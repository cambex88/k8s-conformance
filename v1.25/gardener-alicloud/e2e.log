Conformance test: not doing test setup.
I1209 08:44:53.377557    6292 e2e.go:116] Starting e2e run "2767acbb-378a-4b2a-99e8-8f4fc4df638e" on Ginkgo node 1
Dec  9 08:44:53.388: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /go/src/k8s.io/kubernetes/platforms/linux/amd64
=====================================================================================
Random Seed: 1670575493 - will randomize all specs

Will run 23 of 7067 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Dec  9 08:44:53.475: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  9 08:44:53.476: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec  9 08:44:53.533: INFO: Waiting up to 10m0s for all pods (need at least 1) in namespace 'kube-system' to be running and ready
Dec  9 08:44:53.601: INFO: 34 / 34 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec  9 08:44:53.601: INFO: expected 14 pod replicas in namespace 'kube-system', 14 are Running and Ready.
Dec  9 08:44:53.601: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec  9 08:44:53.619: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'apiserver-proxy' (0 seconds elapsed)
Dec  9 08:44:53.619: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Dec  9 08:44:53.619: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'csi-disk-plugin-alicloud' (0 seconds elapsed)
Dec  9 08:44:53.619: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'egress-filter-applier' (0 seconds elapsed)
Dec  9 08:44:53.619: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-worker-1-v1.25.4' (0 seconds elapsed)
Dec  9 08:44:53.619: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'network-problem-detector-host' (0 seconds elapsed)
Dec  9 08:44:53.619: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'network-problem-detector-pod' (0 seconds elapsed)
Dec  9 08:44:53.619: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Dec  9 08:44:53.619: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Dec  9 08:44:53.619: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-problem-detector' (0 seconds elapsed)
Dec  9 08:44:53.619: INFO: e2e test version: v1.25.4
Dec  9 08:44:53.628: INFO: kube-apiserver version: v1.25.4
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Dec  9 08:44:53.628: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  9 08:44:53.640: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.166 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Dec  9 08:44:53.475: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec  9 08:44:53.476: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Dec  9 08:44:53.533: INFO: Waiting up to 10m0s for all pods (need at least 1) in namespace 'kube-system' to be running and ready
    Dec  9 08:44:53.601: INFO: 34 / 34 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Dec  9 08:44:53.601: INFO: expected 14 pod replicas in namespace 'kube-system', 14 are Running and Ready.
    Dec  9 08:44:53.601: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Dec  9 08:44:53.619: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'apiserver-proxy' (0 seconds elapsed)
    Dec  9 08:44:53.619: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Dec  9 08:44:53.619: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'csi-disk-plugin-alicloud' (0 seconds elapsed)
    Dec  9 08:44:53.619: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'egress-filter-applier' (0 seconds elapsed)
    Dec  9 08:44:53.619: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-worker-1-v1.25.4' (0 seconds elapsed)
    Dec  9 08:44:53.619: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'network-problem-detector-host' (0 seconds elapsed)
    Dec  9 08:44:53.619: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'network-problem-detector-pod' (0 seconds elapsed)
    Dec  9 08:44:53.619: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
    Dec  9 08:44:53.619: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
    Dec  9 08:44:53.619: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-problem-detector' (0 seconds elapsed)
    Dec  9 08:44:53.619: INFO: e2e test version: v1.25.4
    Dec  9 08:44:53.628: INFO: kube-apiserver version: v1.25.4
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Dec  9 08:44:53.628: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec  9 08:44:53.640: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/09/22 08:44:53.68
Dec  9 08:44:53.680: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 12/09/22 08:44:53.681
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:44:53.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:44:53.735
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 12/09/22 08:44:53.812
STEP: Check that daemon pods launch on every node of the cluster. 12/09/22 08:44:53.824
Dec  9 08:44:53.847: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:44:53.847: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
Dec  9 08:44:54.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:44:54.880: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
Dec  9 08:44:55.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:44:55.880: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
Dec  9 08:44:56.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:44:56.880: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
Dec  9 08:44:57.879: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:44:57.880: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
Dec  9 08:44:58.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:44:58.880: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
Dec  9 08:44:59.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:44:59.883: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
Dec  9 08:45:00.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 08:45:00.880: INFO: Node izgw8hm6kg779yfidwdv8yz is running 0 daemon pod, expected 1
Dec  9 08:45:01.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec  9 08:45:01.880: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 12/09/22 08:45:01.891
Dec  9 08:45:01.904: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 12/09/22 08:45:01.904
Dec  9 08:45:01.928: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 12/09/22 08:45:01.928
Dec  9 08:45:01.938: INFO: Observed &DaemonSet event: ADDED
Dec  9 08:45:01.939: INFO: Observed &DaemonSet event: MODIFIED
Dec  9 08:45:01.939: INFO: Observed &DaemonSet event: MODIFIED
Dec  9 08:45:01.939: INFO: Observed &DaemonSet event: MODIFIED
Dec  9 08:45:01.939: INFO: Found daemon set daemon-set in namespace daemonsets-7258 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec  9 08:45:01.939: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 12/09/22 08:45:01.939
STEP: watching for the daemon set status to be patched 12/09/22 08:45:01.952
Dec  9 08:45:01.962: INFO: Observed &DaemonSet event: ADDED
Dec  9 08:45:01.962: INFO: Observed &DaemonSet event: MODIFIED
Dec  9 08:45:01.962: INFO: Observed &DaemonSet event: MODIFIED
Dec  9 08:45:01.962: INFO: Observed &DaemonSet event: MODIFIED
Dec  9 08:45:01.962: INFO: Observed daemon set daemon-set in namespace daemonsets-7258 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec  9 08:45:01.963: INFO: Observed &DaemonSet event: MODIFIED
Dec  9 08:45:01.963: INFO: Found daemon set daemon-set in namespace daemonsets-7258 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Dec  9 08:45:01.963: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 12/09/22 08:45:01.974
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7258, will wait for the garbage collector to delete the pods 12/09/22 08:45:01.974
Dec  9 08:45:02.048: INFO: Deleting DaemonSet.extensions daemon-set took: 12.886171ms
Dec  9 08:45:02.149: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.594138ms
Dec  9 08:45:05.060: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:45:05.060: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec  9 08:45:05.071: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17258"},"items":null}

Dec  9 08:45:05.082: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17258"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Dec  9 08:45:05.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7258" for this suite. 12/09/22 08:45:05.132
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":1,"skipped":313,"failed":0}
------------------------------
• [11.464 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/09/22 08:44:53.68
    Dec  9 08:44:53.680: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 12/09/22 08:44:53.681
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:44:53.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:44:53.735
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 12/09/22 08:44:53.812
    STEP: Check that daemon pods launch on every node of the cluster. 12/09/22 08:44:53.824
    Dec  9 08:44:53.847: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:44:53.847: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
    Dec  9 08:44:54.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:44:54.880: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
    Dec  9 08:44:55.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:44:55.880: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
    Dec  9 08:44:56.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:44:56.880: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
    Dec  9 08:44:57.879: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:44:57.880: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
    Dec  9 08:44:58.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:44:58.880: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
    Dec  9 08:44:59.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:44:59.883: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
    Dec  9 08:45:00.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 08:45:00.880: INFO: Node izgw8hm6kg779yfidwdv8yz is running 0 daemon pod, expected 1
    Dec  9 08:45:01.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec  9 08:45:01.880: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 12/09/22 08:45:01.891
    Dec  9 08:45:01.904: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 12/09/22 08:45:01.904
    Dec  9 08:45:01.928: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 12/09/22 08:45:01.928
    Dec  9 08:45:01.938: INFO: Observed &DaemonSet event: ADDED
    Dec  9 08:45:01.939: INFO: Observed &DaemonSet event: MODIFIED
    Dec  9 08:45:01.939: INFO: Observed &DaemonSet event: MODIFIED
    Dec  9 08:45:01.939: INFO: Observed &DaemonSet event: MODIFIED
    Dec  9 08:45:01.939: INFO: Found daemon set daemon-set in namespace daemonsets-7258 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Dec  9 08:45:01.939: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 12/09/22 08:45:01.939
    STEP: watching for the daemon set status to be patched 12/09/22 08:45:01.952
    Dec  9 08:45:01.962: INFO: Observed &DaemonSet event: ADDED
    Dec  9 08:45:01.962: INFO: Observed &DaemonSet event: MODIFIED
    Dec  9 08:45:01.962: INFO: Observed &DaemonSet event: MODIFIED
    Dec  9 08:45:01.962: INFO: Observed &DaemonSet event: MODIFIED
    Dec  9 08:45:01.962: INFO: Observed daemon set daemon-set in namespace daemonsets-7258 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Dec  9 08:45:01.963: INFO: Observed &DaemonSet event: MODIFIED
    Dec  9 08:45:01.963: INFO: Found daemon set daemon-set in namespace daemonsets-7258 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Dec  9 08:45:01.963: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 12/09/22 08:45:01.974
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7258, will wait for the garbage collector to delete the pods 12/09/22 08:45:01.974
    Dec  9 08:45:02.048: INFO: Deleting DaemonSet.extensions daemon-set took: 12.886171ms
    Dec  9 08:45:02.149: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.594138ms
    Dec  9 08:45:05.060: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:45:05.060: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec  9 08:45:05.071: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17258"},"items":null}

    Dec  9 08:45:05.082: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17258"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Dec  9 08:45:05.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7258" for this suite. 12/09/22 08:45:05.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/09/22 08:45:05.162
Dec  9 08:45:05.163: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename controllerrevisions 12/09/22 08:45:05.163
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:45:05.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:45:05.222
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-f29nn-daemon-set" 12/09/22 08:45:05.3
STEP: Check that daemon pods launch on every node of the cluster. 12/09/22 08:45:05.312
Dec  9 08:45:05.335: INFO: Number of nodes with available pods controlled by daemonset e2e-f29nn-daemon-set: 0
Dec  9 08:45:05.335: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
Dec  9 08:45:06.377: INFO: Number of nodes with available pods controlled by daemonset e2e-f29nn-daemon-set: 2
Dec  9 08:45:06.377: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-f29nn-daemon-set
STEP: Confirm DaemonSet "e2e-f29nn-daemon-set" successfully created with "daemonset-name=e2e-f29nn-daemon-set" label 12/09/22 08:45:06.388
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-f29nn-daemon-set" 12/09/22 08:45:06.411
Dec  9 08:45:06.423: INFO: Located ControllerRevision: "e2e-f29nn-daemon-set-757bbbb896"
STEP: Patching ControllerRevision "e2e-f29nn-daemon-set-757bbbb896" 12/09/22 08:45:06.434
Dec  9 08:45:06.447: INFO: e2e-f29nn-daemon-set-757bbbb896 has been patched
STEP: Create a new ControllerRevision 12/09/22 08:45:06.447
Dec  9 08:45:06.459: INFO: Created ControllerRevision: e2e-f29nn-daemon-set-65fbc978bb
STEP: Confirm that there are two ControllerRevisions 12/09/22 08:45:06.459
Dec  9 08:45:06.459: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec  9 08:45:06.470: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-f29nn-daemon-set-757bbbb896" 12/09/22 08:45:06.47
STEP: Confirm that there is only one ControllerRevision 12/09/22 08:45:06.483
Dec  9 08:45:06.483: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec  9 08:45:06.494: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-f29nn-daemon-set-65fbc978bb" 12/09/22 08:45:06.505
Dec  9 08:45:06.528: INFO: e2e-f29nn-daemon-set-65fbc978bb has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 12/09/22 08:45:06.528
W1209 08:45:06.552252    6292 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 12/09/22 08:45:06.552
Dec  9 08:45:06.552: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec  9 08:45:06.562: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-f29nn-daemon-set-65fbc978bb=updated" 12/09/22 08:45:06.562
STEP: Confirm that there is only one ControllerRevision 12/09/22 08:45:06.576
Dec  9 08:45:06.576: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec  9 08:45:06.587: INFO: Found 1 ControllerRevisions
Dec  9 08:45:06.599: INFO: ControllerRevision "e2e-f29nn-daemon-set-6f7bf88cf9" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-f29nn-daemon-set" 12/09/22 08:45:06.61
STEP: deleting DaemonSet.extensions e2e-f29nn-daemon-set in namespace controllerrevisions-1631, will wait for the garbage collector to delete the pods 12/09/22 08:45:06.61
Dec  9 08:45:06.697: INFO: Deleting DaemonSet.extensions e2e-f29nn-daemon-set took: 12.581264ms
Dec  9 08:45:06.798: INFO: Terminating DaemonSet.extensions e2e-f29nn-daemon-set pods took: 100.682933ms
Dec  9 08:45:09.409: INFO: Number of nodes with available pods controlled by daemonset e2e-f29nn-daemon-set: 0
Dec  9 08:45:09.409: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-f29nn-daemon-set
Dec  9 08:45:09.420: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17319"},"items":null}

Dec  9 08:45:09.431: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17319"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Dec  9 08:45:09.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-1631" for this suite. 12/09/22 08:45:09.484
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":2,"skipped":799,"failed":0}
------------------------------
• [4.334 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/09/22 08:45:05.162
    Dec  9 08:45:05.163: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename controllerrevisions 12/09/22 08:45:05.163
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:45:05.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:45:05.222
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-f29nn-daemon-set" 12/09/22 08:45:05.3
    STEP: Check that daemon pods launch on every node of the cluster. 12/09/22 08:45:05.312
    Dec  9 08:45:05.335: INFO: Number of nodes with available pods controlled by daemonset e2e-f29nn-daemon-set: 0
    Dec  9 08:45:05.335: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
    Dec  9 08:45:06.377: INFO: Number of nodes with available pods controlled by daemonset e2e-f29nn-daemon-set: 2
    Dec  9 08:45:06.377: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-f29nn-daemon-set
    STEP: Confirm DaemonSet "e2e-f29nn-daemon-set" successfully created with "daemonset-name=e2e-f29nn-daemon-set" label 12/09/22 08:45:06.388
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-f29nn-daemon-set" 12/09/22 08:45:06.411
    Dec  9 08:45:06.423: INFO: Located ControllerRevision: "e2e-f29nn-daemon-set-757bbbb896"
    STEP: Patching ControllerRevision "e2e-f29nn-daemon-set-757bbbb896" 12/09/22 08:45:06.434
    Dec  9 08:45:06.447: INFO: e2e-f29nn-daemon-set-757bbbb896 has been patched
    STEP: Create a new ControllerRevision 12/09/22 08:45:06.447
    Dec  9 08:45:06.459: INFO: Created ControllerRevision: e2e-f29nn-daemon-set-65fbc978bb
    STEP: Confirm that there are two ControllerRevisions 12/09/22 08:45:06.459
    Dec  9 08:45:06.459: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec  9 08:45:06.470: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-f29nn-daemon-set-757bbbb896" 12/09/22 08:45:06.47
    STEP: Confirm that there is only one ControllerRevision 12/09/22 08:45:06.483
    Dec  9 08:45:06.483: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec  9 08:45:06.494: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-f29nn-daemon-set-65fbc978bb" 12/09/22 08:45:06.505
    Dec  9 08:45:06.528: INFO: e2e-f29nn-daemon-set-65fbc978bb has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 12/09/22 08:45:06.528
    W1209 08:45:06.552252    6292 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 12/09/22 08:45:06.552
    Dec  9 08:45:06.552: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec  9 08:45:06.562: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-f29nn-daemon-set-65fbc978bb=updated" 12/09/22 08:45:06.562
    STEP: Confirm that there is only one ControllerRevision 12/09/22 08:45:06.576
    Dec  9 08:45:06.576: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec  9 08:45:06.587: INFO: Found 1 ControllerRevisions
    Dec  9 08:45:06.599: INFO: ControllerRevision "e2e-f29nn-daemon-set-6f7bf88cf9" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-f29nn-daemon-set" 12/09/22 08:45:06.61
    STEP: deleting DaemonSet.extensions e2e-f29nn-daemon-set in namespace controllerrevisions-1631, will wait for the garbage collector to delete the pods 12/09/22 08:45:06.61
    Dec  9 08:45:06.697: INFO: Deleting DaemonSet.extensions e2e-f29nn-daemon-set took: 12.581264ms
    Dec  9 08:45:06.798: INFO: Terminating DaemonSet.extensions e2e-f29nn-daemon-set pods took: 100.682933ms
    Dec  9 08:45:09.409: INFO: Number of nodes with available pods controlled by daemonset e2e-f29nn-daemon-set: 0
    Dec  9 08:45:09.409: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-f29nn-daemon-set
    Dec  9 08:45:09.420: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17319"},"items":null}

    Dec  9 08:45:09.431: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17319"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Dec  9 08:45:09.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-1631" for this suite. 12/09/22 08:45:09.484
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/09/22 08:45:09.515
Dec  9 08:45:09.515: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 12/09/22 08:45:09.516
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:45:09.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:45:09.57
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Dec  9 08:45:09.637: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 12/09/22 08:45:09.648
Dec  9 08:45:09.672: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:45:09.672: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
Dec  9 08:45:10.705: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 08:45:10.705: INFO: Node izgw8hm6kg779yfidwdv8yz is running 0 daemon pod, expected 1
Dec  9 08:45:11.704: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec  9 08:45:11.704: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 12/09/22 08:45:11.749
STEP: Check that daemon pods images are updated. 12/09/22 08:45:11.774
Dec  9 08:45:11.785: INFO: Wrong image for pod: daemon-set-h4dgj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec  9 08:45:12.820: INFO: Wrong image for pod: daemon-set-h4dgj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec  9 08:45:13.808: INFO: Pod daemon-set-2j6sg is not available
Dec  9 08:45:13.808: INFO: Wrong image for pod: daemon-set-h4dgj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec  9 08:45:14.808: INFO: Pod daemon-set-2j6sg is not available
Dec  9 08:45:14.808: INFO: Wrong image for pod: daemon-set-h4dgj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec  9 08:45:15.808: INFO: Pod daemon-set-2j6sg is not available
Dec  9 08:45:15.808: INFO: Wrong image for pod: daemon-set-h4dgj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec  9 08:45:16.809: INFO: Pod daemon-set-2j6sg is not available
Dec  9 08:45:16.809: INFO: Wrong image for pod: daemon-set-h4dgj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec  9 08:45:17.809: INFO: Pod daemon-set-2j6sg is not available
Dec  9 08:45:17.809: INFO: Wrong image for pod: daemon-set-h4dgj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec  9 08:45:18.808: INFO: Pod daemon-set-2j6sg is not available
Dec  9 08:45:18.808: INFO: Wrong image for pod: daemon-set-h4dgj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec  9 08:45:20.808: INFO: Pod daemon-set-959fg is not available
STEP: Check that daemon pods are still running on every node of the cluster. 12/09/22 08:45:20.828
Dec  9 08:45:20.851: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 08:45:20.851: INFO: Node izgw8hm6kg779yfidwdv8yz is running 0 daemon pod, expected 1
Dec  9 08:45:21.884: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 08:45:21.884: INFO: Node izgw8hm6kg779yfidwdv8yz is running 0 daemon pod, expected 1
Dec  9 08:45:22.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 08:45:22.883: INFO: Node izgw8hm6kg779yfidwdv8yz is running 0 daemon pod, expected 1
Dec  9 08:45:23.884: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 08:45:23.884: INFO: Node izgw8hm6kg779yfidwdv8yz is running 0 daemon pod, expected 1
Dec  9 08:45:24.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 08:45:24.883: INFO: Node izgw8hm6kg779yfidwdv8yz is running 0 daemon pod, expected 1
Dec  9 08:45:25.884: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 08:45:25.884: INFO: Node izgw8hm6kg779yfidwdv8yz is running 0 daemon pod, expected 1
Dec  9 08:45:26.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec  9 08:45:26.883: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 12/09/22 08:45:26.939
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2387, will wait for the garbage collector to delete the pods 12/09/22 08:45:26.939
Dec  9 08:45:27.014: INFO: Deleting DaemonSet.extensions daemon-set took: 12.454576ms
Dec  9 08:45:27.114: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.203668ms
Dec  9 08:45:30.225: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:45:30.225: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec  9 08:45:30.236: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17495"},"items":null}

Dec  9 08:45:30.247: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17495"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Dec  9 08:45:30.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2387" for this suite. 12/09/22 08:45:30.301
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":3,"skipped":1444,"failed":0}
------------------------------
• [20.798 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/09/22 08:45:09.515
    Dec  9 08:45:09.515: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 12/09/22 08:45:09.516
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:45:09.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:45:09.57
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Dec  9 08:45:09.637: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 12/09/22 08:45:09.648
    Dec  9 08:45:09.672: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:45:09.672: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
    Dec  9 08:45:10.705: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 08:45:10.705: INFO: Node izgw8hm6kg779yfidwdv8yz is running 0 daemon pod, expected 1
    Dec  9 08:45:11.704: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec  9 08:45:11.704: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 12/09/22 08:45:11.749
    STEP: Check that daemon pods images are updated. 12/09/22 08:45:11.774
    Dec  9 08:45:11.785: INFO: Wrong image for pod: daemon-set-h4dgj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec  9 08:45:12.820: INFO: Wrong image for pod: daemon-set-h4dgj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec  9 08:45:13.808: INFO: Pod daemon-set-2j6sg is not available
    Dec  9 08:45:13.808: INFO: Wrong image for pod: daemon-set-h4dgj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec  9 08:45:14.808: INFO: Pod daemon-set-2j6sg is not available
    Dec  9 08:45:14.808: INFO: Wrong image for pod: daemon-set-h4dgj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec  9 08:45:15.808: INFO: Pod daemon-set-2j6sg is not available
    Dec  9 08:45:15.808: INFO: Wrong image for pod: daemon-set-h4dgj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec  9 08:45:16.809: INFO: Pod daemon-set-2j6sg is not available
    Dec  9 08:45:16.809: INFO: Wrong image for pod: daemon-set-h4dgj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec  9 08:45:17.809: INFO: Pod daemon-set-2j6sg is not available
    Dec  9 08:45:17.809: INFO: Wrong image for pod: daemon-set-h4dgj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec  9 08:45:18.808: INFO: Pod daemon-set-2j6sg is not available
    Dec  9 08:45:18.808: INFO: Wrong image for pod: daemon-set-h4dgj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec  9 08:45:20.808: INFO: Pod daemon-set-959fg is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 12/09/22 08:45:20.828
    Dec  9 08:45:20.851: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 08:45:20.851: INFO: Node izgw8hm6kg779yfidwdv8yz is running 0 daemon pod, expected 1
    Dec  9 08:45:21.884: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 08:45:21.884: INFO: Node izgw8hm6kg779yfidwdv8yz is running 0 daemon pod, expected 1
    Dec  9 08:45:22.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 08:45:22.883: INFO: Node izgw8hm6kg779yfidwdv8yz is running 0 daemon pod, expected 1
    Dec  9 08:45:23.884: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 08:45:23.884: INFO: Node izgw8hm6kg779yfidwdv8yz is running 0 daemon pod, expected 1
    Dec  9 08:45:24.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 08:45:24.883: INFO: Node izgw8hm6kg779yfidwdv8yz is running 0 daemon pod, expected 1
    Dec  9 08:45:25.884: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 08:45:25.884: INFO: Node izgw8hm6kg779yfidwdv8yz is running 0 daemon pod, expected 1
    Dec  9 08:45:26.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec  9 08:45:26.883: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 12/09/22 08:45:26.939
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2387, will wait for the garbage collector to delete the pods 12/09/22 08:45:26.939
    Dec  9 08:45:27.014: INFO: Deleting DaemonSet.extensions daemon-set took: 12.454576ms
    Dec  9 08:45:27.114: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.203668ms
    Dec  9 08:45:30.225: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:45:30.225: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec  9 08:45:30.236: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17495"},"items":null}

    Dec  9 08:45:30.247: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17495"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Dec  9 08:45:30.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-2387" for this suite. 12/09/22 08:45:30.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/09/22 08:45:30.34
Dec  9 08:45:30.341: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper 12/09/22 08:45:30.341
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:45:30.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:45:30.396
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 12/09/22 08:45:30.416
STEP: Creating RC which spawns configmap-volume pods 12/09/22 08:45:31.002
Dec  9 08:45:31.028: INFO: Pod name wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8: Found 0 pods out of 5
Dec  9 08:45:36.069: INFO: Pod name wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8: Found 5 pods out of 5
STEP: Ensuring each pod is running 12/09/22 08:45:36.069
Dec  9 08:45:36.070: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-95cjv" in namespace "emptydir-wrapper-1483" to be "running"
Dec  9 08:45:36.081: INFO: Pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-95cjv": Phase="Running", Reason="", readiness=true. Elapsed: 11.646171ms
Dec  9 08:45:36.081: INFO: Pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-95cjv" satisfied condition "running"
Dec  9 08:45:36.081: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-c786x" in namespace "emptydir-wrapper-1483" to be "running"
Dec  9 08:45:36.093: INFO: Pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-c786x": Phase="Running", Reason="", readiness=true. Elapsed: 11.705268ms
Dec  9 08:45:36.093: INFO: Pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-c786x" satisfied condition "running"
Dec  9 08:45:36.093: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-s4c82" in namespace "emptydir-wrapper-1483" to be "running"
Dec  9 08:45:36.105: INFO: Pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-s4c82": Phase="Running", Reason="", readiness=true. Elapsed: 11.618721ms
Dec  9 08:45:36.105: INFO: Pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-s4c82" satisfied condition "running"
Dec  9 08:45:36.105: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-sspkm" in namespace "emptydir-wrapper-1483" to be "running"
Dec  9 08:45:36.116: INFO: Pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-sspkm": Phase="Running", Reason="", readiness=true. Elapsed: 11.658922ms
Dec  9 08:45:36.116: INFO: Pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-sspkm" satisfied condition "running"
Dec  9 08:45:36.116: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-t2kgp" in namespace "emptydir-wrapper-1483" to be "running"
Dec  9 08:45:36.135: INFO: Pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-t2kgp": Phase="Running", Reason="", readiness=true. Elapsed: 18.399391ms
Dec  9 08:45:36.135: INFO: Pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-t2kgp" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8 in namespace emptydir-wrapper-1483, will wait for the garbage collector to delete the pods 12/09/22 08:45:36.135
Dec  9 08:45:36.211: INFO: Deleting ReplicationController wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8 took: 13.169599ms
Dec  9 08:45:36.311: INFO: Terminating ReplicationController wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8 pods took: 100.160345ms
STEP: Creating RC which spawns configmap-volume pods 12/09/22 08:45:37.624
Dec  9 08:45:37.651: INFO: Pod name wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c: Found 0 pods out of 5
Dec  9 08:45:42.693: INFO: Pod name wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c: Found 5 pods out of 5
STEP: Ensuring each pod is running 12/09/22 08:45:42.693
Dec  9 08:45:42.693: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-ctgh9" in namespace "emptydir-wrapper-1483" to be "running"
Dec  9 08:45:42.706: INFO: Pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-ctgh9": Phase="Running", Reason="", readiness=true. Elapsed: 12.243398ms
Dec  9 08:45:42.706: INFO: Pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-ctgh9" satisfied condition "running"
Dec  9 08:45:42.706: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-dbfm7" in namespace "emptydir-wrapper-1483" to be "running"
Dec  9 08:45:42.719: INFO: Pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-dbfm7": Phase="Running", Reason="", readiness=true. Elapsed: 13.338668ms
Dec  9 08:45:42.719: INFO: Pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-dbfm7" satisfied condition "running"
Dec  9 08:45:42.719: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-j9z6g" in namespace "emptydir-wrapper-1483" to be "running"
Dec  9 08:45:42.731: INFO: Pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-j9z6g": Phase="Running", Reason="", readiness=true. Elapsed: 12.116835ms
Dec  9 08:45:42.731: INFO: Pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-j9z6g" satisfied condition "running"
Dec  9 08:45:42.731: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-r8w7n" in namespace "emptydir-wrapper-1483" to be "running"
Dec  9 08:45:42.743: INFO: Pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-r8w7n": Phase="Running", Reason="", readiness=true. Elapsed: 11.417424ms
Dec  9 08:45:42.743: INFO: Pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-r8w7n" satisfied condition "running"
Dec  9 08:45:42.743: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-xf49n" in namespace "emptydir-wrapper-1483" to be "running"
Dec  9 08:45:42.755: INFO: Pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-xf49n": Phase="Running", Reason="", readiness=true. Elapsed: 12.367511ms
Dec  9 08:45:42.755: INFO: Pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-xf49n" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c in namespace emptydir-wrapper-1483, will wait for the garbage collector to delete the pods 12/09/22 08:45:42.755
Dec  9 08:45:42.831: INFO: Deleting ReplicationController wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c took: 12.893555ms
Dec  9 08:45:42.932: INFO: Terminating ReplicationController wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c pods took: 101.184904ms
STEP: Creating RC which spawns configmap-volume pods 12/09/22 08:45:44.345
Dec  9 08:45:44.382: INFO: Pod name wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a: Found 1 pods out of 5
Dec  9 08:45:49.423: INFO: Pod name wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a: Found 5 pods out of 5
STEP: Ensuring each pod is running 12/09/22 08:45:49.423
Dec  9 08:45:49.423: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-f6xqp" in namespace "emptydir-wrapper-1483" to be "running"
Dec  9 08:45:49.435: INFO: Pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-f6xqp": Phase="Running", Reason="", readiness=true. Elapsed: 11.776936ms
Dec  9 08:45:49.435: INFO: Pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-f6xqp" satisfied condition "running"
Dec  9 08:45:49.435: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-fmw2d" in namespace "emptydir-wrapper-1483" to be "running"
Dec  9 08:45:49.447: INFO: Pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-fmw2d": Phase="Running", Reason="", readiness=true. Elapsed: 11.803847ms
Dec  9 08:45:49.447: INFO: Pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-fmw2d" satisfied condition "running"
Dec  9 08:45:49.447: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-l8rrx" in namespace "emptydir-wrapper-1483" to be "running"
Dec  9 08:45:49.458: INFO: Pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-l8rrx": Phase="Running", Reason="", readiness=true. Elapsed: 11.358218ms
Dec  9 08:45:49.458: INFO: Pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-l8rrx" satisfied condition "running"
Dec  9 08:45:49.458: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-m4lzz" in namespace "emptydir-wrapper-1483" to be "running"
Dec  9 08:45:49.470: INFO: Pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-m4lzz": Phase="Running", Reason="", readiness=true. Elapsed: 11.637504ms
Dec  9 08:45:49.470: INFO: Pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-m4lzz" satisfied condition "running"
Dec  9 08:45:49.470: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-qsdnj" in namespace "emptydir-wrapper-1483" to be "running"
Dec  9 08:45:49.481: INFO: Pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-qsdnj": Phase="Running", Reason="", readiness=true. Elapsed: 11.14341ms
Dec  9 08:45:49.481: INFO: Pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-qsdnj" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a in namespace emptydir-wrapper-1483, will wait for the garbage collector to delete the pods 12/09/22 08:45:49.481
Dec  9 08:45:49.556: INFO: Deleting ReplicationController wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a took: 12.454766ms
Dec  9 08:45:49.657: INFO: Terminating ReplicationController wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a pods took: 100.870954ms
STEP: Cleaning up the configMaps 12/09/22 08:45:50.757
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Dec  9 08:45:51.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1483" for this suite. 12/09/22 08:45:51.406
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":4,"skipped":1897,"failed":0}
------------------------------
• [21.079 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/09/22 08:45:30.34
    Dec  9 08:45:30.341: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir-wrapper 12/09/22 08:45:30.341
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:45:30.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:45:30.396
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 12/09/22 08:45:30.416
    STEP: Creating RC which spawns configmap-volume pods 12/09/22 08:45:31.002
    Dec  9 08:45:31.028: INFO: Pod name wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8: Found 0 pods out of 5
    Dec  9 08:45:36.069: INFO: Pod name wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8: Found 5 pods out of 5
    STEP: Ensuring each pod is running 12/09/22 08:45:36.069
    Dec  9 08:45:36.070: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-95cjv" in namespace "emptydir-wrapper-1483" to be "running"
    Dec  9 08:45:36.081: INFO: Pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-95cjv": Phase="Running", Reason="", readiness=true. Elapsed: 11.646171ms
    Dec  9 08:45:36.081: INFO: Pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-95cjv" satisfied condition "running"
    Dec  9 08:45:36.081: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-c786x" in namespace "emptydir-wrapper-1483" to be "running"
    Dec  9 08:45:36.093: INFO: Pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-c786x": Phase="Running", Reason="", readiness=true. Elapsed: 11.705268ms
    Dec  9 08:45:36.093: INFO: Pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-c786x" satisfied condition "running"
    Dec  9 08:45:36.093: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-s4c82" in namespace "emptydir-wrapper-1483" to be "running"
    Dec  9 08:45:36.105: INFO: Pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-s4c82": Phase="Running", Reason="", readiness=true. Elapsed: 11.618721ms
    Dec  9 08:45:36.105: INFO: Pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-s4c82" satisfied condition "running"
    Dec  9 08:45:36.105: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-sspkm" in namespace "emptydir-wrapper-1483" to be "running"
    Dec  9 08:45:36.116: INFO: Pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-sspkm": Phase="Running", Reason="", readiness=true. Elapsed: 11.658922ms
    Dec  9 08:45:36.116: INFO: Pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-sspkm" satisfied condition "running"
    Dec  9 08:45:36.116: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-t2kgp" in namespace "emptydir-wrapper-1483" to be "running"
    Dec  9 08:45:36.135: INFO: Pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-t2kgp": Phase="Running", Reason="", readiness=true. Elapsed: 18.399391ms
    Dec  9 08:45:36.135: INFO: Pod "wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8-t2kgp" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8 in namespace emptydir-wrapper-1483, will wait for the garbage collector to delete the pods 12/09/22 08:45:36.135
    Dec  9 08:45:36.211: INFO: Deleting ReplicationController wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8 took: 13.169599ms
    Dec  9 08:45:36.311: INFO: Terminating ReplicationController wrapped-volume-race-2cb91264-18de-4d78-96eb-e25e215fa1a8 pods took: 100.160345ms
    STEP: Creating RC which spawns configmap-volume pods 12/09/22 08:45:37.624
    Dec  9 08:45:37.651: INFO: Pod name wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c: Found 0 pods out of 5
    Dec  9 08:45:42.693: INFO: Pod name wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c: Found 5 pods out of 5
    STEP: Ensuring each pod is running 12/09/22 08:45:42.693
    Dec  9 08:45:42.693: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-ctgh9" in namespace "emptydir-wrapper-1483" to be "running"
    Dec  9 08:45:42.706: INFO: Pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-ctgh9": Phase="Running", Reason="", readiness=true. Elapsed: 12.243398ms
    Dec  9 08:45:42.706: INFO: Pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-ctgh9" satisfied condition "running"
    Dec  9 08:45:42.706: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-dbfm7" in namespace "emptydir-wrapper-1483" to be "running"
    Dec  9 08:45:42.719: INFO: Pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-dbfm7": Phase="Running", Reason="", readiness=true. Elapsed: 13.338668ms
    Dec  9 08:45:42.719: INFO: Pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-dbfm7" satisfied condition "running"
    Dec  9 08:45:42.719: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-j9z6g" in namespace "emptydir-wrapper-1483" to be "running"
    Dec  9 08:45:42.731: INFO: Pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-j9z6g": Phase="Running", Reason="", readiness=true. Elapsed: 12.116835ms
    Dec  9 08:45:42.731: INFO: Pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-j9z6g" satisfied condition "running"
    Dec  9 08:45:42.731: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-r8w7n" in namespace "emptydir-wrapper-1483" to be "running"
    Dec  9 08:45:42.743: INFO: Pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-r8w7n": Phase="Running", Reason="", readiness=true. Elapsed: 11.417424ms
    Dec  9 08:45:42.743: INFO: Pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-r8w7n" satisfied condition "running"
    Dec  9 08:45:42.743: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-xf49n" in namespace "emptydir-wrapper-1483" to be "running"
    Dec  9 08:45:42.755: INFO: Pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-xf49n": Phase="Running", Reason="", readiness=true. Elapsed: 12.367511ms
    Dec  9 08:45:42.755: INFO: Pod "wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c-xf49n" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c in namespace emptydir-wrapper-1483, will wait for the garbage collector to delete the pods 12/09/22 08:45:42.755
    Dec  9 08:45:42.831: INFO: Deleting ReplicationController wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c took: 12.893555ms
    Dec  9 08:45:42.932: INFO: Terminating ReplicationController wrapped-volume-race-8d6f6ef6-942a-4a95-bcbd-0ca67450147c pods took: 101.184904ms
    STEP: Creating RC which spawns configmap-volume pods 12/09/22 08:45:44.345
    Dec  9 08:45:44.382: INFO: Pod name wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a: Found 1 pods out of 5
    Dec  9 08:45:49.423: INFO: Pod name wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a: Found 5 pods out of 5
    STEP: Ensuring each pod is running 12/09/22 08:45:49.423
    Dec  9 08:45:49.423: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-f6xqp" in namespace "emptydir-wrapper-1483" to be "running"
    Dec  9 08:45:49.435: INFO: Pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-f6xqp": Phase="Running", Reason="", readiness=true. Elapsed: 11.776936ms
    Dec  9 08:45:49.435: INFO: Pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-f6xqp" satisfied condition "running"
    Dec  9 08:45:49.435: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-fmw2d" in namespace "emptydir-wrapper-1483" to be "running"
    Dec  9 08:45:49.447: INFO: Pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-fmw2d": Phase="Running", Reason="", readiness=true. Elapsed: 11.803847ms
    Dec  9 08:45:49.447: INFO: Pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-fmw2d" satisfied condition "running"
    Dec  9 08:45:49.447: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-l8rrx" in namespace "emptydir-wrapper-1483" to be "running"
    Dec  9 08:45:49.458: INFO: Pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-l8rrx": Phase="Running", Reason="", readiness=true. Elapsed: 11.358218ms
    Dec  9 08:45:49.458: INFO: Pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-l8rrx" satisfied condition "running"
    Dec  9 08:45:49.458: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-m4lzz" in namespace "emptydir-wrapper-1483" to be "running"
    Dec  9 08:45:49.470: INFO: Pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-m4lzz": Phase="Running", Reason="", readiness=true. Elapsed: 11.637504ms
    Dec  9 08:45:49.470: INFO: Pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-m4lzz" satisfied condition "running"
    Dec  9 08:45:49.470: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-qsdnj" in namespace "emptydir-wrapper-1483" to be "running"
    Dec  9 08:45:49.481: INFO: Pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-qsdnj": Phase="Running", Reason="", readiness=true. Elapsed: 11.14341ms
    Dec  9 08:45:49.481: INFO: Pod "wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a-qsdnj" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a in namespace emptydir-wrapper-1483, will wait for the garbage collector to delete the pods 12/09/22 08:45:49.481
    Dec  9 08:45:49.556: INFO: Deleting ReplicationController wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a took: 12.454766ms
    Dec  9 08:45:49.657: INFO: Terminating ReplicationController wrapped-volume-race-4b87354a-73de-4324-803a-111659f18d4a pods took: 100.870954ms
    STEP: Cleaning up the configMaps 12/09/22 08:45:50.757
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Dec  9 08:45:51.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-1483" for this suite. 12/09/22 08:45:51.406
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/09/22 08:45:51.432
Dec  9 08:45:51.432: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename taint-single-pod 12/09/22 08:45:51.433
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:45:51.467
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:45:51.487
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Dec  9 08:45:51.508: INFO: Waiting up to 1m0s for all nodes to be ready
Dec  9 08:46:51.603: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Dec  9 08:46:51.614: INFO: Starting informer...
STEP: Starting pod... 12/09/22 08:46:51.614
Dec  9 08:46:51.654: INFO: Pod is running on izgw8hm6kg779yfidwdv8yz. Tainting Node
STEP: Trying to apply a taint on the Node 12/09/22 08:46:51.654
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/09/22 08:46:51.684
STEP: Waiting short time to make sure Pod is queued for deletion 12/09/22 08:46:51.695
Dec  9 08:46:51.695: INFO: Pod wasn't evicted. Proceeding
Dec  9 08:46:51.695: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/09/22 08:46:51.726
STEP: Waiting some time to make sure that toleration time passed. 12/09/22 08:46:51.738
Dec  9 08:48:06.739: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Dec  9 08:48:06.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-8386" for this suite. 12/09/22 08:48:06.761
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":5,"skipped":2213,"failed":0}
------------------------------
• [135.342 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/09/22 08:45:51.432
    Dec  9 08:45:51.432: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename taint-single-pod 12/09/22 08:45:51.433
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:45:51.467
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:45:51.487
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Dec  9 08:45:51.508: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec  9 08:46:51.603: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Dec  9 08:46:51.614: INFO: Starting informer...
    STEP: Starting pod... 12/09/22 08:46:51.614
    Dec  9 08:46:51.654: INFO: Pod is running on izgw8hm6kg779yfidwdv8yz. Tainting Node
    STEP: Trying to apply a taint on the Node 12/09/22 08:46:51.654
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/09/22 08:46:51.684
    STEP: Waiting short time to make sure Pod is queued for deletion 12/09/22 08:46:51.695
    Dec  9 08:46:51.695: INFO: Pod wasn't evicted. Proceeding
    Dec  9 08:46:51.695: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/09/22 08:46:51.726
    STEP: Waiting some time to make sure that toleration time passed. 12/09/22 08:46:51.738
    Dec  9 08:48:06.739: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Dec  9 08:48:06.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-8386" for this suite. 12/09/22 08:48:06.761
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/09/22 08:48:06.774
Dec  9 08:48:06.774: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 12/09/22 08:48:06.775
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:48:06.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:48:06.832
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Dec  9 08:48:06.910: INFO: Create a RollingUpdate DaemonSet
Dec  9 08:48:06.922: INFO: Check that daemon pods launch on every node of the cluster
Dec  9 08:48:06.944: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:48:06.944: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
Dec  9 08:48:07.976: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 08:48:07.976: INFO: Node izgw8hm6kg779yfidwdv8yz is running 0 daemon pod, expected 1
Dec  9 08:48:08.977: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec  9 08:48:08.977: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Dec  9 08:48:08.977: INFO: Update the DaemonSet to trigger a rollout
Dec  9 08:48:09.000: INFO: Updating DaemonSet daemon-set
Dec  9 08:48:11.058: INFO: Roll back the DaemonSet before rollout is complete
Dec  9 08:48:11.081: INFO: Updating DaemonSet daemon-set
Dec  9 08:48:11.081: INFO: Make sure DaemonSet rollback is complete
Dec  9 08:48:14.115: INFO: Pod daemon-set-5jhfz is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 12/09/22 08:48:14.158
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-742, will wait for the garbage collector to delete the pods 12/09/22 08:48:14.158
Dec  9 08:48:14.233: INFO: Deleting DaemonSet.extensions daemon-set took: 12.849517ms
Dec  9 08:48:14.333: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.574795ms
Dec  9 08:48:15.845: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:48:15.845: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec  9 08:48:15.856: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18795"},"items":null}

Dec  9 08:48:15.867: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18795"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Dec  9 08:48:15.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-742" for this suite. 12/09/22 08:48:15.921
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":6,"skipped":2215,"failed":0}
------------------------------
• [9.160 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/09/22 08:48:06.774
    Dec  9 08:48:06.774: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 12/09/22 08:48:06.775
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:48:06.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:48:06.832
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Dec  9 08:48:06.910: INFO: Create a RollingUpdate DaemonSet
    Dec  9 08:48:06.922: INFO: Check that daemon pods launch on every node of the cluster
    Dec  9 08:48:06.944: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:48:06.944: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
    Dec  9 08:48:07.976: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 08:48:07.976: INFO: Node izgw8hm6kg779yfidwdv8yz is running 0 daemon pod, expected 1
    Dec  9 08:48:08.977: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec  9 08:48:08.977: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Dec  9 08:48:08.977: INFO: Update the DaemonSet to trigger a rollout
    Dec  9 08:48:09.000: INFO: Updating DaemonSet daemon-set
    Dec  9 08:48:11.058: INFO: Roll back the DaemonSet before rollout is complete
    Dec  9 08:48:11.081: INFO: Updating DaemonSet daemon-set
    Dec  9 08:48:11.081: INFO: Make sure DaemonSet rollback is complete
    Dec  9 08:48:14.115: INFO: Pod daemon-set-5jhfz is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 12/09/22 08:48:14.158
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-742, will wait for the garbage collector to delete the pods 12/09/22 08:48:14.158
    Dec  9 08:48:14.233: INFO: Deleting DaemonSet.extensions daemon-set took: 12.849517ms
    Dec  9 08:48:14.333: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.574795ms
    Dec  9 08:48:15.845: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:48:15.845: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec  9 08:48:15.856: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18795"},"items":null}

    Dec  9 08:48:15.867: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18795"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Dec  9 08:48:15.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-742" for this suite. 12/09/22 08:48:15.921
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/09/22 08:48:15.948
Dec  9 08:48:15.948: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces 12/09/22 08:48:15.948
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:48:15.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:48:16.001
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 12/09/22 08:48:16.021
Dec  9 08:48:16.032: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 12/09/22 08:48:16.032
Dec  9 08:48:16.045: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 12/09/22 08:48:16.045
Dec  9 08:48:16.067: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Dec  9 08:48:16.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1391" for this suite. 12/09/22 08:48:16.079
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":7,"skipped":2713,"failed":0}
------------------------------
• [0.144 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/09/22 08:48:15.948
    Dec  9 08:48:15.948: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename namespaces 12/09/22 08:48:15.948
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:48:15.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:48:16.001
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 12/09/22 08:48:16.021
    Dec  9 08:48:16.032: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 12/09/22 08:48:16.032
    Dec  9 08:48:16.045: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 12/09/22 08:48:16.045
    Dec  9 08:48:16.067: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Dec  9 08:48:16.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-1391" for this suite. 12/09/22 08:48:16.079
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/09/22 08:48:16.113
Dec  9 08:48:16.113: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces 12/09/22 08:48:16.114
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:48:16.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:48:16.168
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 12/09/22 08:48:16.189
STEP: patching the Namespace 12/09/22 08:48:16.222
STEP: get the Namespace and ensuring it has the label 12/09/22 08:48:16.234
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Dec  9 08:48:16.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1515" for this suite. 12/09/22 08:48:16.256
STEP: Destroying namespace "nspatchtest-e4acf546-a1e6-4103-be5c-b6ada90b6962-1388" for this suite. 12/09/22 08:48:16.268
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":8,"skipped":3272,"failed":0}
------------------------------
• [0.167 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/09/22 08:48:16.113
    Dec  9 08:48:16.113: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename namespaces 12/09/22 08:48:16.114
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:48:16.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:48:16.168
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 12/09/22 08:48:16.189
    STEP: patching the Namespace 12/09/22 08:48:16.222
    STEP: get the Namespace and ensuring it has the label 12/09/22 08:48:16.234
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Dec  9 08:48:16.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-1515" for this suite. 12/09/22 08:48:16.256
    STEP: Destroying namespace "nspatchtest-e4acf546-a1e6-4103-be5c-b6ada90b6962-1388" for this suite. 12/09/22 08:48:16.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/09/22 08:48:16.29
Dec  9 08:48:16.290: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred 12/09/22 08:48:16.291
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:48:16.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:48:16.344
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Dec  9 08:48:16.365: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  9 08:48:16.398: INFO: Waiting for terminating namespaces to be deleted...
Dec  9 08:48:16.409: INFO: 
Logging pods the apiserver thinks is on node izgw86e9lj0cm49ixpgel1z before test
Dec  9 08:48:16.427: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-8668c9bb59-48td7 from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.427: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Dec  9 08:48:16.427: INFO: apiserver-proxy-9xnwn from kube-system started at 2022-12-09 07:59:46 +0000 UTC (2 container statuses recorded)
Dec  9 08:48:16.427: INFO: 	Container proxy ready: true, restart count 0
Dec  9 08:48:16.427: INFO: 	Container sidecar ready: true, restart count 0
Dec  9 08:48:16.427: INFO: blackbox-exporter-59447f4c55-7n6c9 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.427: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec  9 08:48:16.427: INFO: blackbox-exporter-59447f4c55-lnx2d from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.427: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec  9 08:48:16.427: INFO: calico-kube-controllers-5f78564887-zgfpt from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.427: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec  9 08:48:16.427: INFO: calico-node-snqzl from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.427: INFO: 	Container calico-node ready: true, restart count 0
Dec  9 08:48:16.427: INFO: calico-node-vertical-autoscaler-6597dd8998-sw7f8 from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.427: INFO: 	Container autoscaler ready: true, restart count 0
Dec  9 08:48:16.427: INFO: calico-typha-horizontal-autoscaler-6bb4bc55bc-ld47r from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.427: INFO: 	Container autoscaler ready: true, restart count 0
Dec  9 08:48:16.427: INFO: calico-typha-vertical-autoscaler-84df655c88-dwsfj from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.427: INFO: 	Container autoscaler ready: true, restart count 0
Dec  9 08:48:16.427: INFO: coredns-84fdf8dd87-62ckx from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.427: INFO: 	Container coredns ready: true, restart count 0
Dec  9 08:48:16.427: INFO: coredns-84fdf8dd87-lc5vg from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.427: INFO: 	Container coredns ready: true, restart count 0
Dec  9 08:48:16.427: INFO: csi-disk-plugin-alicloud-xpt2l from kube-system started at 2022-12-09 07:59:46 +0000 UTC (3 container statuses recorded)
Dec  9 08:48:16.427: INFO: 	Container csi-diskplugin ready: true, restart count 0
Dec  9 08:48:16.427: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec  9 08:48:16.427: INFO: 	Container driver-registrar ready: true, restart count 0
Dec  9 08:48:16.427: INFO: egress-filter-applier-xkds7 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.427: INFO: 	Container egress-filter-applier ready: true, restart count 1
Dec  9 08:48:16.427: INFO: kube-proxy-worker-1-v1.25.4-nw6q5 from kube-system started at 2022-12-09 08:32:11 +0000 UTC (2 container statuses recorded)
Dec  9 08:48:16.427: INFO: 	Container conntrack-fix ready: true, restart count 0
Dec  9 08:48:16.427: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  9 08:48:16.427: INFO: metrics-server-7bcfb6df5f-gp4jx from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.427: INFO: 	Container metrics-server ready: true, restart count 0
Dec  9 08:48:16.427: INFO: metrics-server-7bcfb6df5f-pjfn4 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.427: INFO: 	Container metrics-server ready: true, restart count 0
Dec  9 08:48:16.427: INFO: network-problem-detector-host-4jvll from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.427: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Dec  9 08:48:16.427: INFO: network-problem-detector-pod-bdbrf from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.427: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Dec  9 08:48:16.427: INFO: node-exporter-8v484 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.427: INFO: 	Container node-exporter ready: true, restart count 0
Dec  9 08:48:16.427: INFO: node-local-dns-hrgm8 from kube-system started at 2022-12-09 08:15:12 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.427: INFO: 	Container node-cache ready: true, restart count 0
Dec  9 08:48:16.427: INFO: node-problem-detector-fvdnw from kube-system started at 2022-12-09 08:20:11 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.427: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  9 08:48:16.427: INFO: vpn-shoot-57fc54fdbb-6f7gk from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.427: INFO: 	Container vpn-shoot ready: true, restart count 0
Dec  9 08:48:16.427: INFO: dashboard-metrics-scraper-6d54964d4b-tqdzb from kubernetes-dashboard started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.427: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Dec  9 08:48:16.427: INFO: kubernetes-dashboard-5d6d5f9c58-vfrn8 from kubernetes-dashboard started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.427: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  9 08:48:16.427: INFO: 
Logging pods the apiserver thinks is on node izgw8hm6kg779yfidwdv8yz before test
Dec  9 08:48:16.452: INFO: addons-nginx-ingress-controller-69b7c7d86f-c7tdg from kube-system started at 2022-12-09 08:46:51 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.452: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  9 08:48:16.452: INFO: apiserver-proxy-nzwxs from kube-system started at 2022-12-09 07:59:48 +0000 UTC (2 container statuses recorded)
Dec  9 08:48:16.452: INFO: 	Container proxy ready: true, restart count 0
Dec  9 08:48:16.452: INFO: 	Container sidecar ready: true, restart count 0
Dec  9 08:48:16.452: INFO: calico-node-pbrgw from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.452: INFO: 	Container calico-node ready: true, restart count 0
Dec  9 08:48:16.452: INFO: calico-typha-deploy-65c54d4db6-mh94l from kube-system started at 2022-12-09 08:02:30 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.452: INFO: 	Container calico-typha ready: true, restart count 0
Dec  9 08:48:16.452: INFO: csi-disk-plugin-alicloud-tcf5b from kube-system started at 2022-12-09 07:59:48 +0000 UTC (3 container statuses recorded)
Dec  9 08:48:16.452: INFO: 	Container csi-diskplugin ready: true, restart count 0
Dec  9 08:48:16.452: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec  9 08:48:16.452: INFO: 	Container driver-registrar ready: true, restart count 0
Dec  9 08:48:16.452: INFO: egress-filter-applier-d9bt4 from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.452: INFO: 	Container egress-filter-applier ready: true, restart count 1
Dec  9 08:48:16.452: INFO: kube-proxy-worker-1-v1.25.4-grfnm from kube-system started at 2022-12-09 08:32:11 +0000 UTC (2 container statuses recorded)
Dec  9 08:48:16.452: INFO: 	Container conntrack-fix ready: true, restart count 0
Dec  9 08:48:16.452: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  9 08:48:16.452: INFO: network-problem-detector-host-wnxb2 from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.452: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Dec  9 08:48:16.452: INFO: network-problem-detector-pod-ntl2d from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.452: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Dec  9 08:48:16.452: INFO: node-exporter-zcl88 from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.452: INFO: 	Container node-exporter ready: true, restart count 0
Dec  9 08:48:16.452: INFO: node-local-dns-xhltp from kube-system started at 2022-12-09 08:15:12 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.452: INFO: 	Container node-cache ready: true, restart count 0
Dec  9 08:48:16.452: INFO: node-problem-detector-zf864 from kube-system started at 2022-12-09 08:20:11 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:16.452: INFO: 	Container node-problem-detector ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 12/09/22 08:48:16.452
Dec  9 08:48:16.468: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2529" to be "running"
Dec  9 08:48:16.479: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 10.599297ms
Dec  9 08:48:18.491: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.022884993s
Dec  9 08:48:18.491: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 12/09/22 08:48:18.502
STEP: Trying to apply a random label on the found node. 12/09/22 08:48:18.517
STEP: verifying the node has the label kubernetes.io/e2e-dca564fc-2ef8-443c-8503-fd207ac4e33c 42 12/09/22 08:48:18.543
STEP: Trying to relaunch the pod, now with labels. 12/09/22 08:48:18.554
Dec  9 08:48:18.569: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-2529" to be "not pending"
Dec  9 08:48:18.580: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 11.056032ms
Dec  9 08:48:20.592: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.022786024s
Dec  9 08:48:20.592: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-dca564fc-2ef8-443c-8503-fd207ac4e33c off the node izgw8hm6kg779yfidwdv8yz 12/09/22 08:48:20.603
STEP: verifying the node doesn't have the label kubernetes.io/e2e-dca564fc-2ef8-443c-8503-fd207ac4e33c 12/09/22 08:48:20.64
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Dec  9 08:48:20.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2529" for this suite. 12/09/22 08:48:20.663
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":9,"skipped":3533,"failed":0}
------------------------------
• [4.385 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/09/22 08:48:16.29
    Dec  9 08:48:16.290: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-pred 12/09/22 08:48:16.291
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:48:16.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:48:16.344
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Dec  9 08:48:16.365: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Dec  9 08:48:16.398: INFO: Waiting for terminating namespaces to be deleted...
    Dec  9 08:48:16.409: INFO: 
    Logging pods the apiserver thinks is on node izgw86e9lj0cm49ixpgel1z before test
    Dec  9 08:48:16.427: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-8668c9bb59-48td7 from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.427: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: apiserver-proxy-9xnwn from kube-system started at 2022-12-09 07:59:46 +0000 UTC (2 container statuses recorded)
    Dec  9 08:48:16.427: INFO: 	Container proxy ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: 	Container sidecar ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: blackbox-exporter-59447f4c55-7n6c9 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.427: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: blackbox-exporter-59447f4c55-lnx2d from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.427: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: calico-kube-controllers-5f78564887-zgfpt from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.427: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: calico-node-snqzl from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.427: INFO: 	Container calico-node ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: calico-node-vertical-autoscaler-6597dd8998-sw7f8 from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.427: INFO: 	Container autoscaler ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: calico-typha-horizontal-autoscaler-6bb4bc55bc-ld47r from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.427: INFO: 	Container autoscaler ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: calico-typha-vertical-autoscaler-84df655c88-dwsfj from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.427: INFO: 	Container autoscaler ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: coredns-84fdf8dd87-62ckx from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.427: INFO: 	Container coredns ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: coredns-84fdf8dd87-lc5vg from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.427: INFO: 	Container coredns ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: csi-disk-plugin-alicloud-xpt2l from kube-system started at 2022-12-09 07:59:46 +0000 UTC (3 container statuses recorded)
    Dec  9 08:48:16.427: INFO: 	Container csi-diskplugin ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: 	Container driver-registrar ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: egress-filter-applier-xkds7 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.427: INFO: 	Container egress-filter-applier ready: true, restart count 1
    Dec  9 08:48:16.427: INFO: kube-proxy-worker-1-v1.25.4-nw6q5 from kube-system started at 2022-12-09 08:32:11 +0000 UTC (2 container statuses recorded)
    Dec  9 08:48:16.427: INFO: 	Container conntrack-fix ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: metrics-server-7bcfb6df5f-gp4jx from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.427: INFO: 	Container metrics-server ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: metrics-server-7bcfb6df5f-pjfn4 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.427: INFO: 	Container metrics-server ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: network-problem-detector-host-4jvll from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.427: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: network-problem-detector-pod-bdbrf from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.427: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: node-exporter-8v484 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.427: INFO: 	Container node-exporter ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: node-local-dns-hrgm8 from kube-system started at 2022-12-09 08:15:12 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.427: INFO: 	Container node-cache ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: node-problem-detector-fvdnw from kube-system started at 2022-12-09 08:20:11 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.427: INFO: 	Container node-problem-detector ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: vpn-shoot-57fc54fdbb-6f7gk from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.427: INFO: 	Container vpn-shoot ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: dashboard-metrics-scraper-6d54964d4b-tqdzb from kubernetes-dashboard started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.427: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: kubernetes-dashboard-5d6d5f9c58-vfrn8 from kubernetes-dashboard started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.427: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Dec  9 08:48:16.427: INFO: 
    Logging pods the apiserver thinks is on node izgw8hm6kg779yfidwdv8yz before test
    Dec  9 08:48:16.452: INFO: addons-nginx-ingress-controller-69b7c7d86f-c7tdg from kube-system started at 2022-12-09 08:46:51 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.452: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Dec  9 08:48:16.452: INFO: apiserver-proxy-nzwxs from kube-system started at 2022-12-09 07:59:48 +0000 UTC (2 container statuses recorded)
    Dec  9 08:48:16.452: INFO: 	Container proxy ready: true, restart count 0
    Dec  9 08:48:16.452: INFO: 	Container sidecar ready: true, restart count 0
    Dec  9 08:48:16.452: INFO: calico-node-pbrgw from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.452: INFO: 	Container calico-node ready: true, restart count 0
    Dec  9 08:48:16.452: INFO: calico-typha-deploy-65c54d4db6-mh94l from kube-system started at 2022-12-09 08:02:30 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.452: INFO: 	Container calico-typha ready: true, restart count 0
    Dec  9 08:48:16.452: INFO: csi-disk-plugin-alicloud-tcf5b from kube-system started at 2022-12-09 07:59:48 +0000 UTC (3 container statuses recorded)
    Dec  9 08:48:16.452: INFO: 	Container csi-diskplugin ready: true, restart count 0
    Dec  9 08:48:16.452: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Dec  9 08:48:16.452: INFO: 	Container driver-registrar ready: true, restart count 0
    Dec  9 08:48:16.452: INFO: egress-filter-applier-d9bt4 from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.452: INFO: 	Container egress-filter-applier ready: true, restart count 1
    Dec  9 08:48:16.452: INFO: kube-proxy-worker-1-v1.25.4-grfnm from kube-system started at 2022-12-09 08:32:11 +0000 UTC (2 container statuses recorded)
    Dec  9 08:48:16.452: INFO: 	Container conntrack-fix ready: true, restart count 0
    Dec  9 08:48:16.452: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec  9 08:48:16.452: INFO: network-problem-detector-host-wnxb2 from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.452: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Dec  9 08:48:16.452: INFO: network-problem-detector-pod-ntl2d from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.452: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Dec  9 08:48:16.452: INFO: node-exporter-zcl88 from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.452: INFO: 	Container node-exporter ready: true, restart count 0
    Dec  9 08:48:16.452: INFO: node-local-dns-xhltp from kube-system started at 2022-12-09 08:15:12 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.452: INFO: 	Container node-cache ready: true, restart count 0
    Dec  9 08:48:16.452: INFO: node-problem-detector-zf864 from kube-system started at 2022-12-09 08:20:11 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:16.452: INFO: 	Container node-problem-detector ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 12/09/22 08:48:16.452
    Dec  9 08:48:16.468: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2529" to be "running"
    Dec  9 08:48:16.479: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 10.599297ms
    Dec  9 08:48:18.491: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.022884993s
    Dec  9 08:48:18.491: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 12/09/22 08:48:18.502
    STEP: Trying to apply a random label on the found node. 12/09/22 08:48:18.517
    STEP: verifying the node has the label kubernetes.io/e2e-dca564fc-2ef8-443c-8503-fd207ac4e33c 42 12/09/22 08:48:18.543
    STEP: Trying to relaunch the pod, now with labels. 12/09/22 08:48:18.554
    Dec  9 08:48:18.569: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-2529" to be "not pending"
    Dec  9 08:48:18.580: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 11.056032ms
    Dec  9 08:48:20.592: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.022786024s
    Dec  9 08:48:20.592: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-dca564fc-2ef8-443c-8503-fd207ac4e33c off the node izgw8hm6kg779yfidwdv8yz 12/09/22 08:48:20.603
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-dca564fc-2ef8-443c-8503-fd207ac4e33c 12/09/22 08:48:20.64
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Dec  9 08:48:20.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-2529" for this suite. 12/09/22 08:48:20.663
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/09/22 08:48:20.683
Dec  9 08:48:20.683: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred 12/09/22 08:48:20.684
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:48:20.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:48:20.736
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Dec  9 08:48:20.757: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  9 08:48:20.780: INFO: Waiting for terminating namespaces to be deleted...
Dec  9 08:48:20.791: INFO: 
Logging pods the apiserver thinks is on node izgw86e9lj0cm49ixpgel1z before test
Dec  9 08:48:20.809: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-8668c9bb59-48td7 from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.809: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Dec  9 08:48:20.809: INFO: apiserver-proxy-9xnwn from kube-system started at 2022-12-09 07:59:46 +0000 UTC (2 container statuses recorded)
Dec  9 08:48:20.809: INFO: 	Container proxy ready: true, restart count 0
Dec  9 08:48:20.809: INFO: 	Container sidecar ready: true, restart count 0
Dec  9 08:48:20.809: INFO: blackbox-exporter-59447f4c55-7n6c9 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.809: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec  9 08:48:20.809: INFO: blackbox-exporter-59447f4c55-lnx2d from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.809: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec  9 08:48:20.809: INFO: calico-kube-controllers-5f78564887-zgfpt from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.809: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec  9 08:48:20.809: INFO: calico-node-snqzl from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.809: INFO: 	Container calico-node ready: true, restart count 0
Dec  9 08:48:20.809: INFO: calico-node-vertical-autoscaler-6597dd8998-sw7f8 from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.809: INFO: 	Container autoscaler ready: true, restart count 0
Dec  9 08:48:20.809: INFO: calico-typha-horizontal-autoscaler-6bb4bc55bc-ld47r from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.809: INFO: 	Container autoscaler ready: true, restart count 0
Dec  9 08:48:20.809: INFO: calico-typha-vertical-autoscaler-84df655c88-dwsfj from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.809: INFO: 	Container autoscaler ready: true, restart count 0
Dec  9 08:48:20.809: INFO: coredns-84fdf8dd87-62ckx from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.809: INFO: 	Container coredns ready: true, restart count 0
Dec  9 08:48:20.809: INFO: coredns-84fdf8dd87-lc5vg from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.809: INFO: 	Container coredns ready: true, restart count 0
Dec  9 08:48:20.809: INFO: csi-disk-plugin-alicloud-xpt2l from kube-system started at 2022-12-09 07:59:46 +0000 UTC (3 container statuses recorded)
Dec  9 08:48:20.809: INFO: 	Container csi-diskplugin ready: true, restart count 0
Dec  9 08:48:20.809: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec  9 08:48:20.809: INFO: 	Container driver-registrar ready: true, restart count 0
Dec  9 08:48:20.809: INFO: egress-filter-applier-xkds7 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.809: INFO: 	Container egress-filter-applier ready: true, restart count 1
Dec  9 08:48:20.809: INFO: kube-proxy-worker-1-v1.25.4-nw6q5 from kube-system started at 2022-12-09 08:32:11 +0000 UTC (2 container statuses recorded)
Dec  9 08:48:20.809: INFO: 	Container conntrack-fix ready: true, restart count 0
Dec  9 08:48:20.809: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  9 08:48:20.809: INFO: metrics-server-7bcfb6df5f-gp4jx from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.809: INFO: 	Container metrics-server ready: true, restart count 0
Dec  9 08:48:20.809: INFO: metrics-server-7bcfb6df5f-pjfn4 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.809: INFO: 	Container metrics-server ready: true, restart count 0
Dec  9 08:48:20.809: INFO: network-problem-detector-host-4jvll from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.809: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Dec  9 08:48:20.809: INFO: network-problem-detector-pod-bdbrf from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.809: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Dec  9 08:48:20.809: INFO: node-exporter-8v484 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.809: INFO: 	Container node-exporter ready: true, restart count 0
Dec  9 08:48:20.809: INFO: node-local-dns-hrgm8 from kube-system started at 2022-12-09 08:15:12 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.809: INFO: 	Container node-cache ready: true, restart count 0
Dec  9 08:48:20.809: INFO: node-problem-detector-fvdnw from kube-system started at 2022-12-09 08:20:11 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.809: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  9 08:48:20.809: INFO: vpn-shoot-57fc54fdbb-6f7gk from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.809: INFO: 	Container vpn-shoot ready: true, restart count 0
Dec  9 08:48:20.809: INFO: dashboard-metrics-scraper-6d54964d4b-tqdzb from kubernetes-dashboard started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.809: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Dec  9 08:48:20.809: INFO: kubernetes-dashboard-5d6d5f9c58-vfrn8 from kubernetes-dashboard started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.809: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  9 08:48:20.809: INFO: 
Logging pods the apiserver thinks is on node izgw8hm6kg779yfidwdv8yz before test
Dec  9 08:48:20.833: INFO: addons-nginx-ingress-controller-69b7c7d86f-c7tdg from kube-system started at 2022-12-09 08:46:51 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.833: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  9 08:48:20.833: INFO: apiserver-proxy-nzwxs from kube-system started at 2022-12-09 07:59:48 +0000 UTC (2 container statuses recorded)
Dec  9 08:48:20.833: INFO: 	Container proxy ready: true, restart count 0
Dec  9 08:48:20.833: INFO: 	Container sidecar ready: true, restart count 0
Dec  9 08:48:20.833: INFO: calico-node-pbrgw from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.833: INFO: 	Container calico-node ready: true, restart count 0
Dec  9 08:48:20.833: INFO: calico-typha-deploy-65c54d4db6-mh94l from kube-system started at 2022-12-09 08:02:30 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.833: INFO: 	Container calico-typha ready: true, restart count 0
Dec  9 08:48:20.833: INFO: csi-disk-plugin-alicloud-tcf5b from kube-system started at 2022-12-09 07:59:48 +0000 UTC (3 container statuses recorded)
Dec  9 08:48:20.833: INFO: 	Container csi-diskplugin ready: true, restart count 0
Dec  9 08:48:20.833: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec  9 08:48:20.833: INFO: 	Container driver-registrar ready: true, restart count 0
Dec  9 08:48:20.833: INFO: egress-filter-applier-d9bt4 from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.833: INFO: 	Container egress-filter-applier ready: true, restart count 1
Dec  9 08:48:20.833: INFO: kube-proxy-worker-1-v1.25.4-grfnm from kube-system started at 2022-12-09 08:32:11 +0000 UTC (2 container statuses recorded)
Dec  9 08:48:20.833: INFO: 	Container conntrack-fix ready: true, restart count 0
Dec  9 08:48:20.833: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  9 08:48:20.833: INFO: network-problem-detector-host-wnxb2 from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.833: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Dec  9 08:48:20.833: INFO: network-problem-detector-pod-ntl2d from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.833: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Dec  9 08:48:20.833: INFO: node-exporter-zcl88 from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.833: INFO: 	Container node-exporter ready: true, restart count 0
Dec  9 08:48:20.833: INFO: node-local-dns-xhltp from kube-system started at 2022-12-09 08:15:12 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.833: INFO: 	Container node-cache ready: true, restart count 0
Dec  9 08:48:20.833: INFO: node-problem-detector-zf864 from kube-system started at 2022-12-09 08:20:11 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.833: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  9 08:48:20.833: INFO: with-labels from sched-pred-2529 started at 2022-12-09 08:48:18 +0000 UTC (1 container statuses recorded)
Dec  9 08:48:20.833: INFO: 	Container with-labels ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 12/09/22 08:48:20.833
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.172f13bfb4d48adb], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.] 12/09/22 08:48:20.892
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Dec  9 08:48:21.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3458" for this suite. 12/09/22 08:48:21.922
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":10,"skipped":3708,"failed":0}
------------------------------
• [1.252 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/09/22 08:48:20.683
    Dec  9 08:48:20.683: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-pred 12/09/22 08:48:20.684
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:48:20.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:48:20.736
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Dec  9 08:48:20.757: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Dec  9 08:48:20.780: INFO: Waiting for terminating namespaces to be deleted...
    Dec  9 08:48:20.791: INFO: 
    Logging pods the apiserver thinks is on node izgw86e9lj0cm49ixpgel1z before test
    Dec  9 08:48:20.809: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-8668c9bb59-48td7 from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.809: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: apiserver-proxy-9xnwn from kube-system started at 2022-12-09 07:59:46 +0000 UTC (2 container statuses recorded)
    Dec  9 08:48:20.809: INFO: 	Container proxy ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: 	Container sidecar ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: blackbox-exporter-59447f4c55-7n6c9 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.809: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: blackbox-exporter-59447f4c55-lnx2d from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.809: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: calico-kube-controllers-5f78564887-zgfpt from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.809: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: calico-node-snqzl from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.809: INFO: 	Container calico-node ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: calico-node-vertical-autoscaler-6597dd8998-sw7f8 from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.809: INFO: 	Container autoscaler ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: calico-typha-horizontal-autoscaler-6bb4bc55bc-ld47r from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.809: INFO: 	Container autoscaler ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: calico-typha-vertical-autoscaler-84df655c88-dwsfj from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.809: INFO: 	Container autoscaler ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: coredns-84fdf8dd87-62ckx from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.809: INFO: 	Container coredns ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: coredns-84fdf8dd87-lc5vg from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.809: INFO: 	Container coredns ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: csi-disk-plugin-alicloud-xpt2l from kube-system started at 2022-12-09 07:59:46 +0000 UTC (3 container statuses recorded)
    Dec  9 08:48:20.809: INFO: 	Container csi-diskplugin ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: 	Container driver-registrar ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: egress-filter-applier-xkds7 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.809: INFO: 	Container egress-filter-applier ready: true, restart count 1
    Dec  9 08:48:20.809: INFO: kube-proxy-worker-1-v1.25.4-nw6q5 from kube-system started at 2022-12-09 08:32:11 +0000 UTC (2 container statuses recorded)
    Dec  9 08:48:20.809: INFO: 	Container conntrack-fix ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: metrics-server-7bcfb6df5f-gp4jx from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.809: INFO: 	Container metrics-server ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: metrics-server-7bcfb6df5f-pjfn4 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.809: INFO: 	Container metrics-server ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: network-problem-detector-host-4jvll from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.809: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: network-problem-detector-pod-bdbrf from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.809: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: node-exporter-8v484 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.809: INFO: 	Container node-exporter ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: node-local-dns-hrgm8 from kube-system started at 2022-12-09 08:15:12 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.809: INFO: 	Container node-cache ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: node-problem-detector-fvdnw from kube-system started at 2022-12-09 08:20:11 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.809: INFO: 	Container node-problem-detector ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: vpn-shoot-57fc54fdbb-6f7gk from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.809: INFO: 	Container vpn-shoot ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: dashboard-metrics-scraper-6d54964d4b-tqdzb from kubernetes-dashboard started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.809: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: kubernetes-dashboard-5d6d5f9c58-vfrn8 from kubernetes-dashboard started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.809: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Dec  9 08:48:20.809: INFO: 
    Logging pods the apiserver thinks is on node izgw8hm6kg779yfidwdv8yz before test
    Dec  9 08:48:20.833: INFO: addons-nginx-ingress-controller-69b7c7d86f-c7tdg from kube-system started at 2022-12-09 08:46:51 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.833: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Dec  9 08:48:20.833: INFO: apiserver-proxy-nzwxs from kube-system started at 2022-12-09 07:59:48 +0000 UTC (2 container statuses recorded)
    Dec  9 08:48:20.833: INFO: 	Container proxy ready: true, restart count 0
    Dec  9 08:48:20.833: INFO: 	Container sidecar ready: true, restart count 0
    Dec  9 08:48:20.833: INFO: calico-node-pbrgw from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.833: INFO: 	Container calico-node ready: true, restart count 0
    Dec  9 08:48:20.833: INFO: calico-typha-deploy-65c54d4db6-mh94l from kube-system started at 2022-12-09 08:02:30 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.833: INFO: 	Container calico-typha ready: true, restart count 0
    Dec  9 08:48:20.833: INFO: csi-disk-plugin-alicloud-tcf5b from kube-system started at 2022-12-09 07:59:48 +0000 UTC (3 container statuses recorded)
    Dec  9 08:48:20.833: INFO: 	Container csi-diskplugin ready: true, restart count 0
    Dec  9 08:48:20.833: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Dec  9 08:48:20.833: INFO: 	Container driver-registrar ready: true, restart count 0
    Dec  9 08:48:20.833: INFO: egress-filter-applier-d9bt4 from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.833: INFO: 	Container egress-filter-applier ready: true, restart count 1
    Dec  9 08:48:20.833: INFO: kube-proxy-worker-1-v1.25.4-grfnm from kube-system started at 2022-12-09 08:32:11 +0000 UTC (2 container statuses recorded)
    Dec  9 08:48:20.833: INFO: 	Container conntrack-fix ready: true, restart count 0
    Dec  9 08:48:20.833: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec  9 08:48:20.833: INFO: network-problem-detector-host-wnxb2 from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.833: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Dec  9 08:48:20.833: INFO: network-problem-detector-pod-ntl2d from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.833: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Dec  9 08:48:20.833: INFO: node-exporter-zcl88 from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.833: INFO: 	Container node-exporter ready: true, restart count 0
    Dec  9 08:48:20.833: INFO: node-local-dns-xhltp from kube-system started at 2022-12-09 08:15:12 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.833: INFO: 	Container node-cache ready: true, restart count 0
    Dec  9 08:48:20.833: INFO: node-problem-detector-zf864 from kube-system started at 2022-12-09 08:20:11 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.833: INFO: 	Container node-problem-detector ready: true, restart count 0
    Dec  9 08:48:20.833: INFO: with-labels from sched-pred-2529 started at 2022-12-09 08:48:18 +0000 UTC (1 container statuses recorded)
    Dec  9 08:48:20.833: INFO: 	Container with-labels ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 12/09/22 08:48:20.833
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.172f13bfb4d48adb], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.] 12/09/22 08:48:20.892
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Dec  9 08:48:21.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-3458" for this suite. 12/09/22 08:48:21.922
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/09/22 08:48:21.942
Dec  9 08:48:21.942: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption 12/09/22 08:48:21.942
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:48:21.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:48:21.995
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Dec  9 08:48:22.048: INFO: Waiting up to 1m0s for all nodes to be ready
Dec  9 08:49:22.155: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/09/22 08:49:22.166
Dec  9 08:49:22.166: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption-path 12/09/22 08:49:22.167
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:49:22.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:49:22.219
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 12/09/22 08:49:22.238
STEP: Trying to launch a pod without a label to get a node which can launch it. 12/09/22 08:49:22.239
Dec  9 08:49:22.264: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-1537" to be "running"
Dec  9 08:49:22.274: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 10.476609ms
Dec  9 08:49:24.286: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.022022606s
Dec  9 08:49:24.286: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 12/09/22 08:49:24.297
Dec  9 08:49:24.311: INFO: found a healthy node: izgw8hm6kg779yfidwdv8yz
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Dec  9 08:49:32.509: INFO: pods created so far: [1 1 1]
Dec  9 08:49:32.509: INFO: length of pods created so far: 3
Dec  9 08:49:34.536: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Dec  9 08:49:41.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1537" for this suite. 12/09/22 08:49:41.56
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Dec  9 08:49:41.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3096" for this suite. 12/09/22 08:49:41.656
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":11,"skipped":3884,"failed":0}
------------------------------
• [79.807 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/09/22 08:48:21.942
    Dec  9 08:48:21.942: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-preemption 12/09/22 08:48:21.942
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:48:21.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:48:21.995
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Dec  9 08:48:22.048: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec  9 08:49:22.155: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/09/22 08:49:22.166
    Dec  9 08:49:22.166: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-preemption-path 12/09/22 08:49:22.167
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:49:22.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:49:22.219
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 12/09/22 08:49:22.238
    STEP: Trying to launch a pod without a label to get a node which can launch it. 12/09/22 08:49:22.239
    Dec  9 08:49:22.264: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-1537" to be "running"
    Dec  9 08:49:22.274: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 10.476609ms
    Dec  9 08:49:24.286: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.022022606s
    Dec  9 08:49:24.286: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 12/09/22 08:49:24.297
    Dec  9 08:49:24.311: INFO: found a healthy node: izgw8hm6kg779yfidwdv8yz
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Dec  9 08:49:32.509: INFO: pods created so far: [1 1 1]
    Dec  9 08:49:32.509: INFO: length of pods created so far: 3
    Dec  9 08:49:34.536: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Dec  9 08:49:41.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-1537" for this suite. 12/09/22 08:49:41.56
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Dec  9 08:49:41.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-3096" for this suite. 12/09/22 08:49:41.656
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/09/22 08:49:41.754
Dec  9 08:49:41.754: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces 12/09/22 08:49:41.755
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:49:41.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:49:41.807
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 12/09/22 08:49:41.827
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:49:41.861
STEP: Creating a pod in the namespace 12/09/22 08:49:41.88
STEP: Waiting for the pod to have running status 12/09/22 08:49:41.896
Dec  9 08:49:41.896: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-3306" to be "running"
Dec  9 08:49:41.907: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.520179ms
Dec  9 08:49:43.918: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.02217478s
Dec  9 08:49:43.918: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 12/09/22 08:49:43.918
STEP: Waiting for the namespace to be removed. 12/09/22 08:49:43.93
STEP: Recreating the namespace 12/09/22 08:49:54.942
STEP: Verifying there are no pods in the namespace 12/09/22 08:49:54.975
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Dec  9 08:49:54.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-81" for this suite. 12/09/22 08:49:55.005
STEP: Destroying namespace "nsdeletetest-3306" for this suite. 12/09/22 08:49:55.018
Dec  9 08:49:55.029: INFO: Namespace nsdeletetest-3306 was already deleted
STEP: Destroying namespace "nsdeletetest-1471" for this suite. 12/09/22 08:49:55.029
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":12,"skipped":4024,"failed":0}
------------------------------
• [13.286 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/09/22 08:49:41.754
    Dec  9 08:49:41.754: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename namespaces 12/09/22 08:49:41.755
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:49:41.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:49:41.807
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 12/09/22 08:49:41.827
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:49:41.861
    STEP: Creating a pod in the namespace 12/09/22 08:49:41.88
    STEP: Waiting for the pod to have running status 12/09/22 08:49:41.896
    Dec  9 08:49:41.896: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-3306" to be "running"
    Dec  9 08:49:41.907: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.520179ms
    Dec  9 08:49:43.918: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.02217478s
    Dec  9 08:49:43.918: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 12/09/22 08:49:43.918
    STEP: Waiting for the namespace to be removed. 12/09/22 08:49:43.93
    STEP: Recreating the namespace 12/09/22 08:49:54.942
    STEP: Verifying there are no pods in the namespace 12/09/22 08:49:54.975
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Dec  9 08:49:54.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-81" for this suite. 12/09/22 08:49:55.005
    STEP: Destroying namespace "nsdeletetest-3306" for this suite. 12/09/22 08:49:55.018
    Dec  9 08:49:55.029: INFO: Namespace nsdeletetest-3306 was already deleted
    STEP: Destroying namespace "nsdeletetest-1471" for this suite. 12/09/22 08:49:55.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/09/22 08:49:55.043
Dec  9 08:49:55.043: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption 12/09/22 08:49:55.044
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:49:55.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:49:55.096
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Dec  9 08:49:55.150: INFO: Waiting up to 1m0s for all nodes to be ready
Dec  9 08:50:55.259: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 12/09/22 08:50:55.27
Dec  9 08:50:55.307: INFO: Created pod: pod0-0-sched-preemption-low-priority
Dec  9 08:50:55.322: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Dec  9 08:50:55.376: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Dec  9 08:50:55.395: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 12/09/22 08:50:55.395
Dec  9 08:50:55.395: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-165" to be "running"
Dec  9 08:50:55.406: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.700872ms
Dec  9 08:50:57.418: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023305666s
Dec  9 08:50:59.417: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.022405022s
Dec  9 08:50:59.417: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Dec  9 08:50:59.417: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-165" to be "running"
Dec  9 08:50:59.429: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 11.227351ms
Dec  9 08:50:59.429: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Dec  9 08:50:59.429: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-165" to be "running"
Dec  9 08:50:59.439: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.785459ms
Dec  9 08:51:01.452: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.023318455s
Dec  9 08:51:01.452: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Dec  9 08:51:01.452: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-165" to be "running"
Dec  9 08:51:01.463: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 11.186077ms
Dec  9 08:51:01.463: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 12/09/22 08:51:01.463
Dec  9 08:51:01.479: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-165" to be "running"
Dec  9 08:51:01.489: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.518274ms
Dec  9 08:51:03.501: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021968328s
Dec  9 08:51:05.502: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.023262226s
Dec  9 08:51:05.502: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Dec  9 08:51:05.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-165" for this suite. 12/09/22 08:51:05.579
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":13,"skipped":4118,"failed":0}
------------------------------
• [70.628 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/09/22 08:49:55.043
    Dec  9 08:49:55.043: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-preemption 12/09/22 08:49:55.044
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:49:55.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:49:55.096
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Dec  9 08:49:55.150: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec  9 08:50:55.259: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 12/09/22 08:50:55.27
    Dec  9 08:50:55.307: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Dec  9 08:50:55.322: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Dec  9 08:50:55.376: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Dec  9 08:50:55.395: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 12/09/22 08:50:55.395
    Dec  9 08:50:55.395: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-165" to be "running"
    Dec  9 08:50:55.406: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.700872ms
    Dec  9 08:50:57.418: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023305666s
    Dec  9 08:50:59.417: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.022405022s
    Dec  9 08:50:59.417: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Dec  9 08:50:59.417: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-165" to be "running"
    Dec  9 08:50:59.429: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 11.227351ms
    Dec  9 08:50:59.429: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Dec  9 08:50:59.429: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-165" to be "running"
    Dec  9 08:50:59.439: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.785459ms
    Dec  9 08:51:01.452: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.023318455s
    Dec  9 08:51:01.452: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Dec  9 08:51:01.452: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-165" to be "running"
    Dec  9 08:51:01.463: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 11.186077ms
    Dec  9 08:51:01.463: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 12/09/22 08:51:01.463
    Dec  9 08:51:01.479: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-165" to be "running"
    Dec  9 08:51:01.489: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.518274ms
    Dec  9 08:51:03.501: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021968328s
    Dec  9 08:51:05.502: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.023262226s
    Dec  9 08:51:05.502: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Dec  9 08:51:05.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-165" for this suite. 12/09/22 08:51:05.579
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/09/22 08:51:05.673
Dec  9 08:51:05.673: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces 12/09/22 08:51:05.674
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:51:05.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:51:05.727
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 12/09/22 08:51:05.747
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:51:05.779
STEP: Creating a service in the namespace 12/09/22 08:51:05.799
STEP: Deleting the namespace 12/09/22 08:51:05.814
STEP: Waiting for the namespace to be removed. 12/09/22 08:51:05.826
STEP: Recreating the namespace 12/09/22 08:51:11.839
STEP: Verifying there is no service in the namespace 12/09/22 08:51:11.872
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Dec  9 08:51:11.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2798" for this suite. 12/09/22 08:51:11.904
STEP: Destroying namespace "nsdeletetest-8210" for this suite. 12/09/22 08:51:11.918
Dec  9 08:51:11.929: INFO: Namespace nsdeletetest-8210 was already deleted
STEP: Destroying namespace "nsdeletetest-6953" for this suite. 12/09/22 08:51:11.929
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":14,"skipped":4173,"failed":0}
------------------------------
• [6.267 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/09/22 08:51:05.673
    Dec  9 08:51:05.673: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename namespaces 12/09/22 08:51:05.674
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:51:05.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:51:05.727
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 12/09/22 08:51:05.747
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:51:05.779
    STEP: Creating a service in the namespace 12/09/22 08:51:05.799
    STEP: Deleting the namespace 12/09/22 08:51:05.814
    STEP: Waiting for the namespace to be removed. 12/09/22 08:51:05.826
    STEP: Recreating the namespace 12/09/22 08:51:11.839
    STEP: Verifying there is no service in the namespace 12/09/22 08:51:11.872
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Dec  9 08:51:11.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-2798" for this suite. 12/09/22 08:51:11.904
    STEP: Destroying namespace "nsdeletetest-8210" for this suite. 12/09/22 08:51:11.918
    Dec  9 08:51:11.929: INFO: Namespace nsdeletetest-8210 was already deleted
    STEP: Destroying namespace "nsdeletetest-6953" for this suite. 12/09/22 08:51:11.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/09/22 08:51:11.947
Dec  9 08:51:11.947: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 12/09/22 08:51:11.947
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:51:11.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:51:12
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 12/09/22 08:51:12.067
STEP: Check that daemon pods launch on every node of the cluster. 12/09/22 08:51:12.079
Dec  9 08:51:12.105: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:51:12.105: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
Dec  9 08:51:13.138: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 08:51:13.138: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
Dec  9 08:51:14.136: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec  9 08:51:14.136: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 12/09/22 08:51:14.147
STEP: DeleteCollection of the DaemonSets 12/09/22 08:51:14.159
STEP: Verify that ReplicaSets have been deleted 12/09/22 08:51:14.172
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Dec  9 08:51:14.205: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20107"},"items":null}

Dec  9 08:51:14.216: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20107"},"items":[{"metadata":{"name":"daemon-set-dm88t","generateName":"daemon-set-","namespace":"daemonsets-1393","uid":"b8c1938b-374c-4bcb-a229-7edfd2a1ded7","resourceVersion":"20107","creationTimestamp":"2022-12-09T08:51:12Z","deletionTimestamp":"2022-12-09T08:51:44Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"a9246e8136a444c47c8c736a6a6e950cd1b11d3add4a82a546339be8ecccc3af","cni.projectcalico.org/podIP":"172.16.1.28/32","cni.projectcalico.org/podIPs":"172.16.1.28/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"03a73259-4130-456c-8593-7224b73e7b0b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-12-09T08:51:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-09T08:51:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03a73259-4130-456c-8593-7224b73e7b0b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-09T08:51:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.1.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-26hkt","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.tmb3w-wjj.it.internal.staging.k8s.ondemand.com"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-26hkt","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"izgw8hm6kg779yfidwdv8yz","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["izgw8hm6kg779yfidwdv8yz"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T08:51:12Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T08:51:12Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T08:51:12Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T08:51:12Z"}],"hostIP":"10.250.13.218","podIP":"172.16.1.28","podIPs":[{"ip":"172.16.1.28"}],"startTime":"2022-12-09T08:51:12Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-09T08:51:12Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://db24bfbce0ea92e8bfeef453a0f02001fd3d08687033fce4fdc2160fff632969","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-m487d","generateName":"daemon-set-","namespace":"daemonsets-1393","uid":"c7a540f5-fa4a-40a1-9272-a5363f5f2e11","resourceVersion":"20106","creationTimestamp":"2022-12-09T08:51:12Z","deletionTimestamp":"2022-12-09T08:51:44Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"3070880150df07112ca007d979b88f1fcb910ebfa3ca2fd1abfdc3254c0bec02","cni.projectcalico.org/podIP":"172.16.0.42/32","cni.projectcalico.org/podIPs":"172.16.0.42/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"03a73259-4130-456c-8593-7224b73e7b0b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-12-09T08:51:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-09T08:51:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03a73259-4130-456c-8593-7224b73e7b0b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-09T08:51:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.0.42\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-b65hh","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.tmb3w-wjj.it.internal.staging.k8s.ondemand.com"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-b65hh","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"izgw86e9lj0cm49ixpgel1z","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["izgw86e9lj0cm49ixpgel1z"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T08:51:12Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T08:51:13Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T08:51:13Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T08:51:12Z"}],"hostIP":"10.250.13.217","podIP":"172.16.0.42","podIPs":[{"ip":"172.16.0.42"}],"startTime":"2022-12-09T08:51:12Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-09T08:51:12Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://25fbfe8ea79044c8dfdcf3b9c65d6717d2a26152e98c37945cdbb7ae76d3d316","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Dec  9 08:51:14.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1393" for this suite. 12/09/22 08:51:14.261
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":15,"skipped":4329,"failed":0}
------------------------------
• [2.327 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/09/22 08:51:11.947
    Dec  9 08:51:11.947: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 12/09/22 08:51:11.947
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:51:11.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:51:12
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 12/09/22 08:51:12.067
    STEP: Check that daemon pods launch on every node of the cluster. 12/09/22 08:51:12.079
    Dec  9 08:51:12.105: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:51:12.105: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
    Dec  9 08:51:13.138: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 08:51:13.138: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
    Dec  9 08:51:14.136: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec  9 08:51:14.136: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 12/09/22 08:51:14.147
    STEP: DeleteCollection of the DaemonSets 12/09/22 08:51:14.159
    STEP: Verify that ReplicaSets have been deleted 12/09/22 08:51:14.172
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Dec  9 08:51:14.205: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20107"},"items":null}

    Dec  9 08:51:14.216: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20107"},"items":[{"metadata":{"name":"daemon-set-dm88t","generateName":"daemon-set-","namespace":"daemonsets-1393","uid":"b8c1938b-374c-4bcb-a229-7edfd2a1ded7","resourceVersion":"20107","creationTimestamp":"2022-12-09T08:51:12Z","deletionTimestamp":"2022-12-09T08:51:44Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"a9246e8136a444c47c8c736a6a6e950cd1b11d3add4a82a546339be8ecccc3af","cni.projectcalico.org/podIP":"172.16.1.28/32","cni.projectcalico.org/podIPs":"172.16.1.28/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"03a73259-4130-456c-8593-7224b73e7b0b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-12-09T08:51:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-09T08:51:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03a73259-4130-456c-8593-7224b73e7b0b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-09T08:51:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.1.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-26hkt","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.tmb3w-wjj.it.internal.staging.k8s.ondemand.com"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-26hkt","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"izgw8hm6kg779yfidwdv8yz","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["izgw8hm6kg779yfidwdv8yz"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T08:51:12Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T08:51:12Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T08:51:12Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T08:51:12Z"}],"hostIP":"10.250.13.218","podIP":"172.16.1.28","podIPs":[{"ip":"172.16.1.28"}],"startTime":"2022-12-09T08:51:12Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-09T08:51:12Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://db24bfbce0ea92e8bfeef453a0f02001fd3d08687033fce4fdc2160fff632969","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-m487d","generateName":"daemon-set-","namespace":"daemonsets-1393","uid":"c7a540f5-fa4a-40a1-9272-a5363f5f2e11","resourceVersion":"20106","creationTimestamp":"2022-12-09T08:51:12Z","deletionTimestamp":"2022-12-09T08:51:44Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"3070880150df07112ca007d979b88f1fcb910ebfa3ca2fd1abfdc3254c0bec02","cni.projectcalico.org/podIP":"172.16.0.42/32","cni.projectcalico.org/podIPs":"172.16.0.42/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"03a73259-4130-456c-8593-7224b73e7b0b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-12-09T08:51:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-09T08:51:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03a73259-4130-456c-8593-7224b73e7b0b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-09T08:51:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.0.42\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-b65hh","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.tmb3w-wjj.it.internal.staging.k8s.ondemand.com"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-b65hh","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"izgw86e9lj0cm49ixpgel1z","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["izgw86e9lj0cm49ixpgel1z"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T08:51:12Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T08:51:13Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T08:51:13Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T08:51:12Z"}],"hostIP":"10.250.13.217","podIP":"172.16.0.42","podIPs":[{"ip":"172.16.0.42"}],"startTime":"2022-12-09T08:51:12Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-09T08:51:12Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://25fbfe8ea79044c8dfdcf3b9c65d6717d2a26152e98c37945cdbb7ae76d3d316","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Dec  9 08:51:14.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1393" for this suite. 12/09/22 08:51:14.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/09/22 08:51:14.303
Dec  9 08:51:14.303: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename taint-multiple-pods 12/09/22 08:51:14.303
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:51:14.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:51:14.356
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Dec  9 08:51:14.376: INFO: Waiting up to 1m0s for all nodes to be ready
Dec  9 08:52:14.473: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Dec  9 08:52:14.484: INFO: Starting informer...
STEP: Starting pods... 12/09/22 08:52:14.484
Dec  9 08:52:14.523: INFO: Pod1 is running on izgw8hm6kg779yfidwdv8yz. Tainting Node
Dec  9 08:52:14.547: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-8528" to be "running"
Dec  9 08:52:14.558: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.578177ms
Dec  9 08:52:16.570: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.022768125s
Dec  9 08:52:16.570: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Dec  9 08:52:16.570: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-8528" to be "running"
Dec  9 08:52:16.581: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 10.834222ms
Dec  9 08:52:16.581: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Dec  9 08:52:16.581: INFO: Pod2 is running on izgw8hm6kg779yfidwdv8yz. Tainting Node
STEP: Trying to apply a taint on the Node 12/09/22 08:52:16.581
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/09/22 08:52:16.619
STEP: Waiting for Pod1 and Pod2 to be deleted 12/09/22 08:52:16.632
Dec  9 08:52:22.047: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Dec  9 08:52:42.099: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/09/22 08:52:42.129
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Dec  9 08:52:42.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-8528" for this suite. 12/09/22 08:52:42.156
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":16,"skipped":5287,"failed":0}
------------------------------
• [87.869 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/09/22 08:51:14.303
    Dec  9 08:51:14.303: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename taint-multiple-pods 12/09/22 08:51:14.303
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:51:14.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:51:14.356
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Dec  9 08:51:14.376: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec  9 08:52:14.473: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Dec  9 08:52:14.484: INFO: Starting informer...
    STEP: Starting pods... 12/09/22 08:52:14.484
    Dec  9 08:52:14.523: INFO: Pod1 is running on izgw8hm6kg779yfidwdv8yz. Tainting Node
    Dec  9 08:52:14.547: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-8528" to be "running"
    Dec  9 08:52:14.558: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.578177ms
    Dec  9 08:52:16.570: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.022768125s
    Dec  9 08:52:16.570: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Dec  9 08:52:16.570: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-8528" to be "running"
    Dec  9 08:52:16.581: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 10.834222ms
    Dec  9 08:52:16.581: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Dec  9 08:52:16.581: INFO: Pod2 is running on izgw8hm6kg779yfidwdv8yz. Tainting Node
    STEP: Trying to apply a taint on the Node 12/09/22 08:52:16.581
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/09/22 08:52:16.619
    STEP: Waiting for Pod1 and Pod2 to be deleted 12/09/22 08:52:16.632
    Dec  9 08:52:22.047: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Dec  9 08:52:42.099: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/09/22 08:52:42.129
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Dec  9 08:52:42.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-8528" for this suite. 12/09/22 08:52:42.156
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/09/22 08:52:42.192
Dec  9 08:52:42.192: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption 12/09/22 08:52:42.193
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:52:42.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:52:42.248
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Dec  9 08:52:42.307: INFO: Waiting up to 1m0s for all nodes to be ready
Dec  9 08:53:42.428: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 12/09/22 08:53:42.44
Dec  9 08:53:42.477: INFO: Created pod: pod0-0-sched-preemption-low-priority
Dec  9 08:53:42.492: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Dec  9 08:53:42.532: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Dec  9 08:53:42.547: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 12/09/22 08:53:42.547
Dec  9 08:53:42.547: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9909" to be "running"
Dec  9 08:53:42.558: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.50357ms
Dec  9 08:53:44.570: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.022541328s
Dec  9 08:53:44.570: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Dec  9 08:53:44.570: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9909" to be "running"
Dec  9 08:53:44.581: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.928429ms
Dec  9 08:53:44.581: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Dec  9 08:53:44.581: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9909" to be "running"
Dec  9 08:53:44.592: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 11.092595ms
Dec  9 08:53:44.592: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Dec  9 08:53:44.592: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9909" to be "running"
Dec  9 08:53:44.603: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.780367ms
Dec  9 08:53:44.603: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 12/09/22 08:53:44.603
Dec  9 08:53:44.627: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Dec  9 08:53:44.638: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.563876ms
Dec  9 08:53:46.649: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021763752s
Dec  9 08:53:48.650: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.022598463s
Dec  9 08:53:48.650: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Dec  9 08:53:48.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9909" for this suite. 12/09/22 08:53:48.761
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":17,"skipped":5820,"failed":0}
------------------------------
• [66.676 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/09/22 08:52:42.192
    Dec  9 08:52:42.192: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-preemption 12/09/22 08:52:42.193
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:52:42.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:52:42.248
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Dec  9 08:52:42.307: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec  9 08:53:42.428: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 12/09/22 08:53:42.44
    Dec  9 08:53:42.477: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Dec  9 08:53:42.492: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Dec  9 08:53:42.532: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Dec  9 08:53:42.547: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 12/09/22 08:53:42.547
    Dec  9 08:53:42.547: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9909" to be "running"
    Dec  9 08:53:42.558: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.50357ms
    Dec  9 08:53:44.570: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.022541328s
    Dec  9 08:53:44.570: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Dec  9 08:53:44.570: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9909" to be "running"
    Dec  9 08:53:44.581: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.928429ms
    Dec  9 08:53:44.581: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Dec  9 08:53:44.581: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9909" to be "running"
    Dec  9 08:53:44.592: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 11.092595ms
    Dec  9 08:53:44.592: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Dec  9 08:53:44.592: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9909" to be "running"
    Dec  9 08:53:44.603: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.780367ms
    Dec  9 08:53:44.603: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 12/09/22 08:53:44.603
    Dec  9 08:53:44.627: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Dec  9 08:53:44.638: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.563876ms
    Dec  9 08:53:46.649: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021763752s
    Dec  9 08:53:48.650: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.022598463s
    Dec  9 08:53:48.650: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Dec  9 08:53:48.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-9909" for this suite. 12/09/22 08:53:48.761
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/09/22 08:53:48.882
Dec  9 08:53:48.883: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 12/09/22 08:53:48.883
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:53:48.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:53:48.938
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Dec  9 08:53:49.004: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 12/09/22 08:53:49.016
Dec  9 08:53:49.027: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:53:49.027: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 12/09/22 08:53:49.027
Dec  9 08:53:49.086: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:53:49.086: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
Dec  9 08:53:50.097: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:53:50.097: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
Dec  9 08:53:51.097: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 08:53:51.097: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 12/09/22 08:53:51.108
Dec  9 08:53:51.164: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:53:51.164: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 12/09/22 08:53:51.164
Dec  9 08:53:51.188: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:53:51.188: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
Dec  9 08:53:52.200: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:53:52.200: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
Dec  9 08:53:53.200: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:53:53.201: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
Dec  9 08:53:54.200: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:53:54.200: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
Dec  9 08:53:55.200: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 08:53:55.200: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 12/09/22 08:53:55.222
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1875, will wait for the garbage collector to delete the pods 12/09/22 08:53:55.222
Dec  9 08:53:55.295: INFO: Deleting DaemonSet.extensions daemon-set took: 11.970587ms
Dec  9 08:53:55.397: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.12367ms
Dec  9 08:53:57.608: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:53:57.608: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec  9 08:53:57.619: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21157"},"items":null}

Dec  9 08:53:57.630: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21157"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Dec  9 08:53:57.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1875" for this suite. 12/09/22 08:53:57.71
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":18,"skipped":6417,"failed":0}
------------------------------
• [8.839 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/09/22 08:53:48.882
    Dec  9 08:53:48.883: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 12/09/22 08:53:48.883
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:53:48.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:53:48.938
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Dec  9 08:53:49.004: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 12/09/22 08:53:49.016
    Dec  9 08:53:49.027: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:53:49.027: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 12/09/22 08:53:49.027
    Dec  9 08:53:49.086: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:53:49.086: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
    Dec  9 08:53:50.097: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:53:50.097: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
    Dec  9 08:53:51.097: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 08:53:51.097: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 12/09/22 08:53:51.108
    Dec  9 08:53:51.164: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:53:51.164: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 12/09/22 08:53:51.164
    Dec  9 08:53:51.188: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:53:51.188: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
    Dec  9 08:53:52.200: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:53:52.200: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
    Dec  9 08:53:53.200: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:53:53.201: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
    Dec  9 08:53:54.200: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:53:54.200: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
    Dec  9 08:53:55.200: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 08:53:55.200: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 12/09/22 08:53:55.222
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1875, will wait for the garbage collector to delete the pods 12/09/22 08:53:55.222
    Dec  9 08:53:55.295: INFO: Deleting DaemonSet.extensions daemon-set took: 11.970587ms
    Dec  9 08:53:55.397: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.12367ms
    Dec  9 08:53:57.608: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:53:57.608: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec  9 08:53:57.619: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21157"},"items":null}

    Dec  9 08:53:57.630: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21157"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Dec  9 08:53:57.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1875" for this suite. 12/09/22 08:53:57.71
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/09/22 08:53:57.728
Dec  9 08:53:57.728: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 12/09/22 08:53:57.729
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:53:57.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:53:57.782
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 12/09/22 08:53:57.871
STEP: Check that daemon pods launch on every node of the cluster. 12/09/22 08:53:57.883
Dec  9 08:53:57.906: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:53:57.906: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
Dec  9 08:53:58.939: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 08:53:58.939: INFO: Node izgw8hm6kg779yfidwdv8yz is running 0 daemon pod, expected 1
Dec  9 08:53:59.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec  9 08:53:59.938: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 12/09/22 08:53:59.948
Dec  9 08:53:59.995: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 08:53:59.995: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
Dec  9 08:54:01.027: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 08:54:01.027: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
Dec  9 08:54:02.028: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec  9 08:54:02.028: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 12/09/22 08:54:02.028
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 12/09/22 08:54:02.05
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2874, will wait for the garbage collector to delete the pods 12/09/22 08:54:02.051
Dec  9 08:54:02.125: INFO: Deleting DaemonSet.extensions daemon-set took: 12.615232ms
Dec  9 08:54:02.226: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.604861ms
Dec  9 08:54:04.638: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:54:04.638: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec  9 08:54:04.649: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21271"},"items":null}

Dec  9 08:54:04.660: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21271"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Dec  9 08:54:04.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2874" for this suite. 12/09/22 08:54:04.716
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":19,"skipped":6574,"failed":0}
------------------------------
• [7.000 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/09/22 08:53:57.728
    Dec  9 08:53:57.728: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 12/09/22 08:53:57.729
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:53:57.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:53:57.782
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 12/09/22 08:53:57.871
    STEP: Check that daemon pods launch on every node of the cluster. 12/09/22 08:53:57.883
    Dec  9 08:53:57.906: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:53:57.906: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
    Dec  9 08:53:58.939: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 08:53:58.939: INFO: Node izgw8hm6kg779yfidwdv8yz is running 0 daemon pod, expected 1
    Dec  9 08:53:59.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec  9 08:53:59.938: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 12/09/22 08:53:59.948
    Dec  9 08:53:59.995: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 08:53:59.995: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
    Dec  9 08:54:01.027: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 08:54:01.027: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
    Dec  9 08:54:02.028: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec  9 08:54:02.028: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 12/09/22 08:54:02.028
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 12/09/22 08:54:02.05
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2874, will wait for the garbage collector to delete the pods 12/09/22 08:54:02.051
    Dec  9 08:54:02.125: INFO: Deleting DaemonSet.extensions daemon-set took: 12.615232ms
    Dec  9 08:54:02.226: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.604861ms
    Dec  9 08:54:04.638: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:54:04.638: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec  9 08:54:04.649: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21271"},"items":null}

    Dec  9 08:54:04.660: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21271"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Dec  9 08:54:04.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-2874" for this suite. 12/09/22 08:54:04.716
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/09/22 08:54:04.731
Dec  9 08:54:04.731: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 12/09/22 08:54:04.732
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:54:04.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:54:04.784
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 12/09/22 08:54:04.85
STEP: Check that daemon pods launch on every node of the cluster. 12/09/22 08:54:04.861
Dec  9 08:54:04.886: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:54:04.886: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
Dec  9 08:54:05.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 08:54:05.917: INFO: Node izgw8hm6kg779yfidwdv8yz is running 0 daemon pod, expected 1
Dec  9 08:54:06.918: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec  9 08:54:06.918: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 12/09/22 08:54:06.929
Dec  9 08:54:06.975: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 08:54:06.975: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
Dec  9 08:54:08.007: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 08:54:08.007: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
Dec  9 08:54:09.007: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 08:54:09.007: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
Dec  9 08:54:10.007: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec  9 08:54:10.007: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 12/09/22 08:54:10.019
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3891, will wait for the garbage collector to delete the pods 12/09/22 08:54:10.019
Dec  9 08:54:10.093: INFO: Deleting DaemonSet.extensions daemon-set took: 12.251751ms
Dec  9 08:54:10.193: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.194368ms
Dec  9 08:54:12.705: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:54:12.705: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec  9 08:54:12.716: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21354"},"items":null}

Dec  9 08:54:12.727: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21354"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Dec  9 08:54:12.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3891" for this suite. 12/09/22 08:54:12.782
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":20,"skipped":6647,"failed":0}
------------------------------
• [8.063 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/09/22 08:54:04.731
    Dec  9 08:54:04.731: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 12/09/22 08:54:04.732
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:54:04.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:54:04.784
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 12/09/22 08:54:04.85
    STEP: Check that daemon pods launch on every node of the cluster. 12/09/22 08:54:04.861
    Dec  9 08:54:04.886: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:54:04.886: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
    Dec  9 08:54:05.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 08:54:05.917: INFO: Node izgw8hm6kg779yfidwdv8yz is running 0 daemon pod, expected 1
    Dec  9 08:54:06.918: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec  9 08:54:06.918: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 12/09/22 08:54:06.929
    Dec  9 08:54:06.975: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 08:54:06.975: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
    Dec  9 08:54:08.007: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 08:54:08.007: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
    Dec  9 08:54:09.007: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 08:54:09.007: INFO: Node izgw86e9lj0cm49ixpgel1z is running 0 daemon pod, expected 1
    Dec  9 08:54:10.007: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec  9 08:54:10.007: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 12/09/22 08:54:10.019
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3891, will wait for the garbage collector to delete the pods 12/09/22 08:54:10.019
    Dec  9 08:54:10.093: INFO: Deleting DaemonSet.extensions daemon-set took: 12.251751ms
    Dec  9 08:54:10.193: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.194368ms
    Dec  9 08:54:12.705: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:54:12.705: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec  9 08:54:12.716: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21354"},"items":null}

    Dec  9 08:54:12.727: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21354"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Dec  9 08:54:12.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3891" for this suite. 12/09/22 08:54:12.782
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/09/22 08:54:12.796
Dec  9 08:54:12.796: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred 12/09/22 08:54:12.797
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:54:12.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:54:12.853
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Dec  9 08:54:12.874: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  9 08:54:12.897: INFO: Waiting for terminating namespaces to be deleted...
Dec  9 08:54:12.908: INFO: 
Logging pods the apiserver thinks is on node izgw86e9lj0cm49ixpgel1z before test
Dec  9 08:54:12.926: INFO: addons-nginx-ingress-controller-69b7c7d86f-zhstx from kube-system started at 2022-12-09 08:52:16 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.926: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  9 08:54:12.926: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-8668c9bb59-48td7 from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.926: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Dec  9 08:54:12.926: INFO: apiserver-proxy-9xnwn from kube-system started at 2022-12-09 07:59:46 +0000 UTC (2 container statuses recorded)
Dec  9 08:54:12.926: INFO: 	Container proxy ready: true, restart count 0
Dec  9 08:54:12.926: INFO: 	Container sidecar ready: true, restart count 0
Dec  9 08:54:12.926: INFO: blackbox-exporter-59447f4c55-7n6c9 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.926: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec  9 08:54:12.926: INFO: blackbox-exporter-59447f4c55-lnx2d from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.926: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec  9 08:54:12.926: INFO: calico-kube-controllers-5f78564887-zgfpt from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.926: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec  9 08:54:12.926: INFO: calico-node-snqzl from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.926: INFO: 	Container calico-node ready: true, restart count 0
Dec  9 08:54:12.926: INFO: calico-node-vertical-autoscaler-6597dd8998-sw7f8 from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.926: INFO: 	Container autoscaler ready: true, restart count 0
Dec  9 08:54:12.926: INFO: calico-typha-horizontal-autoscaler-6bb4bc55bc-ld47r from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.926: INFO: 	Container autoscaler ready: true, restart count 0
Dec  9 08:54:12.926: INFO: calico-typha-vertical-autoscaler-84df655c88-dwsfj from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.926: INFO: 	Container autoscaler ready: true, restart count 0
Dec  9 08:54:12.926: INFO: coredns-84fdf8dd87-62ckx from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.926: INFO: 	Container coredns ready: true, restart count 0
Dec  9 08:54:12.926: INFO: coredns-84fdf8dd87-lc5vg from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.926: INFO: 	Container coredns ready: true, restart count 0
Dec  9 08:54:12.926: INFO: csi-disk-plugin-alicloud-xpt2l from kube-system started at 2022-12-09 07:59:46 +0000 UTC (3 container statuses recorded)
Dec  9 08:54:12.926: INFO: 	Container csi-diskplugin ready: true, restart count 0
Dec  9 08:54:12.926: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec  9 08:54:12.926: INFO: 	Container driver-registrar ready: true, restart count 0
Dec  9 08:54:12.926: INFO: egress-filter-applier-xkds7 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.926: INFO: 	Container egress-filter-applier ready: true, restart count 1
Dec  9 08:54:12.926: INFO: kube-proxy-worker-1-v1.25.4-nw6q5 from kube-system started at 2022-12-09 08:32:11 +0000 UTC (2 container statuses recorded)
Dec  9 08:54:12.926: INFO: 	Container conntrack-fix ready: true, restart count 0
Dec  9 08:54:12.926: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  9 08:54:12.926: INFO: metrics-server-7bcfb6df5f-gp4jx from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.926: INFO: 	Container metrics-server ready: true, restart count 0
Dec  9 08:54:12.926: INFO: metrics-server-7bcfb6df5f-pjfn4 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.926: INFO: 	Container metrics-server ready: true, restart count 0
Dec  9 08:54:12.926: INFO: network-problem-detector-host-4jvll from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.926: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Dec  9 08:54:12.926: INFO: network-problem-detector-pod-bdbrf from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.926: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Dec  9 08:54:12.926: INFO: node-exporter-8v484 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.926: INFO: 	Container node-exporter ready: true, restart count 0
Dec  9 08:54:12.926: INFO: node-local-dns-hrgm8 from kube-system started at 2022-12-09 08:15:12 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.926: INFO: 	Container node-cache ready: true, restart count 0
Dec  9 08:54:12.926: INFO: node-problem-detector-fvdnw from kube-system started at 2022-12-09 08:20:11 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.926: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  9 08:54:12.926: INFO: vpn-shoot-57fc54fdbb-6f7gk from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.926: INFO: 	Container vpn-shoot ready: true, restart count 0
Dec  9 08:54:12.926: INFO: dashboard-metrics-scraper-6d54964d4b-tqdzb from kubernetes-dashboard started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.926: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Dec  9 08:54:12.926: INFO: kubernetes-dashboard-5d6d5f9c58-vfrn8 from kubernetes-dashboard started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.926: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  9 08:54:12.926: INFO: 
Logging pods the apiserver thinks is on node izgw8hm6kg779yfidwdv8yz before test
Dec  9 08:54:12.951: INFO: apiserver-proxy-nzwxs from kube-system started at 2022-12-09 07:59:48 +0000 UTC (2 container statuses recorded)
Dec  9 08:54:12.951: INFO: 	Container proxy ready: true, restart count 0
Dec  9 08:54:12.951: INFO: 	Container sidecar ready: true, restart count 0
Dec  9 08:54:12.951: INFO: calico-node-pbrgw from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.951: INFO: 	Container calico-node ready: true, restart count 0
Dec  9 08:54:12.951: INFO: calico-typha-deploy-65c54d4db6-mh94l from kube-system started at 2022-12-09 08:02:30 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.951: INFO: 	Container calico-typha ready: true, restart count 0
Dec  9 08:54:12.951: INFO: csi-disk-plugin-alicloud-tcf5b from kube-system started at 2022-12-09 07:59:48 +0000 UTC (3 container statuses recorded)
Dec  9 08:54:12.951: INFO: 	Container csi-diskplugin ready: true, restart count 0
Dec  9 08:54:12.951: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec  9 08:54:12.951: INFO: 	Container driver-registrar ready: true, restart count 0
Dec  9 08:54:12.951: INFO: egress-filter-applier-d9bt4 from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.951: INFO: 	Container egress-filter-applier ready: true, restart count 1
Dec  9 08:54:12.951: INFO: kube-proxy-worker-1-v1.25.4-grfnm from kube-system started at 2022-12-09 08:32:11 +0000 UTC (2 container statuses recorded)
Dec  9 08:54:12.951: INFO: 	Container conntrack-fix ready: true, restart count 0
Dec  9 08:54:12.951: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  9 08:54:12.951: INFO: network-problem-detector-host-wnxb2 from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.951: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Dec  9 08:54:12.951: INFO: network-problem-detector-pod-ntl2d from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.951: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Dec  9 08:54:12.951: INFO: node-exporter-zcl88 from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.951: INFO: 	Container node-exporter ready: true, restart count 0
Dec  9 08:54:12.951: INFO: node-local-dns-xhltp from kube-system started at 2022-12-09 08:15:12 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.951: INFO: 	Container node-cache ready: true, restart count 0
Dec  9 08:54:12.951: INFO: node-problem-detector-zf864 from kube-system started at 2022-12-09 08:20:11 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:12.951: INFO: 	Container node-problem-detector ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node izgw86e9lj0cm49ixpgel1z 12/09/22 08:54:12.987
STEP: verifying the node has the label node izgw8hm6kg779yfidwdv8yz 12/09/22 08:54:13.018
Dec  9 08:54:13.047: INFO: Pod addons-nginx-ingress-controller-69b7c7d86f-zhstx requesting resource cpu=100m on Node izgw86e9lj0cm49ixpgel1z
Dec  9 08:54:13.047: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-8668c9bb59-48td7 requesting resource cpu=0m on Node izgw86e9lj0cm49ixpgel1z
Dec  9 08:54:13.047: INFO: Pod apiserver-proxy-9xnwn requesting resource cpu=40m on Node izgw86e9lj0cm49ixpgel1z
Dec  9 08:54:13.047: INFO: Pod apiserver-proxy-nzwxs requesting resource cpu=40m on Node izgw8hm6kg779yfidwdv8yz
Dec  9 08:54:13.047: INFO: Pod blackbox-exporter-59447f4c55-7n6c9 requesting resource cpu=10m on Node izgw86e9lj0cm49ixpgel1z
Dec  9 08:54:13.047: INFO: Pod blackbox-exporter-59447f4c55-lnx2d requesting resource cpu=10m on Node izgw86e9lj0cm49ixpgel1z
Dec  9 08:54:13.047: INFO: Pod calico-kube-controllers-5f78564887-zgfpt requesting resource cpu=10m on Node izgw86e9lj0cm49ixpgel1z
Dec  9 08:54:13.047: INFO: Pod calico-node-pbrgw requesting resource cpu=250m on Node izgw8hm6kg779yfidwdv8yz
Dec  9 08:54:13.048: INFO: Pod calico-node-snqzl requesting resource cpu=250m on Node izgw86e9lj0cm49ixpgel1z
Dec  9 08:54:13.048: INFO: Pod calico-node-vertical-autoscaler-6597dd8998-sw7f8 requesting resource cpu=10m on Node izgw86e9lj0cm49ixpgel1z
Dec  9 08:54:13.048: INFO: Pod calico-typha-deploy-65c54d4db6-mh94l requesting resource cpu=320m on Node izgw8hm6kg779yfidwdv8yz
Dec  9 08:54:13.048: INFO: Pod calico-typha-horizontal-autoscaler-6bb4bc55bc-ld47r requesting resource cpu=10m on Node izgw86e9lj0cm49ixpgel1z
Dec  9 08:54:13.048: INFO: Pod calico-typha-vertical-autoscaler-84df655c88-dwsfj requesting resource cpu=10m on Node izgw86e9lj0cm49ixpgel1z
Dec  9 08:54:13.048: INFO: Pod coredns-84fdf8dd87-62ckx requesting resource cpu=50m on Node izgw86e9lj0cm49ixpgel1z
Dec  9 08:54:13.048: INFO: Pod coredns-84fdf8dd87-lc5vg requesting resource cpu=50m on Node izgw86e9lj0cm49ixpgel1z
Dec  9 08:54:13.048: INFO: Pod csi-disk-plugin-alicloud-tcf5b requesting resource cpu=34m on Node izgw8hm6kg779yfidwdv8yz
Dec  9 08:54:13.048: INFO: Pod csi-disk-plugin-alicloud-xpt2l requesting resource cpu=34m on Node izgw86e9lj0cm49ixpgel1z
Dec  9 08:54:13.048: INFO: Pod egress-filter-applier-d9bt4 requesting resource cpu=50m on Node izgw8hm6kg779yfidwdv8yz
Dec  9 08:54:13.048: INFO: Pod egress-filter-applier-xkds7 requesting resource cpu=50m on Node izgw86e9lj0cm49ixpgel1z
Dec  9 08:54:13.048: INFO: Pod kube-proxy-worker-1-v1.25.4-grfnm requesting resource cpu=22m on Node izgw8hm6kg779yfidwdv8yz
Dec  9 08:54:13.048: INFO: Pod kube-proxy-worker-1-v1.25.4-nw6q5 requesting resource cpu=22m on Node izgw86e9lj0cm49ixpgel1z
Dec  9 08:54:13.048: INFO: Pod metrics-server-7bcfb6df5f-gp4jx requesting resource cpu=50m on Node izgw86e9lj0cm49ixpgel1z
Dec  9 08:54:13.048: INFO: Pod metrics-server-7bcfb6df5f-pjfn4 requesting resource cpu=50m on Node izgw86e9lj0cm49ixpgel1z
Dec  9 08:54:13.048: INFO: Pod network-problem-detector-host-4jvll requesting resource cpu=10m on Node izgw86e9lj0cm49ixpgel1z
Dec  9 08:54:13.048: INFO: Pod network-problem-detector-host-wnxb2 requesting resource cpu=10m on Node izgw8hm6kg779yfidwdv8yz
Dec  9 08:54:13.048: INFO: Pod network-problem-detector-pod-bdbrf requesting resource cpu=10m on Node izgw86e9lj0cm49ixpgel1z
Dec  9 08:54:13.048: INFO: Pod network-problem-detector-pod-ntl2d requesting resource cpu=10m on Node izgw8hm6kg779yfidwdv8yz
Dec  9 08:54:13.048: INFO: Pod node-exporter-8v484 requesting resource cpu=50m on Node izgw86e9lj0cm49ixpgel1z
Dec  9 08:54:13.048: INFO: Pod node-exporter-zcl88 requesting resource cpu=50m on Node izgw8hm6kg779yfidwdv8yz
Dec  9 08:54:13.048: INFO: Pod node-local-dns-hrgm8 requesting resource cpu=11m on Node izgw86e9lj0cm49ixpgel1z
Dec  9 08:54:13.048: INFO: Pod node-local-dns-xhltp requesting resource cpu=11m on Node izgw8hm6kg779yfidwdv8yz
Dec  9 08:54:13.048: INFO: Pod node-problem-detector-fvdnw requesting resource cpu=11m on Node izgw86e9lj0cm49ixpgel1z
Dec  9 08:54:13.048: INFO: Pod node-problem-detector-zf864 requesting resource cpu=11m on Node izgw8hm6kg779yfidwdv8yz
Dec  9 08:54:13.048: INFO: Pod vpn-shoot-57fc54fdbb-6f7gk requesting resource cpu=100m on Node izgw86e9lj0cm49ixpgel1z
Dec  9 08:54:13.048: INFO: Pod dashboard-metrics-scraper-6d54964d4b-tqdzb requesting resource cpu=0m on Node izgw86e9lj0cm49ixpgel1z
Dec  9 08:54:13.048: INFO: Pod kubernetes-dashboard-5d6d5f9c58-vfrn8 requesting resource cpu=50m on Node izgw86e9lj0cm49ixpgel1z
STEP: Starting Pods to consume most of the cluster CPU. 12/09/22 08:54:13.048
Dec  9 08:54:13.048: INFO: Creating a pod which consumes cpu=645m on Node izgw86e9lj0cm49ixpgel1z
Dec  9 08:54:13.065: INFO: Creating a pod which consumes cpu=778m on Node izgw8hm6kg779yfidwdv8yz
Dec  9 08:54:13.080: INFO: Waiting up to 5m0s for pod "filler-pod-909487da-dd7e-4e42-85a7-155c79c4acbe" in namespace "sched-pred-7823" to be "running"
Dec  9 08:54:13.091: INFO: Pod "filler-pod-909487da-dd7e-4e42-85a7-155c79c4acbe": Phase="Pending", Reason="", readiness=false. Elapsed: 10.804095ms
Dec  9 08:54:15.102: INFO: Pod "filler-pod-909487da-dd7e-4e42-85a7-155c79c4acbe": Phase="Running", Reason="", readiness=true. Elapsed: 2.022583856s
Dec  9 08:54:15.103: INFO: Pod "filler-pod-909487da-dd7e-4e42-85a7-155c79c4acbe" satisfied condition "running"
Dec  9 08:54:15.103: INFO: Waiting up to 5m0s for pod "filler-pod-f862ee59-6c05-4ad7-9007-7e8720b4c220" in namespace "sched-pred-7823" to be "running"
Dec  9 08:54:15.114: INFO: Pod "filler-pod-f862ee59-6c05-4ad7-9007-7e8720b4c220": Phase="Running", Reason="", readiness=true. Elapsed: 11.099799ms
Dec  9 08:54:15.114: INFO: Pod "filler-pod-f862ee59-6c05-4ad7-9007-7e8720b4c220" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 12/09/22 08:54:15.114
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-909487da-dd7e-4e42-85a7-155c79c4acbe.172f1411b4315742], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7823/filler-pod-909487da-dd7e-4e42-85a7-155c79c4acbe to izgw86e9lj0cm49ixpgel1z] 12/09/22 08:54:15.126
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-909487da-dd7e-4e42-85a7-155c79c4acbe.172f1411d2039a05], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 12/09/22 08:54:15.126
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-909487da-dd7e-4e42-85a7-155c79c4acbe.172f1411d2d0dc7d], Reason = [Created], Message = [Created container filler-pod-909487da-dd7e-4e42-85a7-155c79c4acbe] 12/09/22 08:54:15.126
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-909487da-dd7e-4e42-85a7-155c79c4acbe.172f1411d6d48a11], Reason = [Started], Message = [Started container filler-pod-909487da-dd7e-4e42-85a7-155c79c4acbe] 12/09/22 08:54:15.126
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f862ee59-6c05-4ad7-9007-7e8720b4c220.172f1411b502b6cc], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7823/filler-pod-f862ee59-6c05-4ad7-9007-7e8720b4c220 to izgw8hm6kg779yfidwdv8yz] 12/09/22 08:54:15.126
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f862ee59-6c05-4ad7-9007-7e8720b4c220.172f1411d4510eb9], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 12/09/22 08:54:15.126
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f862ee59-6c05-4ad7-9007-7e8720b4c220.172f1411d568a810], Reason = [Created], Message = [Created container filler-pod-f862ee59-6c05-4ad7-9007-7e8720b4c220] 12/09/22 08:54:15.126
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f862ee59-6c05-4ad7-9007-7e8720b4c220.172f1411da3b026f], Reason = [Started], Message = [Started container filler-pod-f862ee59-6c05-4ad7-9007-7e8720b4c220] 12/09/22 08:54:15.126
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.172f1412304f1a92], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod.] 12/09/22 08:54:15.155
STEP: removing the label node off the node izgw86e9lj0cm49ixpgel1z 12/09/22 08:54:16.164
STEP: verifying the node doesn't have the label node 12/09/22 08:54:16.199
STEP: removing the label node off the node izgw8hm6kg779yfidwdv8yz 12/09/22 08:54:16.211
STEP: verifying the node doesn't have the label node 12/09/22 08:54:16.239
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Dec  9 08:54:16.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7823" for this suite. 12/09/22 08:54:16.262
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":21,"skipped":6719,"failed":0}
------------------------------
• [3.478 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/09/22 08:54:12.796
    Dec  9 08:54:12.796: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-pred 12/09/22 08:54:12.797
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:54:12.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:54:12.853
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Dec  9 08:54:12.874: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Dec  9 08:54:12.897: INFO: Waiting for terminating namespaces to be deleted...
    Dec  9 08:54:12.908: INFO: 
    Logging pods the apiserver thinks is on node izgw86e9lj0cm49ixpgel1z before test
    Dec  9 08:54:12.926: INFO: addons-nginx-ingress-controller-69b7c7d86f-zhstx from kube-system started at 2022-12-09 08:52:16 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.926: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-8668c9bb59-48td7 from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.926: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: apiserver-proxy-9xnwn from kube-system started at 2022-12-09 07:59:46 +0000 UTC (2 container statuses recorded)
    Dec  9 08:54:12.926: INFO: 	Container proxy ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: 	Container sidecar ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: blackbox-exporter-59447f4c55-7n6c9 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.926: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: blackbox-exporter-59447f4c55-lnx2d from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.926: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: calico-kube-controllers-5f78564887-zgfpt from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.926: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: calico-node-snqzl from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.926: INFO: 	Container calico-node ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: calico-node-vertical-autoscaler-6597dd8998-sw7f8 from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.926: INFO: 	Container autoscaler ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: calico-typha-horizontal-autoscaler-6bb4bc55bc-ld47r from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.926: INFO: 	Container autoscaler ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: calico-typha-vertical-autoscaler-84df655c88-dwsfj from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.926: INFO: 	Container autoscaler ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: coredns-84fdf8dd87-62ckx from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.926: INFO: 	Container coredns ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: coredns-84fdf8dd87-lc5vg from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.926: INFO: 	Container coredns ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: csi-disk-plugin-alicloud-xpt2l from kube-system started at 2022-12-09 07:59:46 +0000 UTC (3 container statuses recorded)
    Dec  9 08:54:12.926: INFO: 	Container csi-diskplugin ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: 	Container driver-registrar ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: egress-filter-applier-xkds7 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.926: INFO: 	Container egress-filter-applier ready: true, restart count 1
    Dec  9 08:54:12.926: INFO: kube-proxy-worker-1-v1.25.4-nw6q5 from kube-system started at 2022-12-09 08:32:11 +0000 UTC (2 container statuses recorded)
    Dec  9 08:54:12.926: INFO: 	Container conntrack-fix ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: metrics-server-7bcfb6df5f-gp4jx from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.926: INFO: 	Container metrics-server ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: metrics-server-7bcfb6df5f-pjfn4 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.926: INFO: 	Container metrics-server ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: network-problem-detector-host-4jvll from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.926: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: network-problem-detector-pod-bdbrf from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.926: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: node-exporter-8v484 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.926: INFO: 	Container node-exporter ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: node-local-dns-hrgm8 from kube-system started at 2022-12-09 08:15:12 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.926: INFO: 	Container node-cache ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: node-problem-detector-fvdnw from kube-system started at 2022-12-09 08:20:11 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.926: INFO: 	Container node-problem-detector ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: vpn-shoot-57fc54fdbb-6f7gk from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.926: INFO: 	Container vpn-shoot ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: dashboard-metrics-scraper-6d54964d4b-tqdzb from kubernetes-dashboard started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.926: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: kubernetes-dashboard-5d6d5f9c58-vfrn8 from kubernetes-dashboard started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.926: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Dec  9 08:54:12.926: INFO: 
    Logging pods the apiserver thinks is on node izgw8hm6kg779yfidwdv8yz before test
    Dec  9 08:54:12.951: INFO: apiserver-proxy-nzwxs from kube-system started at 2022-12-09 07:59:48 +0000 UTC (2 container statuses recorded)
    Dec  9 08:54:12.951: INFO: 	Container proxy ready: true, restart count 0
    Dec  9 08:54:12.951: INFO: 	Container sidecar ready: true, restart count 0
    Dec  9 08:54:12.951: INFO: calico-node-pbrgw from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.951: INFO: 	Container calico-node ready: true, restart count 0
    Dec  9 08:54:12.951: INFO: calico-typha-deploy-65c54d4db6-mh94l from kube-system started at 2022-12-09 08:02:30 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.951: INFO: 	Container calico-typha ready: true, restart count 0
    Dec  9 08:54:12.951: INFO: csi-disk-plugin-alicloud-tcf5b from kube-system started at 2022-12-09 07:59:48 +0000 UTC (3 container statuses recorded)
    Dec  9 08:54:12.951: INFO: 	Container csi-diskplugin ready: true, restart count 0
    Dec  9 08:54:12.951: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Dec  9 08:54:12.951: INFO: 	Container driver-registrar ready: true, restart count 0
    Dec  9 08:54:12.951: INFO: egress-filter-applier-d9bt4 from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.951: INFO: 	Container egress-filter-applier ready: true, restart count 1
    Dec  9 08:54:12.951: INFO: kube-proxy-worker-1-v1.25.4-grfnm from kube-system started at 2022-12-09 08:32:11 +0000 UTC (2 container statuses recorded)
    Dec  9 08:54:12.951: INFO: 	Container conntrack-fix ready: true, restart count 0
    Dec  9 08:54:12.951: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec  9 08:54:12.951: INFO: network-problem-detector-host-wnxb2 from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.951: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Dec  9 08:54:12.951: INFO: network-problem-detector-pod-ntl2d from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.951: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Dec  9 08:54:12.951: INFO: node-exporter-zcl88 from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.951: INFO: 	Container node-exporter ready: true, restart count 0
    Dec  9 08:54:12.951: INFO: node-local-dns-xhltp from kube-system started at 2022-12-09 08:15:12 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.951: INFO: 	Container node-cache ready: true, restart count 0
    Dec  9 08:54:12.951: INFO: node-problem-detector-zf864 from kube-system started at 2022-12-09 08:20:11 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:12.951: INFO: 	Container node-problem-detector ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node izgw86e9lj0cm49ixpgel1z 12/09/22 08:54:12.987
    STEP: verifying the node has the label node izgw8hm6kg779yfidwdv8yz 12/09/22 08:54:13.018
    Dec  9 08:54:13.047: INFO: Pod addons-nginx-ingress-controller-69b7c7d86f-zhstx requesting resource cpu=100m on Node izgw86e9lj0cm49ixpgel1z
    Dec  9 08:54:13.047: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-8668c9bb59-48td7 requesting resource cpu=0m on Node izgw86e9lj0cm49ixpgel1z
    Dec  9 08:54:13.047: INFO: Pod apiserver-proxy-9xnwn requesting resource cpu=40m on Node izgw86e9lj0cm49ixpgel1z
    Dec  9 08:54:13.047: INFO: Pod apiserver-proxy-nzwxs requesting resource cpu=40m on Node izgw8hm6kg779yfidwdv8yz
    Dec  9 08:54:13.047: INFO: Pod blackbox-exporter-59447f4c55-7n6c9 requesting resource cpu=10m on Node izgw86e9lj0cm49ixpgel1z
    Dec  9 08:54:13.047: INFO: Pod blackbox-exporter-59447f4c55-lnx2d requesting resource cpu=10m on Node izgw86e9lj0cm49ixpgel1z
    Dec  9 08:54:13.047: INFO: Pod calico-kube-controllers-5f78564887-zgfpt requesting resource cpu=10m on Node izgw86e9lj0cm49ixpgel1z
    Dec  9 08:54:13.047: INFO: Pod calico-node-pbrgw requesting resource cpu=250m on Node izgw8hm6kg779yfidwdv8yz
    Dec  9 08:54:13.048: INFO: Pod calico-node-snqzl requesting resource cpu=250m on Node izgw86e9lj0cm49ixpgel1z
    Dec  9 08:54:13.048: INFO: Pod calico-node-vertical-autoscaler-6597dd8998-sw7f8 requesting resource cpu=10m on Node izgw86e9lj0cm49ixpgel1z
    Dec  9 08:54:13.048: INFO: Pod calico-typha-deploy-65c54d4db6-mh94l requesting resource cpu=320m on Node izgw8hm6kg779yfidwdv8yz
    Dec  9 08:54:13.048: INFO: Pod calico-typha-horizontal-autoscaler-6bb4bc55bc-ld47r requesting resource cpu=10m on Node izgw86e9lj0cm49ixpgel1z
    Dec  9 08:54:13.048: INFO: Pod calico-typha-vertical-autoscaler-84df655c88-dwsfj requesting resource cpu=10m on Node izgw86e9lj0cm49ixpgel1z
    Dec  9 08:54:13.048: INFO: Pod coredns-84fdf8dd87-62ckx requesting resource cpu=50m on Node izgw86e9lj0cm49ixpgel1z
    Dec  9 08:54:13.048: INFO: Pod coredns-84fdf8dd87-lc5vg requesting resource cpu=50m on Node izgw86e9lj0cm49ixpgel1z
    Dec  9 08:54:13.048: INFO: Pod csi-disk-plugin-alicloud-tcf5b requesting resource cpu=34m on Node izgw8hm6kg779yfidwdv8yz
    Dec  9 08:54:13.048: INFO: Pod csi-disk-plugin-alicloud-xpt2l requesting resource cpu=34m on Node izgw86e9lj0cm49ixpgel1z
    Dec  9 08:54:13.048: INFO: Pod egress-filter-applier-d9bt4 requesting resource cpu=50m on Node izgw8hm6kg779yfidwdv8yz
    Dec  9 08:54:13.048: INFO: Pod egress-filter-applier-xkds7 requesting resource cpu=50m on Node izgw86e9lj0cm49ixpgel1z
    Dec  9 08:54:13.048: INFO: Pod kube-proxy-worker-1-v1.25.4-grfnm requesting resource cpu=22m on Node izgw8hm6kg779yfidwdv8yz
    Dec  9 08:54:13.048: INFO: Pod kube-proxy-worker-1-v1.25.4-nw6q5 requesting resource cpu=22m on Node izgw86e9lj0cm49ixpgel1z
    Dec  9 08:54:13.048: INFO: Pod metrics-server-7bcfb6df5f-gp4jx requesting resource cpu=50m on Node izgw86e9lj0cm49ixpgel1z
    Dec  9 08:54:13.048: INFO: Pod metrics-server-7bcfb6df5f-pjfn4 requesting resource cpu=50m on Node izgw86e9lj0cm49ixpgel1z
    Dec  9 08:54:13.048: INFO: Pod network-problem-detector-host-4jvll requesting resource cpu=10m on Node izgw86e9lj0cm49ixpgel1z
    Dec  9 08:54:13.048: INFO: Pod network-problem-detector-host-wnxb2 requesting resource cpu=10m on Node izgw8hm6kg779yfidwdv8yz
    Dec  9 08:54:13.048: INFO: Pod network-problem-detector-pod-bdbrf requesting resource cpu=10m on Node izgw86e9lj0cm49ixpgel1z
    Dec  9 08:54:13.048: INFO: Pod network-problem-detector-pod-ntl2d requesting resource cpu=10m on Node izgw8hm6kg779yfidwdv8yz
    Dec  9 08:54:13.048: INFO: Pod node-exporter-8v484 requesting resource cpu=50m on Node izgw86e9lj0cm49ixpgel1z
    Dec  9 08:54:13.048: INFO: Pod node-exporter-zcl88 requesting resource cpu=50m on Node izgw8hm6kg779yfidwdv8yz
    Dec  9 08:54:13.048: INFO: Pod node-local-dns-hrgm8 requesting resource cpu=11m on Node izgw86e9lj0cm49ixpgel1z
    Dec  9 08:54:13.048: INFO: Pod node-local-dns-xhltp requesting resource cpu=11m on Node izgw8hm6kg779yfidwdv8yz
    Dec  9 08:54:13.048: INFO: Pod node-problem-detector-fvdnw requesting resource cpu=11m on Node izgw86e9lj0cm49ixpgel1z
    Dec  9 08:54:13.048: INFO: Pod node-problem-detector-zf864 requesting resource cpu=11m on Node izgw8hm6kg779yfidwdv8yz
    Dec  9 08:54:13.048: INFO: Pod vpn-shoot-57fc54fdbb-6f7gk requesting resource cpu=100m on Node izgw86e9lj0cm49ixpgel1z
    Dec  9 08:54:13.048: INFO: Pod dashboard-metrics-scraper-6d54964d4b-tqdzb requesting resource cpu=0m on Node izgw86e9lj0cm49ixpgel1z
    Dec  9 08:54:13.048: INFO: Pod kubernetes-dashboard-5d6d5f9c58-vfrn8 requesting resource cpu=50m on Node izgw86e9lj0cm49ixpgel1z
    STEP: Starting Pods to consume most of the cluster CPU. 12/09/22 08:54:13.048
    Dec  9 08:54:13.048: INFO: Creating a pod which consumes cpu=645m on Node izgw86e9lj0cm49ixpgel1z
    Dec  9 08:54:13.065: INFO: Creating a pod which consumes cpu=778m on Node izgw8hm6kg779yfidwdv8yz
    Dec  9 08:54:13.080: INFO: Waiting up to 5m0s for pod "filler-pod-909487da-dd7e-4e42-85a7-155c79c4acbe" in namespace "sched-pred-7823" to be "running"
    Dec  9 08:54:13.091: INFO: Pod "filler-pod-909487da-dd7e-4e42-85a7-155c79c4acbe": Phase="Pending", Reason="", readiness=false. Elapsed: 10.804095ms
    Dec  9 08:54:15.102: INFO: Pod "filler-pod-909487da-dd7e-4e42-85a7-155c79c4acbe": Phase="Running", Reason="", readiness=true. Elapsed: 2.022583856s
    Dec  9 08:54:15.103: INFO: Pod "filler-pod-909487da-dd7e-4e42-85a7-155c79c4acbe" satisfied condition "running"
    Dec  9 08:54:15.103: INFO: Waiting up to 5m0s for pod "filler-pod-f862ee59-6c05-4ad7-9007-7e8720b4c220" in namespace "sched-pred-7823" to be "running"
    Dec  9 08:54:15.114: INFO: Pod "filler-pod-f862ee59-6c05-4ad7-9007-7e8720b4c220": Phase="Running", Reason="", readiness=true. Elapsed: 11.099799ms
    Dec  9 08:54:15.114: INFO: Pod "filler-pod-f862ee59-6c05-4ad7-9007-7e8720b4c220" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 12/09/22 08:54:15.114
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-909487da-dd7e-4e42-85a7-155c79c4acbe.172f1411b4315742], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7823/filler-pod-909487da-dd7e-4e42-85a7-155c79c4acbe to izgw86e9lj0cm49ixpgel1z] 12/09/22 08:54:15.126
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-909487da-dd7e-4e42-85a7-155c79c4acbe.172f1411d2039a05], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 12/09/22 08:54:15.126
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-909487da-dd7e-4e42-85a7-155c79c4acbe.172f1411d2d0dc7d], Reason = [Created], Message = [Created container filler-pod-909487da-dd7e-4e42-85a7-155c79c4acbe] 12/09/22 08:54:15.126
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-909487da-dd7e-4e42-85a7-155c79c4acbe.172f1411d6d48a11], Reason = [Started], Message = [Started container filler-pod-909487da-dd7e-4e42-85a7-155c79c4acbe] 12/09/22 08:54:15.126
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f862ee59-6c05-4ad7-9007-7e8720b4c220.172f1411b502b6cc], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7823/filler-pod-f862ee59-6c05-4ad7-9007-7e8720b4c220 to izgw8hm6kg779yfidwdv8yz] 12/09/22 08:54:15.126
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f862ee59-6c05-4ad7-9007-7e8720b4c220.172f1411d4510eb9], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 12/09/22 08:54:15.126
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f862ee59-6c05-4ad7-9007-7e8720b4c220.172f1411d568a810], Reason = [Created], Message = [Created container filler-pod-f862ee59-6c05-4ad7-9007-7e8720b4c220] 12/09/22 08:54:15.126
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f862ee59-6c05-4ad7-9007-7e8720b4c220.172f1411da3b026f], Reason = [Started], Message = [Started container filler-pod-f862ee59-6c05-4ad7-9007-7e8720b4c220] 12/09/22 08:54:15.126
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.172f1412304f1a92], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod.] 12/09/22 08:54:15.155
    STEP: removing the label node off the node izgw86e9lj0cm49ixpgel1z 12/09/22 08:54:16.164
    STEP: verifying the node doesn't have the label node 12/09/22 08:54:16.199
    STEP: removing the label node off the node izgw8hm6kg779yfidwdv8yz 12/09/22 08:54:16.211
    STEP: verifying the node doesn't have the label node 12/09/22 08:54:16.239
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Dec  9 08:54:16.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-7823" for this suite. 12/09/22 08:54:16.262
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/09/22 08:54:16.275
Dec  9 08:54:16.275: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred 12/09/22 08:54:16.276
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:54:16.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:54:16.333
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Dec  9 08:54:16.353: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  9 08:54:16.377: INFO: Waiting for terminating namespaces to be deleted...
Dec  9 08:54:16.395: INFO: 
Logging pods the apiserver thinks is on node izgw86e9lj0cm49ixpgel1z before test
Dec  9 08:54:16.414: INFO: addons-nginx-ingress-controller-69b7c7d86f-zhstx from kube-system started at 2022-12-09 08:52:16 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.414: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  9 08:54:16.414: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-8668c9bb59-48td7 from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.414: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Dec  9 08:54:16.414: INFO: apiserver-proxy-9xnwn from kube-system started at 2022-12-09 07:59:46 +0000 UTC (2 container statuses recorded)
Dec  9 08:54:16.414: INFO: 	Container proxy ready: true, restart count 0
Dec  9 08:54:16.414: INFO: 	Container sidecar ready: true, restart count 0
Dec  9 08:54:16.414: INFO: blackbox-exporter-59447f4c55-7n6c9 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.415: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec  9 08:54:16.415: INFO: blackbox-exporter-59447f4c55-lnx2d from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.415: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec  9 08:54:16.415: INFO: calico-kube-controllers-5f78564887-zgfpt from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.415: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec  9 08:54:16.415: INFO: calico-node-snqzl from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.415: INFO: 	Container calico-node ready: true, restart count 0
Dec  9 08:54:16.415: INFO: calico-node-vertical-autoscaler-6597dd8998-sw7f8 from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.415: INFO: 	Container autoscaler ready: true, restart count 0
Dec  9 08:54:16.415: INFO: calico-typha-horizontal-autoscaler-6bb4bc55bc-ld47r from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.417: INFO: 	Container autoscaler ready: true, restart count 0
Dec  9 08:54:16.417: INFO: calico-typha-vertical-autoscaler-84df655c88-dwsfj from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.417: INFO: 	Container autoscaler ready: true, restart count 0
Dec  9 08:54:16.417: INFO: coredns-84fdf8dd87-62ckx from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.417: INFO: 	Container coredns ready: true, restart count 0
Dec  9 08:54:16.417: INFO: coredns-84fdf8dd87-lc5vg from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.417: INFO: 	Container coredns ready: true, restart count 0
Dec  9 08:54:16.417: INFO: csi-disk-plugin-alicloud-xpt2l from kube-system started at 2022-12-09 07:59:46 +0000 UTC (3 container statuses recorded)
Dec  9 08:54:16.417: INFO: 	Container csi-diskplugin ready: true, restart count 0
Dec  9 08:54:16.417: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec  9 08:54:16.417: INFO: 	Container driver-registrar ready: true, restart count 0
Dec  9 08:54:16.417: INFO: egress-filter-applier-xkds7 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.417: INFO: 	Container egress-filter-applier ready: true, restart count 1
Dec  9 08:54:16.417: INFO: kube-proxy-worker-1-v1.25.4-nw6q5 from kube-system started at 2022-12-09 08:32:11 +0000 UTC (2 container statuses recorded)
Dec  9 08:54:16.417: INFO: 	Container conntrack-fix ready: true, restart count 0
Dec  9 08:54:16.417: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  9 08:54:16.417: INFO: metrics-server-7bcfb6df5f-gp4jx from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.417: INFO: 	Container metrics-server ready: true, restart count 0
Dec  9 08:54:16.417: INFO: metrics-server-7bcfb6df5f-pjfn4 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.417: INFO: 	Container metrics-server ready: true, restart count 0
Dec  9 08:54:16.417: INFO: network-problem-detector-host-4jvll from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.417: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Dec  9 08:54:16.417: INFO: network-problem-detector-pod-bdbrf from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.417: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Dec  9 08:54:16.417: INFO: node-exporter-8v484 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.417: INFO: 	Container node-exporter ready: true, restart count 0
Dec  9 08:54:16.417: INFO: node-local-dns-hrgm8 from kube-system started at 2022-12-09 08:15:12 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.417: INFO: 	Container node-cache ready: true, restart count 0
Dec  9 08:54:16.417: INFO: node-problem-detector-fvdnw from kube-system started at 2022-12-09 08:20:11 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.417: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  9 08:54:16.417: INFO: vpn-shoot-57fc54fdbb-6f7gk from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.417: INFO: 	Container vpn-shoot ready: true, restart count 0
Dec  9 08:54:16.417: INFO: dashboard-metrics-scraper-6d54964d4b-tqdzb from kubernetes-dashboard started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.417: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Dec  9 08:54:16.417: INFO: kubernetes-dashboard-5d6d5f9c58-vfrn8 from kubernetes-dashboard started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.417: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  9 08:54:16.417: INFO: filler-pod-909487da-dd7e-4e42-85a7-155c79c4acbe from sched-pred-7823 started at 2022-12-09 08:54:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.417: INFO: 	Container filler-pod-909487da-dd7e-4e42-85a7-155c79c4acbe ready: true, restart count 0
Dec  9 08:54:16.417: INFO: 
Logging pods the apiserver thinks is on node izgw8hm6kg779yfidwdv8yz before test
Dec  9 08:54:16.441: INFO: apiserver-proxy-nzwxs from kube-system started at 2022-12-09 07:59:48 +0000 UTC (2 container statuses recorded)
Dec  9 08:54:16.441: INFO: 	Container proxy ready: true, restart count 0
Dec  9 08:54:16.441: INFO: 	Container sidecar ready: true, restart count 0
Dec  9 08:54:16.441: INFO: calico-node-pbrgw from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.441: INFO: 	Container calico-node ready: true, restart count 0
Dec  9 08:54:16.441: INFO: calico-typha-deploy-65c54d4db6-mh94l from kube-system started at 2022-12-09 08:02:30 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.441: INFO: 	Container calico-typha ready: true, restart count 0
Dec  9 08:54:16.441: INFO: csi-disk-plugin-alicloud-tcf5b from kube-system started at 2022-12-09 07:59:48 +0000 UTC (3 container statuses recorded)
Dec  9 08:54:16.441: INFO: 	Container csi-diskplugin ready: true, restart count 0
Dec  9 08:54:16.441: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec  9 08:54:16.441: INFO: 	Container driver-registrar ready: true, restart count 0
Dec  9 08:54:16.441: INFO: egress-filter-applier-d9bt4 from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.441: INFO: 	Container egress-filter-applier ready: true, restart count 1
Dec  9 08:54:16.441: INFO: kube-proxy-worker-1-v1.25.4-grfnm from kube-system started at 2022-12-09 08:32:11 +0000 UTC (2 container statuses recorded)
Dec  9 08:54:16.441: INFO: 	Container conntrack-fix ready: true, restart count 0
Dec  9 08:54:16.441: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  9 08:54:16.441: INFO: network-problem-detector-host-wnxb2 from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.441: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Dec  9 08:54:16.441: INFO: network-problem-detector-pod-ntl2d from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.441: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Dec  9 08:54:16.441: INFO: node-exporter-zcl88 from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.441: INFO: 	Container node-exporter ready: true, restart count 0
Dec  9 08:54:16.441: INFO: node-local-dns-xhltp from kube-system started at 2022-12-09 08:15:12 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.441: INFO: 	Container node-cache ready: true, restart count 0
Dec  9 08:54:16.441: INFO: node-problem-detector-zf864 from kube-system started at 2022-12-09 08:20:11 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.441: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  9 08:54:16.441: INFO: filler-pod-f862ee59-6c05-4ad7-9007-7e8720b4c220 from sched-pred-7823 started at 2022-12-09 08:54:13 +0000 UTC (1 container statuses recorded)
Dec  9 08:54:16.441: INFO: 	Container filler-pod-f862ee59-6c05-4ad7-9007-7e8720b4c220 ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 12/09/22 08:54:16.441
Dec  9 08:54:16.459: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6959" to be "running"
Dec  9 08:54:16.470: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 10.875666ms
Dec  9 08:54:18.482: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.022830466s
Dec  9 08:54:18.482: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 12/09/22 08:54:18.493
STEP: Trying to apply a random label on the found node. 12/09/22 08:54:18.524
STEP: verifying the node has the label kubernetes.io/e2e-c39d8a45-fcae-45dd-923e-ad4adbb6ffe0 95 12/09/22 08:54:18.542
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 12/09/22 08:54:18.554
Dec  9 08:54:18.569: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-6959" to be "not pending"
Dec  9 08:54:18.585: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.247561ms
Dec  9 08:54:20.597: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.028473396s
Dec  9 08:54:20.598: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.250.13.218 on the node which pod4 resides and expect not scheduled 12/09/22 08:54:20.598
Dec  9 08:54:20.612: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-6959" to be "not pending"
Dec  9 08:54:20.623: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.955034ms
Dec  9 08:54:22.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023367691s
Dec  9 08:54:24.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022822392s
Dec  9 08:54:26.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022442511s
Dec  9 08:54:28.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02249889s
Dec  9 08:54:30.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.023861264s
Dec  9 08:54:32.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.024314446s
Dec  9 08:54:34.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.02260164s
Dec  9 08:54:36.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.023124384s
Dec  9 08:54:38.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.022246374s
Dec  9 08:54:40.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.022623395s
Dec  9 08:54:42.639: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.02689771s
Dec  9 08:54:44.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.022954823s
Dec  9 08:54:46.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.023635979s
Dec  9 08:54:48.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.023317175s
Dec  9 08:54:50.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.023866623s
Dec  9 08:54:52.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.024127262s
Dec  9 08:54:54.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.022867001s
Dec  9 08:54:56.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.023986541s
Dec  9 08:54:58.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.023149522s
Dec  9 08:55:00.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.025040703s
Dec  9 08:55:02.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.024145871s
Dec  9 08:55:04.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.022784714s
Dec  9 08:55:06.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.022793811s
Dec  9 08:55:08.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.022796854s
Dec  9 08:55:10.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.023783979s
Dec  9 08:55:12.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.022729834s
Dec  9 08:55:14.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.023196225s
Dec  9 08:55:16.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.023162238s
Dec  9 08:55:18.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.02220058s
Dec  9 08:55:20.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.023082505s
Dec  9 08:55:22.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.024482074s
Dec  9 08:55:24.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.022388275s
Dec  9 08:55:26.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.022796841s
Dec  9 08:55:28.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.023591155s
Dec  9 08:55:30.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.024170589s
Dec  9 08:55:32.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.022568573s
Dec  9 08:55:34.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.023276131s
Dec  9 08:55:36.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.022749686s
Dec  9 08:55:38.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.023469331s
Dec  9 08:55:40.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.023649407s
Dec  9 08:55:42.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.025005675s
Dec  9 08:55:44.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.023365216s
Dec  9 08:55:46.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.022721852s
Dec  9 08:55:48.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.023160253s
Dec  9 08:55:50.641: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.028999308s
Dec  9 08:55:52.638: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.025992775s
Dec  9 08:55:54.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.024435391s
Dec  9 08:55:56.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.023502938s
Dec  9 08:55:58.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.022577494s
Dec  9 08:56:00.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.023627658s
Dec  9 08:56:02.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.023627333s
Dec  9 08:56:04.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.023637767s
Dec  9 08:56:06.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.022928083s
Dec  9 08:56:08.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.023512401s
Dec  9 08:56:10.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.023630582s
Dec  9 08:56:12.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.023991103s
Dec  9 08:56:14.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.023235364s
Dec  9 08:56:16.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.022412891s
Dec  9 08:56:18.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.022350395s
Dec  9 08:56:20.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.024181933s
Dec  9 08:56:22.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.022846739s
Dec  9 08:56:24.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.023415566s
Dec  9 08:56:26.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.0243188s
Dec  9 08:56:28.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.022246382s
Dec  9 08:56:30.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.023436249s
Dec  9 08:56:32.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.024599988s
Dec  9 08:56:34.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.022891499s
Dec  9 08:56:36.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.023246528s
Dec  9 08:56:38.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.022828054s
Dec  9 08:56:40.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.02403309s
Dec  9 08:56:42.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.022784566s
Dec  9 08:56:44.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.023092046s
Dec  9 08:56:46.638: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.025401046s
Dec  9 08:56:48.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.022381882s
Dec  9 08:56:50.641: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.028849887s
Dec  9 08:56:52.644: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.03119338s
Dec  9 08:56:54.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.022556083s
Dec  9 08:56:56.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.023654284s
Dec  9 08:56:58.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.022706073s
Dec  9 08:57:00.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.023872913s
Dec  9 08:57:02.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.023968731s
Dec  9 08:57:04.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.023466237s
Dec  9 08:57:06.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.02244768s
Dec  9 08:57:08.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.022565839s
Dec  9 08:57:10.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.023901028s
Dec  9 08:57:12.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.022821138s
Dec  9 08:57:14.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.022518567s
Dec  9 08:57:16.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.023911258s
Dec  9 08:57:18.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.02347993s
Dec  9 08:57:20.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.022925266s
Dec  9 08:57:22.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.022547406s
Dec  9 08:57:24.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.023542695s
Dec  9 08:57:26.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.023577063s
Dec  9 08:57:28.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.023171075s
Dec  9 08:57:30.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.0245173s
Dec  9 08:57:32.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.024034966s
Dec  9 08:57:34.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.023231832s
Dec  9 08:57:36.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.023192261s
Dec  9 08:57:38.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.022500837s
Dec  9 08:57:40.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.022901406s
Dec  9 08:57:42.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.024488469s
Dec  9 08:57:44.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.023901447s
Dec  9 08:57:46.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.024457294s
Dec  9 08:57:48.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.022657224s
Dec  9 08:57:50.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.023173885s
Dec  9 08:57:52.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.022507155s
Dec  9 08:57:54.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.022962248s
Dec  9 08:57:56.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.022598321s
Dec  9 08:57:58.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.022576411s
Dec  9 08:58:00.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.023650951s
Dec  9 08:58:02.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.023825189s
Dec  9 08:58:04.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.023612968s
Dec  9 08:58:06.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.023983786s
Dec  9 08:58:08.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.022513435s
Dec  9 08:58:10.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.023650149s
Dec  9 08:58:12.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.02318819s
Dec  9 08:58:14.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.022399739s
Dec  9 08:58:16.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.023638236s
Dec  9 08:58:18.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.023239776s
Dec  9 08:58:20.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.023774196s
Dec  9 08:58:22.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.023963577s
Dec  9 08:58:24.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.024893613s
Dec  9 08:58:26.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.023108658s
Dec  9 08:58:28.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.022360805s
Dec  9 08:58:30.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.023290001s
Dec  9 08:58:32.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.022522136s
Dec  9 08:58:34.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.022844068s
Dec  9 08:58:36.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.02280289s
Dec  9 08:58:38.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.023320136s
Dec  9 08:58:40.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.023616777s
Dec  9 08:58:42.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.022808791s
Dec  9 08:58:44.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.023713684s
Dec  9 08:58:46.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.024194552s
Dec  9 08:58:48.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.022623318s
Dec  9 08:58:50.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.023754155s
Dec  9 08:58:52.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.02334676s
Dec  9 08:58:54.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.022733431s
Dec  9 08:58:56.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.023243231s
Dec  9 08:58:58.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.022293867s
Dec  9 08:59:00.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.022986837s
Dec  9 08:59:02.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.023168176s
Dec  9 08:59:04.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.022771045s
Dec  9 08:59:06.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.024419727s
Dec  9 08:59:08.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.022508916s
Dec  9 08:59:10.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.023666193s
Dec  9 08:59:12.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.023697028s
Dec  9 08:59:14.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.023291681s
Dec  9 08:59:16.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.02294203s
Dec  9 08:59:18.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.023612064s
Dec  9 08:59:20.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.023980096s
Dec  9 08:59:20.648: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.035259414s
STEP: removing the label kubernetes.io/e2e-c39d8a45-fcae-45dd-923e-ad4adbb6ffe0 off the node izgw8hm6kg779yfidwdv8yz 12/09/22 08:59:20.648
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c39d8a45-fcae-45dd-923e-ad4adbb6ffe0 12/09/22 08:59:20.685
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Dec  9 08:59:20.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6959" for this suite. 12/09/22 08:59:20.709
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":22,"skipped":6771,"failed":0}
------------------------------
• [SLOW TEST] [304.446 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/09/22 08:54:16.275
    Dec  9 08:54:16.275: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-pred 12/09/22 08:54:16.276
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:54:16.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:54:16.333
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Dec  9 08:54:16.353: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Dec  9 08:54:16.377: INFO: Waiting for terminating namespaces to be deleted...
    Dec  9 08:54:16.395: INFO: 
    Logging pods the apiserver thinks is on node izgw86e9lj0cm49ixpgel1z before test
    Dec  9 08:54:16.414: INFO: addons-nginx-ingress-controller-69b7c7d86f-zhstx from kube-system started at 2022-12-09 08:52:16 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.414: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Dec  9 08:54:16.414: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-8668c9bb59-48td7 from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.414: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
    Dec  9 08:54:16.414: INFO: apiserver-proxy-9xnwn from kube-system started at 2022-12-09 07:59:46 +0000 UTC (2 container statuses recorded)
    Dec  9 08:54:16.414: INFO: 	Container proxy ready: true, restart count 0
    Dec  9 08:54:16.414: INFO: 	Container sidecar ready: true, restart count 0
    Dec  9 08:54:16.414: INFO: blackbox-exporter-59447f4c55-7n6c9 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.415: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Dec  9 08:54:16.415: INFO: blackbox-exporter-59447f4c55-lnx2d from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.415: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Dec  9 08:54:16.415: INFO: calico-kube-controllers-5f78564887-zgfpt from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.415: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Dec  9 08:54:16.415: INFO: calico-node-snqzl from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.415: INFO: 	Container calico-node ready: true, restart count 0
    Dec  9 08:54:16.415: INFO: calico-node-vertical-autoscaler-6597dd8998-sw7f8 from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.415: INFO: 	Container autoscaler ready: true, restart count 0
    Dec  9 08:54:16.415: INFO: calico-typha-horizontal-autoscaler-6bb4bc55bc-ld47r from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.417: INFO: 	Container autoscaler ready: true, restart count 0
    Dec  9 08:54:16.417: INFO: calico-typha-vertical-autoscaler-84df655c88-dwsfj from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.417: INFO: 	Container autoscaler ready: true, restart count 0
    Dec  9 08:54:16.417: INFO: coredns-84fdf8dd87-62ckx from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.417: INFO: 	Container coredns ready: true, restart count 0
    Dec  9 08:54:16.417: INFO: coredns-84fdf8dd87-lc5vg from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.417: INFO: 	Container coredns ready: true, restart count 0
    Dec  9 08:54:16.417: INFO: csi-disk-plugin-alicloud-xpt2l from kube-system started at 2022-12-09 07:59:46 +0000 UTC (3 container statuses recorded)
    Dec  9 08:54:16.417: INFO: 	Container csi-diskplugin ready: true, restart count 0
    Dec  9 08:54:16.417: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Dec  9 08:54:16.417: INFO: 	Container driver-registrar ready: true, restart count 0
    Dec  9 08:54:16.417: INFO: egress-filter-applier-xkds7 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.417: INFO: 	Container egress-filter-applier ready: true, restart count 1
    Dec  9 08:54:16.417: INFO: kube-proxy-worker-1-v1.25.4-nw6q5 from kube-system started at 2022-12-09 08:32:11 +0000 UTC (2 container statuses recorded)
    Dec  9 08:54:16.417: INFO: 	Container conntrack-fix ready: true, restart count 0
    Dec  9 08:54:16.417: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec  9 08:54:16.417: INFO: metrics-server-7bcfb6df5f-gp4jx from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.417: INFO: 	Container metrics-server ready: true, restart count 0
    Dec  9 08:54:16.417: INFO: metrics-server-7bcfb6df5f-pjfn4 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.417: INFO: 	Container metrics-server ready: true, restart count 0
    Dec  9 08:54:16.417: INFO: network-problem-detector-host-4jvll from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.417: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Dec  9 08:54:16.417: INFO: network-problem-detector-pod-bdbrf from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.417: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Dec  9 08:54:16.417: INFO: node-exporter-8v484 from kube-system started at 2022-12-09 07:59:46 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.417: INFO: 	Container node-exporter ready: true, restart count 0
    Dec  9 08:54:16.417: INFO: node-local-dns-hrgm8 from kube-system started at 2022-12-09 08:15:12 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.417: INFO: 	Container node-cache ready: true, restart count 0
    Dec  9 08:54:16.417: INFO: node-problem-detector-fvdnw from kube-system started at 2022-12-09 08:20:11 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.417: INFO: 	Container node-problem-detector ready: true, restart count 0
    Dec  9 08:54:16.417: INFO: vpn-shoot-57fc54fdbb-6f7gk from kube-system started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.417: INFO: 	Container vpn-shoot ready: true, restart count 0
    Dec  9 08:54:16.417: INFO: dashboard-metrics-scraper-6d54964d4b-tqdzb from kubernetes-dashboard started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.417: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Dec  9 08:54:16.417: INFO: kubernetes-dashboard-5d6d5f9c58-vfrn8 from kubernetes-dashboard started at 2022-12-09 08:00:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.417: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Dec  9 08:54:16.417: INFO: filler-pod-909487da-dd7e-4e42-85a7-155c79c4acbe from sched-pred-7823 started at 2022-12-09 08:54:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.417: INFO: 	Container filler-pod-909487da-dd7e-4e42-85a7-155c79c4acbe ready: true, restart count 0
    Dec  9 08:54:16.417: INFO: 
    Logging pods the apiserver thinks is on node izgw8hm6kg779yfidwdv8yz before test
    Dec  9 08:54:16.441: INFO: apiserver-proxy-nzwxs from kube-system started at 2022-12-09 07:59:48 +0000 UTC (2 container statuses recorded)
    Dec  9 08:54:16.441: INFO: 	Container proxy ready: true, restart count 0
    Dec  9 08:54:16.441: INFO: 	Container sidecar ready: true, restart count 0
    Dec  9 08:54:16.441: INFO: calico-node-pbrgw from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.441: INFO: 	Container calico-node ready: true, restart count 0
    Dec  9 08:54:16.441: INFO: calico-typha-deploy-65c54d4db6-mh94l from kube-system started at 2022-12-09 08:02:30 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.441: INFO: 	Container calico-typha ready: true, restart count 0
    Dec  9 08:54:16.441: INFO: csi-disk-plugin-alicloud-tcf5b from kube-system started at 2022-12-09 07:59:48 +0000 UTC (3 container statuses recorded)
    Dec  9 08:54:16.441: INFO: 	Container csi-diskplugin ready: true, restart count 0
    Dec  9 08:54:16.441: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Dec  9 08:54:16.441: INFO: 	Container driver-registrar ready: true, restart count 0
    Dec  9 08:54:16.441: INFO: egress-filter-applier-d9bt4 from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.441: INFO: 	Container egress-filter-applier ready: true, restart count 1
    Dec  9 08:54:16.441: INFO: kube-proxy-worker-1-v1.25.4-grfnm from kube-system started at 2022-12-09 08:32:11 +0000 UTC (2 container statuses recorded)
    Dec  9 08:54:16.441: INFO: 	Container conntrack-fix ready: true, restart count 0
    Dec  9 08:54:16.441: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec  9 08:54:16.441: INFO: network-problem-detector-host-wnxb2 from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.441: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Dec  9 08:54:16.441: INFO: network-problem-detector-pod-ntl2d from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.441: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Dec  9 08:54:16.441: INFO: node-exporter-zcl88 from kube-system started at 2022-12-09 07:59:48 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.441: INFO: 	Container node-exporter ready: true, restart count 0
    Dec  9 08:54:16.441: INFO: node-local-dns-xhltp from kube-system started at 2022-12-09 08:15:12 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.441: INFO: 	Container node-cache ready: true, restart count 0
    Dec  9 08:54:16.441: INFO: node-problem-detector-zf864 from kube-system started at 2022-12-09 08:20:11 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.441: INFO: 	Container node-problem-detector ready: true, restart count 0
    Dec  9 08:54:16.441: INFO: filler-pod-f862ee59-6c05-4ad7-9007-7e8720b4c220 from sched-pred-7823 started at 2022-12-09 08:54:13 +0000 UTC (1 container statuses recorded)
    Dec  9 08:54:16.441: INFO: 	Container filler-pod-f862ee59-6c05-4ad7-9007-7e8720b4c220 ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 12/09/22 08:54:16.441
    Dec  9 08:54:16.459: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6959" to be "running"
    Dec  9 08:54:16.470: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 10.875666ms
    Dec  9 08:54:18.482: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.022830466s
    Dec  9 08:54:18.482: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 12/09/22 08:54:18.493
    STEP: Trying to apply a random label on the found node. 12/09/22 08:54:18.524
    STEP: verifying the node has the label kubernetes.io/e2e-c39d8a45-fcae-45dd-923e-ad4adbb6ffe0 95 12/09/22 08:54:18.542
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 12/09/22 08:54:18.554
    Dec  9 08:54:18.569: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-6959" to be "not pending"
    Dec  9 08:54:18.585: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.247561ms
    Dec  9 08:54:20.597: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.028473396s
    Dec  9 08:54:20.598: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.250.13.218 on the node which pod4 resides and expect not scheduled 12/09/22 08:54:20.598
    Dec  9 08:54:20.612: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-6959" to be "not pending"
    Dec  9 08:54:20.623: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.955034ms
    Dec  9 08:54:22.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023367691s
    Dec  9 08:54:24.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022822392s
    Dec  9 08:54:26.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022442511s
    Dec  9 08:54:28.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02249889s
    Dec  9 08:54:30.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.023861264s
    Dec  9 08:54:32.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.024314446s
    Dec  9 08:54:34.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.02260164s
    Dec  9 08:54:36.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.023124384s
    Dec  9 08:54:38.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.022246374s
    Dec  9 08:54:40.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.022623395s
    Dec  9 08:54:42.639: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.02689771s
    Dec  9 08:54:44.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.022954823s
    Dec  9 08:54:46.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.023635979s
    Dec  9 08:54:48.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.023317175s
    Dec  9 08:54:50.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.023866623s
    Dec  9 08:54:52.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.024127262s
    Dec  9 08:54:54.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.022867001s
    Dec  9 08:54:56.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.023986541s
    Dec  9 08:54:58.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.023149522s
    Dec  9 08:55:00.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.025040703s
    Dec  9 08:55:02.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.024145871s
    Dec  9 08:55:04.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.022784714s
    Dec  9 08:55:06.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.022793811s
    Dec  9 08:55:08.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.022796854s
    Dec  9 08:55:10.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.023783979s
    Dec  9 08:55:12.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.022729834s
    Dec  9 08:55:14.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.023196225s
    Dec  9 08:55:16.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.023162238s
    Dec  9 08:55:18.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.02220058s
    Dec  9 08:55:20.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.023082505s
    Dec  9 08:55:22.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.024482074s
    Dec  9 08:55:24.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.022388275s
    Dec  9 08:55:26.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.022796841s
    Dec  9 08:55:28.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.023591155s
    Dec  9 08:55:30.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.024170589s
    Dec  9 08:55:32.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.022568573s
    Dec  9 08:55:34.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.023276131s
    Dec  9 08:55:36.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.022749686s
    Dec  9 08:55:38.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.023469331s
    Dec  9 08:55:40.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.023649407s
    Dec  9 08:55:42.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.025005675s
    Dec  9 08:55:44.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.023365216s
    Dec  9 08:55:46.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.022721852s
    Dec  9 08:55:48.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.023160253s
    Dec  9 08:55:50.641: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.028999308s
    Dec  9 08:55:52.638: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.025992775s
    Dec  9 08:55:54.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.024435391s
    Dec  9 08:55:56.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.023502938s
    Dec  9 08:55:58.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.022577494s
    Dec  9 08:56:00.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.023627658s
    Dec  9 08:56:02.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.023627333s
    Dec  9 08:56:04.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.023637767s
    Dec  9 08:56:06.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.022928083s
    Dec  9 08:56:08.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.023512401s
    Dec  9 08:56:10.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.023630582s
    Dec  9 08:56:12.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.023991103s
    Dec  9 08:56:14.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.023235364s
    Dec  9 08:56:16.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.022412891s
    Dec  9 08:56:18.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.022350395s
    Dec  9 08:56:20.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.024181933s
    Dec  9 08:56:22.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.022846739s
    Dec  9 08:56:24.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.023415566s
    Dec  9 08:56:26.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.0243188s
    Dec  9 08:56:28.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.022246382s
    Dec  9 08:56:30.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.023436249s
    Dec  9 08:56:32.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.024599988s
    Dec  9 08:56:34.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.022891499s
    Dec  9 08:56:36.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.023246528s
    Dec  9 08:56:38.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.022828054s
    Dec  9 08:56:40.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.02403309s
    Dec  9 08:56:42.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.022784566s
    Dec  9 08:56:44.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.023092046s
    Dec  9 08:56:46.638: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.025401046s
    Dec  9 08:56:48.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.022381882s
    Dec  9 08:56:50.641: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.028849887s
    Dec  9 08:56:52.644: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.03119338s
    Dec  9 08:56:54.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.022556083s
    Dec  9 08:56:56.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.023654284s
    Dec  9 08:56:58.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.022706073s
    Dec  9 08:57:00.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.023872913s
    Dec  9 08:57:02.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.023968731s
    Dec  9 08:57:04.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.023466237s
    Dec  9 08:57:06.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.02244768s
    Dec  9 08:57:08.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.022565839s
    Dec  9 08:57:10.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.023901028s
    Dec  9 08:57:12.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.022821138s
    Dec  9 08:57:14.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.022518567s
    Dec  9 08:57:16.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.023911258s
    Dec  9 08:57:18.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.02347993s
    Dec  9 08:57:20.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.022925266s
    Dec  9 08:57:22.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.022547406s
    Dec  9 08:57:24.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.023542695s
    Dec  9 08:57:26.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.023577063s
    Dec  9 08:57:28.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.023171075s
    Dec  9 08:57:30.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.0245173s
    Dec  9 08:57:32.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.024034966s
    Dec  9 08:57:34.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.023231832s
    Dec  9 08:57:36.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.023192261s
    Dec  9 08:57:38.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.022500837s
    Dec  9 08:57:40.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.022901406s
    Dec  9 08:57:42.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.024488469s
    Dec  9 08:57:44.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.023901447s
    Dec  9 08:57:46.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.024457294s
    Dec  9 08:57:48.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.022657224s
    Dec  9 08:57:50.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.023173885s
    Dec  9 08:57:52.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.022507155s
    Dec  9 08:57:54.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.022962248s
    Dec  9 08:57:56.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.022598321s
    Dec  9 08:57:58.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.022576411s
    Dec  9 08:58:00.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.023650951s
    Dec  9 08:58:02.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.023825189s
    Dec  9 08:58:04.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.023612968s
    Dec  9 08:58:06.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.023983786s
    Dec  9 08:58:08.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.022513435s
    Dec  9 08:58:10.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.023650149s
    Dec  9 08:58:12.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.02318819s
    Dec  9 08:58:14.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.022399739s
    Dec  9 08:58:16.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.023638236s
    Dec  9 08:58:18.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.023239776s
    Dec  9 08:58:20.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.023774196s
    Dec  9 08:58:22.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.023963577s
    Dec  9 08:58:24.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.024893613s
    Dec  9 08:58:26.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.023108658s
    Dec  9 08:58:28.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.022360805s
    Dec  9 08:58:30.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.023290001s
    Dec  9 08:58:32.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.022522136s
    Dec  9 08:58:34.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.022844068s
    Dec  9 08:58:36.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.02280289s
    Dec  9 08:58:38.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.023320136s
    Dec  9 08:58:40.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.023616777s
    Dec  9 08:58:42.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.022808791s
    Dec  9 08:58:44.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.023713684s
    Dec  9 08:58:46.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.024194552s
    Dec  9 08:58:48.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.022623318s
    Dec  9 08:58:50.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.023754155s
    Dec  9 08:58:52.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.02334676s
    Dec  9 08:58:54.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.022733431s
    Dec  9 08:58:56.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.023243231s
    Dec  9 08:58:58.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.022293867s
    Dec  9 08:59:00.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.022986837s
    Dec  9 08:59:02.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.023168176s
    Dec  9 08:59:04.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.022771045s
    Dec  9 08:59:06.637: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.024419727s
    Dec  9 08:59:08.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.022508916s
    Dec  9 08:59:10.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.023666193s
    Dec  9 08:59:12.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.023697028s
    Dec  9 08:59:14.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.023291681s
    Dec  9 08:59:16.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.02294203s
    Dec  9 08:59:18.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.023612064s
    Dec  9 08:59:20.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.023980096s
    Dec  9 08:59:20.648: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.035259414s
    STEP: removing the label kubernetes.io/e2e-c39d8a45-fcae-45dd-923e-ad4adbb6ffe0 off the node izgw8hm6kg779yfidwdv8yz 12/09/22 08:59:20.648
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-c39d8a45-fcae-45dd-923e-ad4adbb6ffe0 12/09/22 08:59:20.685
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Dec  9 08:59:20.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-6959" for this suite. 12/09/22 08:59:20.709
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/09/22 08:59:20.725
Dec  9 08:59:20.725: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption 12/09/22 08:59:20.726
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:59:20.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:59:20.779
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Dec  9 08:59:20.835: INFO: Waiting up to 1m0s for all nodes to be ready
Dec  9 09:00:20.946: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/09/22 09:00:20.957
Dec  9 09:00:20.957: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption-path 12/09/22 09:00:20.958
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 09:00:20.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 09:00:21.012
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Dec  9 09:00:21.067: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Dec  9 09:00:21.078: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Dec  9 09:00:21.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-6080" for this suite. 12/09/22 09:00:21.145
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Dec  9 09:00:21.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2827" for this suite. 12/09/22 09:00:21.182
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":23,"skipped":6839,"failed":0}
------------------------------
• [60.543 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/09/22 08:59:20.725
    Dec  9 08:59:20.725: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-preemption 12/09/22 08:59:20.726
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:59:20.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:59:20.779
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Dec  9 08:59:20.835: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec  9 09:00:20.946: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/09/22 09:00:20.957
    Dec  9 09:00:20.957: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-preemption-path 12/09/22 09:00:20.958
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 09:00:20.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 09:00:21.012
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Dec  9 09:00:21.067: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Dec  9 09:00:21.078: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Dec  9 09:00:21.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-6080" for this suite. 12/09/22 09:00:21.145
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Dec  9 09:00:21.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-2827" for this suite. 12/09/22 09:00:21.182
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":23,"skipped":7044,"failed":0}
Dec  9 09:00:21.275: INFO: Running AfterSuite actions on all nodes
Dec  9 09:00:21.275: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Dec  9 09:00:21.275: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Dec  9 09:00:21.275: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Dec  9 09:00:21.275: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Dec  9 09:00:21.275: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Dec  9 09:00:21.275: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Dec  9 09:00:21.275: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Dec  9 09:00:21.275: INFO: Running AfterSuite actions on node 1
Dec  9 09:00:21.275: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Dec  9 09:00:21.275: INFO: Running AfterSuite actions on all nodes
    Dec  9 09:00:21.275: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Dec  9 09:00:21.275: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Dec  9 09:00:21.275: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Dec  9 09:00:21.275: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Dec  9 09:00:21.275: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Dec  9 09:00:21.275: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Dec  9 09:00:21.275: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Dec  9 09:00:21.275: INFO: Running AfterSuite actions on node 1
    Dec  9 09:00:21.275: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.090 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 23 of 7067 Specs in 927.801 seconds
SUCCESS! -- 23 Passed | 0 Failed | 0 Pending | 7044 Skipped
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--ginkgo.dryRun is deprecated, use --ginkgo.dry-run instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m
  [38;5;11m--ginkgo.flakeAttempts is deprecated, use --ginkgo.flake-attempts instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

PASS

Ginkgo ran 1 suite in 15m28.092330225s
Test Suite Passed
