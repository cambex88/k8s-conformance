I1127 04:04:19.215007      19 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-192585042
I1127 04:04:19.215753      19 e2e.go:92] Starting e2e run "9e3dca65-22db-46ff-bf22-fcae64da58bd" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1574827446 - Will randomize all specs
Will run 276 of 4897 specs

Nov 27 04:04:19.306: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
Nov 27 04:04:19.318: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Nov 27 04:04:19.477: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Nov 27 04:04:19.616: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Nov 27 04:04:19.616: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Nov 27 04:04:19.616: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Nov 27 04:04:19.651: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Nov 27 04:04:19.651: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'coredns' (0 seconds elapsed)
Nov 27 04:04:19.651: INFO: e2e test version: v1.16.2
Nov 27 04:04:19.655: INFO: kube-apiserver version: v1.16.2
Nov 27 04:04:19.655: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
Nov 27 04:04:19.674: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:04:19.676: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename crd-publish-openapi
Nov 27 04:04:19.876: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Nov 27 04:04:19.913: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7643
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 04:04:20.039: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Nov 27 04:04:33.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 --namespace=crd-publish-openapi-7643 create -f -'
Nov 27 04:04:35.793: INFO: stderr: ""
Nov 27 04:04:35.794: INFO: stdout: "e2e-test-crd-publish-openapi-5794-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 27 04:04:35.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 --namespace=crd-publish-openapi-7643 delete e2e-test-crd-publish-openapi-5794-crds test-foo'
Nov 27 04:04:36.542: INFO: stderr: ""
Nov 27 04:04:36.542: INFO: stdout: "e2e-test-crd-publish-openapi-5794-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Nov 27 04:04:36.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 --namespace=crd-publish-openapi-7643 apply -f -'
Nov 27 04:04:37.726: INFO: stderr: ""
Nov 27 04:04:37.727: INFO: stdout: "e2e-test-crd-publish-openapi-5794-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 27 04:04:37.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 --namespace=crd-publish-openapi-7643 delete e2e-test-crd-publish-openapi-5794-crds test-foo'
Nov 27 04:04:38.437: INFO: stderr: ""
Nov 27 04:04:38.438: INFO: stdout: "e2e-test-crd-publish-openapi-5794-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Nov 27 04:04:38.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 --namespace=crd-publish-openapi-7643 create -f -'
Nov 27 04:04:39.566: INFO: rc: 1
Nov 27 04:04:39.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 --namespace=crd-publish-openapi-7643 apply -f -'
Nov 27 04:04:40.680: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Nov 27 04:04:40.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 --namespace=crd-publish-openapi-7643 create -f -'
Nov 27 04:04:41.821: INFO: rc: 1
Nov 27 04:04:41.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 --namespace=crd-publish-openapi-7643 apply -f -'
Nov 27 04:04:42.925: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Nov 27 04:04:42.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 explain e2e-test-crd-publish-openapi-5794-crds'
Nov 27 04:04:44.109: INFO: stderr: ""
Nov 27 04:04:44.110: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5794-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Nov 27 04:04:44.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 explain e2e-test-crd-publish-openapi-5794-crds.metadata'
Nov 27 04:04:45.243: INFO: stderr: ""
Nov 27 04:04:45.244: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5794-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Nov 27 04:04:45.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 explain e2e-test-crd-publish-openapi-5794-crds.spec'
Nov 27 04:04:46.400: INFO: stderr: ""
Nov 27 04:04:46.400: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5794-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Nov 27 04:04:46.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 explain e2e-test-crd-publish-openapi-5794-crds.spec.bars'
Nov 27 04:04:47.575: INFO: stderr: ""
Nov 27 04:04:47.575: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5794-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Nov 27 04:04:47.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 explain e2e-test-crd-publish-openapi-5794-crds.spec.bars2'
Nov 27 04:04:48.699: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:05:03.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7643" for this suite.
Nov 27 04:05:09.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:05:09.819: INFO: namespace crd-publish-openapi-7643 deletion completed in 6.293817016s

• [SLOW TEST:50.144 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:05:09.822: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-426
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Nov 27 04:05:14.185: INFO: &Pod{ObjectMeta:{send-events-58e20bdb-5be4-447a-8b32-936cd47f8827  events-426 /api/v1/namespaces/events-426/pods/send-events-58e20bdb-5be4-447a-8b32-936cd47f8827 a68c951c-a82e-4335-b659-54c6c306287e 943043 0 2019-11-27 04:05:10 +0000 UTC <nil> <nil> map[name:foo time:128309639] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tlc2q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tlc2q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tlc2q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:05:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:05:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:05:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:05:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.1,PodIP:172.31.161.223,StartTime:2019-11-27 04:05:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-11-27 04:05:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:docker://sha256:b98549801e850b34353972a5dc2e80d12becba5e9e8fa127f91208b5e943e38d,ContainerID:docker://676ffaa19ec6f7d25beca61283c2846d0a080af057f9fd20e23f2b3f3aaac590,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.31.161.223,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Nov 27 04:05:16.206: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Nov 27 04:05:18.220: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:05:18.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-426" for this suite.
Nov 27 04:06:04.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:06:04.558: INFO: namespace events-426 deletion completed in 46.309707499s

• [SLOW TEST:54.737 seconds]
[k8s.io] [sig-node] Events
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:06:04.559: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-158
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-158
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 27 04:06:04.877: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Nov 27 04:06:33.421: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.31.161.210:8080/dial?request=hostName&protocol=udp&host=172.31.161.213&port=8081&tries=1'] Namespace:pod-network-test-158 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 27 04:06:33.422: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
Nov 27 04:06:34.139: INFO: Waiting for endpoints: map[]
Nov 27 04:06:34.150: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.31.161.210:8080/dial?request=hostName&protocol=udp&host=172.31.51.8&port=8081&tries=1'] Namespace:pod-network-test-158 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 27 04:06:34.150: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
Nov 27 04:06:34.815: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:06:34.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-158" for this suite.
Nov 27 04:06:46.861: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:06:47.120: INFO: namespace pod-network-test-158 deletion completed in 12.28964591s

• [SLOW TEST:42.561 seconds]
[sig-network] Networking
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:06:47.121: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-5096
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 04:06:47.460: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-84c52d11-dfe5-49ec-a46e-579c3678b3fd" in namespace "security-context-test-5096" to be "success or failure"
Nov 27 04:06:47.473: INFO: Pod "busybox-readonly-false-84c52d11-dfe5-49ec-a46e-579c3678b3fd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.882268ms
Nov 27 04:06:49.485: INFO: Pod "busybox-readonly-false-84c52d11-dfe5-49ec-a46e-579c3678b3fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02502211s
Nov 27 04:06:51.495: INFO: Pod "busybox-readonly-false-84c52d11-dfe5-49ec-a46e-579c3678b3fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035287546s
Nov 27 04:06:53.509: INFO: Pod "busybox-readonly-false-84c52d11-dfe5-49ec-a46e-579c3678b3fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049393706s
Nov 27 04:06:53.509: INFO: Pod "busybox-readonly-false-84c52d11-dfe5-49ec-a46e-579c3678b3fd" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:06:53.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5096" for this suite.
Nov 27 04:06:59.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:06:59.825: INFO: namespace security-context-test-5096 deletion completed in 6.303223272s

• [SLOW TEST:12.705 seconds]
[k8s.io] Security Context
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:06:59.827: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5264
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:07:11.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5264" for this suite.
Nov 27 04:07:17.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:07:17.555: INFO: namespace resourcequota-5264 deletion completed in 6.279437765s

• [SLOW TEST:17.728 seconds]
[sig-api-machinery] ResourceQuota
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:07:17.556: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4493
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-3e630d7e-fd39-44ad-a8ae-59c302347d28
STEP: Creating a pod to test consume configMaps
Nov 27 04:07:17.917: INFO: Waiting up to 5m0s for pod "pod-configmaps-dbb8ff2d-2fdf-4f9f-9d16-ecbc3616746e" in namespace "configmap-4493" to be "success or failure"
Nov 27 04:07:17.930: INFO: Pod "pod-configmaps-dbb8ff2d-2fdf-4f9f-9d16-ecbc3616746e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.7488ms
Nov 27 04:07:19.944: INFO: Pod "pod-configmaps-dbb8ff2d-2fdf-4f9f-9d16-ecbc3616746e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026836872s
Nov 27 04:07:21.957: INFO: Pod "pod-configmaps-dbb8ff2d-2fdf-4f9f-9d16-ecbc3616746e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039852806s
Nov 27 04:07:23.974: INFO: Pod "pod-configmaps-dbb8ff2d-2fdf-4f9f-9d16-ecbc3616746e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056387286s
STEP: Saw pod success
Nov 27 04:07:23.974: INFO: Pod "pod-configmaps-dbb8ff2d-2fdf-4f9f-9d16-ecbc3616746e" satisfied condition "success or failure"
Nov 27 04:07:23.984: INFO: Trying to get logs from node slave1 pod pod-configmaps-dbb8ff2d-2fdf-4f9f-9d16-ecbc3616746e container configmap-volume-test: <nil>
STEP: delete the pod
Nov 27 04:07:24.246: INFO: Waiting for pod pod-configmaps-dbb8ff2d-2fdf-4f9f-9d16-ecbc3616746e to disappear
Nov 27 04:07:24.254: INFO: Pod pod-configmaps-dbb8ff2d-2fdf-4f9f-9d16-ecbc3616746e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:07:24.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4493" for this suite.
Nov 27 04:07:30.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:07:30.572: INFO: namespace configmap-4493 deletion completed in 6.303537139s

• [SLOW TEST:13.016 seconds]
[sig-storage] ConfigMap
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:07:30.573: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8175
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 27 04:07:59.969: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 27 04:08:01.998: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710424480, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710424480, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710424480, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710424479, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 04:08:04.009: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710424480, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710424480, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710424480, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710424479, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 27 04:08:07.098: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 04:08:07.112: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7686-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:08:09.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8175" for this suite.
Nov 27 04:08:17.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:08:17.371: INFO: namespace webhook-8175 deletion completed in 8.314014754s
STEP: Destroying namespace "webhook-8175-markers" for this suite.
Nov 27 04:08:23.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:08:23.645: INFO: namespace webhook-8175-markers deletion completed in 6.273594544s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:53.113 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:08:23.691: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1002
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Nov 27 04:10:03.111: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:10:03.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1002" for this suite.
Nov 27 04:10:31.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:10:31.587: INFO: namespace replicaset-1002 deletion completed in 28.358178601s

• [SLOW TEST:127.896 seconds]
[sig-apps] ReplicaSet
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:10:31.588: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3509
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 27 04:10:55.457: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 27 04:10:57.494: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710424655, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710424655, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710424655, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710424655, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 04:10:59.503: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710424655, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710424655, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710424655, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710424655, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 27 04:11:02.525: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 04:11:02.538: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:11:04.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3509" for this suite.
Nov 27 04:11:12.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:11:12.639: INFO: namespace webhook-3509 deletion completed in 8.304896409s
STEP: Destroying namespace "webhook-3509-markers" for this suite.
Nov 27 04:11:18.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:11:18.905: INFO: namespace webhook-3509-markers deletion completed in 6.265483796s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:47.352 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:11:18.945: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2650
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 04:11:19.268: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-e7c28cc4-60a3-4b86-9f20-af27ea1de0ca" in namespace "security-context-test-2650" to be "success or failure"
Nov 27 04:11:19.276: INFO: Pod "busybox-privileged-false-e7c28cc4-60a3-4b86-9f20-af27ea1de0ca": Phase="Pending", Reason="", readiness=false. Elapsed: 7.718086ms
Nov 27 04:11:21.285: INFO: Pod "busybox-privileged-false-e7c28cc4-60a3-4b86-9f20-af27ea1de0ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017392086s
Nov 27 04:11:23.296: INFO: Pod "busybox-privileged-false-e7c28cc4-60a3-4b86-9f20-af27ea1de0ca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028194929s
Nov 27 04:11:25.308: INFO: Pod "busybox-privileged-false-e7c28cc4-60a3-4b86-9f20-af27ea1de0ca": Phase="Pending", Reason="", readiness=false. Elapsed: 6.03994915s
Nov 27 04:11:27.321: INFO: Pod "busybox-privileged-false-e7c28cc4-60a3-4b86-9f20-af27ea1de0ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.05323897s
Nov 27 04:11:27.321: INFO: Pod "busybox-privileged-false-e7c28cc4-60a3-4b86-9f20-af27ea1de0ca" satisfied condition "success or failure"
Nov 27 04:11:27.563: INFO: Got logs for pod "busybox-privileged-false-e7c28cc4-60a3-4b86-9f20-af27ea1de0ca": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:11:27.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2650" for this suite.
Nov 27 04:11:33.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:11:33.883: INFO: namespace security-context-test-2650 deletion completed in 6.302949786s

• [SLOW TEST:14.939 seconds]
[k8s.io] Security Context
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:11:33.884: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5992
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:11:50.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5992" for this suite.
Nov 27 04:11:56.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:11:56.712: INFO: namespace resourcequota-5992 deletion completed in 6.262247842s

• [SLOW TEST:22.828 seconds]
[sig-api-machinery] ResourceQuota
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:11:56.714: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-383
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 27 04:11:57.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-383'
Nov 27 04:11:57.863: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Nov 27 04:11:57.864: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Nov 27 04:11:57.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 delete deployment e2e-test-httpd-deployment --namespace=kubectl-383'
Nov 27 04:11:58.636: INFO: stderr: ""
Nov 27 04:11:58.637: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:11:58.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-383" for this suite.
Nov 27 04:12:04.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:12:04.910: INFO: namespace kubectl-383 deletion completed in 6.256165576s

• [SLOW TEST:8.197 seconds]
[sig-cli] Kubectl client
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:12:04.912: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-2224
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Nov 27 04:12:13.335: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2224 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 27 04:12:13.335: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
Nov 27 04:12:13.978: INFO: Exec stderr: ""
Nov 27 04:12:13.979: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2224 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 27 04:12:13.979: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
Nov 27 04:12:14.630: INFO: Exec stderr: ""
Nov 27 04:12:14.630: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2224 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 27 04:12:14.630: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
Nov 27 04:12:15.289: INFO: Exec stderr: ""
Nov 27 04:12:15.289: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2224 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 27 04:12:15.289: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
Nov 27 04:12:15.923: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Nov 27 04:12:15.923: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2224 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 27 04:12:15.923: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
Nov 27 04:12:16.578: INFO: Exec stderr: ""
Nov 27 04:12:16.578: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2224 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 27 04:12:16.579: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
Nov 27 04:12:17.235: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Nov 27 04:12:17.235: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2224 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 27 04:12:17.236: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
Nov 27 04:12:17.850: INFO: Exec stderr: ""
Nov 27 04:12:17.850: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2224 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 27 04:12:17.850: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
Nov 27 04:12:18.485: INFO: Exec stderr: ""
Nov 27 04:12:18.485: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2224 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 27 04:12:18.485: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
Nov 27 04:12:19.137: INFO: Exec stderr: ""
Nov 27 04:12:19.137: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2224 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 27 04:12:19.137: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
Nov 27 04:12:19.795: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:12:19.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2224" for this suite.
Nov 27 04:13:19.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:13:20.067: INFO: namespace e2e-kubelet-etc-hosts-2224 deletion completed in 1m0.253210609s

• [SLOW TEST:75.156 seconds]
[k8s.io] KubeletManagedEtcHosts
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:13:20.070: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4703
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 04:13:20.376: INFO: Creating ReplicaSet my-hostname-basic-b988594a-a6cc-4a85-8397-5377144155e4
Nov 27 04:13:20.400: INFO: Pod name my-hostname-basic-b988594a-a6cc-4a85-8397-5377144155e4: Found 0 pods out of 1
Nov 27 04:13:25.411: INFO: Pod name my-hostname-basic-b988594a-a6cc-4a85-8397-5377144155e4: Found 1 pods out of 1
Nov 27 04:13:25.411: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-b988594a-a6cc-4a85-8397-5377144155e4" is running
Nov 27 04:13:25.420: INFO: Pod "my-hostname-basic-b988594a-a6cc-4a85-8397-5377144155e4-47sf2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-27 04:13:20 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-27 04:13:24 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-27 04:13:24 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-27 04:13:20 +0000 UTC Reason: Message:}])
Nov 27 04:13:25.421: INFO: Trying to dial the pod
Nov 27 04:13:30.460: INFO: Controller my-hostname-basic-b988594a-a6cc-4a85-8397-5377144155e4: Got expected result from replica 1 [my-hostname-basic-b988594a-a6cc-4a85-8397-5377144155e4-47sf2]: "my-hostname-basic-b988594a-a6cc-4a85-8397-5377144155e4-47sf2", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:13:30.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4703" for this suite.
Nov 27 04:13:36.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:13:36.745: INFO: namespace replicaset-4703 deletion completed in 6.272094549s

• [SLOW TEST:16.675 seconds]
[sig-apps] ReplicaSet
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:13:36.747: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3133
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Nov 27 04:13:37.061: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:13:46.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3133" for this suite.
Nov 27 04:13:52.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:13:52.583: INFO: namespace init-container-3133 deletion completed in 6.269185173s

• [SLOW TEST:15.837 seconds]
[k8s.io] InitContainer [NodeConformance]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:13:52.585: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5264
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-5b050544-01e9-42a1-b697-e3f9c500eea3
STEP: Creating a pod to test consume secrets
Nov 27 04:13:52.923: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6d990728-2f04-418a-87f1-e339a7e816e3" in namespace "projected-5264" to be "success or failure"
Nov 27 04:13:52.932: INFO: Pod "pod-projected-secrets-6d990728-2f04-418a-87f1-e339a7e816e3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.116797ms
Nov 27 04:13:54.942: INFO: Pod "pod-projected-secrets-6d990728-2f04-418a-87f1-e339a7e816e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019184352s
Nov 27 04:13:56.954: INFO: Pod "pod-projected-secrets-6d990728-2f04-418a-87f1-e339a7e816e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030923417s
Nov 27 04:13:58.965: INFO: Pod "pod-projected-secrets-6d990728-2f04-418a-87f1-e339a7e816e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042016171s
STEP: Saw pod success
Nov 27 04:13:58.965: INFO: Pod "pod-projected-secrets-6d990728-2f04-418a-87f1-e339a7e816e3" satisfied condition "success or failure"
Nov 27 04:13:58.973: INFO: Trying to get logs from node slave1 pod pod-projected-secrets-6d990728-2f04-418a-87f1-e339a7e816e3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 27 04:13:59.301: INFO: Waiting for pod pod-projected-secrets-6d990728-2f04-418a-87f1-e339a7e816e3 to disappear
Nov 27 04:13:59.307: INFO: Pod pod-projected-secrets-6d990728-2f04-418a-87f1-e339a7e816e3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:13:59.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5264" for this suite.
Nov 27 04:14:05.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:14:05.581: INFO: namespace projected-5264 deletion completed in 6.260971842s

• [SLOW TEST:12.996 seconds]
[sig-storage] Projected secret
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:14:05.583: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5129
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:14:10.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5129" for this suite.
Nov 27 04:14:32.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:14:32.480: INFO: namespace containers-5129 deletion completed in 22.290606249s

• [SLOW TEST:26.897 seconds]
[k8s.io] Docker Containers
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:14:32.481: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5651
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 27 04:14:32.822: INFO: Waiting up to 5m0s for pod "downwardapi-volume-58bac946-13d8-422a-916a-b7eb844f5f69" in namespace "projected-5651" to be "success or failure"
Nov 27 04:14:32.830: INFO: Pod "downwardapi-volume-58bac946-13d8-422a-916a-b7eb844f5f69": Phase="Pending", Reason="", readiness=false. Elapsed: 7.868175ms
Nov 27 04:14:34.840: INFO: Pod "downwardapi-volume-58bac946-13d8-422a-916a-b7eb844f5f69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018013463s
Nov 27 04:14:36.849: INFO: Pod "downwardapi-volume-58bac946-13d8-422a-916a-b7eb844f5f69": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027281685s
Nov 27 04:14:38.859: INFO: Pod "downwardapi-volume-58bac946-13d8-422a-916a-b7eb844f5f69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037373684s
STEP: Saw pod success
Nov 27 04:14:38.859: INFO: Pod "downwardapi-volume-58bac946-13d8-422a-916a-b7eb844f5f69" satisfied condition "success or failure"
Nov 27 04:14:38.867: INFO: Trying to get logs from node slave1 pod downwardapi-volume-58bac946-13d8-422a-916a-b7eb844f5f69 container client-container: <nil>
STEP: delete the pod
Nov 27 04:14:38.941: INFO: Waiting for pod downwardapi-volume-58bac946-13d8-422a-916a-b7eb844f5f69 to disappear
Nov 27 04:14:38.949: INFO: Pod downwardapi-volume-58bac946-13d8-422a-916a-b7eb844f5f69 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:14:38.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5651" for this suite.
Nov 27 04:14:44.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:14:45.242: INFO: namespace projected-5651 deletion completed in 6.278678414s

• [SLOW TEST:12.762 seconds]
[sig-storage] Projected downwardAPI
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:14:45.244: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9142
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Nov 27 04:14:55.641: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 27 04:14:55.651: INFO: Pod pod-with-prestop-http-hook still exists
Nov 27 04:14:57.651: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 27 04:14:57.663: INFO: Pod pod-with-prestop-http-hook still exists
Nov 27 04:14:59.651: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 27 04:14:59.663: INFO: Pod pod-with-prestop-http-hook still exists
Nov 27 04:15:01.651: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 27 04:15:01.665: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:15:01.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9142" for this suite.
Nov 27 04:15:29.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:15:30.024: INFO: namespace container-lifecycle-hook-9142 deletion completed in 28.309212472s

• [SLOW TEST:44.781 seconds]
[k8s.io] Container Lifecycle Hook
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:15:30.027: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5674
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-5674/secret-test-9e1c69da-9ceb-455f-b4eb-ca1b943e6ea7
STEP: Creating a pod to test consume secrets
Nov 27 04:15:30.371: INFO: Waiting up to 5m0s for pod "pod-configmaps-560c8397-ccf3-4e6e-a6d0-ef82669eae29" in namespace "secrets-5674" to be "success or failure"
Nov 27 04:15:30.378: INFO: Pod "pod-configmaps-560c8397-ccf3-4e6e-a6d0-ef82669eae29": Phase="Pending", Reason="", readiness=false. Elapsed: 7.090843ms
Nov 27 04:15:32.388: INFO: Pod "pod-configmaps-560c8397-ccf3-4e6e-a6d0-ef82669eae29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01686702s
Nov 27 04:15:34.403: INFO: Pod "pod-configmaps-560c8397-ccf3-4e6e-a6d0-ef82669eae29": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031363906s
Nov 27 04:15:36.413: INFO: Pod "pod-configmaps-560c8397-ccf3-4e6e-a6d0-ef82669eae29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041439772s
STEP: Saw pod success
Nov 27 04:15:36.413: INFO: Pod "pod-configmaps-560c8397-ccf3-4e6e-a6d0-ef82669eae29" satisfied condition "success or failure"
Nov 27 04:15:36.420: INFO: Trying to get logs from node slave1 pod pod-configmaps-560c8397-ccf3-4e6e-a6d0-ef82669eae29 container env-test: <nil>
STEP: delete the pod
Nov 27 04:15:36.497: INFO: Waiting for pod pod-configmaps-560c8397-ccf3-4e6e-a6d0-ef82669eae29 to disappear
Nov 27 04:15:36.514: INFO: Pod pod-configmaps-560c8397-ccf3-4e6e-a6d0-ef82669eae29 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:15:36.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5674" for this suite.
Nov 27 04:15:42.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:15:42.801: INFO: namespace secrets-5674 deletion completed in 6.26439113s

• [SLOW TEST:12.774 seconds]
[sig-api-machinery] Secrets
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:15:42.802: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3369
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-7aeeddcc-2287-44ef-b553-43f4adbb29c5
STEP: Creating a pod to test consume configMaps
Nov 27 04:15:43.139: INFO: Waiting up to 5m0s for pod "pod-configmaps-ad4855c5-115f-4764-a694-b37348493c7d" in namespace "configmap-3369" to be "success or failure"
Nov 27 04:15:43.148: INFO: Pod "pod-configmaps-ad4855c5-115f-4764-a694-b37348493c7d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.109464ms
Nov 27 04:15:45.166: INFO: Pod "pod-configmaps-ad4855c5-115f-4764-a694-b37348493c7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026657016s
Nov 27 04:15:47.176: INFO: Pod "pod-configmaps-ad4855c5-115f-4764-a694-b37348493c7d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036661015s
Nov 27 04:15:49.184: INFO: Pod "pod-configmaps-ad4855c5-115f-4764-a694-b37348493c7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044728037s
STEP: Saw pod success
Nov 27 04:15:49.184: INFO: Pod "pod-configmaps-ad4855c5-115f-4764-a694-b37348493c7d" satisfied condition "success or failure"
Nov 27 04:15:49.191: INFO: Trying to get logs from node slave1 pod pod-configmaps-ad4855c5-115f-4764-a694-b37348493c7d container configmap-volume-test: <nil>
STEP: delete the pod
Nov 27 04:15:49.243: INFO: Waiting for pod pod-configmaps-ad4855c5-115f-4764-a694-b37348493c7d to disappear
Nov 27 04:15:49.249: INFO: Pod pod-configmaps-ad4855c5-115f-4764-a694-b37348493c7d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:15:49.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3369" for this suite.
Nov 27 04:15:55.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:15:55.512: INFO: namespace configmap-3369 deletion completed in 6.248585845s

• [SLOW TEST:12.710 seconds]
[sig-storage] ConfigMap
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:15:55.514: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5888
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 04:15:55.810: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 27 04:16:10.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 --namespace=crd-publish-openapi-5888 create -f -'
Nov 27 04:16:12.234: INFO: stderr: ""
Nov 27 04:16:12.235: INFO: stdout: "e2e-test-crd-publish-openapi-643-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 27 04:16:12.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 --namespace=crd-publish-openapi-5888 delete e2e-test-crd-publish-openapi-643-crds test-cr'
Nov 27 04:16:12.908: INFO: stderr: ""
Nov 27 04:16:12.909: INFO: stdout: "e2e-test-crd-publish-openapi-643-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Nov 27 04:16:12.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 --namespace=crd-publish-openapi-5888 apply -f -'
Nov 27 04:16:14.051: INFO: stderr: ""
Nov 27 04:16:14.052: INFO: stdout: "e2e-test-crd-publish-openapi-643-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 27 04:16:14.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 --namespace=crd-publish-openapi-5888 delete e2e-test-crd-publish-openapi-643-crds test-cr'
Nov 27 04:16:14.792: INFO: stderr: ""
Nov 27 04:16:14.793: INFO: stdout: "e2e-test-crd-publish-openapi-643-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Nov 27 04:16:14.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 explain e2e-test-crd-publish-openapi-643-crds'
Nov 27 04:16:15.916: INFO: stderr: ""
Nov 27 04:16:15.917: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-643-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:16:31.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5888" for this suite.
Nov 27 04:16:37.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:16:37.325: INFO: namespace crd-publish-openapi-5888 deletion completed in 6.275810593s

• [SLOW TEST:41.811 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:16:37.327: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3297
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-5b717c3c-76b5-4811-a377-e62100dd22d0 in namespace container-probe-3297
Nov 27 04:16:43.680: INFO: Started pod busybox-5b717c3c-76b5-4811-a377-e62100dd22d0 in namespace container-probe-3297
STEP: checking the pod's current state and verifying that restartCount is present
Nov 27 04:16:43.687: INFO: Initial restart count of pod busybox-5b717c3c-76b5-4811-a377-e62100dd22d0 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:20:45.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3297" for this suite.
Nov 27 04:20:51.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:20:51.429: INFO: namespace container-probe-3297 deletion completed in 6.301207556s

• [SLOW TEST:254.102 seconds]
[k8s.io] Probing container
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:20:51.430: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-540
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-86j4
STEP: Creating a pod to test atomic-volume-subpath
Nov 27 04:20:51.773: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-86j4" in namespace "subpath-540" to be "success or failure"
Nov 27 04:20:51.781: INFO: Pod "pod-subpath-test-downwardapi-86j4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.721544ms
Nov 27 04:20:53.790: INFO: Pod "pod-subpath-test-downwardapi-86j4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017111306s
Nov 27 04:20:55.805: INFO: Pod "pod-subpath-test-downwardapi-86j4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031671091s
Nov 27 04:20:57.821: INFO: Pod "pod-subpath-test-downwardapi-86j4": Phase="Running", Reason="", readiness=true. Elapsed: 6.048107727s
Nov 27 04:20:59.833: INFO: Pod "pod-subpath-test-downwardapi-86j4": Phase="Running", Reason="", readiness=true. Elapsed: 8.059387542s
Nov 27 04:21:01.842: INFO: Pod "pod-subpath-test-downwardapi-86j4": Phase="Running", Reason="", readiness=true. Elapsed: 10.068864327s
Nov 27 04:21:03.852: INFO: Pod "pod-subpath-test-downwardapi-86j4": Phase="Running", Reason="", readiness=true. Elapsed: 12.079278805s
Nov 27 04:21:05.861: INFO: Pod "pod-subpath-test-downwardapi-86j4": Phase="Running", Reason="", readiness=true. Elapsed: 14.088223677s
Nov 27 04:21:07.876: INFO: Pod "pod-subpath-test-downwardapi-86j4": Phase="Running", Reason="", readiness=true. Elapsed: 16.102598172s
Nov 27 04:21:09.885: INFO: Pod "pod-subpath-test-downwardapi-86j4": Phase="Running", Reason="", readiness=true. Elapsed: 18.111497088s
Nov 27 04:21:11.894: INFO: Pod "pod-subpath-test-downwardapi-86j4": Phase="Running", Reason="", readiness=true. Elapsed: 20.121023251s
Nov 27 04:21:13.903: INFO: Pod "pod-subpath-test-downwardapi-86j4": Phase="Running", Reason="", readiness=true. Elapsed: 22.129715855s
Nov 27 04:21:15.912: INFO: Pod "pod-subpath-test-downwardapi-86j4": Phase="Running", Reason="", readiness=true. Elapsed: 24.138521881s
Nov 27 04:21:17.926: INFO: Pod "pod-subpath-test-downwardapi-86j4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.152307262s
STEP: Saw pod success
Nov 27 04:21:17.926: INFO: Pod "pod-subpath-test-downwardapi-86j4" satisfied condition "success or failure"
Nov 27 04:21:17.935: INFO: Trying to get logs from node slave1 pod pod-subpath-test-downwardapi-86j4 container test-container-subpath-downwardapi-86j4: <nil>
STEP: delete the pod
Nov 27 04:21:18.225: INFO: Waiting for pod pod-subpath-test-downwardapi-86j4 to disappear
Nov 27 04:21:18.238: INFO: Pod pod-subpath-test-downwardapi-86j4 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-86j4
Nov 27 04:21:18.238: INFO: Deleting pod "pod-subpath-test-downwardapi-86j4" in namespace "subpath-540"
[AfterEach] [sig-storage] Subpath
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:21:18.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-540" for this suite.
Nov 27 04:21:24.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:21:24.556: INFO: namespace subpath-540 deletion completed in 6.286715361s

• [SLOW TEST:33.127 seconds]
[sig-storage] Subpath
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:21:24.558: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3822
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Nov 27 04:21:24.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 cluster-info'
Nov 27 04:21:25.627: INFO: stderr: ""
Nov 27 04:21:25.628: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.30.0.1:443\x1b[0m\n\x1b[0;32mcoredns\x1b[0m is running at \x1b[0;33mhttps://172.30.0.1:443/api/v1/namespaces/kube-system/services/coredns:dns/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://172.30.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:21:25.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3822" for this suite.
Nov 27 04:21:31.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:21:31.910: INFO: namespace kubectl-3822 deletion completed in 6.26406651s

• [SLOW TEST:7.353 seconds]
[sig-cli] Kubectl client
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:21:31.911: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-8986
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 04:21:32.234: INFO: Waiting up to 5m0s for pod "busybox-user-65534-5e50760a-634e-499f-9549-a57747ebc54e" in namespace "security-context-test-8986" to be "success or failure"
Nov 27 04:21:32.244: INFO: Pod "busybox-user-65534-5e50760a-634e-499f-9549-a57747ebc54e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.761197ms
Nov 27 04:21:34.254: INFO: Pod "busybox-user-65534-5e50760a-634e-499f-9549-a57747ebc54e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019822207s
Nov 27 04:21:36.263: INFO: Pod "busybox-user-65534-5e50760a-634e-499f-9549-a57747ebc54e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028330988s
Nov 27 04:21:38.271: INFO: Pod "busybox-user-65534-5e50760a-634e-499f-9549-a57747ebc54e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036614879s
Nov 27 04:21:38.271: INFO: Pod "busybox-user-65534-5e50760a-634e-499f-9549-a57747ebc54e" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:21:38.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8986" for this suite.
Nov 27 04:21:44.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:21:44.545: INFO: namespace security-context-test-8986 deletion completed in 6.261337521s

• [SLOW TEST:12.634 seconds]
[k8s.io] Security Context
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:21:44.546: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5927
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 27 04:21:44.879: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1519d5be-4250-423c-b1f7-ad84ace867ae" in namespace "downward-api-5927" to be "success or failure"
Nov 27 04:21:44.894: INFO: Pod "downwardapi-volume-1519d5be-4250-423c-b1f7-ad84ace867ae": Phase="Pending", Reason="", readiness=false. Elapsed: 14.254105ms
Nov 27 04:21:46.903: INFO: Pod "downwardapi-volume-1519d5be-4250-423c-b1f7-ad84ace867ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023652668s
Nov 27 04:21:48.913: INFO: Pod "downwardapi-volume-1519d5be-4250-423c-b1f7-ad84ace867ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033693633s
Nov 27 04:21:50.926: INFO: Pod "downwardapi-volume-1519d5be-4250-423c-b1f7-ad84ace867ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045989852s
STEP: Saw pod success
Nov 27 04:21:50.926: INFO: Pod "downwardapi-volume-1519d5be-4250-423c-b1f7-ad84ace867ae" satisfied condition "success or failure"
Nov 27 04:21:50.934: INFO: Trying to get logs from node slave1 pod downwardapi-volume-1519d5be-4250-423c-b1f7-ad84ace867ae container client-container: <nil>
STEP: delete the pod
Nov 27 04:21:51.022: INFO: Waiting for pod downwardapi-volume-1519d5be-4250-423c-b1f7-ad84ace867ae to disappear
Nov 27 04:21:51.031: INFO: Pod downwardapi-volume-1519d5be-4250-423c-b1f7-ad84ace867ae no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:21:51.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5927" for this suite.
Nov 27 04:21:57.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:21:57.318: INFO: namespace downward-api-5927 deletion completed in 6.274889089s

• [SLOW TEST:12.772 seconds]
[sig-storage] Downward API volume
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:21:57.319: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-846
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 27 04:22:16.306: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 27 04:22:18.342: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710425336, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710425336, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710425336, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710425336, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 04:22:20.351: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710425336, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710425336, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710425336, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710425336, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 27 04:22:23.375: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
Nov 27 04:22:33.448: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:22:44.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-846" for this suite.
Nov 27 04:22:52.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:22:52.670: INFO: namespace webhook-846 deletion completed in 8.264988681s
STEP: Destroying namespace "webhook-846-markers" for this suite.
Nov 27 04:22:58.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:22:58.964: INFO: namespace webhook-846-markers deletion completed in 6.29359748s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:61.687 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:22:59.008: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1882
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 27 04:22:59.346: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1ba5114-3f33-4f30-9eaf-c66d49277110" in namespace "downward-api-1882" to be "success or failure"
Nov 27 04:22:59.357: INFO: Pod "downwardapi-volume-f1ba5114-3f33-4f30-9eaf-c66d49277110": Phase="Pending", Reason="", readiness=false. Elapsed: 10.449556ms
Nov 27 04:23:01.365: INFO: Pod "downwardapi-volume-f1ba5114-3f33-4f30-9eaf-c66d49277110": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018766691s
Nov 27 04:23:03.374: INFO: Pod "downwardapi-volume-f1ba5114-3f33-4f30-9eaf-c66d49277110": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027876319s
Nov 27 04:23:05.390: INFO: Pod "downwardapi-volume-f1ba5114-3f33-4f30-9eaf-c66d49277110": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.04317384s
STEP: Saw pod success
Nov 27 04:23:05.390: INFO: Pod "downwardapi-volume-f1ba5114-3f33-4f30-9eaf-c66d49277110" satisfied condition "success or failure"
Nov 27 04:23:05.398: INFO: Trying to get logs from node slave1 pod downwardapi-volume-f1ba5114-3f33-4f30-9eaf-c66d49277110 container client-container: <nil>
STEP: delete the pod
Nov 27 04:23:05.467: INFO: Waiting for pod downwardapi-volume-f1ba5114-3f33-4f30-9eaf-c66d49277110 to disappear
Nov 27 04:23:05.477: INFO: Pod downwardapi-volume-f1ba5114-3f33-4f30-9eaf-c66d49277110 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:23:05.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1882" for this suite.
Nov 27 04:23:11.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:23:11.755: INFO: namespace downward-api-1882 deletion completed in 6.265430026s

• [SLOW TEST:12.747 seconds]
[sig-storage] Downward API volume
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:23:11.759: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6329
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Nov 27 04:23:12.083: INFO: Waiting up to 5m0s for pod "var-expansion-5db9c124-ed98-48ee-9393-03e24191397f" in namespace "var-expansion-6329" to be "success or failure"
Nov 27 04:23:12.099: INFO: Pod "var-expansion-5db9c124-ed98-48ee-9393-03e24191397f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.716378ms
Nov 27 04:23:14.108: INFO: Pod "var-expansion-5db9c124-ed98-48ee-9393-03e24191397f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024758539s
Nov 27 04:23:16.124: INFO: Pod "var-expansion-5db9c124-ed98-48ee-9393-03e24191397f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041078331s
Nov 27 04:23:18.134: INFO: Pod "var-expansion-5db9c124-ed98-48ee-9393-03e24191397f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051087518s
Nov 27 04:23:20.151: INFO: Pod "var-expansion-5db9c124-ed98-48ee-9393-03e24191397f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.067393132s
STEP: Saw pod success
Nov 27 04:23:20.151: INFO: Pod "var-expansion-5db9c124-ed98-48ee-9393-03e24191397f" satisfied condition "success or failure"
Nov 27 04:23:20.162: INFO: Trying to get logs from node slave1 pod var-expansion-5db9c124-ed98-48ee-9393-03e24191397f container dapi-container: <nil>
STEP: delete the pod
Nov 27 04:23:20.229: INFO: Waiting for pod var-expansion-5db9c124-ed98-48ee-9393-03e24191397f to disappear
Nov 27 04:23:20.237: INFO: Pod var-expansion-5db9c124-ed98-48ee-9393-03e24191397f no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:23:20.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6329" for this suite.
Nov 27 04:23:26.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:23:26.521: INFO: namespace var-expansion-6329 deletion completed in 6.272702769s

• [SLOW TEST:14.763 seconds]
[k8s.io] Variable Expansion
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:23:26.522: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4068
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 27 04:23:26.842: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6ed4d6d5-1bb0-49e1-af7a-0bc81966c64a" in namespace "projected-4068" to be "success or failure"
Nov 27 04:23:26.855: INFO: Pod "downwardapi-volume-6ed4d6d5-1bb0-49e1-af7a-0bc81966c64a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.476008ms
Nov 27 04:23:28.868: INFO: Pod "downwardapi-volume-6ed4d6d5-1bb0-49e1-af7a-0bc81966c64a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025239607s
Nov 27 04:23:30.879: INFO: Pod "downwardapi-volume-6ed4d6d5-1bb0-49e1-af7a-0bc81966c64a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036996091s
Nov 27 04:23:32.889: INFO: Pod "downwardapi-volume-6ed4d6d5-1bb0-49e1-af7a-0bc81966c64a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046709499s
STEP: Saw pod success
Nov 27 04:23:32.890: INFO: Pod "downwardapi-volume-6ed4d6d5-1bb0-49e1-af7a-0bc81966c64a" satisfied condition "success or failure"
Nov 27 04:23:32.900: INFO: Trying to get logs from node slave1 pod downwardapi-volume-6ed4d6d5-1bb0-49e1-af7a-0bc81966c64a container client-container: <nil>
STEP: delete the pod
Nov 27 04:23:32.967: INFO: Waiting for pod downwardapi-volume-6ed4d6d5-1bb0-49e1-af7a-0bc81966c64a to disappear
Nov 27 04:23:32.978: INFO: Pod downwardapi-volume-6ed4d6d5-1bb0-49e1-af7a-0bc81966c64a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:23:32.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4068" for this suite.
Nov 27 04:23:39.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:23:39.269: INFO: namespace projected-4068 deletion completed in 6.279368308s

• [SLOW TEST:12.747 seconds]
[sig-storage] Projected downwardAPI
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:23:39.271: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6455
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-88cf50ff-ab72-4f28-862d-5f4116cde075 in namespace container-probe-6455
Nov 27 04:23:45.612: INFO: Started pod busybox-88cf50ff-ab72-4f28-862d-5f4116cde075 in namespace container-probe-6455
STEP: checking the pod's current state and verifying that restartCount is present
Nov 27 04:23:45.618: INFO: Initial restart count of pod busybox-88cf50ff-ab72-4f28-862d-5f4116cde075 is 0
Nov 27 04:24:35.941: INFO: Restart count of pod container-probe-6455/busybox-88cf50ff-ab72-4f28-862d-5f4116cde075 is now 1 (50.323090081s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:24:35.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6455" for this suite.
Nov 27 04:24:42.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:24:42.266: INFO: namespace container-probe-6455 deletion completed in 6.280150045s

• [SLOW TEST:62.994 seconds]
[k8s.io] Probing container
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:24:42.267: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-970
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-8b8726c7-7dca-4641-a8b4-f708a40b8d1b
STEP: Creating secret with name s-test-opt-upd-5a1f0f79-bf99-4e07-90b9-4b6e907e821c
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-8b8726c7-7dca-4641-a8b4-f708a40b8d1b
STEP: Updating secret s-test-opt-upd-5a1f0f79-bf99-4e07-90b9-4b6e907e821c
STEP: Creating secret with name s-test-opt-create-743ba9ba-4438-4709-9203-43b53571aa53
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:26:20.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-970" for this suite.
Nov 27 04:26:48.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:26:48.760: INFO: namespace secrets-970 deletion completed in 28.25927086s

• [SLOW TEST:126.493 seconds]
[sig-storage] Secrets
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:26:48.760: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4477
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Nov 27 04:26:49.085: INFO: Waiting up to 5m0s for pod "downward-api-c7f84c0f-bdbd-42f5-b4e5-1a38db72c8d9" in namespace "downward-api-4477" to be "success or failure"
Nov 27 04:26:49.095: INFO: Pod "downward-api-c7f84c0f-bdbd-42f5-b4e5-1a38db72c8d9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.668308ms
Nov 27 04:26:51.105: INFO: Pod "downward-api-c7f84c0f-bdbd-42f5-b4e5-1a38db72c8d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02020012s
Nov 27 04:26:53.118: INFO: Pod "downward-api-c7f84c0f-bdbd-42f5-b4e5-1a38db72c8d9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033030208s
Nov 27 04:26:55.133: INFO: Pod "downward-api-c7f84c0f-bdbd-42f5-b4e5-1a38db72c8d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048043194s
STEP: Saw pod success
Nov 27 04:26:55.133: INFO: Pod "downward-api-c7f84c0f-bdbd-42f5-b4e5-1a38db72c8d9" satisfied condition "success or failure"
Nov 27 04:26:55.142: INFO: Trying to get logs from node slave1 pod downward-api-c7f84c0f-bdbd-42f5-b4e5-1a38db72c8d9 container dapi-container: <nil>
STEP: delete the pod
Nov 27 04:26:55.205: INFO: Waiting for pod downward-api-c7f84c0f-bdbd-42f5-b4e5-1a38db72c8d9 to disappear
Nov 27 04:26:55.215: INFO: Pod downward-api-c7f84c0f-bdbd-42f5-b4e5-1a38db72c8d9 no longer exists
[AfterEach] [sig-node] Downward API
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:26:55.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4477" for this suite.
Nov 27 04:27:01.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:27:01.485: INFO: namespace downward-api-4477 deletion completed in 6.257626837s

• [SLOW TEST:12.724 seconds]
[sig-node] Downward API
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:27:01.487: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1082
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Nov 27 04:27:02.348: INFO: created pod pod-service-account-defaultsa
Nov 27 04:27:02.348: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Nov 27 04:27:02.363: INFO: created pod pod-service-account-mountsa
Nov 27 04:27:02.363: INFO: pod pod-service-account-mountsa service account token volume mount: true
Nov 27 04:27:02.379: INFO: created pod pod-service-account-nomountsa
Nov 27 04:27:02.379: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Nov 27 04:27:02.397: INFO: created pod pod-service-account-defaultsa-mountspec
Nov 27 04:27:02.398: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Nov 27 04:27:02.416: INFO: created pod pod-service-account-mountsa-mountspec
Nov 27 04:27:02.416: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Nov 27 04:27:02.434: INFO: created pod pod-service-account-nomountsa-mountspec
Nov 27 04:27:02.434: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Nov 27 04:27:02.461: INFO: created pod pod-service-account-defaultsa-nomountspec
Nov 27 04:27:02.461: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Nov 27 04:27:02.492: INFO: created pod pod-service-account-mountsa-nomountspec
Nov 27 04:27:02.492: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Nov 27 04:27:02.507: INFO: created pod pod-service-account-nomountsa-nomountspec
Nov 27 04:27:02.507: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:27:02.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1082" for this suite.
Nov 27 04:28:14.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:28:14.865: INFO: namespace svcaccounts-1082 deletion completed in 1m12.338850067s

• [SLOW TEST:73.379 seconds]
[sig-auth] ServiceAccounts
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:28:14.868: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7057
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Nov 27 04:28:15.171: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 27 04:28:15.207: INFO: Waiting for terminating namespaces to be deleted...
Nov 27 04:28:15.216: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Nov 27 04:28:15.446: INFO: coredns-lxl9z from kube-system started at 2019-11-14 04:51:49 +0000 UTC (1 container statuses recorded)
Nov 27 04:28:15.446: INFO: 	Container coredns ready: true, restart count 6
Nov 27 04:28:15.446: INFO: calico-node-cppjk from kube-system started at 2019-11-20 07:23:24 +0000 UTC (1 container statuses recorded)
Nov 27 04:28:15.446: INFO: 	Container calico-node ready: true, restart count 5
Nov 27 04:28:15.447: INFO: e2e-conformance-test from conformance started at 2019-11-27 04:04:03 +0000 UTC (1 container statuses recorded)
Nov 27 04:28:15.447: INFO: 	Container conformance-container ready: true, restart count 0
Nov 27 04:28:15.447: INFO: kube-proxy-master1 from kube-system started at 2019-11-27 03:11:20 +0000 UTC (1 container statuses recorded)
Nov 27 04:28:15.447: INFO: 	Container kube-proxy ready: true, restart count 14
Nov 27 04:28:15.447: INFO: kube-scheduler-master1 from kube-system started at 2019-11-27 03:11:20 +0000 UTC (1 container statuses recorded)
Nov 27 04:28:15.447: INFO: 	Container kube-scheduler ready: true, restart count 64
Nov 27 04:28:15.448: INFO: resource-reserver-master1 from kube-system started at 2019-11-27 03:11:20 +0000 UTC (1 container statuses recorded)
Nov 27 04:28:15.448: INFO: 	Container sleep-forever ready: true, restart count 7
Nov 27 04:28:15.448: INFO: metrics-server-687c949fd7-pf9hm from kube-system started at 2019-11-22 00:26:19 +0000 UTC (1 container statuses recorded)
Nov 27 04:28:15.448: INFO: 	Container metrics-server ready: true, restart count 3
Nov 27 04:28:15.448: INFO: kube-apiserver-master1 from kube-system started at 2019-11-27 03:11:20 +0000 UTC (1 container statuses recorded)
Nov 27 04:28:15.448: INFO: 	Container kube-apiserver ready: true, restart count 6
Nov 27 04:28:15.448: INFO: kube-controller-manager-master1 from kube-system started at 2019-11-27 03:11:20 +0000 UTC (1 container statuses recorded)
Nov 27 04:28:15.448: INFO: 	Container kube-controller-manager ready: true, restart count 73
Nov 27 04:28:15.449: INFO: 
Logging pods the kubelet thinks is on node slave1 before test
Nov 27 04:28:15.477: INFO: nginx-proxy-slave1 from kube-system started at 2019-11-27 03:14:35 +0000 UTC (1 container statuses recorded)
Nov 27 04:28:15.477: INFO: 	Container nginx-proxy ready: true, restart count 15
Nov 27 04:28:15.478: INFO: dns-autoscaler-799d586fb4-mqr72 from kube-system started at 2019-11-08 02:43:45 +0000 UTC (1 container statuses recorded)
Nov 27 04:28:15.478: INFO: 	Container autoscaler ready: true, restart count 5
Nov 27 04:28:15.478: INFO: calico-kube-controllers-685dd5746c-8jjc9 from kube-system started at 2019-11-08 02:42:34 +0000 UTC (1 container statuses recorded)
Nov 27 04:28:15.478: INFO: 	Container calico-kube-controllers ready: true, restart count 5
Nov 27 04:28:15.478: INFO: calico-node-5nkgf from kube-system started at 2019-11-20 07:23:24 +0000 UTC (1 container statuses recorded)
Nov 27 04:28:15.478: INFO: 	Container calico-node ready: true, restart count 5
Nov 27 04:28:15.478: INFO: kube-proxy-slave1 from kube-system started at 2019-11-27 03:14:35 +0000 UTC (1 container statuses recorded)
Nov 27 04:28:15.478: INFO: 	Container kube-proxy ready: true, restart count 260
Nov 27 04:28:15.478: INFO: test-webserver-d1bd1012-16a8-4768-81ad-d85c5340fd42 from container-probe-4706 started at 2019-11-27 04:02:33 +0000 UTC (1 container statuses recorded)
Nov 27 04:28:15.478: INFO: 	Container test-webserver ready: false, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-6f5cfa1a-f3ee-4835-bbe2-4d91e4b4efee 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-6f5cfa1a-f3ee-4835-bbe2-4d91e4b4efee off the node slave1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-6f5cfa1a-f3ee-4835-bbe2-4d91e4b4efee
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:28:43.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7057" for this suite.
Nov 27 04:29:05.785: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:29:06.049: INFO: namespace sched-pred-7057 deletion completed in 22.291339433s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
I1127 04:29:06.049546      19 request.go:706] Error in request: resource name may not be empty

• [SLOW TEST:51.182 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:29:06.051: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-9039
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:29:06.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-9039" for this suite.
Nov 27 04:29:12.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:29:12.628: INFO: namespace tables-9039 deletion completed in 6.255209835s

• [SLOW TEST:6.578 seconds]
[sig-api-machinery] Servers with support for Table transformation
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:29:12.629: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1835
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-54c3d63c-b5b8-47fc-9319-cdf1bd93805e
STEP: Creating a pod to test consume configMaps
Nov 27 04:29:12.959: INFO: Waiting up to 5m0s for pod "pod-configmaps-16e23fc3-ce15-4ced-aca3-57bdbb171fa7" in namespace "configmap-1835" to be "success or failure"
Nov 27 04:29:12.990: INFO: Pod "pod-configmaps-16e23fc3-ce15-4ced-aca3-57bdbb171fa7": Phase="Pending", Reason="", readiness=false. Elapsed: 30.694032ms
Nov 27 04:29:15.000: INFO: Pod "pod-configmaps-16e23fc3-ce15-4ced-aca3-57bdbb171fa7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040434702s
Nov 27 04:29:17.009: INFO: Pod "pod-configmaps-16e23fc3-ce15-4ced-aca3-57bdbb171fa7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04918928s
Nov 27 04:29:19.021: INFO: Pod "pod-configmaps-16e23fc3-ce15-4ced-aca3-57bdbb171fa7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.061314271s
Nov 27 04:29:21.032: INFO: Pod "pod-configmaps-16e23fc3-ce15-4ced-aca3-57bdbb171fa7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.072087479s
STEP: Saw pod success
Nov 27 04:29:21.032: INFO: Pod "pod-configmaps-16e23fc3-ce15-4ced-aca3-57bdbb171fa7" satisfied condition "success or failure"
Nov 27 04:29:21.040: INFO: Trying to get logs from node slave1 pod pod-configmaps-16e23fc3-ce15-4ced-aca3-57bdbb171fa7 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 27 04:29:21.132: INFO: Waiting for pod pod-configmaps-16e23fc3-ce15-4ced-aca3-57bdbb171fa7 to disappear
Nov 27 04:29:21.144: INFO: Pod pod-configmaps-16e23fc3-ce15-4ced-aca3-57bdbb171fa7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:29:21.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1835" for this suite.
Nov 27 04:29:27.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:29:27.452: INFO: namespace configmap-1835 deletion completed in 6.295426971s

• [SLOW TEST:14.823 seconds]
[sig-storage] ConfigMap
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:29:27.454: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-1485
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-1485
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1485
STEP: Deleting pre-stop pod
Nov 27 04:29:42.894: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:29:42.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1485" for this suite.
Nov 27 04:30:28.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:30:29.687: INFO: namespace prestop-1485 deletion completed in 46.76177865s

• [SLOW TEST:62.235 seconds]
[k8s.io] [sig-node] PreStop
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:30:29.694: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8557
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-3ca415a6-40c7-44a7-a723-5341e3a1c4e2
STEP: Creating secret with name s-test-opt-upd-5ab3890a-8b35-45d9-ae2f-ad6f1974661d
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-3ca415a6-40c7-44a7-a723-5341e3a1c4e2
STEP: Updating secret s-test-opt-upd-5ab3890a-8b35-45d9-ae2f-ad6f1974661d
STEP: Creating secret with name s-test-opt-create-e59ce337-1029-4c78-afdb-8059a86e9665
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:30:38.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8557" for this suite.
Nov 27 04:30:50.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:30:50.759: INFO: namespace projected-8557 deletion completed in 12.269196542s

• [SLOW TEST:21.066 seconds]
[sig-storage] Projected secret
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:30:50.760: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5951
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-1e60f13e-43cc-4ad1-adc5-c27ceb03511d
STEP: Creating a pod to test consume configMaps
Nov 27 04:30:51.093: INFO: Waiting up to 5m0s for pod "pod-configmaps-c831b8f4-0a80-40aa-afa1-7458e6d00238" in namespace "configmap-5951" to be "success or failure"
Nov 27 04:30:51.103: INFO: Pod "pod-configmaps-c831b8f4-0a80-40aa-afa1-7458e6d00238": Phase="Pending", Reason="", readiness=false. Elapsed: 10.000751ms
Nov 27 04:30:53.113: INFO: Pod "pod-configmaps-c831b8f4-0a80-40aa-afa1-7458e6d00238": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020276268s
Nov 27 04:30:55.124: INFO: Pod "pod-configmaps-c831b8f4-0a80-40aa-afa1-7458e6d00238": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030621696s
Nov 27 04:30:57.133: INFO: Pod "pod-configmaps-c831b8f4-0a80-40aa-afa1-7458e6d00238": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039706764s
STEP: Saw pod success
Nov 27 04:30:57.133: INFO: Pod "pod-configmaps-c831b8f4-0a80-40aa-afa1-7458e6d00238" satisfied condition "success or failure"
Nov 27 04:30:57.139: INFO: Trying to get logs from node slave1 pod pod-configmaps-c831b8f4-0a80-40aa-afa1-7458e6d00238 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 27 04:30:57.199: INFO: Waiting for pod pod-configmaps-c831b8f4-0a80-40aa-afa1-7458e6d00238 to disappear
Nov 27 04:30:57.213: INFO: Pod pod-configmaps-c831b8f4-0a80-40aa-afa1-7458e6d00238 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:30:57.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5951" for this suite.
Nov 27 04:31:03.262: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:31:03.493: INFO: namespace configmap-5951 deletion completed in 6.262778354s

• [SLOW TEST:12.733 seconds]
[sig-storage] ConfigMap
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:31:03.498: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6743
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Nov 27 04:31:03.834: INFO: Waiting up to 5m0s for pod "client-containers-1e4bf2e2-8841-496a-b327-6f25ec33e045" in namespace "containers-6743" to be "success or failure"
Nov 27 04:31:03.847: INFO: Pod "client-containers-1e4bf2e2-8841-496a-b327-6f25ec33e045": Phase="Pending", Reason="", readiness=false. Elapsed: 11.438578ms
Nov 27 04:31:05.861: INFO: Pod "client-containers-1e4bf2e2-8841-496a-b327-6f25ec33e045": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025631088s
Nov 27 04:31:07.871: INFO: Pod "client-containers-1e4bf2e2-8841-496a-b327-6f25ec33e045": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035296959s
STEP: Saw pod success
Nov 27 04:31:07.871: INFO: Pod "client-containers-1e4bf2e2-8841-496a-b327-6f25ec33e045" satisfied condition "success or failure"
Nov 27 04:31:07.879: INFO: Trying to get logs from node master1 pod client-containers-1e4bf2e2-8841-496a-b327-6f25ec33e045 container test-container: <nil>
STEP: delete the pod
Nov 27 04:31:08.171: INFO: Waiting for pod client-containers-1e4bf2e2-8841-496a-b327-6f25ec33e045 to disappear
Nov 27 04:31:08.182: INFO: Pod client-containers-1e4bf2e2-8841-496a-b327-6f25ec33e045 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:31:08.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6743" for this suite.
Nov 27 04:31:14.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:31:14.480: INFO: namespace containers-6743 deletion completed in 6.284428394s

• [SLOW TEST:10.982 seconds]
[k8s.io] Docker Containers
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:31:14.481: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2557
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 04:31:14.786: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:31:24.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2557" for this suite.
Nov 27 04:31:31.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:31:31.380: INFO: namespace custom-resource-definition-2557 deletion completed in 6.393727269s

• [SLOW TEST:16.899 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:31:31.382: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1661
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1661.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-1661.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1661.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1661.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1661.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-1661.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1661.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-1661.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1661.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1661.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-1661.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1661.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-1661.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1661.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-1661.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1661.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-1661.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1661.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 27 04:31:37.815: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1661.svc.cluster.local from pod dns-1661/dns-test-5262e853-d269-483c-ab7a-7b3234c13789: the server could not find the requested resource (get pods dns-test-5262e853-d269-483c-ab7a-7b3234c13789)
Nov 27 04:31:37.830: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1661.svc.cluster.local from pod dns-1661/dns-test-5262e853-d269-483c-ab7a-7b3234c13789: the server could not find the requested resource (get pods dns-test-5262e853-d269-483c-ab7a-7b3234c13789)
Nov 27 04:31:37.844: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1661.svc.cluster.local from pod dns-1661/dns-test-5262e853-d269-483c-ab7a-7b3234c13789: the server could not find the requested resource (get pods dns-test-5262e853-d269-483c-ab7a-7b3234c13789)
Nov 27 04:31:37.860: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1661.svc.cluster.local from pod dns-1661/dns-test-5262e853-d269-483c-ab7a-7b3234c13789: the server could not find the requested resource (get pods dns-test-5262e853-d269-483c-ab7a-7b3234c13789)
Nov 27 04:31:37.918: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1661.svc.cluster.local from pod dns-1661/dns-test-5262e853-d269-483c-ab7a-7b3234c13789: the server could not find the requested resource (get pods dns-test-5262e853-d269-483c-ab7a-7b3234c13789)
Nov 27 04:31:37.930: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1661.svc.cluster.local from pod dns-1661/dns-test-5262e853-d269-483c-ab7a-7b3234c13789: the server could not find the requested resource (get pods dns-test-5262e853-d269-483c-ab7a-7b3234c13789)
Nov 27 04:31:37.951: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1661.svc.cluster.local from pod dns-1661/dns-test-5262e853-d269-483c-ab7a-7b3234c13789: the server could not find the requested resource (get pods dns-test-5262e853-d269-483c-ab7a-7b3234c13789)
Nov 27 04:31:37.967: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1661.svc.cluster.local from pod dns-1661/dns-test-5262e853-d269-483c-ab7a-7b3234c13789: the server could not find the requested resource (get pods dns-test-5262e853-d269-483c-ab7a-7b3234c13789)
Nov 27 04:31:37.996: INFO: Lookups using dns-1661/dns-test-5262e853-d269-483c-ab7a-7b3234c13789 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1661.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1661.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1661.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1661.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1661.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1661.svc.cluster.local jessie_udp@dns-test-service-2.dns-1661.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1661.svc.cluster.local]

Nov 27 04:31:43.135: INFO: DNS probes using dns-1661/dns-test-5262e853-d269-483c-ab7a-7b3234c13789 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:31:43.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1661" for this suite.
Nov 27 04:31:49.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:31:49.529: INFO: namespace dns-1661 deletion completed in 6.2859728s

• [SLOW TEST:18.148 seconds]
[sig-network] DNS
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:31:49.539: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1128
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Nov 27 04:31:56.412: INFO: Successfully updated pod "adopt-release-lq987"
STEP: Checking that the Job readopts the Pod
Nov 27 04:31:56.412: INFO: Waiting up to 15m0s for pod "adopt-release-lq987" in namespace "job-1128" to be "adopted"
Nov 27 04:31:56.422: INFO: Pod "adopt-release-lq987": Phase="Running", Reason="", readiness=true. Elapsed: 9.168658ms
Nov 27 04:31:58.432: INFO: Pod "adopt-release-lq987": Phase="Running", Reason="", readiness=true. Elapsed: 2.019713554s
Nov 27 04:31:58.432: INFO: Pod "adopt-release-lq987" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Nov 27 04:31:58.958: INFO: Successfully updated pod "adopt-release-lq987"
STEP: Checking that the Job releases the Pod
Nov 27 04:31:58.958: INFO: Waiting up to 15m0s for pod "adopt-release-lq987" in namespace "job-1128" to be "released"
Nov 27 04:31:58.966: INFO: Pod "adopt-release-lq987": Phase="Running", Reason="", readiness=true. Elapsed: 7.477585ms
Nov 27 04:32:00.975: INFO: Pod "adopt-release-lq987": Phase="Running", Reason="", readiness=true. Elapsed: 2.016463408s
Nov 27 04:32:00.975: INFO: Pod "adopt-release-lq987" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:32:00.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1128" for this suite.
Nov 27 04:32:47.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:32:47.262: INFO: namespace job-1128 deletion completed in 46.274682553s

• [SLOW TEST:57.724 seconds]
[sig-apps] Job
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:32:47.263: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4976
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 27 04:32:47.590: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4c9602c2-c4b0-400d-81be-e1d442195adb" in namespace "projected-4976" to be "success or failure"
Nov 27 04:32:47.597: INFO: Pod "downwardapi-volume-4c9602c2-c4b0-400d-81be-e1d442195adb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.569008ms
Nov 27 04:32:49.607: INFO: Pod "downwardapi-volume-4c9602c2-c4b0-400d-81be-e1d442195adb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016787543s
Nov 27 04:32:51.616: INFO: Pod "downwardapi-volume-4c9602c2-c4b0-400d-81be-e1d442195adb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025813944s
Nov 27 04:32:53.629: INFO: Pod "downwardapi-volume-4c9602c2-c4b0-400d-81be-e1d442195adb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038842405s
STEP: Saw pod success
Nov 27 04:32:53.629: INFO: Pod "downwardapi-volume-4c9602c2-c4b0-400d-81be-e1d442195adb" satisfied condition "success or failure"
Nov 27 04:32:53.637: INFO: Trying to get logs from node slave1 pod downwardapi-volume-4c9602c2-c4b0-400d-81be-e1d442195adb container client-container: <nil>
STEP: delete the pod
Nov 27 04:32:53.888: INFO: Waiting for pod downwardapi-volume-4c9602c2-c4b0-400d-81be-e1d442195adb to disappear
Nov 27 04:32:53.897: INFO: Pod downwardapi-volume-4c9602c2-c4b0-400d-81be-e1d442195adb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:32:53.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4976" for this suite.
Nov 27 04:32:59.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:33:00.184: INFO: namespace projected-4976 deletion completed in 6.276767964s

• [SLOW TEST:12.921 seconds]
[sig-storage] Projected downwardAPI
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:33:00.185: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-8229
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 04:33:00.514: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-233b76bb-1bb7-474a-bf3a-0a2c58264286" in namespace "security-context-test-8229" to be "success or failure"
Nov 27 04:33:00.530: INFO: Pod "alpine-nnp-false-233b76bb-1bb7-474a-bf3a-0a2c58264286": Phase="Pending", Reason="", readiness=false. Elapsed: 15.261705ms
Nov 27 04:33:02.541: INFO: Pod "alpine-nnp-false-233b76bb-1bb7-474a-bf3a-0a2c58264286": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026733049s
Nov 27 04:33:04.554: INFO: Pod "alpine-nnp-false-233b76bb-1bb7-474a-bf3a-0a2c58264286": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040044578s
Nov 27 04:33:04.554: INFO: Pod "alpine-nnp-false-233b76bb-1bb7-474a-bf3a-0a2c58264286" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:33:04.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8229" for this suite.
Nov 27 04:33:10.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:33:10.856: INFO: namespace security-context-test-8229 deletion completed in 6.256549884s

• [SLOW TEST:10.672 seconds]
[k8s.io] Security Context
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:33:10.860: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7055
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Nov 27 04:33:11.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 create -f - --namespace=kubectl-7055'
Nov 27 04:33:13.029: INFO: stderr: ""
Nov 27 04:33:13.029: INFO: stdout: "pod/pause created\n"
Nov 27 04:33:13.029: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Nov 27 04:33:13.030: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7055" to be "running and ready"
Nov 27 04:33:13.041: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 11.104666ms
Nov 27 04:33:15.057: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027687852s
Nov 27 04:33:17.071: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041400761s
Nov 27 04:33:19.080: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 6.050743963s
Nov 27 04:33:19.080: INFO: Pod "pause" satisfied condition "running and ready"
Nov 27 04:33:19.080: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Nov 27 04:33:19.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 label pods pause testing-label=testing-label-value --namespace=kubectl-7055'
Nov 27 04:33:19.816: INFO: stderr: ""
Nov 27 04:33:19.817: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Nov 27 04:33:19.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pod pause -L testing-label --namespace=kubectl-7055'
Nov 27 04:33:20.533: INFO: stderr: ""
Nov 27 04:33:20.533: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          8s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Nov 27 04:33:20.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 label pods pause testing-label- --namespace=kubectl-7055'
Nov 27 04:33:21.283: INFO: stderr: ""
Nov 27 04:33:21.283: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Nov 27 04:33:21.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pod pause -L testing-label --namespace=kubectl-7055'
Nov 27 04:33:21.997: INFO: stderr: ""
Nov 27 04:33:21.997: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          9s    \n"
[AfterEach] Kubectl label
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Nov 27 04:33:21.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 delete --grace-period=0 --force -f - --namespace=kubectl-7055'
Nov 27 04:33:22.742: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 27 04:33:22.742: INFO: stdout: "pod \"pause\" force deleted\n"
Nov 27 04:33:22.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get rc,svc -l name=pause --no-headers --namespace=kubectl-7055'
Nov 27 04:33:23.604: INFO: stderr: "No resources found in kubectl-7055 namespace.\n"
Nov 27 04:33:23.605: INFO: stdout: ""
Nov 27 04:33:23.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods -l name=pause --namespace=kubectl-7055 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 27 04:33:24.359: INFO: stderr: ""
Nov 27 04:33:24.359: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:33:24.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7055" for this suite.
Nov 27 04:33:30.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:33:30.718: INFO: namespace kubectl-7055 deletion completed in 6.3422764s

• [SLOW TEST:19.858 seconds]
[sig-cli] Kubectl client
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:33:30.720: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3672
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov 27 04:33:31.061: INFO: Waiting up to 5m0s for pod "pod-99c91883-92cf-4ae8-96ab-70c9b675d02c" in namespace "emptydir-3672" to be "success or failure"
Nov 27 04:33:31.072: INFO: Pod "pod-99c91883-92cf-4ae8-96ab-70c9b675d02c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.056266ms
Nov 27 04:33:33.087: INFO: Pod "pod-99c91883-92cf-4ae8-96ab-70c9b675d02c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025735223s
Nov 27 04:33:35.125: INFO: Pod "pod-99c91883-92cf-4ae8-96ab-70c9b675d02c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06325418s
STEP: Saw pod success
Nov 27 04:33:35.125: INFO: Pod "pod-99c91883-92cf-4ae8-96ab-70c9b675d02c" satisfied condition "success or failure"
Nov 27 04:33:35.133: INFO: Trying to get logs from node slave1 pod pod-99c91883-92cf-4ae8-96ab-70c9b675d02c container test-container: <nil>
STEP: delete the pod
Nov 27 04:33:35.192: INFO: Waiting for pod pod-99c91883-92cf-4ae8-96ab-70c9b675d02c to disappear
Nov 27 04:33:35.200: INFO: Pod pod-99c91883-92cf-4ae8-96ab-70c9b675d02c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:33:35.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3672" for this suite.
Nov 27 04:33:41.242: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:33:41.486: INFO: namespace emptydir-3672 deletion completed in 6.271994924s

• [SLOW TEST:10.766 seconds]
[sig-storage] EmptyDir volumes
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:33:41.487: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9825
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 27 04:34:07.066: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 27 04:34:09.101: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426047, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426047, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426047, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426047, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 04:34:11.112: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426047, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426047, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426047, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426047, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 27 04:34:14.145: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Nov 27 04:34:19.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 attach --namespace=webhook-9825 to-be-attached-pod -i -c=container1'
Nov 27 04:34:20.204: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:34:20.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9825" for this suite.
Nov 27 04:34:48.287: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:34:48.534: INFO: namespace webhook-9825 deletion completed in 28.286715051s
STEP: Destroying namespace "webhook-9825-markers" for this suite.
Nov 27 04:34:54.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:34:54.812: INFO: namespace webhook-9825-markers deletion completed in 6.277326768s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:73.371 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:34:54.858: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1928
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Nov 27 04:34:55.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 create -f - --namespace=kubectl-1928'
Nov 27 04:34:56.312: INFO: stderr: ""
Nov 27 04:34:56.312: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 27 04:34:56.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1928'
Nov 27 04:34:57.257: INFO: stderr: ""
Nov 27 04:34:57.259: INFO: stdout: "update-demo-nautilus-2mr5l update-demo-nautilus-zvvxb "
Nov 27 04:34:57.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods update-demo-nautilus-2mr5l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1928'
Nov 27 04:34:58.143: INFO: stderr: ""
Nov 27 04:34:58.144: INFO: stdout: ""
Nov 27 04:34:58.144: INFO: update-demo-nautilus-2mr5l is created but not running
Nov 27 04:35:03.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1928'
Nov 27 04:35:03.877: INFO: stderr: ""
Nov 27 04:35:03.877: INFO: stdout: "update-demo-nautilus-2mr5l update-demo-nautilus-zvvxb "
Nov 27 04:35:03.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods update-demo-nautilus-2mr5l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1928'
Nov 27 04:35:04.601: INFO: stderr: ""
Nov 27 04:35:04.601: INFO: stdout: "true"
Nov 27 04:35:04.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods update-demo-nautilus-2mr5l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1928'
Nov 27 04:35:05.332: INFO: stderr: ""
Nov 27 04:35:05.332: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 27 04:35:05.333: INFO: validating pod update-demo-nautilus-2mr5l
Nov 27 04:35:05.357: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 27 04:35:05.358: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 27 04:35:05.358: INFO: update-demo-nautilus-2mr5l is verified up and running
Nov 27 04:35:05.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods update-demo-nautilus-zvvxb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1928'
Nov 27 04:35:06.089: INFO: stderr: ""
Nov 27 04:35:06.090: INFO: stdout: "true"
Nov 27 04:35:06.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods update-demo-nautilus-zvvxb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1928'
Nov 27 04:35:06.836: INFO: stderr: ""
Nov 27 04:35:06.836: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 27 04:35:06.836: INFO: validating pod update-demo-nautilus-zvvxb
Nov 27 04:35:06.883: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 27 04:35:06.884: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 27 04:35:06.884: INFO: update-demo-nautilus-zvvxb is verified up and running
STEP: using delete to clean up resources
Nov 27 04:35:06.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 delete --grace-period=0 --force -f - --namespace=kubectl-1928'
Nov 27 04:35:07.590: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 27 04:35:07.591: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 27 04:35:07.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1928'
Nov 27 04:35:08.506: INFO: stderr: "No resources found in kubectl-1928 namespace.\n"
Nov 27 04:35:08.506: INFO: stdout: ""
Nov 27 04:35:08.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods -l name=update-demo --namespace=kubectl-1928 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 27 04:35:09.428: INFO: stderr: ""
Nov 27 04:35:09.428: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:35:09.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1928" for this suite.
Nov 27 04:35:15.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:35:15.737: INFO: namespace kubectl-1928 deletion completed in 6.291774469s

• [SLOW TEST:20.879 seconds]
[sig-cli] Kubectl client
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:35:15.740: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7210
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 04:35:16.057: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:35:22.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7210" for this suite.
Nov 27 04:36:08.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:36:08.971: INFO: namespace pods-7210 deletion completed in 46.276462122s

• [SLOW TEST:53.231 seconds]
[k8s.io] Pods
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:36:08.971: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4641
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 04:36:09.579: INFO: (0) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 260.780887ms)
Nov 27 04:36:09.602: INFO: (1) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 22.488014ms)
Nov 27 04:36:09.626: INFO: (2) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 23.916555ms)
Nov 27 04:36:09.646: INFO: (3) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 19.544401ms)
Nov 27 04:36:09.663: INFO: (4) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 17.565369ms)
Nov 27 04:36:09.682: INFO: (5) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 18.546085ms)
Nov 27 04:36:09.701: INFO: (6) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 18.956265ms)
Nov 27 04:36:09.717: INFO: (7) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 15.74425ms)
Nov 27 04:36:09.735: INFO: (8) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 17.937416ms)
Nov 27 04:36:09.751: INFO: (9) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 16.327497ms)
Nov 27 04:36:09.769: INFO: (10) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 17.510125ms)
Nov 27 04:36:09.785: INFO: (11) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 15.332115ms)
Nov 27 04:36:09.802: INFO: (12) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 16.861366ms)
Nov 27 04:36:09.822: INFO: (13) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 19.430489ms)
Nov 27 04:36:09.837: INFO: (14) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 15.408382ms)
Nov 27 04:36:09.853: INFO: (15) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 15.97794ms)
Nov 27 04:36:09.870: INFO: (16) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 16.433097ms)
Nov 27 04:36:09.885: INFO: (17) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 15.425493ms)
Nov 27 04:36:09.901: INFO: (18) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 15.204025ms)
Nov 27 04:36:09.919: INFO: (19) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 17.974082ms)
[AfterEach] version v1
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:36:09.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4641" for this suite.
Nov 27 04:36:15.967: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:36:16.245: INFO: namespace proxy-4641 deletion completed in 6.312599972s

• [SLOW TEST:7.273 seconds]
[sig-network] Proxy
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:36:16.246: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3551
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov 27 04:36:23.143: INFO: Successfully updated pod "pod-update-activedeadlineseconds-a9214b7a-3278-43ea-87b8-c401f794ef4d"
Nov 27 04:36:23.143: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-a9214b7a-3278-43ea-87b8-c401f794ef4d" in namespace "pods-3551" to be "terminated due to deadline exceeded"
Nov 27 04:36:23.153: INFO: Pod "pod-update-activedeadlineseconds-a9214b7a-3278-43ea-87b8-c401f794ef4d": Phase="Running", Reason="", readiness=true. Elapsed: 9.962624ms
Nov 27 04:36:25.167: INFO: Pod "pod-update-activedeadlineseconds-a9214b7a-3278-43ea-87b8-c401f794ef4d": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.023953074s
Nov 27 04:36:25.168: INFO: Pod "pod-update-activedeadlineseconds-a9214b7a-3278-43ea-87b8-c401f794ef4d" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:36:25.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3551" for this suite.
Nov 27 04:36:31.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:36:31.505: INFO: namespace pods-3551 deletion completed in 6.324528915s

• [SLOW TEST:15.259 seconds]
[k8s.io] Pods
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:36:31.506: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-121
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Nov 27 04:36:38.641: INFO: Successfully updated pod "labelsupdate8ce2ffee-2af3-452b-8bf9-e749957ac48b"
[AfterEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:36:40.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-121" for this suite.
Nov 27 04:37:08.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:37:08.997: INFO: namespace downward-api-121 deletion completed in 28.265141689s

• [SLOW TEST:37.491 seconds]
[sig-storage] Downward API volume
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:37:08.998: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1186
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 27 04:37:09.351: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0457eda6-dc8a-4393-9016-3f170931841d" in namespace "downward-api-1186" to be "success or failure"
Nov 27 04:37:09.371: INFO: Pod "downwardapi-volume-0457eda6-dc8a-4393-9016-3f170931841d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.761468ms
Nov 27 04:37:11.381: INFO: Pod "downwardapi-volume-0457eda6-dc8a-4393-9016-3f170931841d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029522032s
Nov 27 04:37:13.392: INFO: Pod "downwardapi-volume-0457eda6-dc8a-4393-9016-3f170931841d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040185579s
Nov 27 04:37:15.402: INFO: Pod "downwardapi-volume-0457eda6-dc8a-4393-9016-3f170931841d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050318678s
STEP: Saw pod success
Nov 27 04:37:15.402: INFO: Pod "downwardapi-volume-0457eda6-dc8a-4393-9016-3f170931841d" satisfied condition "success or failure"
Nov 27 04:37:15.414: INFO: Trying to get logs from node slave1 pod downwardapi-volume-0457eda6-dc8a-4393-9016-3f170931841d container client-container: <nil>
STEP: delete the pod
Nov 27 04:37:15.478: INFO: Waiting for pod downwardapi-volume-0457eda6-dc8a-4393-9016-3f170931841d to disappear
Nov 27 04:37:15.486: INFO: Pod downwardapi-volume-0457eda6-dc8a-4393-9016-3f170931841d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:37:15.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1186" for this suite.
Nov 27 04:37:21.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:37:21.756: INFO: namespace downward-api-1186 deletion completed in 6.253244676s

• [SLOW TEST:12.759 seconds]
[sig-storage] Downward API volume
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:37:21.760: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2763
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 27 04:37:53.999: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Nov 27 04:37:56.033: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426274, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426274, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426274, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426274, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 04:37:58.043: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426274, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426274, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426274, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426274, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 27 04:38:01.073: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:38:01.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2763" for this suite.
Nov 27 04:38:13.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:38:13.626: INFO: namespace webhook-2763 deletion completed in 12.272520723s
STEP: Destroying namespace "webhook-2763-markers" for this suite.
Nov 27 04:38:19.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:38:19.909: INFO: namespace webhook-2763-markers deletion completed in 6.283052858s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:58.188 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:38:19.950: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6481
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Nov 27 04:38:24.879: INFO: Successfully updated pod "annotationupdatebea89014-2e71-4c35-8812-ff91e9f248a6"
[AfterEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:38:28.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6481" for this suite.
Nov 27 04:38:57.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:38:57.311: INFO: namespace projected-6481 deletion completed in 28.310062473s

• [SLOW TEST:37.361 seconds]
[sig-storage] Projected downwardAPI
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:38:57.317: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1808
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1808.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1808.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1808.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1808.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1808.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1808.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1808.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1808.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1808.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1808.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1808.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1808.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1808.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 229.7.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.7.229_udp@PTR;check="$$(dig +tcp +noall +answer +search 229.7.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.7.229_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1808.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1808.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1808.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1808.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1808.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1808.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1808.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1808.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1808.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1808.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1808.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1808.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1808.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 229.7.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.7.229_udp@PTR;check="$$(dig +tcp +noall +answer +search 229.7.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.7.229_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 27 04:39:01.801: INFO: Unable to read wheezy_udp@dns-test-service.dns-1808.svc.cluster.local from pod dns-1808/dns-test-d4533937-37ec-420c-9f47-297027a7cbf4: the server could not find the requested resource (get pods dns-test-d4533937-37ec-420c-9f47-297027a7cbf4)
Nov 27 04:39:01.820: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1808.svc.cluster.local from pod dns-1808/dns-test-d4533937-37ec-420c-9f47-297027a7cbf4: the server could not find the requested resource (get pods dns-test-d4533937-37ec-420c-9f47-297027a7cbf4)
Nov 27 04:39:01.838: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1808.svc.cluster.local from pod dns-1808/dns-test-d4533937-37ec-420c-9f47-297027a7cbf4: the server could not find the requested resource (get pods dns-test-d4533937-37ec-420c-9f47-297027a7cbf4)
Nov 27 04:39:01.853: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1808.svc.cluster.local from pod dns-1808/dns-test-d4533937-37ec-420c-9f47-297027a7cbf4: the server could not find the requested resource (get pods dns-test-d4533937-37ec-420c-9f47-297027a7cbf4)
Nov 27 04:39:01.955: INFO: Unable to read jessie_udp@dns-test-service.dns-1808.svc.cluster.local from pod dns-1808/dns-test-d4533937-37ec-420c-9f47-297027a7cbf4: the server could not find the requested resource (get pods dns-test-d4533937-37ec-420c-9f47-297027a7cbf4)
Nov 27 04:39:01.967: INFO: Unable to read jessie_tcp@dns-test-service.dns-1808.svc.cluster.local from pod dns-1808/dns-test-d4533937-37ec-420c-9f47-297027a7cbf4: the server could not find the requested resource (get pods dns-test-d4533937-37ec-420c-9f47-297027a7cbf4)
Nov 27 04:39:01.981: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1808.svc.cluster.local from pod dns-1808/dns-test-d4533937-37ec-420c-9f47-297027a7cbf4: the server could not find the requested resource (get pods dns-test-d4533937-37ec-420c-9f47-297027a7cbf4)
Nov 27 04:39:01.996: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1808.svc.cluster.local from pod dns-1808/dns-test-d4533937-37ec-420c-9f47-297027a7cbf4: the server could not find the requested resource (get pods dns-test-d4533937-37ec-420c-9f47-297027a7cbf4)
Nov 27 04:39:02.008: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-1808.svc.cluster.local from pod dns-1808/dns-test-d4533937-37ec-420c-9f47-297027a7cbf4: the server could not find the requested resource (get pods dns-test-d4533937-37ec-420c-9f47-297027a7cbf4)
Nov 27 04:39:02.026: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-1808.svc.cluster.local from pod dns-1808/dns-test-d4533937-37ec-420c-9f47-297027a7cbf4: the server could not find the requested resource (get pods dns-test-d4533937-37ec-420c-9f47-297027a7cbf4)
Nov 27 04:39:02.040: INFO: Unable to read jessie_udp@PodARecord from pod dns-1808/dns-test-d4533937-37ec-420c-9f47-297027a7cbf4: the server could not find the requested resource (get pods dns-test-d4533937-37ec-420c-9f47-297027a7cbf4)
Nov 27 04:39:02.066: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1808/dns-test-d4533937-37ec-420c-9f47-297027a7cbf4: the server could not find the requested resource (get pods dns-test-d4533937-37ec-420c-9f47-297027a7cbf4)
Nov 27 04:39:02.125: INFO: Lookups using dns-1808/dns-test-d4533937-37ec-420c-9f47-297027a7cbf4 failed for: [wheezy_udp@dns-test-service.dns-1808.svc.cluster.local wheezy_tcp@dns-test-service.dns-1808.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1808.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1808.svc.cluster.local jessie_udp@dns-test-service.dns-1808.svc.cluster.local jessie_tcp@dns-test-service.dns-1808.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1808.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1808.svc.cluster.local jessie_udp@_http._tcp.test-service-2.dns-1808.svc.cluster.local jessie_tcp@_http._tcp.test-service-2.dns-1808.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Nov 27 04:39:07.138: INFO: Unable to read wheezy_udp@dns-test-service.dns-1808.svc.cluster.local from pod dns-1808/dns-test-d4533937-37ec-420c-9f47-297027a7cbf4: the server could not find the requested resource (get pods dns-test-d4533937-37ec-420c-9f47-297027a7cbf4)
Nov 27 04:39:07.150: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1808.svc.cluster.local from pod dns-1808/dns-test-d4533937-37ec-420c-9f47-297027a7cbf4: the server could not find the requested resource (get pods dns-test-d4533937-37ec-420c-9f47-297027a7cbf4)
Nov 27 04:39:07.161: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1808.svc.cluster.local from pod dns-1808/dns-test-d4533937-37ec-420c-9f47-297027a7cbf4: the server could not find the requested resource (get pods dns-test-d4533937-37ec-420c-9f47-297027a7cbf4)
Nov 27 04:39:07.172: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1808.svc.cluster.local from pod dns-1808/dns-test-d4533937-37ec-420c-9f47-297027a7cbf4: the server could not find the requested resource (get pods dns-test-d4533937-37ec-420c-9f47-297027a7cbf4)
Nov 27 04:39:07.256: INFO: Unable to read jessie_udp@dns-test-service.dns-1808.svc.cluster.local from pod dns-1808/dns-test-d4533937-37ec-420c-9f47-297027a7cbf4: the server could not find the requested resource (get pods dns-test-d4533937-37ec-420c-9f47-297027a7cbf4)
Nov 27 04:39:07.265: INFO: Unable to read jessie_tcp@dns-test-service.dns-1808.svc.cluster.local from pod dns-1808/dns-test-d4533937-37ec-420c-9f47-297027a7cbf4: the server could not find the requested resource (get pods dns-test-d4533937-37ec-420c-9f47-297027a7cbf4)
Nov 27 04:39:07.276: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1808.svc.cluster.local from pod dns-1808/dns-test-d4533937-37ec-420c-9f47-297027a7cbf4: the server could not find the requested resource (get pods dns-test-d4533937-37ec-420c-9f47-297027a7cbf4)
Nov 27 04:39:07.289: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1808.svc.cluster.local from pod dns-1808/dns-test-d4533937-37ec-420c-9f47-297027a7cbf4: the server could not find the requested resource (get pods dns-test-d4533937-37ec-420c-9f47-297027a7cbf4)
Nov 27 04:39:07.359: INFO: Lookups using dns-1808/dns-test-d4533937-37ec-420c-9f47-297027a7cbf4 failed for: [wheezy_udp@dns-test-service.dns-1808.svc.cluster.local wheezy_tcp@dns-test-service.dns-1808.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1808.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1808.svc.cluster.local jessie_udp@dns-test-service.dns-1808.svc.cluster.local jessie_tcp@dns-test-service.dns-1808.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1808.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1808.svc.cluster.local]

Nov 27 04:39:12.418: INFO: DNS probes using dns-1808/dns-test-d4533937-37ec-420c-9f47-297027a7cbf4 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:39:12.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1808" for this suite.
Nov 27 04:39:18.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:39:18.928: INFO: namespace dns-1808 deletion completed in 6.302181701s

• [SLOW TEST:21.611 seconds]
[sig-network] DNS
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:39:18.930: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1735
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 04:39:19.228: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Nov 27 04:39:19.251: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 27 04:39:24.267: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 27 04:39:24.267: INFO: Creating deployment "test-rolling-update-deployment"
Nov 27 04:39:24.283: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Nov 27 04:39:24.303: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Nov 27 04:39:26.324: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Nov 27 04:39:26.331: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426364, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426364, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426364, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426364, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 04:39:28.340: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Nov 27 04:39:28.373: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1735 /apis/apps/v1/namespaces/deployment-1735/deployments/test-rolling-update-deployment 9edf1349-45c9-4156-84d3-658506e6bd0e 949080 1 2019-11-27 04:39:24 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005a84ea8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-11-27 04:39:24 +0000 UTC,LastTransitionTime:2019-11-27 04:39:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2019-11-27 04:39:28 +0000 UTC,LastTransitionTime:2019-11-27 04:39:24 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 27 04:39:28.382: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-1735 /apis/apps/v1/namespaces/deployment-1735/replicasets/test-rolling-update-deployment-55d946486 e542a7ca-8905-47aa-a3a3-f1a1d55b7659 949069 1 2019-11-27 04:39:24 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 9edf1349-45c9-4156-84d3-658506e6bd0e 0xc004024780 0xc004024781}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0040247e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 27 04:39:28.382: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Nov 27 04:39:28.382: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1735 /apis/apps/v1/namespaces/deployment-1735/replicasets/test-rolling-update-controller c521ecaa-e245-4af1-ad97-67b37b5c45ef 949079 2 2019-11-27 04:39:19 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 9edf1349-45c9-4156-84d3-658506e6bd0e 0xc0040246b7 0xc0040246b8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004024718 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 27 04:39:28.392: INFO: Pod "test-rolling-update-deployment-55d946486-gjw6n" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-gjw6n test-rolling-update-deployment-55d946486- deployment-1735 /api/v1/namespaces/deployment-1735/pods/test-rolling-update-deployment-55d946486-gjw6n d137b6d4-8e83-45ef-bd7d-3a9ab322f027 949068 0 2019-11-27 04:39:24 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 e542a7ca-8905-47aa-a3a3-f1a1d55b7659 0xc0028c2af0 0xc0028c2af1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jnm7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jnm7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jnm7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:39:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:39:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:39:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:39:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.17,PodIP:172.31.51.41,StartTime:2019-11-27 04:39:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-11-27 04:39:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker://sha256:0819d392499b83e17f70aecae0173f2bd403a637c12598d5e0cb23a04f7490f3,ContainerID:docker://7804a6932a88488cb88f6bd9d44963d0d3df711aa7fcdeaeffa03ec951f7c491,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.31.51.41,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:39:28.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1735" for this suite.
Nov 27 04:39:34.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:39:34.660: INFO: namespace deployment-1735 deletion completed in 6.257300518s

• [SLOW TEST:15.731 seconds]
[sig-apps] Deployment
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:39:34.662: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7806
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-7806
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7806
STEP: creating replication controller externalsvc in namespace services-7806
I1127 04:39:35.046971      19 runners.go:184] Created replication controller with name: externalsvc, namespace: services-7806, replica count: 2
I1127 04:39:35.048475      19 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 04:39:35.048586      19 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 04:39:38.099026      19 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1127 04:39:41.099701      19 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Nov 27 04:39:41.165: INFO: Creating new exec pod
Nov 27 04:39:45.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=services-7806 execpodjdcnp -- /bin/sh -x -c nslookup nodeport-service'
Nov 27 04:39:46.683: INFO: stderr: "+ nslookup nodeport-service\n"
Nov 27 04:39:46.683: INFO: stdout: "Server:\t\t172.30.0.3\nAddress:\t172.30.0.3#53\n\nnodeport-service.services-7806.svc.cluster.local\tcanonical name = externalsvc.services-7806.svc.cluster.local.\nName:\texternalsvc.services-7806.svc.cluster.local\nAddress: 172.30.238.230\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7806, will wait for the garbage collector to delete the pods
I1127 04:39:46.698750      19 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 04:39:46.699015      19 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Nov 27 04:39:46.767: INFO: Deleting ReplicationController externalsvc took: 19.06431ms
I1127 04:39:47.268322      19 controller_utils.go:810] Ignoring inactive pod services-7806/externalsvc-jnhzk in state Running, deletion time 2019-11-27 04:39:48 +0000 UTC
I1127 04:39:47.268460      19 controller_utils.go:810] Ignoring inactive pod services-7806/externalsvc-5hdjb in state Running, deletion time 2019-11-27 04:39:48 +0000 UTC
Nov 27 04:39:47.268: INFO: Terminating ReplicationController externalsvc pods took: 500.719855ms
Nov 27 04:40:03.725: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:40:03.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7806" for this suite.
Nov 27 04:40:09.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:40:10.060: INFO: namespace services-7806 deletion completed in 6.291849477s
[AfterEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:35.398 seconds]
[sig-network] Services
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:40:10.061: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3542
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Nov 27 04:40:10.404: INFO: Waiting up to 5m0s for pod "client-containers-18cf44ca-46d4-4353-b1e9-d3fc1a212313" in namespace "containers-3542" to be "success or failure"
Nov 27 04:40:10.413: INFO: Pod "client-containers-18cf44ca-46d4-4353-b1e9-d3fc1a212313": Phase="Pending", Reason="", readiness=false. Elapsed: 8.867508ms
Nov 27 04:40:12.424: INFO: Pod "client-containers-18cf44ca-46d4-4353-b1e9-d3fc1a212313": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020478525s
Nov 27 04:40:14.438: INFO: Pod "client-containers-18cf44ca-46d4-4353-b1e9-d3fc1a212313": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033670616s
Nov 27 04:40:16.448: INFO: Pod "client-containers-18cf44ca-46d4-4353-b1e9-d3fc1a212313": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043649715s
STEP: Saw pod success
Nov 27 04:40:16.448: INFO: Pod "client-containers-18cf44ca-46d4-4353-b1e9-d3fc1a212313" satisfied condition "success or failure"
Nov 27 04:40:16.455: INFO: Trying to get logs from node master1 pod client-containers-18cf44ca-46d4-4353-b1e9-d3fc1a212313 container test-container: <nil>
STEP: delete the pod
Nov 27 04:40:16.722: INFO: Waiting for pod client-containers-18cf44ca-46d4-4353-b1e9-d3fc1a212313 to disappear
Nov 27 04:40:16.732: INFO: Pod client-containers-18cf44ca-46d4-4353-b1e9-d3fc1a212313 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:40:16.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3542" for this suite.
Nov 27 04:40:22.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:40:23.060: INFO: namespace containers-3542 deletion completed in 6.303789176s

• [SLOW TEST:12.999 seconds]
[k8s.io] Docker Containers
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:40:23.060: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2481
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2481.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2481.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2481.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2481.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2481.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2481.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 27 04:40:27.533: INFO: DNS probes using dns-2481/dns-test-4b9b6011-b8dd-4391-9ce6-5b3e194fe265 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:40:27.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2481" for this suite.
Nov 27 04:40:33.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:40:33.887: INFO: namespace dns-2481 deletion completed in 6.309038888s

• [SLOW TEST:10.826 seconds]
[sig-network] DNS
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:40:33.887: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3660
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Nov 27 04:40:34.288: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3660 /api/v1/namespaces/watch-3660/configmaps/e2e-watch-test-resource-version 94ab4aa1-bcfe-4483-9840-daebf61df025 949453 0 2019-11-27 04:40:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov 27 04:40:34.289: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3660 /api/v1/namespaces/watch-3660/configmaps/e2e-watch-test-resource-version 94ab4aa1-bcfe-4483-9840-daebf61df025 949454 0 2019-11-27 04:40:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:40:34.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3660" for this suite.
Nov 27 04:40:40.391: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:40:40.840: INFO: namespace watch-3660 deletion completed in 6.532843961s

• [SLOW TEST:6.952 seconds]
[sig-api-machinery] Watchers
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:40:40.841: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6035
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-c8008df2-0a5b-48d8-95ee-26a12f91607d in namespace container-probe-6035
Nov 27 04:40:47.196: INFO: Started pod test-webserver-c8008df2-0a5b-48d8-95ee-26a12f91607d in namespace container-probe-6035
STEP: checking the pod's current state and verifying that restartCount is present
Nov 27 04:40:47.204: INFO: Initial restart count of pod test-webserver-c8008df2-0a5b-48d8-95ee-26a12f91607d is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:44:48.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6035" for this suite.
Nov 27 04:44:54.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:44:54.822: INFO: namespace container-probe-6035 deletion completed in 6.285315935s

• [SLOW TEST:253.981 seconds]
[k8s.io] Probing container
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:44:54.823: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9608
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-ffe7bf93-c6a8-476a-a78a-39a7579e2dfc in namespace container-probe-9608
Nov 27 04:44:59.180: INFO: Started pod liveness-ffe7bf93-c6a8-476a-a78a-39a7579e2dfc in namespace container-probe-9608
STEP: checking the pod's current state and verifying that restartCount is present
Nov 27 04:44:59.187: INFO: Initial restart count of pod liveness-ffe7bf93-c6a8-476a-a78a-39a7579e2dfc is 0
Nov 27 04:45:15.278: INFO: Restart count of pod container-probe-9608/liveness-ffe7bf93-c6a8-476a-a78a-39a7579e2dfc is now 1 (16.090984395s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:45:15.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9608" for this suite.
Nov 27 04:45:21.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:45:21.604: INFO: namespace container-probe-9608 deletion completed in 6.282490633s

• [SLOW TEST:26.781 seconds]
[k8s.io] Probing container
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:45:21.604: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6814
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:45:27.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6814" for this suite.
Nov 27 04:45:33.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:45:33.486: INFO: namespace watch-6814 deletion completed in 6.319269425s

• [SLOW TEST:11.881 seconds]
[sig-api-machinery] Watchers
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:45:33.488: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2549
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 04:45:55.841: INFO: Container started at 2019-11-27 04:45:37 +0000 UTC, pod became ready at 2019-11-27 04:45:55 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:45:55.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2549" for this suite.
Nov 27 04:46:07.884: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:46:08.120: INFO: namespace container-probe-2549 deletion completed in 12.267215765s

• [SLOW TEST:34.632 seconds]
[k8s.io] Probing container
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:46:08.121: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8726
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov 27 04:46:08.446: INFO: Waiting up to 5m0s for pod "pod-515e841e-266e-4370-93d8-7e001a044119" in namespace "emptydir-8726" to be "success or failure"
Nov 27 04:46:08.457: INFO: Pod "pod-515e841e-266e-4370-93d8-7e001a044119": Phase="Pending", Reason="", readiness=false. Elapsed: 10.095602ms
Nov 27 04:46:10.467: INFO: Pod "pod-515e841e-266e-4370-93d8-7e001a044119": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020365591s
Nov 27 04:46:12.482: INFO: Pod "pod-515e841e-266e-4370-93d8-7e001a044119": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03533529s
Nov 27 04:46:14.495: INFO: Pod "pod-515e841e-266e-4370-93d8-7e001a044119": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048087379s
STEP: Saw pod success
Nov 27 04:46:14.495: INFO: Pod "pod-515e841e-266e-4370-93d8-7e001a044119" satisfied condition "success or failure"
Nov 27 04:46:14.503: INFO: Trying to get logs from node slave1 pod pod-515e841e-266e-4370-93d8-7e001a044119 container test-container: <nil>
STEP: delete the pod
Nov 27 04:46:14.773: INFO: Waiting for pod pod-515e841e-266e-4370-93d8-7e001a044119 to disappear
Nov 27 04:46:14.780: INFO: Pod pod-515e841e-266e-4370-93d8-7e001a044119 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:46:14.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8726" for this suite.
Nov 27 04:46:20.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:46:21.041: INFO: namespace emptydir-8726 deletion completed in 6.249659638s

• [SLOW TEST:12.921 seconds]
[sig-storage] EmptyDir volumes
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:46:21.042: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5469
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 27 04:46:45.447: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 27 04:46:47.481: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426805, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426805, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426805, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426805, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 04:46:49.491: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426805, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426805, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426805, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710426805, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 27 04:46:52.520: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:46:52.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5469" for this suite.
Nov 27 04:46:58.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:46:58.950: INFO: namespace webhook-5469 deletion completed in 6.400260108s
STEP: Destroying namespace "webhook-5469-markers" for this suite.
Nov 27 04:47:04.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:47:05.221: INFO: namespace webhook-5469-markers deletion completed in 6.271401604s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:44.215 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:47:05.261: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8989
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-h5hq
STEP: Creating a pod to test atomic-volume-subpath
Nov 27 04:47:05.604: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-h5hq" in namespace "subpath-8989" to be "success or failure"
Nov 27 04:47:05.613: INFO: Pod "pod-subpath-test-configmap-h5hq": Phase="Pending", Reason="", readiness=false. Elapsed: 8.538973ms
Nov 27 04:47:07.627: INFO: Pod "pod-subpath-test-configmap-h5hq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022238533s
Nov 27 04:47:09.638: INFO: Pod "pod-subpath-test-configmap-h5hq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033892083s
Nov 27 04:47:11.653: INFO: Pod "pod-subpath-test-configmap-h5hq": Phase="Running", Reason="", readiness=true. Elapsed: 6.048294669s
Nov 27 04:47:13.665: INFO: Pod "pod-subpath-test-configmap-h5hq": Phase="Running", Reason="", readiness=true. Elapsed: 8.060725068s
Nov 27 04:47:15.677: INFO: Pod "pod-subpath-test-configmap-h5hq": Phase="Running", Reason="", readiness=true. Elapsed: 10.072990221s
Nov 27 04:47:17.692: INFO: Pod "pod-subpath-test-configmap-h5hq": Phase="Running", Reason="", readiness=true. Elapsed: 12.087356629s
Nov 27 04:47:19.702: INFO: Pod "pod-subpath-test-configmap-h5hq": Phase="Running", Reason="", readiness=true. Elapsed: 14.097278527s
Nov 27 04:47:21.713: INFO: Pod "pod-subpath-test-configmap-h5hq": Phase="Running", Reason="", readiness=true. Elapsed: 16.108530698s
Nov 27 04:47:23.724: INFO: Pod "pod-subpath-test-configmap-h5hq": Phase="Running", Reason="", readiness=true. Elapsed: 18.119939803s
Nov 27 04:47:25.735: INFO: Pod "pod-subpath-test-configmap-h5hq": Phase="Running", Reason="", readiness=true. Elapsed: 20.130999351s
Nov 27 04:47:27.747: INFO: Pod "pod-subpath-test-configmap-h5hq": Phase="Running", Reason="", readiness=true. Elapsed: 22.143048415s
Nov 27 04:47:29.756: INFO: Pod "pod-subpath-test-configmap-h5hq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.152143598s
STEP: Saw pod success
Nov 27 04:47:29.757: INFO: Pod "pod-subpath-test-configmap-h5hq" satisfied condition "success or failure"
Nov 27 04:47:29.765: INFO: Trying to get logs from node slave1 pod pod-subpath-test-configmap-h5hq container test-container-subpath-configmap-h5hq: <nil>
STEP: delete the pod
Nov 27 04:47:29.850: INFO: Waiting for pod pod-subpath-test-configmap-h5hq to disappear
Nov 27 04:47:29.857: INFO: Pod pod-subpath-test-configmap-h5hq no longer exists
STEP: Deleting pod pod-subpath-test-configmap-h5hq
Nov 27 04:47:29.857: INFO: Deleting pod "pod-subpath-test-configmap-h5hq" in namespace "subpath-8989"
[AfterEach] [sig-storage] Subpath
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:47:29.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8989" for this suite.
Nov 27 04:47:35.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:47:36.168: INFO: namespace subpath-8989 deletion completed in 6.286213051s

• [SLOW TEST:30.906 seconds]
[sig-storage] Subpath
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:47:36.170: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3953
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-afd7d57c-6ebd-4279-bf13-d1e03b897b09
STEP: Creating a pod to test consume configMaps
Nov 27 04:47:36.514: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a1762a4c-51f3-455b-ab58-fc6220f24f4e" in namespace "projected-3953" to be "success or failure"
Nov 27 04:47:36.523: INFO: Pod "pod-projected-configmaps-a1762a4c-51f3-455b-ab58-fc6220f24f4e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.381821ms
Nov 27 04:47:38.535: INFO: Pod "pod-projected-configmaps-a1762a4c-51f3-455b-ab58-fc6220f24f4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021166395s
Nov 27 04:47:40.545: INFO: Pod "pod-projected-configmaps-a1762a4c-51f3-455b-ab58-fc6220f24f4e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03098367s
Nov 27 04:47:42.556: INFO: Pod "pod-projected-configmaps-a1762a4c-51f3-455b-ab58-fc6220f24f4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041815884s
STEP: Saw pod success
Nov 27 04:47:42.556: INFO: Pod "pod-projected-configmaps-a1762a4c-51f3-455b-ab58-fc6220f24f4e" satisfied condition "success or failure"
Nov 27 04:47:42.566: INFO: Trying to get logs from node slave1 pod pod-projected-configmaps-a1762a4c-51f3-455b-ab58-fc6220f24f4e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 27 04:47:42.639: INFO: Waiting for pod pod-projected-configmaps-a1762a4c-51f3-455b-ab58-fc6220f24f4e to disappear
Nov 27 04:47:42.646: INFO: Pod pod-projected-configmaps-a1762a4c-51f3-455b-ab58-fc6220f24f4e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:47:42.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3953" for this suite.
Nov 27 04:47:48.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:47:48.918: INFO: namespace projected-3953 deletion completed in 6.261437025s

• [SLOW TEST:12.748 seconds]
[sig-storage] Projected configMap
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:47:48.921: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-473
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov 27 04:47:49.261: INFO: Waiting up to 5m0s for pod "pod-a73d8597-fdc7-44eb-9613-c2f71e9a1567" in namespace "emptydir-473" to be "success or failure"
Nov 27 04:47:49.269: INFO: Pod "pod-a73d8597-fdc7-44eb-9613-c2f71e9a1567": Phase="Pending", Reason="", readiness=false. Elapsed: 7.761147ms
Nov 27 04:47:51.278: INFO: Pod "pod-a73d8597-fdc7-44eb-9613-c2f71e9a1567": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016706729s
Nov 27 04:47:53.286: INFO: Pod "pod-a73d8597-fdc7-44eb-9613-c2f71e9a1567": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025310577s
STEP: Saw pod success
Nov 27 04:47:53.287: INFO: Pod "pod-a73d8597-fdc7-44eb-9613-c2f71e9a1567" satisfied condition "success or failure"
Nov 27 04:47:53.294: INFO: Trying to get logs from node slave1 pod pod-a73d8597-fdc7-44eb-9613-c2f71e9a1567 container test-container: <nil>
STEP: delete the pod
Nov 27 04:47:53.355: INFO: Waiting for pod pod-a73d8597-fdc7-44eb-9613-c2f71e9a1567 to disappear
Nov 27 04:47:53.362: INFO: Pod pod-a73d8597-fdc7-44eb-9613-c2f71e9a1567 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:47:53.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-473" for this suite.
Nov 27 04:47:59.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:47:59.666: INFO: namespace emptydir-473 deletion completed in 6.293310772s

• [SLOW TEST:10.745 seconds]
[sig-storage] EmptyDir volumes
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:47:59.668: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1436
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 27 04:48:00.027: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3a6b9979-7c8d-41e1-aad0-79c771c42c85" in namespace "projected-1436" to be "success or failure"
Nov 27 04:48:00.047: INFO: Pod "downwardapi-volume-3a6b9979-7c8d-41e1-aad0-79c771c42c85": Phase="Pending", Reason="", readiness=false. Elapsed: 19.533156ms
Nov 27 04:48:02.061: INFO: Pod "downwardapi-volume-3a6b9979-7c8d-41e1-aad0-79c771c42c85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033598407s
Nov 27 04:48:04.070: INFO: Pod "downwardapi-volume-3a6b9979-7c8d-41e1-aad0-79c771c42c85": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042733679s
Nov 27 04:48:06.080: INFO: Pod "downwardapi-volume-3a6b9979-7c8d-41e1-aad0-79c771c42c85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052819845s
STEP: Saw pod success
Nov 27 04:48:06.081: INFO: Pod "downwardapi-volume-3a6b9979-7c8d-41e1-aad0-79c771c42c85" satisfied condition "success or failure"
Nov 27 04:48:06.090: INFO: Trying to get logs from node slave1 pod downwardapi-volume-3a6b9979-7c8d-41e1-aad0-79c771c42c85 container client-container: <nil>
STEP: delete the pod
Nov 27 04:48:06.147: INFO: Waiting for pod downwardapi-volume-3a6b9979-7c8d-41e1-aad0-79c771c42c85 to disappear
Nov 27 04:48:06.156: INFO: Pod downwardapi-volume-3a6b9979-7c8d-41e1-aad0-79c771c42c85 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:48:06.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1436" for this suite.
Nov 27 04:48:12.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:48:12.456: INFO: namespace projected-1436 deletion completed in 6.287436256s

• [SLOW TEST:12.788 seconds]
[sig-storage] Projected downwardAPI
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:48:12.457: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1456
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 04:48:12.765: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:48:13.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1456" for this suite.
Nov 27 04:48:19.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:48:20.124: INFO: namespace custom-resource-definition-1456 deletion completed in 6.256295846s

• [SLOW TEST:7.667 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:48:20.126: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9644
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Nov 27 04:48:20.466: INFO: Waiting up to 5m0s for pod "downward-api-28986b79-0202-4ffc-9572-b85e7dada3b4" in namespace "downward-api-9644" to be "success or failure"
Nov 27 04:48:20.477: INFO: Pod "downward-api-28986b79-0202-4ffc-9572-b85e7dada3b4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.624093ms
Nov 27 04:48:22.488: INFO: Pod "downward-api-28986b79-0202-4ffc-9572-b85e7dada3b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021897286s
Nov 27 04:48:24.497: INFO: Pod "downward-api-28986b79-0202-4ffc-9572-b85e7dada3b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031216871s
Nov 27 04:48:26.512: INFO: Pod "downward-api-28986b79-0202-4ffc-9572-b85e7dada3b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045512878s
STEP: Saw pod success
Nov 27 04:48:26.512: INFO: Pod "downward-api-28986b79-0202-4ffc-9572-b85e7dada3b4" satisfied condition "success or failure"
Nov 27 04:48:26.520: INFO: Trying to get logs from node slave1 pod downward-api-28986b79-0202-4ffc-9572-b85e7dada3b4 container dapi-container: <nil>
STEP: delete the pod
Nov 27 04:48:26.583: INFO: Waiting for pod downward-api-28986b79-0202-4ffc-9572-b85e7dada3b4 to disappear
Nov 27 04:48:26.590: INFO: Pod downward-api-28986b79-0202-4ffc-9572-b85e7dada3b4 no longer exists
[AfterEach] [sig-node] Downward API
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:48:26.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9644" for this suite.
Nov 27 04:48:32.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:48:32.860: INFO: namespace downward-api-9644 deletion completed in 6.256983227s

• [SLOW TEST:12.735 seconds]
[sig-node] Downward API
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:48:32.864: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3333
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov 27 04:48:33.228: INFO: Waiting up to 5m0s for pod "pod-453bc101-61b9-43a9-aa1e-2b12852c2d4a" in namespace "emptydir-3333" to be "success or failure"
Nov 27 04:48:33.241: INFO: Pod "pod-453bc101-61b9-43a9-aa1e-2b12852c2d4a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.145033ms
Nov 27 04:48:35.251: INFO: Pod "pod-453bc101-61b9-43a9-aa1e-2b12852c2d4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022891691s
Nov 27 04:48:37.263: INFO: Pod "pod-453bc101-61b9-43a9-aa1e-2b12852c2d4a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034659153s
Nov 27 04:48:39.272: INFO: Pod "pod-453bc101-61b9-43a9-aa1e-2b12852c2d4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.04399976s
STEP: Saw pod success
Nov 27 04:48:39.273: INFO: Pod "pod-453bc101-61b9-43a9-aa1e-2b12852c2d4a" satisfied condition "success or failure"
Nov 27 04:48:39.280: INFO: Trying to get logs from node slave1 pod pod-453bc101-61b9-43a9-aa1e-2b12852c2d4a container test-container: <nil>
STEP: delete the pod
Nov 27 04:48:39.335: INFO: Waiting for pod pod-453bc101-61b9-43a9-aa1e-2b12852c2d4a to disappear
Nov 27 04:48:39.342: INFO: Pod pod-453bc101-61b9-43a9-aa1e-2b12852c2d4a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:48:39.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3333" for this suite.
Nov 27 04:48:45.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:48:45.633: INFO: namespace emptydir-3333 deletion completed in 6.277346521s

• [SLOW TEST:12.769 seconds]
[sig-storage] EmptyDir volumes
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:48:45.634: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1866
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 04:48:45.941: INFO: Creating deployment "webserver-deployment"
Nov 27 04:48:45.953: INFO: Waiting for observed generation 1
Nov 27 04:48:47.974: INFO: Waiting for all required pods to come up
Nov 27 04:48:47.988: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Nov 27 04:48:54.026: INFO: Waiting for deployment "webserver-deployment" to complete
Nov 27 04:48:54.044: INFO: Updating deployment "webserver-deployment" with a non-existent image
Nov 27 04:48:54.064: INFO: Updating deployment webserver-deployment
Nov 27 04:48:54.064: INFO: Waiting for observed generation 2
Nov 27 04:48:56.080: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Nov 27 04:48:56.087: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Nov 27 04:48:56.094: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 27 04:48:56.114: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Nov 27 04:48:56.114: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Nov 27 04:48:56.121: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 27 04:48:56.134: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Nov 27 04:48:56.134: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Nov 27 04:48:56.156: INFO: Updating deployment webserver-deployment
Nov 27 04:48:56.156: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Nov 27 04:48:56.298: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Nov 27 04:48:58.329: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Nov 27 04:48:58.349: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1866 /apis/apps/v1/namespaces/deployment-1866/deployments/webserver-deployment 79ffe115-ef49-41e4-ba83-ae7e9de85df4 951073 3 2019-11-27 04:48:45 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004996e58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2019-11-27 04:48:56 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2019-11-27 04:48:56 +0000 UTC,LastTransitionTime:2019-11-27 04:48:45 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Nov 27 04:48:58.364: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-1866 /apis/apps/v1/namespaces/deployment-1866/replicasets/webserver-deployment-c7997dcc8 eb1a26b3-54d2-4143-a2a0-44cad6024e10 951037 3 2019-11-27 04:48:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 79ffe115-ef49-41e4-ba83-ae7e9de85df4 0xc005a85027 0xc005a85028}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005a85098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 27 04:48:58.364: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Nov 27 04:48:58.365: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-1866 /apis/apps/v1/namespaces/deployment-1866/replicasets/webserver-deployment-595b5b9587 358fc9a0-8e66-4b22-8d1a-549af9ea17e7 951066 3 2019-11-27 04:48:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 79ffe115-ef49-41e4-ba83-ae7e9de85df4 0xc005a84f67 0xc005a84f68}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005a84fc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Nov 27 04:48:58.397: INFO: Pod "webserver-deployment-595b5b9587-58xwt" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-58xwt webserver-deployment-595b5b9587- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-595b5b9587-58xwt c3a26e59-32e1-423f-b1e3-92099bfa75df 950918 0 2019-11-27 04:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 358fc9a0-8e66-4b22-8d1a-549af9ea17e7 0xc005a85587 0xc005a85588}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.17,PodIP:172.31.51.55,StartTime:2019-11-27 04:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-11-27 04:48:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c2b0d33a386966885dd604f38d06db8c78942dbee8e1f5a8db17ad0d8a368da,ContainerID:docker://efac331ff7315462fd2323498205071e627b798022f45fba0b05c0b557c17c77,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.31.51.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.406: INFO: Pod "webserver-deployment-595b5b9587-5ktxd" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5ktxd webserver-deployment-595b5b9587- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-595b5b9587-5ktxd cf6e001f-5fae-41c1-9eae-0401cb02e033 950912 0 2019-11-27 04:48:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 358fc9a0-8e66-4b22-8d1a-549af9ea17e7 0xc005a85707 0xc005a85708}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.17,PodIP:172.31.51.72,StartTime:2019-11-27 04:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-11-27 04:48:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c2b0d33a386966885dd604f38d06db8c78942dbee8e1f5a8db17ad0d8a368da,ContainerID:docker://4e0199b70a196172cec92930e2acbb9c24cb75c74585b72607586df98b31eaff,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.31.51.72,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.407: INFO: Pod "webserver-deployment-595b5b9587-5z79p" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5z79p webserver-deployment-595b5b9587- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-595b5b9587-5z79p 10d44c06-dd63-4dd9-b916-07f7b53a251c 951105 0 2019-11-27 04:48:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 358fc9a0-8e66-4b22-8d1a-549af9ea17e7 0xc005a85887 0xc005a85888}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.1,PodIP:,StartTime:2019-11-27 04:48:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.410: INFO: Pod "webserver-deployment-595b5b9587-8wvt8" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-8wvt8 webserver-deployment-595b5b9587- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-595b5b9587-8wvt8 86d1fcd9-e78b-4474-8454-99dc9215f398 951091 0 2019-11-27 04:48:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 358fc9a0-8e66-4b22-8d1a-549af9ea17e7 0xc005a859e7 0xc005a859e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.17,PodIP:,StartTime:2019-11-27 04:48:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.411: INFO: Pod "webserver-deployment-595b5b9587-bbdr2" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bbdr2 webserver-deployment-595b5b9587- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-595b5b9587-bbdr2 b2df3fcb-b0fe-4eb4-bafc-ef70185c15b8 951064 0 2019-11-27 04:48:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 358fc9a0-8e66-4b22-8d1a-549af9ea17e7 0xc005a85b47 0xc005a85b48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.412: INFO: Pod "webserver-deployment-595b5b9587-cd2gp" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-cd2gp webserver-deployment-595b5b9587- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-595b5b9587-cd2gp 8692544c-1c97-4b66-ae4b-f715f8e82025 950890 0 2019-11-27 04:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 358fc9a0-8e66-4b22-8d1a-549af9ea17e7 0xc005a85c60 0xc005a85c61}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.17,PodIP:172.31.51.77,StartTime:2019-11-27 04:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-11-27 04:48:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c2b0d33a386966885dd604f38d06db8c78942dbee8e1f5a8db17ad0d8a368da,ContainerID:docker://2be04c586c3f99ff82dda9f4ddcca95587aaec8f8b07987039a32709f961df90,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.31.51.77,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.413: INFO: Pod "webserver-deployment-595b5b9587-czmzq" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-czmzq webserver-deployment-595b5b9587- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-595b5b9587-czmzq 394f4a53-39e8-4ccf-88d1-33dbf3fec0de 951108 0 2019-11-27 04:48:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 358fc9a0-8e66-4b22-8d1a-549af9ea17e7 0xc005a85dd7 0xc005a85dd8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.17,PodIP:,StartTime:2019-11-27 04:48:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.414: INFO: Pod "webserver-deployment-595b5b9587-g2854" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-g2854 webserver-deployment-595b5b9587- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-595b5b9587-g2854 4037b4a5-3aad-4610-a44a-2939f830cae5 951065 0 2019-11-27 04:48:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 358fc9a0-8e66-4b22-8d1a-549af9ea17e7 0xc005a85f37 0xc005a85f38}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.415: INFO: Pod "webserver-deployment-595b5b9587-h424r" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-h424r webserver-deployment-595b5b9587- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-595b5b9587-h424r 7d129995-04ba-4c5b-9f75-8511c69defaa 950893 0 2019-11-27 04:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 358fc9a0-8e66-4b22-8d1a-549af9ea17e7 0xc002fb80c0 0xc002fb80c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.1,PodIP:172.31.161.239,StartTime:2019-11-27 04:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-11-27 04:48:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c2b0d33a386966885dd604f38d06db8c78942dbee8e1f5a8db17ad0d8a368da,ContainerID:docker://6f6d2dd6e415e67920d1cbe37e08fc520e7d512534304391748c50d2ea38972d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.31.161.239,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.421: INFO: Pod "webserver-deployment-595b5b9587-h5j8w" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-h5j8w webserver-deployment-595b5b9587- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-595b5b9587-h5j8w c8abfc59-3c7f-4d04-b166-dfeddd43f6e6 950896 0 2019-11-27 04:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 358fc9a0-8e66-4b22-8d1a-549af9ea17e7 0xc002fb8537 0xc002fb8538}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.1,PodIP:172.31.161.112,StartTime:2019-11-27 04:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-11-27 04:48:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c2b0d33a386966885dd604f38d06db8c78942dbee8e1f5a8db17ad0d8a368da,ContainerID:docker://b9baabcf8aa3bde9818878955c7f3e5c381dda5e8eeca5694112a5db9df758a6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.31.161.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.424: INFO: Pod "webserver-deployment-595b5b9587-j78tc" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-j78tc webserver-deployment-595b5b9587- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-595b5b9587-j78tc 7bcf3c8a-188d-4062-a613-7848c164a440 950909 0 2019-11-27 04:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 358fc9a0-8e66-4b22-8d1a-549af9ea17e7 0xc002fb8777 0xc002fb8778}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.17,PodIP:172.31.51.69,StartTime:2019-11-27 04:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-11-27 04:48:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c2b0d33a386966885dd604f38d06db8c78942dbee8e1f5a8db17ad0d8a368da,ContainerID:docker://d84825f55e3e29a9b13bff8231e5d1543e70f2295e84ad02cb4454e2108ab061,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.31.51.69,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.425: INFO: Pod "webserver-deployment-595b5b9587-jqw8t" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-jqw8t webserver-deployment-595b5b9587- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-595b5b9587-jqw8t de18ac81-6965-4ad0-8e28-539da42e9da3 951068 0 2019-11-27 04:48:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 358fc9a0-8e66-4b22-8d1a-549af9ea17e7 0xc002fb8b27 0xc002fb8b28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.1,PodIP:,StartTime:2019-11-27 04:48:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.426: INFO: Pod "webserver-deployment-595b5b9587-k2djv" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-k2djv webserver-deployment-595b5b9587- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-595b5b9587-k2djv 4b45e365-4e36-4cb6-8d52-b9fb1eb035d3 951043 0 2019-11-27 04:48:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 358fc9a0-8e66-4b22-8d1a-549af9ea17e7 0xc002fb8da7 0xc002fb8da8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.17,PodIP:,StartTime:2019-11-27 04:48:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.427: INFO: Pod "webserver-deployment-595b5b9587-pp7tp" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-pp7tp webserver-deployment-595b5b9587- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-595b5b9587-pp7tp 0cd6c4b9-03b2-4ae5-85c7-c8c5b4c6f386 950860 0 2019-11-27 04:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 358fc9a0-8e66-4b22-8d1a-549af9ea17e7 0xc002fb8fd7 0xc002fb8fd8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.1,PodIP:172.31.161.246,StartTime:2019-11-27 04:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-11-27 04:48:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c2b0d33a386966885dd604f38d06db8c78942dbee8e1f5a8db17ad0d8a368da,ContainerID:docker://e10a3891d2aa8c87d6633125cdb2a55734d50f8605cd7d60e4cfcb2975fd1171,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.31.161.246,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.428: INFO: Pod "webserver-deployment-595b5b9587-q6dvl" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-q6dvl webserver-deployment-595b5b9587- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-595b5b9587-q6dvl 566bbcbc-5e3c-428a-9502-73aad8bd84e8 950864 0 2019-11-27 04:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 358fc9a0-8e66-4b22-8d1a-549af9ea17e7 0xc002fb9167 0xc002fb9168}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.1,PodIP:172.31.161.113,StartTime:2019-11-27 04:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-11-27 04:48:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c2b0d33a386966885dd604f38d06db8c78942dbee8e1f5a8db17ad0d8a368da,ContainerID:docker://8ff56e2d7b6ee6905f0a786252d5a14d83e53addd781c95356cd65dc5c1b9d51,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.31.161.113,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.430: INFO: Pod "webserver-deployment-595b5b9587-qrk7j" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qrk7j webserver-deployment-595b5b9587- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-595b5b9587-qrk7j 4c255839-366a-4002-ad4c-7b2b995119f0 951114 0 2019-11-27 04:48:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 358fc9a0-8e66-4b22-8d1a-549af9ea17e7 0xc002fb92f7 0xc002fb92f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.17,PodIP:,StartTime:2019-11-27 04:48:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.432: INFO: Pod "webserver-deployment-595b5b9587-rmq65" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rmq65 webserver-deployment-595b5b9587- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-595b5b9587-rmq65 229c44c5-f084-48ec-ba1b-457d8a4e3bd8 951001 0 2019-11-27 04:48:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 358fc9a0-8e66-4b22-8d1a-549af9ea17e7 0xc002fb9457 0xc002fb9458}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.17,PodIP:,StartTime:2019-11-27 04:48:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.434: INFO: Pod "webserver-deployment-595b5b9587-rqdg7" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rqdg7 webserver-deployment-595b5b9587- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-595b5b9587-rqdg7 4b9ede77-a526-4a30-ab06-2e81d1fd83d1 951109 0 2019-11-27 04:48:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 358fc9a0-8e66-4b22-8d1a-549af9ea17e7 0xc002fb95b7 0xc002fb95b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.1,PodIP:,StartTime:2019-11-27 04:48:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.435: INFO: Pod "webserver-deployment-595b5b9587-stdk7" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-stdk7 webserver-deployment-595b5b9587- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-595b5b9587-stdk7 0be60fab-fee8-4e1e-9894-a055200d6137 951099 0 2019-11-27 04:48:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 358fc9a0-8e66-4b22-8d1a-549af9ea17e7 0xc002fb9717 0xc002fb9718}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.17,PodIP:,StartTime:2019-11-27 04:48:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.437: INFO: Pod "webserver-deployment-595b5b9587-z6qzm" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-z6qzm webserver-deployment-595b5b9587- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-595b5b9587-z6qzm 367ced48-0f77-4108-8ad4-d5856c4d1b4e 951097 0 2019-11-27 04:48:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 358fc9a0-8e66-4b22-8d1a-549af9ea17e7 0xc002fb9877 0xc002fb9878}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.1,PodIP:,StartTime:2019-11-27 04:48:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.438: INFO: Pod "webserver-deployment-c7997dcc8-4gd9d" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-4gd9d webserver-deployment-c7997dcc8- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-c7997dcc8-4gd9d 4b9d3ab9-e0a6-420b-98af-5f961e0c0b3b 951083 0 2019-11-27 04:48:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 eb1a26b3-54d2-4143-a2a0-44cad6024e10 0xc002fb99d7 0xc002fb99d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.17,PodIP:,StartTime:2019-11-27 04:48:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.439: INFO: Pod "webserver-deployment-c7997dcc8-4v8bd" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-4v8bd webserver-deployment-c7997dcc8- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-c7997dcc8-4v8bd daa71b51-1a0a-448e-af51-f3cdbaec02b6 950945 0 2019-11-27 04:48:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 eb1a26b3-54d2-4143-a2a0-44cad6024e10 0xc002fb9b57 0xc002fb9b58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.1,PodIP:,StartTime:2019-11-27 04:48:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.442: INFO: Pod "webserver-deployment-c7997dcc8-8jnk5" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-8jnk5 webserver-deployment-c7997dcc8- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-c7997dcc8-8jnk5 3ee676d1-ae99-4183-a0c5-ef131652874f 950946 0 2019-11-27 04:48:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 eb1a26b3-54d2-4143-a2a0-44cad6024e10 0xc002fb9cd0 0xc002fb9cd1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.17,PodIP:,StartTime:2019-11-27 04:48:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.443: INFO: Pod "webserver-deployment-c7997dcc8-9z6lc" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-9z6lc webserver-deployment-c7997dcc8- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-c7997dcc8-9z6lc 29413ed5-8f03-4b77-a239-0d21c9932b1b 951011 0 2019-11-27 04:48:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 eb1a26b3-54d2-4143-a2a0-44cad6024e10 0xc002fb9e47 0xc002fb9e48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.1,PodIP:,StartTime:2019-11-27 04:48:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.444: INFO: Pod "webserver-deployment-c7997dcc8-b22r8" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-b22r8 webserver-deployment-c7997dcc8- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-c7997dcc8-b22r8 52984d6d-c723-4a21-8072-d7f0cd5e2ac2 951069 0 2019-11-27 04:48:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 eb1a26b3-54d2-4143-a2a0-44cad6024e10 0xc002fb9fc0 0xc002fb9fc1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.17,PodIP:,StartTime:2019-11-27 04:48:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.446: INFO: Pod "webserver-deployment-c7997dcc8-b5hg9" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-b5hg9 webserver-deployment-c7997dcc8- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-c7997dcc8-b5hg9 52a1ba15-f97a-459a-ad84-2de8f6541f26 951085 0 2019-11-27 04:48:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 eb1a26b3-54d2-4143-a2a0-44cad6024e10 0xc002bfc1e7 0xc002bfc1e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.1,PodIP:,StartTime:2019-11-27 04:48:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.447: INFO: Pod "webserver-deployment-c7997dcc8-f8p74" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-f8p74 webserver-deployment-c7997dcc8- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-c7997dcc8-f8p74 17288a88-ac5e-4918-a2f3-a0738ced2674 951025 0 2019-11-27 04:48:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 eb1a26b3-54d2-4143-a2a0-44cad6024e10 0xc002bfc460 0xc002bfc461}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.17,PodIP:,StartTime:2019-11-27 04:48:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.448: INFO: Pod "webserver-deployment-c7997dcc8-hgpvj" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-hgpvj webserver-deployment-c7997dcc8- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-c7997dcc8-hgpvj fe56b7c9-5832-4355-a2c6-8d32ebbfbf06 951079 0 2019-11-27 04:48:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 eb1a26b3-54d2-4143-a2a0-44cad6024e10 0xc002bfc637 0xc002bfc638}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.1,PodIP:,StartTime:2019-11-27 04:48:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.452: INFO: Pod "webserver-deployment-c7997dcc8-hhx8m" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-hhx8m webserver-deployment-c7997dcc8- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-c7997dcc8-hhx8m 41aa76b7-2b40-45a3-b0fe-2a104f6cb109 950961 0 2019-11-27 04:48:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 eb1a26b3-54d2-4143-a2a0-44cad6024e10 0xc002bfc8e0 0xc002bfc8e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.17,PodIP:,StartTime:2019-11-27 04:48:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.453: INFO: Pod "webserver-deployment-c7997dcc8-n7l7k" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-n7l7k webserver-deployment-c7997dcc8- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-c7997dcc8-n7l7k 296313a6-9095-44c4-a0be-af5c644fc1cb 951032 0 2019-11-27 04:48:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 eb1a26b3-54d2-4143-a2a0-44cad6024e10 0xc002bfcb17 0xc002bfcb18}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.1,PodIP:,StartTime:2019-11-27 04:48:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.454: INFO: Pod "webserver-deployment-c7997dcc8-nb49m" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-nb49m webserver-deployment-c7997dcc8- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-c7997dcc8-nb49m 4f7c5253-e60d-4d68-a108-39068ba0c64c 950973 0 2019-11-27 04:48:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 eb1a26b3-54d2-4143-a2a0-44cad6024e10 0xc002bfce00 0xc002bfce01}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.17,PodIP:,StartTime:2019-11-27 04:48:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.455: INFO: Pod "webserver-deployment-c7997dcc8-s5czt" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-s5czt webserver-deployment-c7997dcc8- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-c7997dcc8-s5czt fc6f13e4-dce8-49f0-9d40-4dcb28c82abf 950972 0 2019-11-27 04:48:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 eb1a26b3-54d2-4143-a2a0-44cad6024e10 0xc002bfcfa7 0xc002bfcfa8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.1,PodIP:,StartTime:2019-11-27 04:48:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 04:48:58.456: INFO: Pod "webserver-deployment-c7997dcc8-sz6sm" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-sz6sm webserver-deployment-c7997dcc8- deployment-1866 /api/v1/namespaces/deployment-1866/pods/webserver-deployment-c7997dcc8-sz6sm ff3f051c-eb19-4c28-a910-735d1022edb1 951077 0 2019-11-27 04:48:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 eb1a26b3-54d2-4143-a2a0-44cad6024e10 0xc002bfd2a0 0xc002bfd2a1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6tl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6tl9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6tl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 04:48:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.17,PodIP:,StartTime:2019-11-27 04:48:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:48:58.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1866" for this suite.
Nov 27 04:49:08.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:49:09.215: INFO: namespace deployment-1866 deletion completed in 10.741211023s

• [SLOW TEST:23.581 seconds]
[sig-apps] Deployment
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:49:09.216: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9996
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9996
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9996
STEP: creating replication controller externalsvc in namespace services-9996
I1127 04:49:09.852638      19 runners.go:184] Created replication controller with name: externalsvc, namespace: services-9996, replica count: 2
I1127 04:49:09.853162      19 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 04:49:09.853311      19 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 04:49:12.905707      19 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1127 04:49:15.906569      19 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1127 04:49:18.907221      19 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1127 04:49:21.907728      19 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Nov 27 04:49:21.976: INFO: Creating new exec pod
Nov 27 04:49:26.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=services-9996 execpodg5nhn -- /bin/sh -x -c nslookup clusterip-service'
Nov 27 04:49:27.811: INFO: stderr: "+ nslookup clusterip-service\n"
Nov 27 04:49:27.812: INFO: stdout: "Server:\t\t172.30.0.3\nAddress:\t172.30.0.3#53\n\nclusterip-service.services-9996.svc.cluster.local\tcanonical name = externalsvc.services-9996.svc.cluster.local.\nName:\texternalsvc.services-9996.svc.cluster.local\nAddress: 172.30.68.230\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9996, will wait for the garbage collector to delete the pods
I1127 04:49:27.827446      19 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 04:49:27.827696      19 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Nov 27 04:49:27.903: INFO: Deleting ReplicationController externalsvc took: 25.282027ms
I1127 04:49:28.405551      19 controller_utils.go:810] Ignoring inactive pod services-9996/externalsvc-pzzmn in state Running, deletion time 2019-11-27 04:49:29 +0000 UTC
I1127 04:49:28.405868      19 controller_utils.go:810] Ignoring inactive pod services-9996/externalsvc-5fjbw in state Running, deletion time 2019-11-27 04:49:29 +0000 UTC
Nov 27 04:49:28.405: INFO: Terminating ReplicationController externalsvc pods took: 501.926927ms
Nov 27 04:49:39.268: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:49:39.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9996" for this suite.
Nov 27 04:49:45.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:49:45.662: INFO: namespace services-9996 deletion completed in 6.343823004s
[AfterEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:36.447 seconds]
[sig-network] Services
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:49:45.664: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4779
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:50:00.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4779" for this suite.
Nov 27 04:50:08.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:50:08.370: INFO: namespace job-4779 deletion completed in 8.311331641s

• [SLOW TEST:22.706 seconds]
[sig-apps] Job
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:50:08.373: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9526
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 27 04:50:26.114: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 27 04:50:28.161: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427026, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427026, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427026, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427026, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 04:50:30.171: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427026, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427026, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427026, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427026, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 27 04:50:33.193: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
Nov 27 04:50:43.268: INFO: Waiting for webhook configuration to be ready...
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:50:43.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9526" for this suite.
Nov 27 04:50:49.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:50:49.956: INFO: namespace webhook-9526 deletion completed in 6.395967155s
STEP: Destroying namespace "webhook-9526-markers" for this suite.
Nov 27 04:50:57.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:50:58.241: INFO: namespace webhook-9526-markers deletion completed in 8.285302099s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:49.913 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:50:58.287: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6735
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6735
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-6735
I1127 04:50:58.673332      19 runners.go:184] Created replication controller with name: externalname-service, namespace: services-6735, replica count: 2
I1127 04:50:58.673641      19 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 04:50:58.673704      19 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 04:51:01.724129      19 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1127 04:51:04.724476      19 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 27 04:51:04.724: INFO: Creating new exec pod
I1127 04:51:08.780055      19 reflector.go:120] Starting reflector *v1.Endpoints (0s) from k8s.io/kubernetes/test/e2e/framework/service/jig.go:389
I1127 04:51:08.780159      19 reflector.go:158] Listing and watching *v1.Endpoints from k8s.io/kubernetes/test/e2e/framework/service/jig.go:389
Nov 27 04:51:09.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=services-6735 execpod252cc -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Nov 27 04:51:11.184: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 27 04:51:11.185: INFO: stdout: ""
Nov 27 04:51:11.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=services-6735 execpod252cc -- /bin/sh -x -c nc -zv -t -w 2 172.30.187.8 80'
Nov 27 04:51:12.515: INFO: stderr: "+ nc -zv -t -w 2 172.30.187.8 80\nConnection to 172.30.187.8 80 port [tcp/http] succeeded!\n"
Nov 27 04:51:12.515: INFO: stdout: ""
Nov 27 04:51:12.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=services-6735 execpod252cc -- /bin/sh -x -c nc -zv -t -w 2 10.200.72.1 30816'
Nov 27 04:51:13.984: INFO: stderr: "+ nc -zv -t -w 2 10.200.72.1 30816\nConnection to 10.200.72.1 30816 port [tcp/*] succeeded!\n"
Nov 27 04:51:13.985: INFO: stdout: ""
Nov 27 04:51:13.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=services-6735 execpod252cc -- /bin/sh -x -c nc -zv -t -w 2 10.200.72.17 30816'
Nov 27 04:51:15.445: INFO: stderr: "+ nc -zv -t -w 2 10.200.72.17 30816\nConnection to 10.200.72.17 30816 port [tcp/*] succeeded!\n"
Nov 27 04:51:15.445: INFO: stdout: ""
Nov 27 04:51:15.445: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:51:15.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6735" for this suite.
Nov 27 04:51:21.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:51:21.830: INFO: namespace services-6735 deletion completed in 6.294764734s
[AfterEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:23.543 seconds]
[sig-network] Services
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:51:21.831: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7514
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 04:51:22.425: INFO: (0) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 227.773891ms)
Nov 27 04:51:22.445: INFO: (1) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 19.534134ms)
Nov 27 04:51:22.462: INFO: (2) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 16.839856ms)
Nov 27 04:51:22.477: INFO: (3) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 15.164914ms)
Nov 27 04:51:22.492: INFO: (4) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 15.492472ms)
Nov 27 04:51:22.508: INFO: (5) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 15.741806ms)
Nov 27 04:51:22.525: INFO: (6) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 16.835455ms)
Nov 27 04:51:22.546: INFO: (7) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 20.542716ms)
Nov 27 04:51:22.566: INFO: (8) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 20.003469ms)
Nov 27 04:51:22.583: INFO: (9) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 16.725454ms)
Nov 27 04:51:22.599: INFO: (10) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 15.559938ms)
Nov 27 04:51:22.625: INFO: (11) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 26.035897ms)
Nov 27 04:51:22.649: INFO: (12) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 24.174689ms)
Nov 27 04:51:22.667: INFO: (13) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 17.667548ms)
Nov 27 04:51:22.682: INFO: (14) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 15.577538ms)
Nov 27 04:51:22.698: INFO: (15) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 16.00434ms)
Nov 27 04:51:22.715: INFO: (16) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 16.292298ms)
Nov 27 04:51:22.731: INFO: (17) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 15.630828ms)
Nov 27 04:51:22.748: INFO: (18) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 16.847189ms)
Nov 27 04:51:22.763: INFO: (19) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.0.log.old">Xorg.0.log.old</a>
<a href="Xo... (200; 15.501271ms)
[AfterEach] version v1
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:51:22.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7514" for this suite.
Nov 27 04:51:28.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:51:29.085: INFO: namespace proxy-7514 deletion completed in 6.310038182s

• [SLOW TEST:7.254 seconds]
[sig-network] Proxy
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:51:29.088: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8864
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Nov 27 04:51:29.395: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 27 04:51:29.446: INFO: Waiting for terminating namespaces to be deleted...
Nov 27 04:51:29.456: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Nov 27 04:51:29.487: INFO: calico-node-cppjk from kube-system started at 2019-11-20 07:23:24 +0000 UTC (1 container statuses recorded)
Nov 27 04:51:29.487: INFO: 	Container calico-node ready: true, restart count 5
Nov 27 04:51:29.487: INFO: coredns-lxl9z from kube-system started at 2019-11-14 04:51:49 +0000 UTC (1 container statuses recorded)
Nov 27 04:51:29.487: INFO: 	Container coredns ready: true, restart count 6
Nov 27 04:51:29.487: INFO: metrics-server-687c949fd7-pf9hm from kube-system started at 2019-11-22 00:26:19 +0000 UTC (1 container statuses recorded)
Nov 27 04:51:29.487: INFO: 	Container metrics-server ready: true, restart count 3
Nov 27 04:51:29.487: INFO: kube-scheduler-master1 from kube-system started at 2019-11-27 03:11:20 +0000 UTC (1 container statuses recorded)
Nov 27 04:51:29.487: INFO: 	Container kube-scheduler ready: true, restart count 64
Nov 27 04:51:29.487: INFO: e2e-conformance-test from conformance started at 2019-11-27 04:04:03 +0000 UTC (1 container statuses recorded)
Nov 27 04:51:29.488: INFO: 	Container conformance-container ready: true, restart count 0
Nov 27 04:51:29.488: INFO: kube-proxy-master1 from kube-system started at 2019-11-27 03:11:20 +0000 UTC (1 container statuses recorded)
Nov 27 04:51:29.488: INFO: 	Container kube-proxy ready: true, restart count 14
Nov 27 04:51:29.488: INFO: kube-controller-manager-master1 from kube-system started at 2019-11-27 03:11:20 +0000 UTC (1 container statuses recorded)
Nov 27 04:51:29.488: INFO: 	Container kube-controller-manager ready: true, restart count 73
Nov 27 04:51:29.488: INFO: resource-reserver-master1 from kube-system started at 2019-11-27 03:11:20 +0000 UTC (1 container statuses recorded)
Nov 27 04:51:29.488: INFO: 	Container sleep-forever ready: true, restart count 7
Nov 27 04:51:29.488: INFO: kube-apiserver-master1 from kube-system started at 2019-11-27 03:11:20 +0000 UTC (1 container statuses recorded)
Nov 27 04:51:29.488: INFO: 	Container kube-apiserver ready: true, restart count 6
Nov 27 04:51:29.488: INFO: 
Logging pods the kubelet thinks is on node slave1 before test
Nov 27 04:51:29.723: INFO: calico-kube-controllers-685dd5746c-8jjc9 from kube-system started at 2019-11-08 02:42:34 +0000 UTC (1 container statuses recorded)
Nov 27 04:51:29.723: INFO: 	Container calico-kube-controllers ready: true, restart count 5
Nov 27 04:51:29.723: INFO: nginx-proxy-slave1 from kube-system started at 2019-11-27 03:14:35 +0000 UTC (1 container statuses recorded)
Nov 27 04:51:29.723: INFO: 	Container nginx-proxy ready: true, restart count 15
Nov 27 04:51:29.723: INFO: dns-autoscaler-799d586fb4-mqr72 from kube-system started at 2019-11-08 02:43:45 +0000 UTC (1 container statuses recorded)
Nov 27 04:51:29.723: INFO: 	Container autoscaler ready: true, restart count 5
Nov 27 04:51:29.723: INFO: calico-node-5nkgf from kube-system started at 2019-11-20 07:23:24 +0000 UTC (1 container statuses recorded)
Nov 27 04:51:29.723: INFO: 	Container calico-node ready: true, restart count 5
Nov 27 04:51:29.723: INFO: kube-proxy-slave1 from kube-system started at 2019-11-27 03:14:35 +0000 UTC (1 container statuses recorded)
Nov 27 04:51:29.723: INFO: 	Container kube-proxy ready: true, restart count 260
Nov 27 04:51:29.723: INFO: test-webserver-d1bd1012-16a8-4768-81ad-d85c5340fd42 from container-probe-4706 started at 2019-11-27 04:02:33 +0000 UTC (1 container statuses recorded)
Nov 27 04:51:29.723: INFO: 	Container test-webserver ready: false, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-a93062f8-404c-4513-a390-d5233e93692b 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-a93062f8-404c-4513-a390-d5233e93692b off the node slave1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a93062f8-404c-4513-a390-d5233e93692b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:51:39.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8864" for this suite.
Nov 27 04:51:57.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:51:58.226: INFO: namespace sched-pred-8864 deletion completed in 18.271548366s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
I1127 04:51:58.226556      19 request.go:706] Error in request: resource name may not be empty

• [SLOW TEST:29.139 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:51:58.227: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1381
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-1381/configmap-test-7eb625cd-68e2-4bd2-9227-9e33cc5d5bd2
STEP: Creating a pod to test consume configMaps
Nov 27 04:51:58.576: INFO: Waiting up to 5m0s for pod "pod-configmaps-a3c91a82-9ff2-4e81-a1b3-fe8e8953730c" in namespace "configmap-1381" to be "success or failure"
Nov 27 04:51:58.584: INFO: Pod "pod-configmaps-a3c91a82-9ff2-4e81-a1b3-fe8e8953730c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.261771ms
Nov 27 04:52:00.596: INFO: Pod "pod-configmaps-a3c91a82-9ff2-4e81-a1b3-fe8e8953730c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019873277s
Nov 27 04:52:02.605: INFO: Pod "pod-configmaps-a3c91a82-9ff2-4e81-a1b3-fe8e8953730c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029257884s
STEP: Saw pod success
Nov 27 04:52:02.605: INFO: Pod "pod-configmaps-a3c91a82-9ff2-4e81-a1b3-fe8e8953730c" satisfied condition "success or failure"
Nov 27 04:52:02.614: INFO: Trying to get logs from node slave1 pod pod-configmaps-a3c91a82-9ff2-4e81-a1b3-fe8e8953730c container env-test: <nil>
STEP: delete the pod
Nov 27 04:52:02.678: INFO: Waiting for pod pod-configmaps-a3c91a82-9ff2-4e81-a1b3-fe8e8953730c to disappear
Nov 27 04:52:02.696: INFO: Pod pod-configmaps-a3c91a82-9ff2-4e81-a1b3-fe8e8953730c no longer exists
[AfterEach] [sig-node] ConfigMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:52:02.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1381" for this suite.
Nov 27 04:52:08.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:52:09.029: INFO: namespace configmap-1381 deletion completed in 6.315805142s

• [SLOW TEST:10.802 seconds]
[sig-node] ConfigMap
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:52:09.031: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4755
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 04:52:09.443: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"77f7b68a-fe27-4e02-9d5a-dd97ac54664e", Controller:(*bool)(0xc002b64eea), BlockOwnerDeletion:(*bool)(0xc002b64eeb)}}
Nov 27 04:52:09.497: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"e8e4b24d-a8ee-4dba-b982-67ed00e8e162", Controller:(*bool)(0xc002b651ca), BlockOwnerDeletion:(*bool)(0xc002b651cb)}}
Nov 27 04:52:09.514: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"9d261ab1-2957-44a3-b289-0d76846f0712", Controller:(*bool)(0xc00499fada), BlockOwnerDeletion:(*bool)(0xc00499fadb)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:52:14.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4755" for this suite.
Nov 27 04:52:20.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:52:20.822: INFO: namespace gc-4755 deletion completed in 6.269336528s

• [SLOW TEST:11.792 seconds]
[sig-api-machinery] Garbage collector
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:52:20.823: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4617
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 04:52:21.122: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:52:25.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4617" for this suite.
Nov 27 04:53:15.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:53:15.616: INFO: namespace pods-4617 deletion completed in 50.290361962s

• [SLOW TEST:54.793 seconds]
[k8s.io] Pods
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:53:15.617: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-108
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-d39f7ac7-1491-43e5-831f-e01b77fcaf47
STEP: Creating configMap with name cm-test-opt-upd-72fd99fe-f234-4131-bd7a-563b76ad45ff
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-d39f7ac7-1491-43e5-831f-e01b77fcaf47
STEP: Updating configmap cm-test-opt-upd-72fd99fe-f234-4131-bd7a-563b76ad45ff
STEP: Creating configMap with name cm-test-opt-create-a8469492-e658-4076-89c0-465aaf60e75f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:54:39.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-108" for this suite.
Nov 27 04:55:07.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:55:07.802: INFO: namespace configmap-108 deletion completed in 28.268548771s

• [SLOW TEST:112.186 seconds]
[sig-storage] ConfigMap
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:55:07.805: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5420
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 27 04:55:08.142: INFO: Waiting up to 5m0s for pod "downwardapi-volume-68b8d2bc-f9e8-494c-9379-10fd4b89b14c" in namespace "downward-api-5420" to be "success or failure"
Nov 27 04:55:08.153: INFO: Pod "downwardapi-volume-68b8d2bc-f9e8-494c-9379-10fd4b89b14c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.870495ms
Nov 27 04:55:10.170: INFO: Pod "downwardapi-volume-68b8d2bc-f9e8-494c-9379-10fd4b89b14c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027693091s
Nov 27 04:55:12.188: INFO: Pod "downwardapi-volume-68b8d2bc-f9e8-494c-9379-10fd4b89b14c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045372225s
STEP: Saw pod success
Nov 27 04:55:12.188: INFO: Pod "downwardapi-volume-68b8d2bc-f9e8-494c-9379-10fd4b89b14c" satisfied condition "success or failure"
Nov 27 04:55:12.198: INFO: Trying to get logs from node slave1 pod downwardapi-volume-68b8d2bc-f9e8-494c-9379-10fd4b89b14c container client-container: <nil>
STEP: delete the pod
Nov 27 04:55:12.262: INFO: Waiting for pod downwardapi-volume-68b8d2bc-f9e8-494c-9379-10fd4b89b14c to disappear
Nov 27 04:55:12.270: INFO: Pod downwardapi-volume-68b8d2bc-f9e8-494c-9379-10fd4b89b14c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:55:12.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5420" for this suite.
Nov 27 04:55:18.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:55:18.587: INFO: namespace downward-api-5420 deletion completed in 6.305532561s

• [SLOW TEST:10.783 seconds]
[sig-storage] Downward API volume
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:55:18.588: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6371
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 27 04:55:18.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-6371'
Nov 27 04:55:19.714: INFO: stderr: ""
Nov 27 04:55:19.714: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Nov 27 04:55:19.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 delete pods e2e-test-httpd-pod --namespace=kubectl-6371'
Nov 27 04:55:23.605: INFO: stderr: ""
Nov 27 04:55:23.606: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:55:23.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6371" for this suite.
Nov 27 04:55:29.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:55:29.902: INFO: namespace kubectl-6371 deletion completed in 6.276617096s

• [SLOW TEST:11.315 seconds]
[sig-cli] Kubectl client
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:55:29.906: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2977
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Nov 27 04:55:30.234: INFO: Waiting up to 5m0s for pod "downward-api-9b7fbdf6-3c33-4220-b5e6-21a5df901014" in namespace "downward-api-2977" to be "success or failure"
Nov 27 04:55:30.244: INFO: Pod "downward-api-9b7fbdf6-3c33-4220-b5e6-21a5df901014": Phase="Pending", Reason="", readiness=false. Elapsed: 9.693244ms
Nov 27 04:55:32.253: INFO: Pod "downward-api-9b7fbdf6-3c33-4220-b5e6-21a5df901014": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01941812s
Nov 27 04:55:34.269: INFO: Pod "downward-api-9b7fbdf6-3c33-4220-b5e6-21a5df901014": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0352292s
STEP: Saw pod success
Nov 27 04:55:34.269: INFO: Pod "downward-api-9b7fbdf6-3c33-4220-b5e6-21a5df901014" satisfied condition "success or failure"
Nov 27 04:55:34.279: INFO: Trying to get logs from node slave1 pod downward-api-9b7fbdf6-3c33-4220-b5e6-21a5df901014 container dapi-container: <nil>
STEP: delete the pod
Nov 27 04:55:34.362: INFO: Waiting for pod downward-api-9b7fbdf6-3c33-4220-b5e6-21a5df901014 to disappear
Nov 27 04:55:34.376: INFO: Pod downward-api-9b7fbdf6-3c33-4220-b5e6-21a5df901014 no longer exists
[AfterEach] [sig-node] Downward API
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:55:34.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2977" for this suite.
Nov 27 04:55:40.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:55:40.688: INFO: namespace downward-api-2977 deletion completed in 6.290892227s

• [SLOW TEST:10.782 seconds]
[sig-node] Downward API
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:55:40.688: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2162
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-4f43b8af-30c4-475c-ae6b-9286f40d5713
STEP: Creating a pod to test consume configMaps
Nov 27 04:55:41.047: INFO: Waiting up to 5m0s for pod "pod-configmaps-b41f461d-1a95-4a50-886f-b7cfdff92ab8" in namespace "configmap-2162" to be "success or failure"
Nov 27 04:55:41.057: INFO: Pod "pod-configmaps-b41f461d-1a95-4a50-886f-b7cfdff92ab8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.000757ms
Nov 27 04:55:43.070: INFO: Pod "pod-configmaps-b41f461d-1a95-4a50-886f-b7cfdff92ab8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023184537s
Nov 27 04:55:45.084: INFO: Pod "pod-configmaps-b41f461d-1a95-4a50-886f-b7cfdff92ab8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036860141s
STEP: Saw pod success
Nov 27 04:55:45.084: INFO: Pod "pod-configmaps-b41f461d-1a95-4a50-886f-b7cfdff92ab8" satisfied condition "success or failure"
Nov 27 04:55:45.094: INFO: Trying to get logs from node slave1 pod pod-configmaps-b41f461d-1a95-4a50-886f-b7cfdff92ab8 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 27 04:55:45.172: INFO: Waiting for pod pod-configmaps-b41f461d-1a95-4a50-886f-b7cfdff92ab8 to disappear
Nov 27 04:55:45.181: INFO: Pod pod-configmaps-b41f461d-1a95-4a50-886f-b7cfdff92ab8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:55:45.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2162" for this suite.
Nov 27 04:55:51.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:55:51.532: INFO: namespace configmap-2162 deletion completed in 6.337689865s

• [SLOW TEST:10.844 seconds]
[sig-storage] ConfigMap
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:55:51.534: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9668
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-9668
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 27 04:55:51.879: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Nov 27 04:56:18.070: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.31.51.98:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9668 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 27 04:56:18.070: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
Nov 27 04:56:18.759: INFO: Found all expected endpoints: [netserver-0]
Nov 27 04:56:18.770: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.31.161.7:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9668 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 27 04:56:18.770: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
Nov 27 04:56:19.500: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:56:19.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9668" for this suite.
Nov 27 04:56:31.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:56:31.811: INFO: namespace pod-network-test-9668 deletion completed in 12.293661843s

• [SLOW TEST:40.277 seconds]
[sig-network] Networking
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:56:31.812: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3656
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Nov 27 04:56:32.133: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
Nov 27 04:56:46.078: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 04:57:41.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3656" for this suite.
Nov 27 04:57:47.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 04:57:48.167: INFO: namespace crd-publish-openapi-3656 deletion completed in 6.267714388s

• [SLOW TEST:76.355 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 04:57:48.171: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6370
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-e3a41976-bdf7-45ef-afcb-f3d3348d9543 in namespace container-probe-6370
Nov 27 04:57:52.524: INFO: Started pod liveness-e3a41976-bdf7-45ef-afcb-f3d3348d9543 in namespace container-probe-6370
STEP: checking the pod's current state and verifying that restartCount is present
Nov 27 04:57:52.532: INFO: Initial restart count of pod liveness-e3a41976-bdf7-45ef-afcb-f3d3348d9543 is 0
Nov 27 04:58:10.640: INFO: Restart count of pod container-probe-6370/liveness-e3a41976-bdf7-45ef-afcb-f3d3348d9543 is now 1 (18.108055838s elapsed)
Nov 27 04:58:28.753: INFO: Restart count of pod container-probe-6370/liveness-e3a41976-bdf7-45ef-afcb-f3d3348d9543 is now 2 (36.221290499s elapsed)
Nov 27 04:58:48.874: INFO: Restart count of pod container-probe-6370/liveness-e3a41976-bdf7-45ef-afcb-f3d3348d9543 is now 3 (56.342473894s elapsed)
Nov 27 04:59:08.977: INFO: Restart count of pod container-probe-6370/liveness-e3a41976-bdf7-45ef-afcb-f3d3348d9543 is now 4 (1m16.445334138s elapsed)
Nov 27 05:00:11.511: INFO: Restart count of pod container-probe-6370/liveness-e3a41976-bdf7-45ef-afcb-f3d3348d9543 is now 5 (2m18.97866531s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:00:11.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6370" for this suite.
Nov 27 05:00:17.590: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:00:17.848: INFO: namespace container-probe-6370 deletion completed in 6.290210713s

• [SLOW TEST:149.677 seconds]
[k8s.io] Probing container
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:00:17.849: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2046
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 05:00:18.187: INFO: Pod name rollover-pod: Found 0 pods out of 1
Nov 27 05:00:23.199: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 27 05:00:23.199: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Nov 27 05:00:25.207: INFO: Creating deployment "test-rollover-deployment"
Nov 27 05:00:25.228: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Nov 27 05:00:27.247: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Nov 27 05:00:27.270: INFO: Ensure that both replica sets have 1 created replica
Nov 27 05:00:27.288: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Nov 27 05:00:27.308: INFO: Updating deployment test-rollover-deployment
Nov 27 05:00:27.308: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Nov 27 05:00:29.330: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Nov 27 05:00:29.348: INFO: Make sure deployment "test-rollover-deployment" is complete
Nov 27 05:00:29.365: INFO: all replica sets need to contain the pod-template-hash label
Nov 27 05:00:29.365: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427625, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427625, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427627, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427625, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 05:00:31.384: INFO: all replica sets need to contain the pod-template-hash label
Nov 27 05:00:31.385: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427625, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427625, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427627, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427625, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 05:00:33.385: INFO: all replica sets need to contain the pod-template-hash label
Nov 27 05:00:33.385: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427625, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427625, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427631, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427625, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 05:00:35.388: INFO: all replica sets need to contain the pod-template-hash label
Nov 27 05:00:35.389: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427625, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427625, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427631, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427625, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 05:00:37.388: INFO: all replica sets need to contain the pod-template-hash label
Nov 27 05:00:37.389: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427625, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427625, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427631, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427625, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 05:00:39.388: INFO: all replica sets need to contain the pod-template-hash label
Nov 27 05:00:39.388: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427625, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427625, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427631, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427625, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 05:00:41.383: INFO: all replica sets need to contain the pod-template-hash label
Nov 27 05:00:41.383: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427625, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427625, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427631, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710427625, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 05:00:43.386: INFO: 
Nov 27 05:00:43.387: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Nov 27 05:00:43.412: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2046 /apis/apps/v1/namespaces/deployment-2046/deployments/test-rollover-deployment b148a0ad-9c65-4a96-b23c-238994f20470 953631 2 2019-11-27 05:00:25 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003ff7d98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-11-27 05:00:25 +0000 UTC,LastTransitionTime:2019-11-27 05:00:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2019-11-27 05:00:41 +0000 UTC,LastTransitionTime:2019-11-27 05:00:25 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 27 05:00:43.424: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-2046 /apis/apps/v1/namespaces/deployment-2046/replicasets/test-rollover-deployment-7d7dc6548c f1585610-eac4-4017-bb50-b87392edcaa8 953620 2 2019-11-27 05:00:27 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment b148a0ad-9c65-4a96-b23c-238994f20470 0xc00266d4e7 0xc00266d4e8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00266d5e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 27 05:00:43.424: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Nov 27 05:00:43.425: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2046 /apis/apps/v1/namespaces/deployment-2046/replicasets/test-rollover-controller 8db90e39-e134-412f-bd4e-380baf64ee50 953629 2 2019-11-27 05:00:18 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment b148a0ad-9c65-4a96-b23c-238994f20470 0xc00266d417 0xc00266d418}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00266d478 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 27 05:00:43.426: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-2046 /apis/apps/v1/namespaces/deployment-2046/replicasets/test-rollover-deployment-f6c94f66c 1ae9af81-71fe-4ad3-a795-ae878c8afdf2 953584 2 2019-11-27 05:00:25 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment b148a0ad-9c65-4a96-b23c-238994f20470 0xc00266d720 0xc00266d721}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00266d798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 27 05:00:43.438: INFO: Pod "test-rollover-deployment-7d7dc6548c-h4ccv" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-h4ccv test-rollover-deployment-7d7dc6548c- deployment-2046 /api/v1/namespaces/deployment-2046/pods/test-rollover-deployment-7d7dc6548c-h4ccv 7d1b9f95-2589-410d-a085-4995d1cb6b67 953604 0 2019-11-27 05:00:27 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c f1585610-eac4-4017-bb50-b87392edcaa8 0xc0058ee187 0xc0058ee188}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-66k4k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-66k4k,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-66k4k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 05:00:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 05:00:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 05:00:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 05:00:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.17,PodIP:172.31.51.99,StartTime:2019-11-27 05:00:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-11-27 05:00:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker://sha256:0819d392499b83e17f70aecae0173f2bd403a637c12598d5e0cb23a04f7490f3,ContainerID:docker://764769303d9c30bc5ea16d7139fba2b93616f92ec1677b11b7c60e908c881a88,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.31.51.99,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:00:43.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2046" for this suite.
Nov 27 05:00:51.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:00:51.712: INFO: namespace deployment-2046 deletion completed in 8.260417051s

• [SLOW TEST:33.864 seconds]
[sig-apps] Deployment
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:00:51.713: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8974
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 05:00:52.046: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 27 05:01:05.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 --namespace=crd-publish-openapi-8974 create -f -'
Nov 27 05:01:07.094: INFO: stderr: ""
Nov 27 05:01:07.095: INFO: stdout: "e2e-test-crd-publish-openapi-3666-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 27 05:01:07.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 --namespace=crd-publish-openapi-8974 delete e2e-test-crd-publish-openapi-3666-crds test-cr'
Nov 27 05:01:07.893: INFO: stderr: ""
Nov 27 05:01:07.893: INFO: stdout: "e2e-test-crd-publish-openapi-3666-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Nov 27 05:01:07.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 --namespace=crd-publish-openapi-8974 apply -f -'
Nov 27 05:01:09.062: INFO: stderr: ""
Nov 27 05:01:09.062: INFO: stdout: "e2e-test-crd-publish-openapi-3666-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 27 05:01:09.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 --namespace=crd-publish-openapi-8974 delete e2e-test-crd-publish-openapi-3666-crds test-cr'
Nov 27 05:01:09.817: INFO: stderr: ""
Nov 27 05:01:09.818: INFO: stdout: "e2e-test-crd-publish-openapi-3666-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Nov 27 05:01:09.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 explain e2e-test-crd-publish-openapi-3666-crds'
Nov 27 05:01:10.945: INFO: stderr: ""
Nov 27 05:01:10.946: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3666-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:01:25.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8974" for this suite.
Nov 27 05:01:31.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:01:31.700: INFO: namespace crd-publish-openapi-8974 deletion completed in 6.30563474s

• [SLOW TEST:39.987 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:01:31.703: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6187
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-8178e255-62ca-4b73-8533-6c0b68aa5f09
STEP: Creating a pod to test consume secrets
Nov 27 05:01:32.104: INFO: Waiting up to 5m0s for pod "pod-secrets-d9ea8efa-e721-4c3a-aa60-2e5100092ff4" in namespace "secrets-6187" to be "success or failure"
Nov 27 05:01:32.112: INFO: Pod "pod-secrets-d9ea8efa-e721-4c3a-aa60-2e5100092ff4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.705195ms
Nov 27 05:01:34.128: INFO: Pod "pod-secrets-d9ea8efa-e721-4c3a-aa60-2e5100092ff4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024687877s
Nov 27 05:01:36.142: INFO: Pod "pod-secrets-d9ea8efa-e721-4c3a-aa60-2e5100092ff4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038155214s
STEP: Saw pod success
Nov 27 05:01:36.142: INFO: Pod "pod-secrets-d9ea8efa-e721-4c3a-aa60-2e5100092ff4" satisfied condition "success or failure"
Nov 27 05:01:36.154: INFO: Trying to get logs from node slave1 pod pod-secrets-d9ea8efa-e721-4c3a-aa60-2e5100092ff4 container secret-env-test: <nil>
STEP: delete the pod
Nov 27 05:01:36.449: INFO: Waiting for pod pod-secrets-d9ea8efa-e721-4c3a-aa60-2e5100092ff4 to disappear
Nov 27 05:01:36.457: INFO: Pod pod-secrets-d9ea8efa-e721-4c3a-aa60-2e5100092ff4 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:01:36.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6187" for this suite.
Nov 27 05:01:42.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:01:42.738: INFO: namespace secrets-6187 deletion completed in 6.268773815s

• [SLOW TEST:11.035 seconds]
[sig-api-machinery] Secrets
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:01:42.739: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7043
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 27 05:01:43.073: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3e5d8bae-05bc-4a86-b6c7-135276a75af8" in namespace "downward-api-7043" to be "success or failure"
Nov 27 05:01:43.084: INFO: Pod "downwardapi-volume-3e5d8bae-05bc-4a86-b6c7-135276a75af8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.493831ms
Nov 27 05:01:45.095: INFO: Pod "downwardapi-volume-3e5d8bae-05bc-4a86-b6c7-135276a75af8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021577552s
Nov 27 05:01:47.103: INFO: Pod "downwardapi-volume-3e5d8bae-05bc-4a86-b6c7-135276a75af8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030455668s
STEP: Saw pod success
Nov 27 05:01:47.104: INFO: Pod "downwardapi-volume-3e5d8bae-05bc-4a86-b6c7-135276a75af8" satisfied condition "success or failure"
Nov 27 05:01:47.111: INFO: Trying to get logs from node slave1 pod downwardapi-volume-3e5d8bae-05bc-4a86-b6c7-135276a75af8 container client-container: <nil>
STEP: delete the pod
Nov 27 05:01:47.168: INFO: Waiting for pod downwardapi-volume-3e5d8bae-05bc-4a86-b6c7-135276a75af8 to disappear
Nov 27 05:01:47.175: INFO: Pod downwardapi-volume-3e5d8bae-05bc-4a86-b6c7-135276a75af8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:01:47.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7043" for this suite.
Nov 27 05:01:53.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:01:53.450: INFO: namespace downward-api-7043 deletion completed in 6.261028312s

• [SLOW TEST:10.711 seconds]
[sig-storage] Downward API volume
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:01:53.451: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3341
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-013c1b2a-2f51-4e29-bdb4-b3b46aad9d91
STEP: Creating a pod to test consume secrets
Nov 27 05:01:53.774: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-086fac65-fe78-4a12-bccf-70d9e7c3de5d" in namespace "projected-3341" to be "success or failure"
Nov 27 05:01:53.783: INFO: Pod "pod-projected-secrets-086fac65-fe78-4a12-bccf-70d9e7c3de5d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.272038ms
Nov 27 05:01:55.795: INFO: Pod "pod-projected-secrets-086fac65-fe78-4a12-bccf-70d9e7c3de5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020787503s
Nov 27 05:01:57.811: INFO: Pod "pod-projected-secrets-086fac65-fe78-4a12-bccf-70d9e7c3de5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036566806s
STEP: Saw pod success
Nov 27 05:01:57.811: INFO: Pod "pod-projected-secrets-086fac65-fe78-4a12-bccf-70d9e7c3de5d" satisfied condition "success or failure"
Nov 27 05:01:57.820: INFO: Trying to get logs from node slave1 pod pod-projected-secrets-086fac65-fe78-4a12-bccf-70d9e7c3de5d container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 27 05:01:57.886: INFO: Waiting for pod pod-projected-secrets-086fac65-fe78-4a12-bccf-70d9e7c3de5d to disappear
Nov 27 05:01:57.896: INFO: Pod pod-projected-secrets-086fac65-fe78-4a12-bccf-70d9e7c3de5d no longer exists
[AfterEach] [sig-storage] Projected secret
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:01:57.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3341" for this suite.
Nov 27 05:02:03.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:02:04.192: INFO: namespace projected-3341 deletion completed in 6.260878223s

• [SLOW TEST:10.741 seconds]
[sig-storage] Projected secret
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:02:04.193: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3584
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3584
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-3584
I1127 05:02:04.602096      19 runners.go:184] Created replication controller with name: externalname-service, namespace: services-3584, replica count: 2
I1127 05:02:04.602335      19 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 05:02:04.602392      19 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 05:02:07.666712      19 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1127 05:02:10.667202      19 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 27 05:02:10.667: INFO: Creating new exec pod
I1127 05:02:14.697090      19 reflector.go:120] Starting reflector *v1.Endpoints (0s) from k8s.io/kubernetes/test/e2e/framework/service/jig.go:389
I1127 05:02:14.697189      19 reflector.go:158] Listing and watching *v1.Endpoints from k8s.io/kubernetes/test/e2e/framework/service/jig.go:389
Nov 27 05:02:15.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=services-3584 execpod2bnc5 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Nov 27 05:02:17.039: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 27 05:02:17.039: INFO: stdout: ""
Nov 27 05:02:17.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=services-3584 execpod2bnc5 -- /bin/sh -x -c nc -zv -t -w 2 172.30.174.150 80'
Nov 27 05:02:18.486: INFO: stderr: "+ nc -zv -t -w 2 172.30.174.150 80\nConnection to 172.30.174.150 80 port [tcp/http] succeeded!\n"
Nov 27 05:02:18.486: INFO: stdout: ""
Nov 27 05:02:18.487: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:02:18.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3584" for this suite.
Nov 27 05:02:26.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:02:26.925: INFO: namespace services-3584 deletion completed in 8.363245524s
[AfterEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:22.733 seconds]
[sig-network] Services
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:02:26.928: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1925
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Nov 27 05:02:27.256: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 27 05:02:27.297: INFO: Waiting for terminating namespaces to be deleted...
Nov 27 05:02:27.306: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Nov 27 05:02:27.611: INFO: metrics-server-687c949fd7-pf9hm from kube-system started at 2019-11-22 00:26:19 +0000 UTC (1 container statuses recorded)
Nov 27 05:02:27.611: INFO: 	Container metrics-server ready: true, restart count 3
Nov 27 05:02:27.611: INFO: kube-scheduler-master1 from kube-system started at 2019-11-27 03:11:20 +0000 UTC (1 container statuses recorded)
Nov 27 05:02:27.611: INFO: 	Container kube-scheduler ready: true, restart count 64
Nov 27 05:02:27.611: INFO: e2e-conformance-test from conformance started at 2019-11-27 04:04:03 +0000 UTC (1 container statuses recorded)
Nov 27 05:02:27.611: INFO: 	Container conformance-container ready: true, restart count 0
Nov 27 05:02:27.611: INFO: kube-proxy-master1 from kube-system started at 2019-11-27 03:11:20 +0000 UTC (1 container statuses recorded)
Nov 27 05:02:27.611: INFO: 	Container kube-proxy ready: true, restart count 14
Nov 27 05:02:27.611: INFO: kube-controller-manager-master1 from kube-system started at 2019-11-27 03:11:20 +0000 UTC (1 container statuses recorded)
Nov 27 05:02:27.611: INFO: 	Container kube-controller-manager ready: true, restart count 73
Nov 27 05:02:27.611: INFO: resource-reserver-master1 from kube-system started at 2019-11-27 03:11:20 +0000 UTC (1 container statuses recorded)
Nov 27 05:02:27.611: INFO: 	Container sleep-forever ready: true, restart count 7
Nov 27 05:02:27.611: INFO: kube-apiserver-master1 from kube-system started at 2019-11-27 03:11:20 +0000 UTC (1 container statuses recorded)
Nov 27 05:02:27.612: INFO: 	Container kube-apiserver ready: true, restart count 6
Nov 27 05:02:27.612: INFO: calico-node-cppjk from kube-system started at 2019-11-20 07:23:24 +0000 UTC (1 container statuses recorded)
Nov 27 05:02:27.612: INFO: 	Container calico-node ready: true, restart count 5
Nov 27 05:02:27.612: INFO: coredns-lxl9z from kube-system started at 2019-11-14 04:51:49 +0000 UTC (1 container statuses recorded)
Nov 27 05:02:27.612: INFO: 	Container coredns ready: true, restart count 6
Nov 27 05:02:27.612: INFO: 
Logging pods the kubelet thinks is on node slave1 before test
Nov 27 05:02:27.642: INFO: calico-kube-controllers-685dd5746c-8jjc9 from kube-system started at 2019-11-08 02:42:34 +0000 UTC (1 container statuses recorded)
Nov 27 05:02:27.642: INFO: 	Container calico-kube-controllers ready: true, restart count 5
Nov 27 05:02:27.642: INFO: nginx-proxy-slave1 from kube-system started at 2019-11-27 03:14:35 +0000 UTC (1 container statuses recorded)
Nov 27 05:02:27.642: INFO: 	Container nginx-proxy ready: true, restart count 15
Nov 27 05:02:27.642: INFO: dns-autoscaler-799d586fb4-mqr72 from kube-system started at 2019-11-08 02:43:45 +0000 UTC (1 container statuses recorded)
Nov 27 05:02:27.642: INFO: 	Container autoscaler ready: true, restart count 5
Nov 27 05:02:27.642: INFO: calico-node-5nkgf from kube-system started at 2019-11-20 07:23:24 +0000 UTC (1 container statuses recorded)
Nov 27 05:02:27.642: INFO: 	Container calico-node ready: true, restart count 5
Nov 27 05:02:27.643: INFO: kube-proxy-slave1 from kube-system started at 2019-11-27 03:14:35 +0000 UTC (1 container statuses recorded)
Nov 27 05:02:27.643: INFO: 	Container kube-proxy ready: true, restart count 260
Nov 27 05:02:27.643: INFO: test-webserver-d1bd1012-16a8-4768-81ad-d85c5340fd42 from container-probe-4706 started at 2019-11-27 04:02:33 +0000 UTC (1 container statuses recorded)
Nov 27 05:02:27.643: INFO: 	Container test-webserver ready: false, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-f5de6de0-7556-491a-af4e-1641f58f70b4 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-f5de6de0-7556-491a-af4e-1641f58f70b4 off the node slave1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f5de6de0-7556-491a-af4e-1641f58f70b4
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:07:39.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1925" for this suite.
Nov 27 05:07:55.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:07:56.252: INFO: namespace sched-pred-1925 deletion completed in 16.311368207s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

I1127 05:07:56.253576      19 request.go:706] Error in request: resource name may not be empty
• [SLOW TEST:329.326 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:07:56.254: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3543
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Nov 27 05:07:56.577: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:08:04.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3543" for this suite.
Nov 27 05:08:32.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:08:32.466: INFO: namespace init-container-3543 deletion completed in 28.262180964s

• [SLOW TEST:36.212 seconds]
[k8s.io] InitContainer [NodeConformance]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:08:32.467: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7354
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:08:49.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7354" for this suite.
Nov 27 05:08:55.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:08:55.427: INFO: namespace resourcequota-7354 deletion completed in 6.311605567s

• [SLOW TEST:22.960 seconds]
[sig-api-machinery] ResourceQuota
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:08:55.428: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3566
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Nov 27 05:08:59.800: INFO: Pod pod-hostip-25154da0-fd31-433d-95a0-3d92a6229d90 has hostIP: 10.200.72.17
[AfterEach] [k8s.io] Pods
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:08:59.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3566" for this suite.
Nov 27 05:09:27.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:09:28.063: INFO: namespace pods-3566 deletion completed in 28.251476204s

• [SLOW TEST:32.636 seconds]
[k8s.io] Pods
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:09:28.067: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8213
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-mq5v
STEP: Creating a pod to test atomic-volume-subpath
Nov 27 05:09:28.433: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-mq5v" in namespace "subpath-8213" to be "success or failure"
Nov 27 05:09:28.440: INFO: Pod "pod-subpath-test-projected-mq5v": Phase="Pending", Reason="", readiness=false. Elapsed: 6.76332ms
Nov 27 05:09:30.452: INFO: Pod "pod-subpath-test-projected-mq5v": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019109141s
Nov 27 05:09:32.462: INFO: Pod "pod-subpath-test-projected-mq5v": Phase="Running", Reason="", readiness=true. Elapsed: 4.029224151s
Nov 27 05:09:34.473: INFO: Pod "pod-subpath-test-projected-mq5v": Phase="Running", Reason="", readiness=true. Elapsed: 6.03993512s
Nov 27 05:09:36.492: INFO: Pod "pod-subpath-test-projected-mq5v": Phase="Running", Reason="", readiness=true. Elapsed: 8.058815459s
Nov 27 05:09:38.510: INFO: Pod "pod-subpath-test-projected-mq5v": Phase="Running", Reason="", readiness=true. Elapsed: 10.076849528s
Nov 27 05:09:40.525: INFO: Pod "pod-subpath-test-projected-mq5v": Phase="Running", Reason="", readiness=true. Elapsed: 12.091400247s
Nov 27 05:09:42.534: INFO: Pod "pod-subpath-test-projected-mq5v": Phase="Running", Reason="", readiness=true. Elapsed: 14.100668009s
Nov 27 05:09:44.544: INFO: Pod "pod-subpath-test-projected-mq5v": Phase="Running", Reason="", readiness=true. Elapsed: 16.110367951s
Nov 27 05:09:46.555: INFO: Pod "pod-subpath-test-projected-mq5v": Phase="Running", Reason="", readiness=true. Elapsed: 18.122074791s
Nov 27 05:09:48.565: INFO: Pod "pod-subpath-test-projected-mq5v": Phase="Running", Reason="", readiness=true. Elapsed: 20.131544465s
Nov 27 05:09:50.580: INFO: Pod "pod-subpath-test-projected-mq5v": Phase="Running", Reason="", readiness=true. Elapsed: 22.146861766s
Nov 27 05:09:52.591: INFO: Pod "pod-subpath-test-projected-mq5v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.1573938s
STEP: Saw pod success
Nov 27 05:09:52.591: INFO: Pod "pod-subpath-test-projected-mq5v" satisfied condition "success or failure"
Nov 27 05:09:52.598: INFO: Trying to get logs from node slave1 pod pod-subpath-test-projected-mq5v container test-container-subpath-projected-mq5v: <nil>
STEP: delete the pod
Nov 27 05:09:52.893: INFO: Waiting for pod pod-subpath-test-projected-mq5v to disappear
Nov 27 05:09:52.901: INFO: Pod pod-subpath-test-projected-mq5v no longer exists
STEP: Deleting pod pod-subpath-test-projected-mq5v
Nov 27 05:09:52.901: INFO: Deleting pod "pod-subpath-test-projected-mq5v" in namespace "subpath-8213"
[AfterEach] [sig-storage] Subpath
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:09:52.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8213" for this suite.
Nov 27 05:09:58.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:09:59.223: INFO: namespace subpath-8213 deletion completed in 6.297922971s

• [SLOW TEST:31.156 seconds]
[sig-storage] Subpath
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:09:59.226: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5650
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-65d9d2f2-e2ce-433e-b819-45f0c72093a7
STEP: Creating a pod to test consume configMaps
Nov 27 05:09:59.575: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c952dda8-29ee-416a-abd7-c0246360fe6b" in namespace "projected-5650" to be "success or failure"
Nov 27 05:09:59.586: INFO: Pod "pod-projected-configmaps-c952dda8-29ee-416a-abd7-c0246360fe6b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.276274ms
Nov 27 05:10:01.601: INFO: Pod "pod-projected-configmaps-c952dda8-29ee-416a-abd7-c0246360fe6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025799127s
Nov 27 05:10:03.612: INFO: Pod "pod-projected-configmaps-c952dda8-29ee-416a-abd7-c0246360fe6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036563874s
STEP: Saw pod success
Nov 27 05:10:03.612: INFO: Pod "pod-projected-configmaps-c952dda8-29ee-416a-abd7-c0246360fe6b" satisfied condition "success or failure"
Nov 27 05:10:03.620: INFO: Trying to get logs from node slave1 pod pod-projected-configmaps-c952dda8-29ee-416a-abd7-c0246360fe6b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 27 05:10:03.681: INFO: Waiting for pod pod-projected-configmaps-c952dda8-29ee-416a-abd7-c0246360fe6b to disappear
Nov 27 05:10:03.689: INFO: Pod pod-projected-configmaps-c952dda8-29ee-416a-abd7-c0246360fe6b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:10:03.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5650" for this suite.
Nov 27 05:10:09.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:10:10.023: INFO: namespace projected-5650 deletion completed in 6.322771351s

• [SLOW TEST:10.798 seconds]
[sig-storage] Projected configMap
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:10:10.029: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5638
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 27 05:10:15.449: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:10:15.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5638" for this suite.
Nov 27 05:10:21.531: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:10:21.808: INFO: namespace container-runtime-5638 deletion completed in 6.308087018s

• [SLOW TEST:11.779 seconds]
[k8s.io] Container Runtime
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:10:21.812: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-7050
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7050
I1127 05:10:22.164064      19 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7050, replica count: 1
I1127 05:10:22.164530      19 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 05:10:22.164614      19 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 05:10:23.215389      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1127 05:10:24.215924      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1127 05:10:25.216337      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1127 05:10:26.216756      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1127 05:10:27.217151      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1127 05:10:27.218577      19 reflector.go:120] Starting reflector *v1.Endpoints (0s) from k8s.io/kubernetes/test/e2e/network/service_latency.go:323
I1127 05:10:27.218663      19 reflector.go:158] Listing and watching *v1.Endpoints from k8s.io/kubernetes/test/e2e/network/service_latency.go:323
Nov 27 05:10:27.339: INFO: Created: latency-svc-dmp5g
Nov 27 05:10:27.355: INFO: Got endpoints: latency-svc-dmp5g [38.23422ms]
Nov 27 05:10:27.415: INFO: Created: latency-svc-qpwhr
Nov 27 05:10:27.417: INFO: Created: latency-svc-l7jm8
Nov 27 05:10:27.420: INFO: Got endpoints: latency-svc-l7jm8 [64.228073ms]
Nov 27 05:10:27.424: INFO: Created: latency-svc-lxk69
Nov 27 05:10:27.437: INFO: Got endpoints: latency-svc-qpwhr [77.908225ms]
Nov 27 05:10:27.440: INFO: Got endpoints: latency-svc-lxk69 [80.859171ms]
Nov 27 05:10:27.461: INFO: Created: latency-svc-d5rtp
Nov 27 05:10:27.475: INFO: Created: latency-svc-dnn6d
Nov 27 05:10:27.483: INFO: Got endpoints: latency-svc-d5rtp [124.617861ms]
Nov 27 05:10:27.488: INFO: Got endpoints: latency-svc-dnn6d [129.102948ms]
Nov 27 05:10:27.494: INFO: Created: latency-svc-gfqbs
Nov 27 05:10:27.514: INFO: Got endpoints: latency-svc-gfqbs [156.992721ms]
Nov 27 05:10:27.524: INFO: Created: latency-svc-4r7p8
Nov 27 05:10:27.535: INFO: Created: latency-svc-m8wbd
Nov 27 05:10:27.545: INFO: Got endpoints: latency-svc-4r7p8 [187.100104ms]
Nov 27 05:10:27.555: INFO: Created: latency-svc-9jx8s
Nov 27 05:10:27.566: INFO: Got endpoints: latency-svc-9jx8s [209.191138ms]
Nov 27 05:10:27.587: INFO: Created: latency-svc-vjczc
Nov 27 05:10:27.587: INFO: Got endpoints: latency-svc-m8wbd [228.905184ms]
Nov 27 05:10:27.601: INFO: Created: latency-svc-v57jp
Nov 27 05:10:27.621: INFO: Got endpoints: latency-svc-v57jp [75.813814ms]
Nov 27 05:10:27.623: INFO: Got endpoints: latency-svc-vjczc [263.362232ms]
Nov 27 05:10:27.623: INFO: Created: latency-svc-vd4tn
Nov 27 05:10:27.636: INFO: Got endpoints: latency-svc-vd4tn [277.059006ms]
Nov 27 05:10:27.638: INFO: Created: latency-svc-58vht
Nov 27 05:10:27.649: INFO: Got endpoints: latency-svc-58vht [290.007776ms]
Nov 27 05:10:27.697: INFO: Created: latency-svc-4v6lx
Nov 27 05:10:27.711: INFO: Created: latency-svc-89j22
Nov 27 05:10:27.718: INFO: Got endpoints: latency-svc-4v6lx [361.051481ms]
Nov 27 05:10:27.729: INFO: Created: latency-svc-tr6qr
Nov 27 05:10:27.733: INFO: Got endpoints: latency-svc-89j22 [375.417546ms]
Nov 27 05:10:27.744: INFO: Created: latency-svc-55gl2
Nov 27 05:10:27.744: INFO: Got endpoints: latency-svc-tr6qr [386.415642ms]
Nov 27 05:10:27.759: INFO: Got endpoints: latency-svc-55gl2 [337.03177ms]
Nov 27 05:10:27.771: INFO: Created: latency-svc-5hz2g
Nov 27 05:10:27.778: INFO: Got endpoints: latency-svc-5hz2g [340.341074ms]
Nov 27 05:10:27.788: INFO: Created: latency-svc-h66vv
Nov 27 05:10:27.805: INFO: Created: latency-svc-l528l
Nov 27 05:10:27.810: INFO: Got endpoints: latency-svc-h66vv [369.752765ms]
Nov 27 05:10:27.817: INFO: Got endpoints: latency-svc-l528l [333.930734ms]
Nov 27 05:10:27.817: INFO: Created: latency-svc-rglqx
Nov 27 05:10:27.846: INFO: Got endpoints: latency-svc-rglqx [357.231285ms]
Nov 27 05:10:27.856: INFO: Created: latency-svc-kgq8r
Nov 27 05:10:27.860: INFO: Created: latency-svc-x7pd9
Nov 27 05:10:27.875: INFO: Got endpoints: latency-svc-kgq8r [360.201299ms]
Nov 27 05:10:27.896: INFO: Got endpoints: latency-svc-x7pd9 [329.648536ms]
Nov 27 05:10:27.908: INFO: Created: latency-svc-zglfv
Nov 27 05:10:27.922: INFO: Created: latency-svc-r7gcg
Nov 27 05:10:27.924: INFO: Got endpoints: latency-svc-zglfv [336.258345ms]
Nov 27 05:10:27.935: INFO: Got endpoints: latency-svc-r7gcg [312.264056ms]
Nov 27 05:10:27.945: INFO: Created: latency-svc-wd56g
Nov 27 05:10:27.960: INFO: Got endpoints: latency-svc-wd56g [337.924974ms]
Nov 27 05:10:27.969: INFO: Created: latency-svc-dmd6l
Nov 27 05:10:27.969: INFO: Got endpoints: latency-svc-dmd6l [332.100814ms]
Nov 27 05:10:27.988: INFO: Created: latency-svc-chwt2
Nov 27 05:10:28.000: INFO: Got endpoints: latency-svc-chwt2 [351.034101ms]
Nov 27 05:10:28.009: INFO: Created: latency-svc-n6hnp
Nov 27 05:10:28.017: INFO: Got endpoints: latency-svc-n6hnp [298.133147ms]
Nov 27 05:10:28.039: INFO: Created: latency-svc-5hshr
Nov 27 05:10:28.048: INFO: Got endpoints: latency-svc-5hshr [314.307132ms]
Nov 27 05:10:28.056: INFO: Created: latency-svc-rkzll
Nov 27 05:10:28.068: INFO: Got endpoints: latency-svc-rkzll [323.997444ms]
Nov 27 05:10:28.085: INFO: Created: latency-svc-d52l7
Nov 27 05:10:28.101: INFO: Got endpoints: latency-svc-d52l7 [342.490729ms]
Nov 27 05:10:28.118: INFO: Created: latency-svc-kbqch
Nov 27 05:10:28.131: INFO: Got endpoints: latency-svc-kbqch [352.778954ms]
Nov 27 05:10:28.152: INFO: Created: latency-svc-n4vvd
Nov 27 05:10:28.176: INFO: Created: latency-svc-rh5ts
Nov 27 05:10:28.176: INFO: Got endpoints: latency-svc-n4vvd [365.430479ms]
Nov 27 05:10:28.187: INFO: Created: latency-svc-dzj54
Nov 27 05:10:28.199: INFO: Created: latency-svc-882bd
Nov 27 05:10:28.214: INFO: Got endpoints: latency-svc-rh5ts [397.44307ms]
Nov 27 05:10:28.216: INFO: Created: latency-svc-9mtdw
Nov 27 05:10:28.226: INFO: Got endpoints: latency-svc-dzj54 [380.507392ms]
Nov 27 05:10:28.229: INFO: Got endpoints: latency-svc-882bd [353.978203ms]
Nov 27 05:10:28.246: INFO: Created: latency-svc-lqqrh
Nov 27 05:10:28.246: INFO: Got endpoints: latency-svc-9mtdw [350.335965ms]
Nov 27 05:10:28.252: INFO: Got endpoints: latency-svc-lqqrh [328.109996ms]
Nov 27 05:10:28.258: INFO: Created: latency-svc-flqcq
Nov 27 05:10:28.289: INFO: Created: latency-svc-v842l
Nov 27 05:10:28.291: INFO: Got endpoints: latency-svc-flqcq [355.961635ms]
Nov 27 05:10:28.298: INFO: Created: latency-svc-z7nz6
Nov 27 05:10:28.306: INFO: Got endpoints: latency-svc-v842l [345.907589ms]
Nov 27 05:10:28.312: INFO: Got endpoints: latency-svc-z7nz6 [343.05882ms]
Nov 27 05:10:28.318: INFO: Created: latency-svc-gfr25
Nov 27 05:10:28.338: INFO: Created: latency-svc-sqr59
Nov 27 05:10:28.339: INFO: Got endpoints: latency-svc-gfr25 [338.287732ms]
Nov 27 05:10:28.398: INFO: Got endpoints: latency-svc-sqr59 [381.596641ms]
Nov 27 05:10:28.401: INFO: Created: latency-svc-z558g
Nov 27 05:10:28.426: INFO: Got endpoints: latency-svc-z558g [378.094225ms]
Nov 27 05:10:28.438: INFO: Created: latency-svc-s7r4p
Nov 27 05:10:28.444: INFO: Created: latency-svc-7h622
Nov 27 05:10:28.445: INFO: Got endpoints: latency-svc-s7r4p [376.942886ms]
Nov 27 05:10:28.495: INFO: Created: latency-svc-lkthk
Nov 27 05:10:28.501: INFO: Got endpoints: latency-svc-lkthk [369.759609ms]
Nov 27 05:10:28.495: INFO: Got endpoints: latency-svc-7h622 [393.359851ms]
Nov 27 05:10:28.508: INFO: Created: latency-svc-brs72
Nov 27 05:10:28.512: INFO: Created: latency-svc-lhw2z
Nov 27 05:10:28.518: INFO: Got endpoints: latency-svc-brs72 [341.464058ms]
Nov 27 05:10:28.540: INFO: Created: latency-svc-dgclz
Nov 27 05:10:28.541: INFO: Got endpoints: latency-svc-lhw2z [326.241943ms]
Nov 27 05:10:28.546: INFO: Got endpoints: latency-svc-dgclz [319.640935ms]
Nov 27 05:10:28.565: INFO: Created: latency-svc-l5vvv
Nov 27 05:10:28.573: INFO: Got endpoints: latency-svc-l5vvv [337.122704ms]
Nov 27 05:10:28.589: INFO: Created: latency-svc-57ndm
Nov 27 05:10:28.615: INFO: Got endpoints: latency-svc-57ndm [368.361869ms]
Nov 27 05:10:28.625: INFO: Created: latency-svc-6zns6
Nov 27 05:10:28.638: INFO: Created: latency-svc-2kkbw
Nov 27 05:10:28.639: INFO: Got endpoints: latency-svc-6zns6 [387.080045ms]
Nov 27 05:10:28.656: INFO: Created: latency-svc-pk7h2
Nov 27 05:10:28.658: INFO: Got endpoints: latency-svc-2kkbw [366.807685ms]
Nov 27 05:10:28.675: INFO: Created: latency-svc-n4nnd
Nov 27 05:10:28.698: INFO: Created: latency-svc-8n26s
Nov 27 05:10:28.710: INFO: Created: latency-svc-mlnrj
Nov 27 05:10:28.714: INFO: Got endpoints: latency-svc-pk7h2 [408.51352ms]
Nov 27 05:10:28.727: INFO: Created: latency-svc-lm6v7
Nov 27 05:10:28.742: INFO: Created: latency-svc-x6xnc
Nov 27 05:10:28.754: INFO: Got endpoints: latency-svc-n4nnd [441.102514ms]
Nov 27 05:10:28.767: INFO: Created: latency-svc-lwrpl
Nov 27 05:10:28.790: INFO: Created: latency-svc-29m74
Nov 27 05:10:28.802: INFO: Created: latency-svc-hmxvv
Nov 27 05:10:28.809: INFO: Got endpoints: latency-svc-8n26s [469.613179ms]
Nov 27 05:10:28.821: INFO: Created: latency-svc-wp9ch
Nov 27 05:10:28.842: INFO: Created: latency-svc-ml56s
Nov 27 05:10:28.862: INFO: Got endpoints: latency-svc-mlnrj [463.093327ms]
Nov 27 05:10:28.882: INFO: Created: latency-svc-kd857
Nov 27 05:10:28.904: INFO: Got endpoints: latency-svc-lm6v7 [477.961973ms]
Nov 27 05:10:28.913: INFO: Created: latency-svc-w8t6d
Nov 27 05:10:28.931: INFO: Created: latency-svc-2smfk
Nov 27 05:10:28.948: INFO: Created: latency-svc-hcdch
Nov 27 05:10:28.955: INFO: Got endpoints: latency-svc-x6xnc [509.979942ms]
Nov 27 05:10:28.972: INFO: Created: latency-svc-fgrgr
Nov 27 05:10:28.990: INFO: Created: latency-svc-gn6j6
Nov 27 05:10:29.000: INFO: Created: latency-svc-g8lzd
Nov 27 05:10:29.004: INFO: Got endpoints: latency-svc-lwrpl [502.412885ms]
Nov 27 05:10:29.033: INFO: Created: latency-svc-nq8jn
Nov 27 05:10:29.047: INFO: Created: latency-svc-xgtjk
Nov 27 05:10:29.062: INFO: Got endpoints: latency-svc-29m74 [561.152177ms]
Nov 27 05:10:29.071: INFO: Created: latency-svc-59b97
Nov 27 05:10:29.088: INFO: Created: latency-svc-4zn2t
Nov 27 05:10:29.105: INFO: Got endpoints: latency-svc-hmxvv [587.195408ms]
Nov 27 05:10:29.110: INFO: Created: latency-svc-kd5tp
Nov 27 05:10:29.150: INFO: Created: latency-svc-6pzt7
Nov 27 05:10:29.155: INFO: Got endpoints: latency-svc-wp9ch [613.913798ms]
Nov 27 05:10:29.182: INFO: Created: latency-svc-bs59q
Nov 27 05:10:29.213: INFO: Got endpoints: latency-svc-ml56s [666.853374ms]
Nov 27 05:10:29.258: INFO: Created: latency-svc-7gnxg
Nov 27 05:10:29.273: INFO: Got endpoints: latency-svc-kd857 [700.18206ms]
Nov 27 05:10:29.303: INFO: Created: latency-svc-4bfkq
Nov 27 05:10:29.309: INFO: Got endpoints: latency-svc-w8t6d [693.484252ms]
Nov 27 05:10:29.333: INFO: Created: latency-svc-fkjvd
Nov 27 05:10:29.351: INFO: Got endpoints: latency-svc-2smfk [711.60989ms]
Nov 27 05:10:29.380: INFO: Created: latency-svc-th66x
Nov 27 05:10:29.407: INFO: Got endpoints: latency-svc-hcdch [749.462286ms]
Nov 27 05:10:29.459: INFO: Created: latency-svc-gfwpk
Nov 27 05:10:29.459: INFO: Got endpoints: latency-svc-fgrgr [744.543064ms]
Nov 27 05:10:29.493: INFO: Created: latency-svc-r5nd8
Nov 27 05:10:29.502: INFO: Got endpoints: latency-svc-gn6j6 [748.127614ms]
Nov 27 05:10:29.529: INFO: Created: latency-svc-p47w9
Nov 27 05:10:29.554: INFO: Got endpoints: latency-svc-g8lzd [745.640625ms]
Nov 27 05:10:29.582: INFO: Created: latency-svc-8rscl
Nov 27 05:10:29.621: INFO: Got endpoints: latency-svc-nq8jn [759.300732ms]
Nov 27 05:10:29.653: INFO: Got endpoints: latency-svc-xgtjk [748.373527ms]
Nov 27 05:10:29.655: INFO: Created: latency-svc-wlcsv
Nov 27 05:10:29.679: INFO: Created: latency-svc-4m5xn
Nov 27 05:10:29.707: INFO: Got endpoints: latency-svc-59b97 [751.014027ms]
Nov 27 05:10:29.732: INFO: Created: latency-svc-hgpht
Nov 27 05:10:29.752: INFO: Got endpoints: latency-svc-4zn2t [747.608901ms]
Nov 27 05:10:29.779: INFO: Created: latency-svc-gq5d7
Nov 27 05:10:29.804: INFO: Got endpoints: latency-svc-kd5tp [742.316165ms]
Nov 27 05:10:29.828: INFO: Created: latency-svc-wh5v8
Nov 27 05:10:29.851: INFO: Got endpoints: latency-svc-6pzt7 [746.078671ms]
Nov 27 05:10:29.876: INFO: Created: latency-svc-d6bj9
Nov 27 05:10:29.902: INFO: Got endpoints: latency-svc-bs59q [746.708363ms]
Nov 27 05:10:29.925: INFO: Created: latency-svc-9rlnq
Nov 27 05:10:29.951: INFO: Got endpoints: latency-svc-7gnxg [737.496676ms]
Nov 27 05:10:29.977: INFO: Created: latency-svc-n7xgk
Nov 27 05:10:30.002: INFO: Got endpoints: latency-svc-4bfkq [728.776325ms]
Nov 27 05:10:30.027: INFO: Created: latency-svc-qddgl
Nov 27 05:10:30.053: INFO: Got endpoints: latency-svc-fkjvd [743.837594ms]
Nov 27 05:10:30.074: INFO: Created: latency-svc-5qqjh
Nov 27 05:10:30.101: INFO: Got endpoints: latency-svc-th66x [750.235224ms]
Nov 27 05:10:30.125: INFO: Created: latency-svc-nr4rs
Nov 27 05:10:30.150: INFO: Got endpoints: latency-svc-gfwpk [742.336698ms]
Nov 27 05:10:30.173: INFO: Created: latency-svc-4nmph
Nov 27 05:10:30.199: INFO: Got endpoints: latency-svc-r5nd8 [740.062376ms]
Nov 27 05:10:30.222: INFO: Created: latency-svc-fhjbj
Nov 27 05:10:30.249: INFO: Got endpoints: latency-svc-p47w9 [747.171343ms]
Nov 27 05:10:30.272: INFO: Created: latency-svc-c8ms7
Nov 27 05:10:30.304: INFO: Got endpoints: latency-svc-8rscl [749.647576ms]
Nov 27 05:10:30.332: INFO: Created: latency-svc-9tbpw
Nov 27 05:10:30.352: INFO: Got endpoints: latency-svc-wlcsv [730.507488ms]
Nov 27 05:10:30.406: INFO: Got endpoints: latency-svc-4m5xn [753.043415ms]
Nov 27 05:10:30.407: INFO: Created: latency-svc-tkv45
Nov 27 05:10:30.430: INFO: Created: latency-svc-4nmd7
Nov 27 05:10:30.454: INFO: Got endpoints: latency-svc-hgpht [747.383522ms]
Nov 27 05:10:30.475: INFO: Created: latency-svc-dfjw7
Nov 27 05:10:30.504: INFO: Got endpoints: latency-svc-gq5d7 [751.60803ms]
Nov 27 05:10:30.533: INFO: Created: latency-svc-vspxx
Nov 27 05:10:30.550: INFO: Got endpoints: latency-svc-wh5v8 [745.234845ms]
Nov 27 05:10:30.579: INFO: Created: latency-svc-5rv29
Nov 27 05:10:30.602: INFO: Got endpoints: latency-svc-d6bj9 [750.210779ms]
Nov 27 05:10:30.626: INFO: Created: latency-svc-fwq77
Nov 27 05:10:30.652: INFO: Got endpoints: latency-svc-9rlnq [750.77887ms]
Nov 27 05:10:30.677: INFO: Created: latency-svc-q6hpp
Nov 27 05:10:30.704: INFO: Got endpoints: latency-svc-n7xgk [752.594123ms]
Nov 27 05:10:30.726: INFO: Created: latency-svc-8q7bd
Nov 27 05:10:30.750: INFO: Got endpoints: latency-svc-qddgl [747.964814ms]
Nov 27 05:10:30.771: INFO: Created: latency-svc-lmtcr
Nov 27 05:10:30.803: INFO: Got endpoints: latency-svc-5qqjh [750.527092ms]
Nov 27 05:10:30.826: INFO: Created: latency-svc-4j2l2
Nov 27 05:10:30.850: INFO: Got endpoints: latency-svc-nr4rs [748.775395ms]
Nov 27 05:10:30.872: INFO: Created: latency-svc-l7v2g
Nov 27 05:10:30.902: INFO: Got endpoints: latency-svc-4nmph [752.290033ms]
Nov 27 05:10:30.925: INFO: Created: latency-svc-cpxrn
Nov 27 05:10:30.950: INFO: Got endpoints: latency-svc-fhjbj [750.838026ms]
Nov 27 05:10:30.972: INFO: Created: latency-svc-l8msd
Nov 27 05:10:31.005: INFO: Got endpoints: latency-svc-c8ms7 [755.050313ms]
Nov 27 05:10:31.028: INFO: Created: latency-svc-bm5x8
Nov 27 05:10:31.050: INFO: Got endpoints: latency-svc-9tbpw [745.515469ms]
Nov 27 05:10:31.074: INFO: Created: latency-svc-ttksv
Nov 27 05:10:31.102: INFO: Got endpoints: latency-svc-tkv45 [749.681799ms]
Nov 27 05:10:31.125: INFO: Created: latency-svc-gxnks
Nov 27 05:10:31.155: INFO: Got endpoints: latency-svc-4nmd7 [748.950418ms]
Nov 27 05:10:31.179: INFO: Created: latency-svc-hgkjw
Nov 27 05:10:31.207: INFO: Got endpoints: latency-svc-dfjw7 [752.3355ms]
Nov 27 05:10:31.232: INFO: Created: latency-svc-9jj8d
Nov 27 05:10:31.251: INFO: Got endpoints: latency-svc-vspxx [747.227565ms]
Nov 27 05:10:31.273: INFO: Created: latency-svc-vc57x
Nov 27 05:10:31.303: INFO: Got endpoints: latency-svc-5rv29 [753.101104ms]
Nov 27 05:10:31.328: INFO: Created: latency-svc-mqgf5
Nov 27 05:10:31.352: INFO: Got endpoints: latency-svc-fwq77 [749.812332ms]
Nov 27 05:10:31.375: INFO: Created: latency-svc-dtf98
Nov 27 05:10:31.414: INFO: Got endpoints: latency-svc-q6hpp [760.834383ms]
Nov 27 05:10:31.438: INFO: Created: latency-svc-qdfl9
Nov 27 05:10:31.451: INFO: Got endpoints: latency-svc-8q7bd [746.83303ms]
Nov 27 05:10:31.477: INFO: Created: latency-svc-ljqdc
Nov 27 05:10:31.502: INFO: Got endpoints: latency-svc-lmtcr [751.951232ms]
Nov 27 05:10:31.525: INFO: Created: latency-svc-wnp4k
Nov 27 05:10:31.550: INFO: Got endpoints: latency-svc-4j2l2 [746.617918ms]
Nov 27 05:10:31.575: INFO: Created: latency-svc-955s2
Nov 27 05:10:31.606: INFO: Got endpoints: latency-svc-l7v2g [755.971872ms]
Nov 27 05:10:31.648: INFO: Created: latency-svc-p8ssv
Nov 27 05:10:31.659: INFO: Got endpoints: latency-svc-cpxrn [756.432408ms]
Nov 27 05:10:31.688: INFO: Created: latency-svc-2db5j
Nov 27 05:10:31.704: INFO: Got endpoints: latency-svc-l8msd [754.00213ms]
Nov 27 05:10:31.738: INFO: Created: latency-svc-tj2ff
Nov 27 05:10:31.752: INFO: Got endpoints: latency-svc-bm5x8 [746.79783ms]
Nov 27 05:10:31.771: INFO: Created: latency-svc-vkxwz
Nov 27 05:10:31.802: INFO: Got endpoints: latency-svc-ttksv [751.67843ms]
Nov 27 05:10:31.829: INFO: Created: latency-svc-kn4r6
Nov 27 05:10:31.849: INFO: Got endpoints: latency-svc-gxnks [747.669523ms]
Nov 27 05:10:31.872: INFO: Created: latency-svc-sxcmx
Nov 27 05:10:31.903: INFO: Got endpoints: latency-svc-hgkjw [747.076498ms]
Nov 27 05:10:31.925: INFO: Created: latency-svc-4qkpk
Nov 27 05:10:31.953: INFO: Got endpoints: latency-svc-9jj8d [746.362717ms]
Nov 27 05:10:31.976: INFO: Created: latency-svc-92pc4
Nov 27 05:10:32.004: INFO: Got endpoints: latency-svc-vc57x [752.557945ms]
Nov 27 05:10:32.028: INFO: Created: latency-svc-bdcv9
Nov 27 05:10:32.049: INFO: Got endpoints: latency-svc-mqgf5 [745.776048ms]
Nov 27 05:10:32.073: INFO: Created: latency-svc-vrs5f
Nov 27 05:10:32.103: INFO: Got endpoints: latency-svc-dtf98 [750.875182ms]
Nov 27 05:10:32.127: INFO: Created: latency-svc-g8tkl
Nov 27 05:10:32.153: INFO: Got endpoints: latency-svc-qdfl9 [738.969216ms]
Nov 27 05:10:32.173: INFO: Created: latency-svc-n84j8
Nov 27 05:10:32.202: INFO: Got endpoints: latency-svc-ljqdc [751.70483ms]
Nov 27 05:10:32.223: INFO: Created: latency-svc-8n4wk
Nov 27 05:10:32.249: INFO: Got endpoints: latency-svc-wnp4k [746.566096ms]
Nov 27 05:10:32.267: INFO: Created: latency-svc-9jhh2
Nov 27 05:10:32.302: INFO: Got endpoints: latency-svc-955s2 [751.300517ms]
Nov 27 05:10:32.384: INFO: Created: latency-svc-xgzqn
Nov 27 05:10:32.408: INFO: Got endpoints: latency-svc-p8ssv [801.423103ms]
Nov 27 05:10:32.416: INFO: Got endpoints: latency-svc-2db5j [754.893378ms]
Nov 27 05:10:32.504: INFO: Got endpoints: latency-svc-tj2ff [798.944914ms]
Nov 27 05:10:32.508: INFO: Created: latency-svc-2z5pd
Nov 27 05:10:32.603: INFO: Got endpoints: latency-svc-vkxwz [850.798174ms]
Nov 27 05:10:32.603: INFO: Created: latency-svc-774rj
Nov 27 05:10:32.610: INFO: Got endpoints: latency-svc-kn4r6 [807.983045ms]
Nov 27 05:10:32.691: INFO: Created: latency-svc-b85b5
Nov 27 05:10:32.812: INFO: Created: latency-svc-nkqdk
Nov 27 05:10:32.813: INFO: Got endpoints: latency-svc-bdcv9 [808.314023ms]
Nov 27 05:10:32.813: INFO: Created: latency-svc-dg2gt
Nov 27 05:10:32.813: INFO: Got endpoints: latency-svc-sxcmx [963.251447ms]
Nov 27 05:10:32.813: INFO: Got endpoints: latency-svc-4qkpk [909.772623ms]
Nov 27 05:10:32.814: INFO: Got endpoints: latency-svc-92pc4 [860.311507ms]
Nov 27 05:10:32.883: INFO: Got endpoints: latency-svc-vrs5f [833.300761ms]
Nov 27 05:10:32.907: INFO: Got endpoints: latency-svc-g8tkl [804.018138ms]
Nov 27 05:10:32.916: INFO: Got endpoints: latency-svc-n84j8 [763.317951ms]
Nov 27 05:10:32.926: INFO: Created: latency-svc-tmjdc
Nov 27 05:10:32.996: INFO: Created: latency-svc-2cwpd
Nov 27 05:10:33.005: INFO: Got endpoints: latency-svc-8n4wk [802.569064ms]
Nov 27 05:10:33.028: INFO: Got endpoints: latency-svc-9jhh2 [778.56842ms]
Nov 27 05:10:33.028: INFO: Created: latency-svc-zxb8p
Nov 27 05:10:33.119: INFO: Got endpoints: latency-svc-2z5pd [710.882909ms]
Nov 27 05:10:33.120: INFO: Got endpoints: latency-svc-xgzqn [818.393981ms]
Nov 27 05:10:33.186: INFO: Created: latency-svc-5ftjs
Nov 27 05:10:33.187: INFO: Created: latency-svc-qn5rv
Nov 27 05:10:33.206: INFO: Got endpoints: latency-svc-774rj [789.143624ms]
Nov 27 05:10:33.206: INFO: Got endpoints: latency-svc-b85b5 [702.244692ms]
Nov 27 05:10:33.206: INFO: Created: latency-svc-6gpfw
Nov 27 05:10:33.295: INFO: Created: latency-svc-rsh28
Nov 27 05:10:33.302: INFO: Got endpoints: latency-svc-nkqdk [699.1373ms]
Nov 27 05:10:33.308: INFO: Created: latency-svc-26k88
Nov 27 05:10:33.313: INFO: Got endpoints: latency-svc-dg2gt [702.869983ms]
Nov 27 05:10:33.396: INFO: Got endpoints: latency-svc-tmjdc [582.993878ms]
Nov 27 05:10:33.404: INFO: Got endpoints: latency-svc-2cwpd [589.895065ms]
Nov 27 05:10:33.413: INFO: Created: latency-svc-jz2hf
Nov 27 05:10:33.434: INFO: Created: latency-svc-mxqfc
Nov 27 05:10:33.492: INFO: Created: latency-svc-t6tnd
Nov 27 05:10:33.497: INFO: Got endpoints: latency-svc-zxb8p [683.948431ms]
Nov 27 05:10:33.621: INFO: Got endpoints: latency-svc-5ftjs [737.602277ms]
Nov 27 05:10:33.621: INFO: Got endpoints: latency-svc-qn5rv [807.383664ms]
Nov 27 05:10:33.625: INFO: Created: latency-svc-t7w6l
Nov 27 05:10:33.681: INFO: Got endpoints: latency-svc-6gpfw [773.979199ms]
Nov 27 05:10:33.698: INFO: Created: latency-svc-sn57m
Nov 27 05:10:33.717: INFO: Got endpoints: latency-svc-rsh28 [800.211142ms]
Nov 27 05:10:33.725: INFO: Got endpoints: latency-svc-26k88 [719.616461ms]
Nov 27 05:10:33.735: INFO: Created: latency-svc-5pjr9
Nov 27 05:10:33.795: INFO: Created: latency-svc-mwgq8
Nov 27 05:10:33.799: INFO: Got endpoints: latency-svc-jz2hf [770.725629ms]
Nov 27 05:10:33.809: INFO: Created: latency-svc-6nqsb
Nov 27 05:10:33.810: INFO: Got endpoints: latency-svc-mxqfc [689.334544ms]
Nov 27 05:10:33.824: INFO: Created: latency-svc-qbrrm
Nov 27 05:10:33.843: INFO: Created: latency-svc-lr5rd
Nov 27 05:10:33.881: INFO: Got endpoints: latency-svc-t6tnd [761.908477ms]
Nov 27 05:10:33.889: INFO: Created: latency-svc-6598m
Nov 27 05:10:33.892: INFO: Created: latency-svc-85p9p
Nov 27 05:10:33.901: INFO: Created: latency-svc-7hbhj
Nov 27 05:10:33.905: INFO: Got endpoints: latency-svc-sn57m [675.241768ms]
Nov 27 05:10:33.937: INFO: Created: latency-svc-455bq
Nov 27 05:10:33.945: INFO: Created: latency-svc-5kx2d
Nov 27 05:10:33.958: INFO: Got endpoints: latency-svc-t7w6l [729.913486ms]
Nov 27 05:10:33.980: INFO: Created: latency-svc-h25nq
Nov 27 05:10:33.997: INFO: Created: latency-svc-6pgz5
Nov 27 05:10:34.015: INFO: Got endpoints: latency-svc-5pjr9 [713.098075ms]
Nov 27 05:10:34.025: INFO: Created: latency-svc-9tf45
Nov 27 05:10:34.039: INFO: Created: latency-svc-kzf2b
Nov 27 05:10:34.056: INFO: Created: latency-svc-g769k
Nov 27 05:10:34.057: INFO: Got endpoints: latency-svc-mwgq8 [743.555993ms]
Nov 27 05:10:34.082: INFO: Created: latency-svc-gsxwb
Nov 27 05:10:34.104: INFO: Got endpoints: latency-svc-6nqsb [700.479796ms]
Nov 27 05:10:34.126: INFO: Created: latency-svc-mdk8r
Nov 27 05:10:34.153: INFO: Created: latency-svc-p5qf5
Nov 27 05:10:34.154: INFO: Got endpoints: latency-svc-qbrrm [758.090237ms]
Nov 27 05:10:34.180: INFO: Created: latency-svc-fqr6v
Nov 27 05:10:34.212: INFO: Got endpoints: latency-svc-lr5rd [712.131537ms]
Nov 27 05:10:34.240: INFO: Created: latency-svc-pn74r
Nov 27 05:10:34.252: INFO: Got endpoints: latency-svc-6598m [628.422131ms]
Nov 27 05:10:34.278: INFO: Created: latency-svc-d4gjl
Nov 27 05:10:34.302: INFO: Got endpoints: latency-svc-85p9p [680.803394ms]
Nov 27 05:10:34.328: INFO: Created: latency-svc-d84g4
Nov 27 05:10:34.351: INFO: Got endpoints: latency-svc-7hbhj [670.346501ms]
Nov 27 05:10:34.372: INFO: Created: latency-svc-lm6w8
Nov 27 05:10:34.402: INFO: Got endpoints: latency-svc-455bq [684.418255ms]
Nov 27 05:10:34.425: INFO: Created: latency-svc-472mn
Nov 27 05:10:34.451: INFO: Got endpoints: latency-svc-5kx2d [725.487555ms]
Nov 27 05:10:34.476: INFO: Created: latency-svc-rd5wb
Nov 27 05:10:34.504: INFO: Got endpoints: latency-svc-h25nq [704.962438ms]
Nov 27 05:10:34.525: INFO: Created: latency-svc-7m8th
Nov 27 05:10:34.549: INFO: Got endpoints: latency-svc-6pgz5 [738.99415ms]
Nov 27 05:10:34.574: INFO: Created: latency-svc-x2l6z
Nov 27 05:10:34.603: INFO: Got endpoints: latency-svc-9tf45 [721.225402ms]
Nov 27 05:10:34.628: INFO: Created: latency-svc-wxs4b
Nov 27 05:10:34.651: INFO: Got endpoints: latency-svc-kzf2b [745.884092ms]
Nov 27 05:10:34.675: INFO: Created: latency-svc-jr5cc
Nov 27 05:10:34.711: INFO: Got endpoints: latency-svc-g769k [753.02777ms]
Nov 27 05:10:34.736: INFO: Created: latency-svc-mwfg9
Nov 27 05:10:34.750: INFO: Got endpoints: latency-svc-gsxwb [734.219639ms]
Nov 27 05:10:34.769: INFO: Created: latency-svc-6b8tz
Nov 27 05:10:34.808: INFO: Got endpoints: latency-svc-mdk8r [750.665448ms]
Nov 27 05:10:34.832: INFO: Created: latency-svc-l9kkt
Nov 27 05:10:34.849: INFO: Got endpoints: latency-svc-p5qf5 [742.369455ms]
Nov 27 05:10:34.870: INFO: Created: latency-svc-nnwvs
Nov 27 05:10:34.901: INFO: Got endpoints: latency-svc-fqr6v [746.980187ms]
Nov 27 05:10:34.927: INFO: Created: latency-svc-jc4ln
Nov 27 05:10:34.951: INFO: Got endpoints: latency-svc-pn74r [738.965794ms]
Nov 27 05:10:34.976: INFO: Created: latency-svc-8msj2
Nov 27 05:10:35.003: INFO: Got endpoints: latency-svc-d4gjl [750.676204ms]
Nov 27 05:10:35.027: INFO: Created: latency-svc-pv85z
Nov 27 05:10:35.050: INFO: Got endpoints: latency-svc-d84g4 [747.279877ms]
Nov 27 05:10:35.080: INFO: Created: latency-svc-kmdgs
Nov 27 05:10:35.101: INFO: Got endpoints: latency-svc-lm6w8 [749.936511ms]
Nov 27 05:10:35.125: INFO: Created: latency-svc-ldvgw
Nov 27 05:10:35.150: INFO: Got endpoints: latency-svc-472mn [747.869968ms]
Nov 27 05:10:35.172: INFO: Created: latency-svc-pb8pn
Nov 27 05:10:35.202: INFO: Got endpoints: latency-svc-rd5wb [750.96465ms]
Nov 27 05:10:35.251: INFO: Got endpoints: latency-svc-7m8th [746.242938ms]
Nov 27 05:10:35.300: INFO: Got endpoints: latency-svc-x2l6z [750.811137ms]
Nov 27 05:10:35.350: INFO: Got endpoints: latency-svc-wxs4b [746.616941ms]
Nov 27 05:10:35.401: INFO: Got endpoints: latency-svc-jr5cc [749.133752ms]
Nov 27 05:10:35.450: INFO: Got endpoints: latency-svc-mwfg9 [738.642637ms]
Nov 27 05:10:35.504: INFO: Got endpoints: latency-svc-6b8tz [753.847151ms]
Nov 27 05:10:35.550: INFO: Got endpoints: latency-svc-l9kkt [742.081498ms]
Nov 27 05:10:35.608: INFO: Got endpoints: latency-svc-nnwvs [758.757085ms]
Nov 27 05:10:35.651: INFO: Got endpoints: latency-svc-jc4ln [749.527798ms]
Nov 27 05:10:35.699: INFO: Got endpoints: latency-svc-8msj2 [747.672456ms]
Nov 27 05:10:35.750: INFO: Got endpoints: latency-svc-pv85z [746.67903ms]
Nov 27 05:10:35.801: INFO: Got endpoints: latency-svc-kmdgs [751.251139ms]
Nov 27 05:10:35.850: INFO: Got endpoints: latency-svc-ldvgw [748.937217ms]
Nov 27 05:10:35.899: INFO: Got endpoints: latency-svc-pb8pn [749.500909ms]
Nov 27 05:10:35.900: INFO: Latencies: [64.228073ms 75.813814ms 77.908225ms 80.859171ms 124.617861ms 129.102948ms 156.992721ms 187.100104ms 209.191138ms 228.905184ms 263.362232ms 277.059006ms 290.007776ms 298.133147ms 312.264056ms 314.307132ms 319.640935ms 323.997444ms 326.241943ms 328.109996ms 329.648536ms 332.100814ms 333.930734ms 336.258345ms 337.03177ms 337.122704ms 337.924974ms 338.287732ms 340.341074ms 341.464058ms 342.490729ms 343.05882ms 345.907589ms 350.335965ms 351.034101ms 352.778954ms 353.978203ms 355.961635ms 357.231285ms 360.201299ms 361.051481ms 365.430479ms 366.807685ms 368.361869ms 369.752765ms 369.759609ms 375.417546ms 376.942886ms 378.094225ms 380.507392ms 381.596641ms 386.415642ms 387.080045ms 393.359851ms 397.44307ms 408.51352ms 441.102514ms 463.093327ms 469.613179ms 477.961973ms 502.412885ms 509.979942ms 561.152177ms 582.993878ms 587.195408ms 589.895065ms 613.913798ms 628.422131ms 666.853374ms 670.346501ms 675.241768ms 680.803394ms 683.948431ms 684.418255ms 689.334544ms 693.484252ms 699.1373ms 700.18206ms 700.479796ms 702.244692ms 702.869983ms 704.962438ms 710.882909ms 711.60989ms 712.131537ms 713.098075ms 719.616461ms 721.225402ms 725.487555ms 728.776325ms 729.913486ms 730.507488ms 734.219639ms 737.496676ms 737.602277ms 738.642637ms 738.965794ms 738.969216ms 738.99415ms 740.062376ms 742.081498ms 742.316165ms 742.336698ms 742.369455ms 743.555993ms 743.837594ms 744.543064ms 745.234845ms 745.515469ms 745.640625ms 745.776048ms 745.884092ms 746.078671ms 746.242938ms 746.362717ms 746.566096ms 746.616941ms 746.617918ms 746.67903ms 746.708363ms 746.79783ms 746.83303ms 746.980187ms 747.076498ms 747.171343ms 747.227565ms 747.279877ms 747.383522ms 747.608901ms 747.669523ms 747.672456ms 747.869968ms 747.964814ms 748.127614ms 748.373527ms 748.775395ms 748.937217ms 748.950418ms 749.133752ms 749.462286ms 749.500909ms 749.527798ms 749.647576ms 749.681799ms 749.812332ms 749.936511ms 750.210779ms 750.235224ms 750.527092ms 750.665448ms 750.676204ms 750.77887ms 750.811137ms 750.838026ms 750.875182ms 750.96465ms 751.014027ms 751.251139ms 751.300517ms 751.60803ms 751.67843ms 751.70483ms 751.951232ms 752.290033ms 752.3355ms 752.557945ms 752.594123ms 753.02777ms 753.043415ms 753.101104ms 753.847151ms 754.00213ms 754.893378ms 755.050313ms 755.971872ms 756.432408ms 758.090237ms 758.757085ms 759.300732ms 760.834383ms 761.908477ms 763.317951ms 770.725629ms 773.979199ms 778.56842ms 789.143624ms 798.944914ms 800.211142ms 801.423103ms 802.569064ms 804.018138ms 807.383664ms 807.983045ms 808.314023ms 818.393981ms 833.300761ms 850.798174ms 860.311507ms 909.772623ms 963.251447ms]
Nov 27 05:10:35.900: INFO: 50 %ile: 742.081498ms
Nov 27 05:10:35.900: INFO: 90 %ile: 761.908477ms
Nov 27 05:10:35.900: INFO: 99 %ile: 909.772623ms
Nov 27 05:10:35.900: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:10:35.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7050" for this suite.
Nov 27 05:10:55.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:10:56.191: INFO: namespace svc-latency-7050 deletion completed in 20.278433761s

• [SLOW TEST:34.379 seconds]
[sig-network] Service endpoints latency
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:10:56.195: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9120
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 05:10:56.512: INFO: Creating deployment "test-recreate-deployment"
Nov 27 05:10:56.526: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Nov 27 05:10:56.541: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Nov 27 05:10:58.560: INFO: Waiting deployment "test-recreate-deployment" to complete
Nov 27 05:10:58.567: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710428256, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710428256, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710428256, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710428256, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 05:11:00.578: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Nov 27 05:11:00.600: INFO: Updating deployment test-recreate-deployment
Nov 27 05:11:00.600: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Nov 27 05:11:00.799: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9120 /apis/apps/v1/namespaces/deployment-9120/deployments/test-recreate-deployment 3497e636-3b1a-4255-9e7d-5fd4dddff5b1 956599 2 2019-11-27 05:10:56 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004f731e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2019-11-27 05:11:00 +0000 UTC,LastTransitionTime:2019-11-27 05:11:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2019-11-27 05:11:00 +0000 UTC,LastTransitionTime:2019-11-27 05:10:56 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Nov 27 05:11:00.813: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-9120 /apis/apps/v1/namespaces/deployment-9120/replicasets/test-recreate-deployment-5f94c574ff 95b3bd83-4613-47f5-94ec-8072bc4b4da1 956598 1 2019-11-27 05:11:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 3497e636-3b1a-4255-9e7d-5fd4dddff5b1 0xc002956297 0xc002956298}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0029562f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 27 05:11:00.814: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Nov 27 05:11:00.814: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-9120 /apis/apps/v1/namespaces/deployment-9120/replicasets/test-recreate-deployment-68fc85c7bb 57f9b0eb-ccb4-44fc-874b-6320dc74cd69 956588 2 2019-11-27 05:10:56 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 3497e636-3b1a-4255-9e7d-5fd4dddff5b1 0xc002956367 0xc002956368}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0029563c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 27 05:11:00.825: INFO: Pod "test-recreate-deployment-5f94c574ff-zwfrm" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-zwfrm test-recreate-deployment-5f94c574ff- deployment-9120 /api/v1/namespaces/deployment-9120/pods/test-recreate-deployment-5f94c574ff-zwfrm 3bc30df6-288b-46de-8025-26e7196f0afe 956600 0 2019-11-27 05:11:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 95b3bd83-4613-47f5-94ec-8072bc4b4da1 0xc004f735b7 0xc004f735b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kf6nt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kf6nt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kf6nt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 05:11:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 05:11:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 05:11:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 05:11:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.17,PodIP:,StartTime:2019-11-27 05:11:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:11:00.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9120" for this suite.
Nov 27 05:11:06.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:11:07.112: INFO: namespace deployment-9120 deletion completed in 6.275091266s

• [SLOW TEST:10.918 seconds]
[sig-apps] Deployment
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:11:07.113: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1279
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:11:11.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1279" for this suite.
Nov 27 05:11:57.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:11:57.811: INFO: namespace kubelet-test-1279 deletion completed in 46.286795307s

• [SLOW TEST:50.698 seconds]
[k8s.io] Kubelet
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:11:57.812: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8915
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Nov 27 05:11:58.148: INFO: Waiting up to 5m0s for pod "pod-32b092b5-9a9d-4845-9b05-7a2d969e309e" in namespace "emptydir-8915" to be "success or failure"
Nov 27 05:11:58.157: INFO: Pod "pod-32b092b5-9a9d-4845-9b05-7a2d969e309e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.951326ms
Nov 27 05:12:00.167: INFO: Pod "pod-32b092b5-9a9d-4845-9b05-7a2d969e309e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018709228s
Nov 27 05:12:02.176: INFO: Pod "pod-32b092b5-9a9d-4845-9b05-7a2d969e309e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027504721s
Nov 27 05:12:04.187: INFO: Pod "pod-32b092b5-9a9d-4845-9b05-7a2d969e309e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038639069s
STEP: Saw pod success
Nov 27 05:12:04.187: INFO: Pod "pod-32b092b5-9a9d-4845-9b05-7a2d969e309e" satisfied condition "success or failure"
Nov 27 05:12:04.194: INFO: Trying to get logs from node slave1 pod pod-32b092b5-9a9d-4845-9b05-7a2d969e309e container test-container: <nil>
STEP: delete the pod
Nov 27 05:12:04.259: INFO: Waiting for pod pod-32b092b5-9a9d-4845-9b05-7a2d969e309e to disappear
Nov 27 05:12:04.268: INFO: Pod pod-32b092b5-9a9d-4845-9b05-7a2d969e309e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:12:04.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8915" for this suite.
Nov 27 05:12:10.306: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:12:10.578: INFO: namespace emptydir-8915 deletion completed in 6.298128794s

• [SLOW TEST:12.766 seconds]
[sig-storage] EmptyDir volumes
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:12:10.579: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9802
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:12:10.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9802" for this suite.
Nov 27 05:12:38.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:12:39.225: INFO: namespace pods-9802 deletion completed in 28.271576963s

• [SLOW TEST:28.646 seconds]
[k8s.io] [sig-node] Pods Extended
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:12:39.226: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9030
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Nov 27 05:12:45.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec pod-sharedvolume-c4481c4f-127b-42b5-a348-2ce3b3a4e321 -c busybox-main-container --namespace=emptydir-9030 -- cat /usr/share/volumeshare/shareddata.txt'
Nov 27 05:12:47.188: INFO: stderr: ""
Nov 27 05:12:47.188: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:12:47.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9030" for this suite.
Nov 27 05:12:53.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:12:53.498: INFO: namespace emptydir-9030 deletion completed in 6.290224891s

• [SLOW TEST:14.272 seconds]
[sig-storage] EmptyDir volumes
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:12:53.500: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2577
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 27 05:12:58.902: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:12:58.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2577" for this suite.
Nov 27 05:13:04.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:13:05.259: INFO: namespace container-runtime-2577 deletion completed in 6.305490028s

• [SLOW TEST:11.759 seconds]
[k8s.io] Container Runtime
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:13:05.260: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7314
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7314
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Nov 27 05:13:05.614: INFO: Found 0 stateful pods, waiting for 3
Nov 27 05:13:15.630: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 27 05:13:15.630: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 27 05:13:15.630: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 27 05:13:25.626: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 27 05:13:25.626: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 27 05:13:25.626: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Nov 27 05:13:25.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-7314 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 27 05:13:27.503: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 27 05:13:27.503: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 27 05:13:27.503: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Nov 27 05:13:37.609: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Nov 27 05:13:47.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-7314 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 05:13:49.059: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 27 05:13:49.059: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 27 05:13:49.059: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 27 05:13:59.245: INFO: Waiting for StatefulSet statefulset-7314/ss2 to complete update
Nov 27 05:13:59.245: INFO: Waiting for Pod statefulset-7314/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 27 05:13:59.245: INFO: Waiting for Pod statefulset-7314/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 27 05:13:59.246: INFO: Waiting for Pod statefulset-7314/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 27 05:14:09.270: INFO: Waiting for StatefulSet statefulset-7314/ss2 to complete update
Nov 27 05:14:09.271: INFO: Waiting for Pod statefulset-7314/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 27 05:14:09.271: INFO: Waiting for Pod statefulset-7314/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 27 05:14:19.277: INFO: Waiting for StatefulSet statefulset-7314/ss2 to complete update
Nov 27 05:14:19.277: INFO: Waiting for Pod statefulset-7314/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 27 05:14:29.267: INFO: Waiting for StatefulSet statefulset-7314/ss2 to complete update
Nov 27 05:14:29.267: INFO: Waiting for Pod statefulset-7314/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Nov 27 05:14:39.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-7314 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 27 05:14:40.643: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 27 05:14:40.644: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 27 05:14:40.644: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 27 05:14:50.727: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Nov 27 05:15:00.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-7314 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 05:15:02.093: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 27 05:15:02.094: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 27 05:15:02.094: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 27 05:15:12.162: INFO: Waiting for StatefulSet statefulset-7314/ss2 to complete update
Nov 27 05:15:12.162: INFO: Waiting for Pod statefulset-7314/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Nov 27 05:15:12.162: INFO: Waiting for Pod statefulset-7314/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Nov 27 05:15:12.162: INFO: Waiting for Pod statefulset-7314/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Nov 27 05:15:22.192: INFO: Waiting for StatefulSet statefulset-7314/ss2 to complete update
Nov 27 05:15:22.192: INFO: Waiting for Pod statefulset-7314/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Nov 27 05:15:22.192: INFO: Waiting for Pod statefulset-7314/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Nov 27 05:15:32.186: INFO: Waiting for StatefulSet statefulset-7314/ss2 to complete update
Nov 27 05:15:32.186: INFO: Waiting for Pod statefulset-7314/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Nov 27 05:15:42.185: INFO: Waiting for StatefulSet statefulset-7314/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Nov 27 05:15:52.189: INFO: Deleting all statefulset in ns statefulset-7314
Nov 27 05:15:52.200: INFO: Scaling statefulset ss2 to 0
Nov 27 05:16:42.245: INFO: Waiting for statefulset status.replicas updated to 0
Nov 27 05:16:42.255: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:16:42.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7314" for this suite.
Nov 27 05:16:50.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:16:50.580: INFO: namespace statefulset-7314 deletion completed in 8.277019794s

• [SLOW TEST:225.320 seconds]
[sig-apps] StatefulSet
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:16:50.580: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-7255
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Nov 27 05:17:08.411: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Nov 27 05:17:10.442: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710428628, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710428628, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710428628, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710428628, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 05:17:12.453: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710428628, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710428628, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710428628, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710428628, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 27 05:17:15.485: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 05:17:15.495: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:17:18.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7255" for this suite.
Nov 27 05:17:27.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:17:27.316: INFO: namespace crd-webhook-7255 deletion completed in 8.387452479s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:36.777 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:17:27.359: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4948
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Nov 27 05:17:27.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 create -f - --namespace=kubectl-4948'
Nov 27 05:17:29.476: INFO: stderr: ""
Nov 27 05:17:29.477: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 27 05:17:29.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4948'
Nov 27 05:17:30.287: INFO: stderr: ""
Nov 27 05:17:30.287: INFO: stdout: "update-demo-nautilus-6rrl7 update-demo-nautilus-xb659 "
Nov 27 05:17:30.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods update-demo-nautilus-6rrl7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4948'
Nov 27 05:17:31.014: INFO: stderr: ""
Nov 27 05:17:31.014: INFO: stdout: ""
Nov 27 05:17:31.014: INFO: update-demo-nautilus-6rrl7 is created but not running
Nov 27 05:17:36.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4948'
Nov 27 05:17:36.736: INFO: stderr: ""
Nov 27 05:17:36.737: INFO: stdout: "update-demo-nautilus-6rrl7 update-demo-nautilus-xb659 "
Nov 27 05:17:36.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods update-demo-nautilus-6rrl7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4948'
Nov 27 05:17:37.472: INFO: stderr: ""
Nov 27 05:17:37.473: INFO: stdout: "true"
Nov 27 05:17:37.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods update-demo-nautilus-6rrl7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4948'
Nov 27 05:17:38.219: INFO: stderr: ""
Nov 27 05:17:38.219: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 27 05:17:38.219: INFO: validating pod update-demo-nautilus-6rrl7
Nov 27 05:17:38.236: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 27 05:17:38.236: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 27 05:17:38.236: INFO: update-demo-nautilus-6rrl7 is verified up and running
Nov 27 05:17:38.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods update-demo-nautilus-xb659 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4948'
Nov 27 05:17:38.940: INFO: stderr: ""
Nov 27 05:17:38.941: INFO: stdout: "true"
Nov 27 05:17:38.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods update-demo-nautilus-xb659 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4948'
Nov 27 05:17:39.672: INFO: stderr: ""
Nov 27 05:17:39.673: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 27 05:17:39.673: INFO: validating pod update-demo-nautilus-xb659
Nov 27 05:17:39.691: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 27 05:17:39.692: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 27 05:17:39.692: INFO: update-demo-nautilus-xb659 is verified up and running
STEP: rolling-update to new replication controller
Nov 27 05:17:39.701: INFO: scanned /root for discovery docs: <nil>
Nov 27 05:17:39.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-4948'
Nov 27 05:18:03.891: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Nov 27 05:18:03.892: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 27 05:18:03.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4948'
Nov 27 05:18:04.600: INFO: stderr: ""
Nov 27 05:18:04.600: INFO: stdout: "update-demo-kitten-2txlq update-demo-kitten-5kfmv "
Nov 27 05:18:04.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods update-demo-kitten-2txlq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4948'
Nov 27 05:18:05.359: INFO: stderr: ""
Nov 27 05:18:05.360: INFO: stdout: "true"
Nov 27 05:18:05.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods update-demo-kitten-2txlq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4948'
Nov 27 05:18:06.119: INFO: stderr: ""
Nov 27 05:18:06.119: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Nov 27 05:18:06.119: INFO: validating pod update-demo-kitten-2txlq
Nov 27 05:18:06.140: INFO: got data: {
  "image": "kitten.jpg"
}

Nov 27 05:18:06.140: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Nov 27 05:18:06.140: INFO: update-demo-kitten-2txlq is verified up and running
Nov 27 05:18:06.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods update-demo-kitten-5kfmv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4948'
Nov 27 05:18:06.854: INFO: stderr: ""
Nov 27 05:18:06.854: INFO: stdout: "true"
Nov 27 05:18:06.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods update-demo-kitten-5kfmv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4948'
Nov 27 05:18:07.572: INFO: stderr: ""
Nov 27 05:18:07.573: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Nov 27 05:18:07.573: INFO: validating pod update-demo-kitten-5kfmv
Nov 27 05:18:07.592: INFO: got data: {
  "image": "kitten.jpg"
}

Nov 27 05:18:07.592: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Nov 27 05:18:07.592: INFO: update-demo-kitten-5kfmv is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:18:07.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4948" for this suite.
Nov 27 05:18:37.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:18:37.911: INFO: namespace kubectl-4948 deletion completed in 30.302197045s

• [SLOW TEST:70.552 seconds]
[sig-cli] Kubectl client
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:18:37.912: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-7365
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Nov 27 05:18:38.244: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-7365" to be "success or failure"
Nov 27 05:18:38.258: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 14.088376ms
Nov 27 05:18:40.268: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024469343s
Nov 27 05:18:42.281: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036636718s
Nov 27 05:18:44.295: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051299883s
STEP: Saw pod success
Nov 27 05:18:44.296: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Nov 27 05:18:44.305: INFO: Trying to get logs from node slave1 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Nov 27 05:18:44.610: INFO: Waiting for pod pod-host-path-test to disappear
Nov 27 05:18:44.622: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:18:44.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-7365" for this suite.
Nov 27 05:18:50.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:18:50.895: INFO: namespace hostpath-7365 deletion completed in 6.258603901s

• [SLOW TEST:12.983 seconds]
[sig-storage] HostPath
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:18:50.896: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4626
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-4626
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 27 05:18:51.231: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Nov 27 05:19:17.424: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.31.161.10:8080/dial?request=hostName&protocol=http&host=172.31.51.148&port=8080&tries=1'] Namespace:pod-network-test-4626 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 27 05:19:17.424: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
Nov 27 05:19:18.088: INFO: Waiting for endpoints: map[]
Nov 27 05:19:18.099: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.31.161.10:8080/dial?request=hostName&protocol=http&host=172.31.161.9&port=8080&tries=1'] Namespace:pod-network-test-4626 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 27 05:19:18.100: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
Nov 27 05:19:18.763: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:19:18.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4626" for this suite.
Nov 27 05:19:30.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:19:31.076: INFO: namespace pod-network-test-4626 deletion completed in 12.297785151s

• [SLOW TEST:40.181 seconds]
[sig-network] Networking
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:19:31.077: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9541
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Nov 27 05:19:31.393: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Nov 27 05:20:28.051: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
Nov 27 05:20:42.583: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:21:40.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9541" for this suite.
Nov 27 05:21:46.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:21:46.714: INFO: namespace crd-publish-openapi-9541 deletion completed in 6.278606393s

• [SLOW TEST:135.637 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:21:46.716: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9401
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Nov 27 05:21:51.097: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-192585042 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Nov 27 05:21:57.049: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:21:57.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9401" for this suite.
Nov 27 05:22:03.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:22:03.389: INFO: namespace pods-9401 deletion completed in 6.313453576s

• [SLOW TEST:16.674 seconds]
[k8s.io] [sig-node] Pods Extended
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:22:03.390: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5201
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 27 05:22:03.807: INFO: Number of nodes with available pods: 0
Nov 27 05:22:03.807: INFO: Node master1 is running more than one daemon pod
Nov 27 05:22:04.829: INFO: Number of nodes with available pods: 0
Nov 27 05:22:04.829: INFO: Node master1 is running more than one daemon pod
Nov 27 05:22:05.832: INFO: Number of nodes with available pods: 0
Nov 27 05:22:05.832: INFO: Node master1 is running more than one daemon pod
Nov 27 05:22:06.831: INFO: Number of nodes with available pods: 0
Nov 27 05:22:06.831: INFO: Node master1 is running more than one daemon pod
Nov 27 05:22:07.830: INFO: Number of nodes with available pods: 2
Nov 27 05:22:07.830: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Nov 27 05:22:07.899: INFO: Number of nodes with available pods: 1
Nov 27 05:22:07.899: INFO: Node master1 is running more than one daemon pod
Nov 27 05:22:08.927: INFO: Number of nodes with available pods: 1
Nov 27 05:22:08.927: INFO: Node master1 is running more than one daemon pod
Nov 27 05:22:09.926: INFO: Number of nodes with available pods: 1
Nov 27 05:22:09.926: INFO: Node master1 is running more than one daemon pod
Nov 27 05:22:10.926: INFO: Number of nodes with available pods: 1
Nov 27 05:22:10.926: INFO: Node master1 is running more than one daemon pod
Nov 27 05:22:11.930: INFO: Number of nodes with available pods: 2
Nov 27 05:22:11.930: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5201, will wait for the garbage collector to delete the pods
I1127 05:22:11.950198      19 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 05:22:11.950379      19 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Nov 27 05:22:12.017: INFO: Deleting DaemonSet.extensions daemon-set took: 17.484214ms
I1127 05:22:12.518556      19 controller_utils.go:810] Ignoring inactive pod daemonsets-5201/daemon-set-g82zj in state Running, deletion time 2019-11-27 05:22:42 +0000 UTC
I1127 05:22:12.518753      19 controller_utils.go:810] Ignoring inactive pod daemonsets-5201/daemon-set-pjh4z in state Running, deletion time 2019-11-27 05:22:42 +0000 UTC
Nov 27 05:22:12.518: INFO: Terminating DaemonSet.extensions daemon-set pods took: 501.021501ms
Nov 27 05:22:29.131: INFO: Number of nodes with available pods: 0
Nov 27 05:22:29.131: INFO: Number of running nodes: 0, number of available pods: 0
Nov 27 05:22:29.140: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5201/daemonsets","resourceVersion":"958800"},"items":null}

Nov 27 05:22:29.148: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5201/pods","resourceVersion":"958800"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:22:29.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5201" for this suite.
Nov 27 05:22:35.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:22:35.488: INFO: namespace daemonsets-5201 deletion completed in 6.292959259s

• [SLOW TEST:32.098 seconds]
[sig-apps] Daemon set [Serial]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:22:35.489: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4201
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-4201
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Nov 27 05:22:35.844: INFO: Found 0 stateful pods, waiting for 3
Nov 27 05:22:45.862: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 27 05:22:45.862: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 27 05:22:45.862: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 27 05:22:55.859: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 27 05:22:55.860: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 27 05:22:55.860: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Nov 27 05:22:55.926: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Nov 27 05:23:06.003: INFO: Updating stateful set ss2
Nov 27 05:23:06.042: INFO: Waiting for Pod statefulset-4201/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Nov 27 05:23:16.200: INFO: Found 2 stateful pods, waiting for 3
Nov 27 05:23:26.218: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 27 05:23:26.218: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 27 05:23:26.218: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Nov 27 05:23:26.269: INFO: Updating stateful set ss2
Nov 27 05:23:26.303: INFO: Waiting for Pod statefulset-4201/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 27 05:23:36.326: INFO: Waiting for Pod statefulset-4201/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 27 05:23:46.360: INFO: Updating stateful set ss2
Nov 27 05:23:46.381: INFO: Waiting for StatefulSet statefulset-4201/ss2 to complete update
Nov 27 05:23:46.381: INFO: Waiting for Pod statefulset-4201/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 27 05:23:56.406: INFO: Waiting for StatefulSet statefulset-4201/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Nov 27 05:24:06.404: INFO: Deleting all statefulset in ns statefulset-4201
Nov 27 05:24:06.412: INFO: Scaling statefulset ss2 to 0
Nov 27 05:24:36.461: INFO: Waiting for statefulset status.replicas updated to 0
Nov 27 05:24:36.475: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:24:36.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4201" for this suite.
Nov 27 05:24:44.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:24:44.822: INFO: namespace statefulset-4201 deletion completed in 8.300809237s

• [SLOW TEST:129.333 seconds]
[sig-apps] StatefulSet
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:24:44.823: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7904
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Nov 27 05:24:45.129: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:26:09.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7904" for this suite.
Nov 27 05:26:15.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:26:15.376: INFO: namespace crd-publish-openapi-7904 deletion completed in 6.268150968s

• [SLOW TEST:90.553 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:26:15.377: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7758
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 27 05:26:34.853: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Nov 27 05:26:36.893: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710429194, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710429194, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710429194, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710429194, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 05:26:38.903: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710429194, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710429194, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710429194, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710429194, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 27 05:26:41.930: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 05:26:41.939: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9140-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:26:44.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7758" for this suite.
Nov 27 05:26:52.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:26:52.839: INFO: namespace webhook-7758 deletion completed in 8.40968676s
STEP: Destroying namespace "webhook-7758-markers" for this suite.
Nov 27 05:26:58.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:26:59.101: INFO: namespace webhook-7758-markers deletion completed in 6.261903428s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:43.758 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:26:59.136: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9820
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 27 05:27:22.707: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 27 05:27:24.734: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710429242, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710429242, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710429242, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710429242, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 05:27:26.745: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710429242, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710429242, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710429242, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710429242, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 27 05:27:29.765: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
Nov 27 05:27:39.843: INFO: Waiting for webhook configuration to be ready...
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:27:40.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9820" for this suite.
Nov 27 05:27:46.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:27:46.779: INFO: namespace webhook-9820 deletion completed in 6.382711361s
STEP: Destroying namespace "webhook-9820-markers" for this suite.
Nov 27 05:27:52.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:27:53.047: INFO: namespace webhook-9820-markers deletion completed in 6.267236252s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:53.958 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:27:53.095: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4945
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 27 05:27:53.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-4945'
Nov 27 05:27:54.323: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Nov 27 05:27:54.323: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: rolling-update to same image controller
Nov 27 05:27:54.354: INFO: scanned /root for discovery docs: <nil>
Nov 27 05:27:54.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-4945'
Nov 27 05:28:11.943: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Nov 27 05:28:11.943: INFO: stdout: "Created e2e-test-httpd-rc-9cbdb226fe54ee6fa2fd475adfc51de9\nScaling up e2e-test-httpd-rc-9cbdb226fe54ee6fa2fd475adfc51de9 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-9cbdb226fe54ee6fa2fd475adfc51de9 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-9cbdb226fe54ee6fa2fd475adfc51de9 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Nov 27 05:28:11.943: INFO: stdout: "Created e2e-test-httpd-rc-9cbdb226fe54ee6fa2fd475adfc51de9\nScaling up e2e-test-httpd-rc-9cbdb226fe54ee6fa2fd475adfc51de9 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-9cbdb226fe54ee6fa2fd475adfc51de9 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-9cbdb226fe54ee6fa2fd475adfc51de9 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Nov 27 05:28:11.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-4945'
Nov 27 05:28:12.696: INFO: stderr: ""
Nov 27 05:28:12.697: INFO: stdout: "e2e-test-httpd-rc-9cbdb226fe54ee6fa2fd475adfc51de9-m5tth e2e-test-httpd-rc-t8wx9 "
STEP: Replicas for run=e2e-test-httpd-rc: expected=1 actual=2
Nov 27 05:28:17.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-4945'
Nov 27 05:28:18.434: INFO: stderr: ""
Nov 27 05:28:18.435: INFO: stdout: "e2e-test-httpd-rc-9cbdb226fe54ee6fa2fd475adfc51de9-m5tth "
Nov 27 05:28:18.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods e2e-test-httpd-rc-9cbdb226fe54ee6fa2fd475adfc51de9-m5tth -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4945'
Nov 27 05:28:19.105: INFO: stderr: ""
Nov 27 05:28:19.106: INFO: stdout: "true"
Nov 27 05:28:19.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods e2e-test-httpd-rc-9cbdb226fe54ee6fa2fd475adfc51de9-m5tth -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4945'
Nov 27 05:28:19.843: INFO: stderr: ""
Nov 27 05:28:19.843: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Nov 27 05:28:19.844: INFO: e2e-test-httpd-rc-9cbdb226fe54ee6fa2fd475adfc51de9-m5tth is verified up and running
[AfterEach] Kubectl rolling-update
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Nov 27 05:28:19.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 delete rc e2e-test-httpd-rc --namespace=kubectl-4945'
Nov 27 05:28:20.589: INFO: stderr: ""
Nov 27 05:28:20.590: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:28:20.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4945" for this suite.
Nov 27 05:28:48.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:28:48.877: INFO: namespace kubectl-4945 deletion completed in 28.268425571s

• [SLOW TEST:55.783 seconds]
[sig-cli] Kubectl client
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:28:48.879: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-131
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov 27 05:28:49.217: INFO: Waiting up to 5m0s for pod "pod-4b3b3015-e816-4a48-98c4-7bc57a45c85d" in namespace "emptydir-131" to be "success or failure"
Nov 27 05:28:49.225: INFO: Pod "pod-4b3b3015-e816-4a48-98c4-7bc57a45c85d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.570968ms
Nov 27 05:28:51.239: INFO: Pod "pod-4b3b3015-e816-4a48-98c4-7bc57a45c85d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021271017s
Nov 27 05:28:53.252: INFO: Pod "pod-4b3b3015-e816-4a48-98c4-7bc57a45c85d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034272929s
Nov 27 05:28:55.262: INFO: Pod "pod-4b3b3015-e816-4a48-98c4-7bc57a45c85d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044331717s
STEP: Saw pod success
Nov 27 05:28:55.262: INFO: Pod "pod-4b3b3015-e816-4a48-98c4-7bc57a45c85d" satisfied condition "success or failure"
Nov 27 05:28:55.269: INFO: Trying to get logs from node slave1 pod pod-4b3b3015-e816-4a48-98c4-7bc57a45c85d container test-container: <nil>
STEP: delete the pod
Nov 27 05:28:55.548: INFO: Waiting for pod pod-4b3b3015-e816-4a48-98c4-7bc57a45c85d to disappear
Nov 27 05:28:55.565: INFO: Pod pod-4b3b3015-e816-4a48-98c4-7bc57a45c85d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:28:55.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-131" for this suite.
Nov 27 05:29:01.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:29:01.860: INFO: namespace emptydir-131 deletion completed in 6.281031294s

• [SLOW TEST:12.981 seconds]
[sig-storage] EmptyDir volumes
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:29:01.862: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-285
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Nov 27 05:29:02.197: INFO: Pod name pod-release: Found 0 pods out of 1
Nov 27 05:29:07.208: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:29:08.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-285" for this suite.
Nov 27 05:29:14.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:29:14.556: INFO: namespace replication-controller-285 deletion completed in 6.295011624s

• [SLOW TEST:12.695 seconds]
[sig-apps] ReplicationController
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:29:14.557: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9849
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-9849
[It] should have a working scale subresource [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-9849
Nov 27 05:29:14.945: INFO: Found 0 stateful pods, waiting for 1
Nov 27 05:29:24.959: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Nov 27 05:29:25.005: INFO: Deleting all statefulset in ns statefulset-9849
Nov 27 05:29:25.015: INFO: Scaling statefulset ss to 0
Nov 27 05:29:35.072: INFO: Waiting for statefulset status.replicas updated to 0
Nov 27 05:29:35.081: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:29:35.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9849" for this suite.
Nov 27 05:29:43.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:29:43.434: INFO: namespace statefulset-9849 deletion completed in 8.29987301s

• [SLOW TEST:28.877 seconds]
[sig-apps] StatefulSet
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:29:43.435: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5454
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-6d0f1702-0583-4806-a08a-9122d6bc4693
STEP: Creating a pod to test consume secrets
Nov 27 05:29:43.840: INFO: Waiting up to 5m0s for pod "pod-secrets-2e22f552-0c2b-4697-a763-cd9ec65dbd5e" in namespace "secrets-5454" to be "success or failure"
Nov 27 05:29:43.847: INFO: Pod "pod-secrets-2e22f552-0c2b-4697-a763-cd9ec65dbd5e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.65848ms
Nov 27 05:29:45.859: INFO: Pod "pod-secrets-2e22f552-0c2b-4697-a763-cd9ec65dbd5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019365809s
Nov 27 05:29:47.868: INFO: Pod "pod-secrets-2e22f552-0c2b-4697-a763-cd9ec65dbd5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028062546s
STEP: Saw pod success
Nov 27 05:29:47.868: INFO: Pod "pod-secrets-2e22f552-0c2b-4697-a763-cd9ec65dbd5e" satisfied condition "success or failure"
Nov 27 05:29:47.876: INFO: Trying to get logs from node slave1 pod pod-secrets-2e22f552-0c2b-4697-a763-cd9ec65dbd5e container secret-volume-test: <nil>
STEP: delete the pod
Nov 27 05:29:47.946: INFO: Waiting for pod pod-secrets-2e22f552-0c2b-4697-a763-cd9ec65dbd5e to disappear
Nov 27 05:29:47.954: INFO: Pod pod-secrets-2e22f552-0c2b-4697-a763-cd9ec65dbd5e no longer exists
[AfterEach] [sig-storage] Secrets
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:29:47.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5454" for this suite.
Nov 27 05:29:53.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:29:54.229: INFO: namespace secrets-5454 deletion completed in 6.261664849s

• [SLOW TEST:10.794 seconds]
[sig-storage] Secrets
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:29:54.230: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6259
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Nov 27 05:29:54.564: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6259 /api/v1/namespaces/watch-6259/configmaps/e2e-watch-test-configmap-a 2f3e13f3-9964-4e86-bc72-1fb7a565825b 960333 0 2019-11-27 05:29:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov 27 05:29:54.564: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6259 /api/v1/namespaces/watch-6259/configmaps/e2e-watch-test-configmap-a 2f3e13f3-9964-4e86-bc72-1fb7a565825b 960333 0 2019-11-27 05:29:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Nov 27 05:30:04.587: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6259 /api/v1/namespaces/watch-6259/configmaps/e2e-watch-test-configmap-a 2f3e13f3-9964-4e86-bc72-1fb7a565825b 960387 0 2019-11-27 05:29:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Nov 27 05:30:04.587: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6259 /api/v1/namespaces/watch-6259/configmaps/e2e-watch-test-configmap-a 2f3e13f3-9964-4e86-bc72-1fb7a565825b 960387 0 2019-11-27 05:29:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Nov 27 05:30:14.610: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6259 /api/v1/namespaces/watch-6259/configmaps/e2e-watch-test-configmap-a 2f3e13f3-9964-4e86-bc72-1fb7a565825b 960401 0 2019-11-27 05:29:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov 27 05:30:14.610: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6259 /api/v1/namespaces/watch-6259/configmaps/e2e-watch-test-configmap-a 2f3e13f3-9964-4e86-bc72-1fb7a565825b 960401 0 2019-11-27 05:29:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Nov 27 05:30:24.629: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6259 /api/v1/namespaces/watch-6259/configmaps/e2e-watch-test-configmap-a 2f3e13f3-9964-4e86-bc72-1fb7a565825b 960417 0 2019-11-27 05:29:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov 27 05:30:24.629: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6259 /api/v1/namespaces/watch-6259/configmaps/e2e-watch-test-configmap-a 2f3e13f3-9964-4e86-bc72-1fb7a565825b 960417 0 2019-11-27 05:29:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Nov 27 05:30:34.654: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6259 /api/v1/namespaces/watch-6259/configmaps/e2e-watch-test-configmap-b 165f9bfa-4d20-4321-aca2-3550fa65d696 960431 0 2019-11-27 05:30:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov 27 05:30:34.654: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6259 /api/v1/namespaces/watch-6259/configmaps/e2e-watch-test-configmap-b 165f9bfa-4d20-4321-aca2-3550fa65d696 960431 0 2019-11-27 05:30:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Nov 27 05:30:44.677: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6259 /api/v1/namespaces/watch-6259/configmaps/e2e-watch-test-configmap-b 165f9bfa-4d20-4321-aca2-3550fa65d696 960445 0 2019-11-27 05:30:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov 27 05:30:44.677: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6259 /api/v1/namespaces/watch-6259/configmaps/e2e-watch-test-configmap-b 165f9bfa-4d20-4321-aca2-3550fa65d696 960445 0 2019-11-27 05:30:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:30:54.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6259" for this suite.
Nov 27 05:31:00.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:31:00.941: INFO: namespace watch-6259 deletion completed in 6.245668331s

• [SLOW TEST:66.711 seconds]
[sig-api-machinery] Watchers
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:31:00.943: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3591
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:31:12.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3591" for this suite.
Nov 27 05:31:18.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:31:18.639: INFO: namespace resourcequota-3591 deletion completed in 6.264113705s

• [SLOW TEST:17.696 seconds]
[sig-api-machinery] ResourceQuota
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:31:18.639: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7991
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-7991
STEP: creating replication controller nodeport-test in namespace services-7991
I1127 05:31:18.996923      19 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-7991, replica count: 2
I1127 05:31:18.997525      19 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 05:31:18.997658      19 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 05:31:22.048345      19 runners.go:184] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1127 05:31:25.048710      19 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 27 05:31:25.049: INFO: Creating new exec pod
I1127 05:31:31.103743      19 reflector.go:120] Starting reflector *v1.Endpoints (0s) from k8s.io/kubernetes/test/e2e/framework/service/jig.go:389
I1127 05:31:31.103887      19 reflector.go:158] Listing and watching *v1.Endpoints from k8s.io/kubernetes/test/e2e/framework/service/jig.go:389
Nov 27 05:31:32.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=services-7991 execpodkgq7l -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Nov 27 05:31:33.476: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Nov 27 05:31:33.476: INFO: stdout: ""
Nov 27 05:31:33.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=services-7991 execpodkgq7l -- /bin/sh -x -c nc -zv -t -w 2 172.30.216.210 80'
Nov 27 05:31:34.843: INFO: stderr: "+ nc -zv -t -w 2 172.30.216.210 80\nConnection to 172.30.216.210 80 port [tcp/http] succeeded!\n"
Nov 27 05:31:34.843: INFO: stdout: ""
Nov 27 05:31:34.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=services-7991 execpodkgq7l -- /bin/sh -x -c nc -zv -t -w 2 10.200.72.1 30271'
Nov 27 05:31:36.241: INFO: stderr: "+ nc -zv -t -w 2 10.200.72.1 30271\nConnection to 10.200.72.1 30271 port [tcp/*] succeeded!\n"
Nov 27 05:31:36.242: INFO: stdout: ""
Nov 27 05:31:36.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=services-7991 execpodkgq7l -- /bin/sh -x -c nc -zv -t -w 2 10.200.72.17 30271'
Nov 27 05:31:37.640: INFO: stderr: "+ nc -zv -t -w 2 10.200.72.17 30271\nConnection to 10.200.72.17 30271 port [tcp/*] succeeded!\n"
Nov 27 05:31:37.641: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:31:37.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7991" for this suite.
Nov 27 05:31:45.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:31:46.008: INFO: namespace services-7991 deletion completed in 8.3482527s
[AfterEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:27.370 seconds]
[sig-network] Services
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:31:46.009: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3542
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-f9edefc0-c014-4804-9140-c08b7b094d9a
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:31:52.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3542" for this suite.
Nov 27 05:32:04.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:32:05.005: INFO: namespace configmap-3542 deletion completed in 12.311837348s

• [SLOW TEST:18.996 seconds]
[sig-storage] ConfigMap
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:32:05.007: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8989
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov 27 05:32:13.671: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 27 05:32:13.683: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 27 05:32:15.684: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 27 05:32:15.696: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 27 05:32:17.684: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 27 05:32:17.694: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 27 05:32:19.684: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 27 05:32:19.699: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 27 05:32:21.684: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 27 05:32:21.697: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 27 05:32:23.684: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 27 05:32:23.695: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 27 05:32:25.684: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 27 05:32:25.694: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 27 05:32:27.684: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 27 05:32:27.695: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 27 05:32:29.684: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 27 05:32:29.695: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:32:29.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8989" for this suite.
Nov 27 05:32:53.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:32:54.010: INFO: namespace container-lifecycle-hook-8989 deletion completed in 24.297657155s

• [SLOW TEST:49.003 seconds]
[k8s.io] Container Lifecycle Hook
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:32:54.011: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-898
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-d6229bb5-2ba4-4d8a-882d-1b4cb05b4fff
STEP: Creating a pod to test consume secrets
Nov 27 05:32:54.350: INFO: Waiting up to 5m0s for pod "pod-secrets-2b905144-74c2-485f-a5e7-46adc5281737" in namespace "secrets-898" to be "success or failure"
Nov 27 05:32:54.359: INFO: Pod "pod-secrets-2b905144-74c2-485f-a5e7-46adc5281737": Phase="Pending", Reason="", readiness=false. Elapsed: 9.618444ms
Nov 27 05:32:56.368: INFO: Pod "pod-secrets-2b905144-74c2-485f-a5e7-46adc5281737": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018240381s
Nov 27 05:32:58.378: INFO: Pod "pod-secrets-2b905144-74c2-485f-a5e7-46adc5281737": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028768504s
Nov 27 05:33:00.388: INFO: Pod "pod-secrets-2b905144-74c2-485f-a5e7-46adc5281737": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038649336s
STEP: Saw pod success
Nov 27 05:33:00.388: INFO: Pod "pod-secrets-2b905144-74c2-485f-a5e7-46adc5281737" satisfied condition "success or failure"
Nov 27 05:33:00.398: INFO: Trying to get logs from node slave1 pod pod-secrets-2b905144-74c2-485f-a5e7-46adc5281737 container secret-volume-test: <nil>
STEP: delete the pod
Nov 27 05:33:00.469: INFO: Waiting for pod pod-secrets-2b905144-74c2-485f-a5e7-46adc5281737 to disappear
Nov 27 05:33:00.478: INFO: Pod pod-secrets-2b905144-74c2-485f-a5e7-46adc5281737 no longer exists
[AfterEach] [sig-storage] Secrets
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:33:00.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-898" for this suite.
Nov 27 05:33:06.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:33:06.764: INFO: namespace secrets-898 deletion completed in 6.274259173s

• [SLOW TEST:12.753 seconds]
[sig-storage] Secrets
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:33:06.765: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2262
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-5381
STEP: Creating secret with name secret-test-06d3a4cb-b9d8-4900-a9a6-61a2eaee22d6
STEP: Creating a pod to test consume secrets
Nov 27 05:33:07.420: INFO: Waiting up to 5m0s for pod "pod-secrets-37bf34be-5986-4595-89c3-9d03bef102d0" in namespace "secrets-2262" to be "success or failure"
Nov 27 05:33:07.434: INFO: Pod "pod-secrets-37bf34be-5986-4595-89c3-9d03bef102d0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.797486ms
Nov 27 05:33:09.445: INFO: Pod "pod-secrets-37bf34be-5986-4595-89c3-9d03bef102d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025339569s
Nov 27 05:33:11.460: INFO: Pod "pod-secrets-37bf34be-5986-4595-89c3-9d03bef102d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040535625s
STEP: Saw pod success
Nov 27 05:33:11.461: INFO: Pod "pod-secrets-37bf34be-5986-4595-89c3-9d03bef102d0" satisfied condition "success or failure"
Nov 27 05:33:11.478: INFO: Trying to get logs from node slave1 pod pod-secrets-37bf34be-5986-4595-89c3-9d03bef102d0 container secret-volume-test: <nil>
STEP: delete the pod
Nov 27 05:33:11.555: INFO: Waiting for pod pod-secrets-37bf34be-5986-4595-89c3-9d03bef102d0 to disappear
Nov 27 05:33:11.563: INFO: Pod pod-secrets-37bf34be-5986-4595-89c3-9d03bef102d0 no longer exists
[AfterEach] [sig-storage] Secrets
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:33:11.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2262" for this suite.
Nov 27 05:33:17.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:33:17.875: INFO: namespace secrets-2262 deletion completed in 6.299430222s
STEP: Destroying namespace "secret-namespace-5381" for this suite.
Nov 27 05:33:23.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:33:24.162: INFO: namespace secret-namespace-5381 deletion completed in 6.28667652s

• [SLOW TEST:17.398 seconds]
[sig-storage] Secrets
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:33:24.163: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4322
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 27 05:33:28.569: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:33:28.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4322" for this suite.
Nov 27 05:33:34.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:33:34.924: INFO: namespace container-runtime-4322 deletion completed in 6.282061387s

• [SLOW TEST:10.761 seconds]
[k8s.io] Container Runtime
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:33:34.926: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8553
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 27 05:33:35.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-8553'
Nov 27 05:33:36.555: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Nov 27 05:33:36.556: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Nov 27 05:33:40.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 delete deployment e2e-test-httpd-deployment --namespace=kubectl-8553'
Nov 27 05:33:41.398: INFO: stderr: ""
Nov 27 05:33:41.399: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:33:41.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8553" for this suite.
Nov 27 05:34:09.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:34:09.718: INFO: namespace kubectl-8553 deletion completed in 28.299306868s

• [SLOW TEST:34.792 seconds]
[sig-cli] Kubectl client
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:34:09.720: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7182
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 27 05:34:10.049: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e09453c-92b7-4d82-a802-6dcb635fd50c" in namespace "projected-7182" to be "success or failure"
Nov 27 05:34:10.057: INFO: Pod "downwardapi-volume-0e09453c-92b7-4d82-a802-6dcb635fd50c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.166437ms
Nov 27 05:34:12.066: INFO: Pod "downwardapi-volume-0e09453c-92b7-4d82-a802-6dcb635fd50c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017337888s
Nov 27 05:34:14.082: INFO: Pod "downwardapi-volume-0e09453c-92b7-4d82-a802-6dcb635fd50c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032968568s
Nov 27 05:34:16.092: INFO: Pod "downwardapi-volume-0e09453c-92b7-4d82-a802-6dcb635fd50c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043126111s
STEP: Saw pod success
Nov 27 05:34:16.092: INFO: Pod "downwardapi-volume-0e09453c-92b7-4d82-a802-6dcb635fd50c" satisfied condition "success or failure"
Nov 27 05:34:16.100: INFO: Trying to get logs from node slave1 pod downwardapi-volume-0e09453c-92b7-4d82-a802-6dcb635fd50c container client-container: <nil>
STEP: delete the pod
Nov 27 05:34:16.161: INFO: Waiting for pod downwardapi-volume-0e09453c-92b7-4d82-a802-6dcb635fd50c to disappear
Nov 27 05:34:16.170: INFO: Pod downwardapi-volume-0e09453c-92b7-4d82-a802-6dcb635fd50c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:34:16.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7182" for this suite.
Nov 27 05:34:22.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:34:22.463: INFO: namespace projected-7182 deletion completed in 6.278329192s

• [SLOW TEST:12.743 seconds]
[sig-storage] Projected downwardAPI
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:34:22.464: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4258
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Nov 27 05:34:30.891: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 27 05:34:30.900: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 27 05:34:32.901: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 27 05:34:32.910: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 27 05:34:34.901: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 27 05:34:34.913: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 27 05:34:36.901: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 27 05:34:36.911: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 27 05:34:38.901: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 27 05:34:38.911: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 27 05:34:40.901: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 27 05:34:40.914: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:34:41.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4258" for this suite.
Nov 27 05:35:09.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:35:09.494: INFO: namespace container-lifecycle-hook-4258 deletion completed in 28.270672514s

• [SLOW TEST:47.030 seconds]
[k8s.io] Container Lifecycle Hook
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:35:09.495: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1202
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Nov 27 05:35:09.812: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:36:27.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1202" for this suite.
Nov 27 05:36:33.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:36:33.421: INFO: namespace crd-publish-openapi-1202 deletion completed in 6.296669943s

• [SLOW TEST:83.926 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:36:33.422: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2256
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 27 05:36:47.711: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 27 05:36:49.746: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710429807, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710429807, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710429807, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710429807, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 05:36:51.758: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710429807, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710429807, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710429807, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710429807, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 27 05:36:54.787: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:36:56.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2256" for this suite.
Nov 27 05:37:04.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:37:04.722: INFO: namespace webhook-2256 deletion completed in 8.296336371s
STEP: Destroying namespace "webhook-2256-markers" for this suite.
Nov 27 05:37:10.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:37:11.027: INFO: namespace webhook-2256-markers deletion completed in 6.304654024s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:37.653 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:37:11.075: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7703
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:37:19.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7703" for this suite.
Nov 27 05:37:25.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:37:25.808: INFO: namespace kubelet-test-7703 deletion completed in 6.353367137s

• [SLOW TEST:14.733 seconds]
[k8s.io] Kubelet
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:37:25.808: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5615
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-b5c8a07c-8fae-4772-8eee-21efe48a5056
STEP: Creating a pod to test consume secrets
Nov 27 05:37:26.155: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-988f8255-e107-4092-a1d6-59c983743cf3" in namespace "projected-5615" to be "success or failure"
Nov 27 05:37:26.167: INFO: Pod "pod-projected-secrets-988f8255-e107-4092-a1d6-59c983743cf3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.338852ms
Nov 27 05:37:28.183: INFO: Pod "pod-projected-secrets-988f8255-e107-4092-a1d6-59c983743cf3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026738776s
Nov 27 05:37:30.198: INFO: Pod "pod-projected-secrets-988f8255-e107-4092-a1d6-59c983743cf3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041912343s
STEP: Saw pod success
Nov 27 05:37:30.198: INFO: Pod "pod-projected-secrets-988f8255-e107-4092-a1d6-59c983743cf3" satisfied condition "success or failure"
Nov 27 05:37:30.206: INFO: Trying to get logs from node slave1 pod pod-projected-secrets-988f8255-e107-4092-a1d6-59c983743cf3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 27 05:37:30.486: INFO: Waiting for pod pod-projected-secrets-988f8255-e107-4092-a1d6-59c983743cf3 to disappear
Nov 27 05:37:30.495: INFO: Pod pod-projected-secrets-988f8255-e107-4092-a1d6-59c983743cf3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:37:30.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5615" for this suite.
Nov 27 05:37:36.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:37:36.782: INFO: namespace projected-5615 deletion completed in 6.273818682s

• [SLOW TEST:10.974 seconds]
[sig-storage] Projected secret
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:37:36.782: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4269
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Nov 27 05:37:37.092: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-192585042 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:37:37.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4269" for this suite.
Nov 27 05:37:43.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:37:44.061: INFO: namespace kubectl-4269 deletion completed in 6.269459729s

• [SLOW TEST:7.279 seconds]
[sig-cli] Kubectl client
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:37:44.064: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-6113
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 05:37:44.373: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Creating first CR 
Nov 27 05:37:45.390: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-11-27T05:37:45Z generation:1 name:name1 resourceVersion:961794 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:ea503e2b-dcfb-4b9f-b893-bb6adfd30e4e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Nov 27 05:37:55.409: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-11-27T05:37:55Z generation:1 name:name2 resourceVersion:961808 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:46a7fb02-e201-4dce-b0f4-7588a6ce5f29] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Nov 27 05:38:05.431: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-11-27T05:37:45Z generation:2 name:name1 resourceVersion:961822 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:ea503e2b-dcfb-4b9f-b893-bb6adfd30e4e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Nov 27 05:38:15.449: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-11-27T05:37:55Z generation:2 name:name2 resourceVersion:961836 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:46a7fb02-e201-4dce-b0f4-7588a6ce5f29] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Nov 27 05:38:25.470: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-11-27T05:37:45Z generation:2 name:name1 resourceVersion:961852 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:ea503e2b-dcfb-4b9f-b893-bb6adfd30e4e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Nov 27 05:38:35.495: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-11-27T05:37:55Z generation:2 name:name2 resourceVersion:961866 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:46a7fb02-e201-4dce-b0f4-7588a6ce5f29] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:38:46.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6113" for this suite.
Nov 27 05:38:52.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:38:52.323: INFO: namespace crd-watch-6113 deletion completed in 6.27973231s

• [SLOW TEST:68.259 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:38:52.325: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6350
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-02e41369-b53f-4c38-9445-986b7c189748
STEP: Creating a pod to test consume configMaps
Nov 27 05:38:52.680: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-20c09455-fda9-4fcc-be89-e4a6824e719a" in namespace "projected-6350" to be "success or failure"
Nov 27 05:38:52.689: INFO: Pod "pod-projected-configmaps-20c09455-fda9-4fcc-be89-e4a6824e719a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.881197ms
Nov 27 05:38:54.697: INFO: Pod "pod-projected-configmaps-20c09455-fda9-4fcc-be89-e4a6824e719a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017234732s
Nov 27 05:38:56.709: INFO: Pod "pod-projected-configmaps-20c09455-fda9-4fcc-be89-e4a6824e719a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028741127s
STEP: Saw pod success
Nov 27 05:38:56.709: INFO: Pod "pod-projected-configmaps-20c09455-fda9-4fcc-be89-e4a6824e719a" satisfied condition "success or failure"
Nov 27 05:38:56.716: INFO: Trying to get logs from node slave1 pod pod-projected-configmaps-20c09455-fda9-4fcc-be89-e4a6824e719a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 27 05:38:56.801: INFO: Waiting for pod pod-projected-configmaps-20c09455-fda9-4fcc-be89-e4a6824e719a to disappear
Nov 27 05:38:56.810: INFO: Pod pod-projected-configmaps-20c09455-fda9-4fcc-be89-e4a6824e719a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:38:56.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6350" for this suite.
Nov 27 05:39:02.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:39:03.137: INFO: namespace projected-6350 deletion completed in 6.313599265s

• [SLOW TEST:10.812 seconds]
[sig-storage] Projected configMap
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:39:03.137: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1464
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 27 05:39:03.464: INFO: Waiting up to 5m0s for pod "downwardapi-volume-37e84529-8210-4d36-b43e-2901072deed8" in namespace "projected-1464" to be "success or failure"
Nov 27 05:39:03.473: INFO: Pod "downwardapi-volume-37e84529-8210-4d36-b43e-2901072deed8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.889019ms
Nov 27 05:39:05.484: INFO: Pod "downwardapi-volume-37e84529-8210-4d36-b43e-2901072deed8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019340386s
Nov 27 05:39:07.496: INFO: Pod "downwardapi-volume-37e84529-8210-4d36-b43e-2901072deed8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031350339s
Nov 27 05:39:09.506: INFO: Pod "downwardapi-volume-37e84529-8210-4d36-b43e-2901072deed8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041083525s
STEP: Saw pod success
Nov 27 05:39:09.506: INFO: Pod "downwardapi-volume-37e84529-8210-4d36-b43e-2901072deed8" satisfied condition "success or failure"
Nov 27 05:39:09.516: INFO: Trying to get logs from node slave1 pod downwardapi-volume-37e84529-8210-4d36-b43e-2901072deed8 container client-container: <nil>
STEP: delete the pod
Nov 27 05:39:09.574: INFO: Waiting for pod downwardapi-volume-37e84529-8210-4d36-b43e-2901072deed8 to disappear
Nov 27 05:39:09.584: INFO: Pod downwardapi-volume-37e84529-8210-4d36-b43e-2901072deed8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:39:09.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1464" for this suite.
Nov 27 05:39:15.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:39:15.884: INFO: namespace projected-1464 deletion completed in 6.284322019s

• [SLOW TEST:12.747 seconds]
[sig-storage] Projected downwardAPI
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:39:15.884: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4664
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with configMap that has name projected-configmap-test-upd-f0fed223-7584-4bdb-82e0-bcc09846e818
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-f0fed223-7584-4bdb-82e0-bcc09846e818
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:40:47.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4664" for this suite.
Nov 27 05:41:15.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:41:16.169: INFO: namespace projected-4664 deletion completed in 28.269698644s

• [SLOW TEST:120.285 seconds]
[sig-storage] Projected configMap
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:41:16.174: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9824
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:41:49.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9824" for this suite.
Nov 27 05:41:55.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:41:55.526: INFO: namespace container-runtime-9824 deletion completed in 6.33038192s

• [SLOW TEST:39.352 seconds]
[k8s.io] Container Runtime
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:41:55.531: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-907
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Nov 27 05:41:55.851: INFO: Waiting up to 5m0s for pod "downward-api-14231d49-26f2-4b7d-ae63-6068dee76450" in namespace "downward-api-907" to be "success or failure"
Nov 27 05:41:55.863: INFO: Pod "downward-api-14231d49-26f2-4b7d-ae63-6068dee76450": Phase="Pending", Reason="", readiness=false. Elapsed: 11.557876ms
Nov 27 05:41:57.872: INFO: Pod "downward-api-14231d49-26f2-4b7d-ae63-6068dee76450": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020882349s
Nov 27 05:41:59.884: INFO: Pod "downward-api-14231d49-26f2-4b7d-ae63-6068dee76450": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032793056s
Nov 27 05:42:01.894: INFO: Pod "downward-api-14231d49-26f2-4b7d-ae63-6068dee76450": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042726688s
STEP: Saw pod success
Nov 27 05:42:01.894: INFO: Pod "downward-api-14231d49-26f2-4b7d-ae63-6068dee76450" satisfied condition "success or failure"
Nov 27 05:42:01.902: INFO: Trying to get logs from node slave1 pod downward-api-14231d49-26f2-4b7d-ae63-6068dee76450 container dapi-container: <nil>
STEP: delete the pod
Nov 27 05:42:01.966: INFO: Waiting for pod downward-api-14231d49-26f2-4b7d-ae63-6068dee76450 to disappear
Nov 27 05:42:01.973: INFO: Pod downward-api-14231d49-26f2-4b7d-ae63-6068dee76450 no longer exists
[AfterEach] [sig-node] Downward API
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:42:01.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-907" for this suite.
Nov 27 05:42:08.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:42:08.285: INFO: namespace downward-api-907 deletion completed in 6.289166931s

• [SLOW TEST:12.755 seconds]
[sig-node] Downward API
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:42:08.287: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3141
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Nov 27 05:42:08.586: INFO: PodSpec: initContainers in spec.initContainers
Nov 27 05:42:56.692: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-675c8c1d-871c-4450-a250-601e04c91c42", GenerateName:"", Namespace:"init-container-3141", SelfLink:"/api/v1/namespaces/init-container-3141/pods/pod-init-675c8c1d-871c-4450-a250-601e04c91c42", UID:"151bfea6-68c0-4e79-98ac-757b97269c3b", ResourceVersion:"962544", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63710430128, loc:(*time.Location)(0x1283e4440)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"586159342"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-n2j8l", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0059effc0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-n2j8l", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-n2j8l", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-n2j8l", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0056aceb8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"slave1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002fa5f20), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0056acf40)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0056acf60)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0056acf68), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0056acf6c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710430128, loc:(*time.Location)(0x1283e4440)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710430128, loc:(*time.Location)(0x1283e4440)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710430128, loc:(*time.Location)(0x1283e4440)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710430128, loc:(*time.Location)(0x1283e4440)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.200.72.17", PodIP:"172.31.51.168", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.31.51.168"}}, StartTime:(*v1.Time)(0xc002361ae0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002c9b500)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002c9b570)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker://sha256:9024aa0782a40c0c54ab944b62c94b033971d984fdd05b531ac7ac99a58d1503", ContainerID:"docker://ede171c208b83da481d7a9ff520565ab92dbd2e723985a3f07884f357fcec0ee", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002361b20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002361b00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc0056acfef)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:42:56.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3141" for this suite.
Nov 27 05:43:24.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:43:24.986: INFO: namespace init-container-3141 deletion completed in 28.272796747s

• [SLOW TEST:76.699 seconds]
[k8s.io] InitContainer [NodeConformance]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:43:24.991: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-1647
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:43:31.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1647" for this suite.
Nov 27 05:43:37.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:43:37.803: INFO: namespace emptydir-wrapper-1647 deletion completed in 6.286948831s

• [SLOW TEST:12.812 seconds]
[sig-storage] EmptyDir wrapper volumes
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:43:37.805: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8032
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 05:43:42.205: INFO: Waiting up to 5m0s for pod "client-envvars-ed8f99a8-ba6d-4f53-bae3-12c6f8aa7609" in namespace "pods-8032" to be "success or failure"
Nov 27 05:43:42.221: INFO: Pod "client-envvars-ed8f99a8-ba6d-4f53-bae3-12c6f8aa7609": Phase="Pending", Reason="", readiness=false. Elapsed: 15.17518ms
Nov 27 05:43:44.232: INFO: Pod "client-envvars-ed8f99a8-ba6d-4f53-bae3-12c6f8aa7609": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026245973s
Nov 27 05:43:46.243: INFO: Pod "client-envvars-ed8f99a8-ba6d-4f53-bae3-12c6f8aa7609": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037361254s
Nov 27 05:43:48.254: INFO: Pod "client-envvars-ed8f99a8-ba6d-4f53-bae3-12c6f8aa7609": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048534225s
STEP: Saw pod success
Nov 27 05:43:48.254: INFO: Pod "client-envvars-ed8f99a8-ba6d-4f53-bae3-12c6f8aa7609" satisfied condition "success or failure"
Nov 27 05:43:48.263: INFO: Trying to get logs from node slave1 pod client-envvars-ed8f99a8-ba6d-4f53-bae3-12c6f8aa7609 container env3cont: <nil>
STEP: delete the pod
Nov 27 05:43:48.517: INFO: Waiting for pod client-envvars-ed8f99a8-ba6d-4f53-bae3-12c6f8aa7609 to disappear
Nov 27 05:43:48.527: INFO: Pod client-envvars-ed8f99a8-ba6d-4f53-bae3-12c6f8aa7609 no longer exists
[AfterEach] [k8s.io] Pods
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:43:48.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8032" for this suite.
Nov 27 05:44:00.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:44:00.887: INFO: namespace pods-8032 deletion completed in 12.348253872s

• [SLOW TEST:23.082 seconds]
[k8s.io] Pods
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:44:00.888: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1543
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-daf77e4c-2624-4f2a-9512-f227bb6ded56
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-daf77e4c-2624-4f2a-9512-f227bb6ded56
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:44:07.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1543" for this suite.
Nov 27 05:44:19.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:44:19.690: INFO: namespace configmap-1543 deletion completed in 12.282552769s

• [SLOW TEST:18.802 seconds]
[sig-storage] ConfigMap
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:44:19.691: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5186
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov 27 05:44:20.029: INFO: Waiting up to 5m0s for pod "pod-725bf7a4-64c1-4e60-b718-8cc523357b31" in namespace "emptydir-5186" to be "success or failure"
Nov 27 05:44:20.038: INFO: Pod "pod-725bf7a4-64c1-4e60-b718-8cc523357b31": Phase="Pending", Reason="", readiness=false. Elapsed: 9.034219ms
Nov 27 05:44:22.052: INFO: Pod "pod-725bf7a4-64c1-4e60-b718-8cc523357b31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022459023s
Nov 27 05:44:24.061: INFO: Pod "pod-725bf7a4-64c1-4e60-b718-8cc523357b31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031485272s
STEP: Saw pod success
Nov 27 05:44:24.061: INFO: Pod "pod-725bf7a4-64c1-4e60-b718-8cc523357b31" satisfied condition "success or failure"
Nov 27 05:44:24.068: INFO: Trying to get logs from node slave1 pod pod-725bf7a4-64c1-4e60-b718-8cc523357b31 container test-container: <nil>
STEP: delete the pod
Nov 27 05:44:24.134: INFO: Waiting for pod pod-725bf7a4-64c1-4e60-b718-8cc523357b31 to disappear
Nov 27 05:44:24.145: INFO: Pod pod-725bf7a4-64c1-4e60-b718-8cc523357b31 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:44:24.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5186" for this suite.
Nov 27 05:44:30.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:44:30.431: INFO: namespace emptydir-5186 deletion completed in 6.269873331s

• [SLOW TEST:10.740 seconds]
[sig-storage] EmptyDir volumes
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:44:30.431: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1542
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:44:37.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1542" for this suite.
Nov 27 05:44:43.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:44:44.095: INFO: namespace resourcequota-1542 deletion completed in 6.288559239s

• [SLOW TEST:13.665 seconds]
[sig-api-machinery] ResourceQuota
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:44:44.097: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5103
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:45:01.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5103" for this suite.
Nov 27 05:45:07.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:45:07.784: INFO: namespace resourcequota-5103 deletion completed in 6.254184325s

• [SLOW TEST:23.687 seconds]
[sig-api-machinery] ResourceQuota
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:45:07.786: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7479
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-d9e36671-6492-4217-a33c-1d5260a4226c
STEP: Creating a pod to test consume configMaps
Nov 27 05:45:08.130: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-271b1e59-bc06-4e36-9376-ee36cbfad339" in namespace "projected-7479" to be "success or failure"
Nov 27 05:45:08.139: INFO: Pod "pod-projected-configmaps-271b1e59-bc06-4e36-9376-ee36cbfad339": Phase="Pending", Reason="", readiness=false. Elapsed: 9.300665ms
Nov 27 05:45:10.147: INFO: Pod "pod-projected-configmaps-271b1e59-bc06-4e36-9376-ee36cbfad339": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017451799s
Nov 27 05:45:12.157: INFO: Pod "pod-projected-configmaps-271b1e59-bc06-4e36-9376-ee36cbfad339": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027127785s
Nov 27 05:45:14.165: INFO: Pod "pod-projected-configmaps-271b1e59-bc06-4e36-9376-ee36cbfad339": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035547321s
STEP: Saw pod success
Nov 27 05:45:14.166: INFO: Pod "pod-projected-configmaps-271b1e59-bc06-4e36-9376-ee36cbfad339" satisfied condition "success or failure"
Nov 27 05:45:14.173: INFO: Trying to get logs from node slave1 pod pod-projected-configmaps-271b1e59-bc06-4e36-9376-ee36cbfad339 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 27 05:45:14.230: INFO: Waiting for pod pod-projected-configmaps-271b1e59-bc06-4e36-9376-ee36cbfad339 to disappear
Nov 27 05:45:14.240: INFO: Pod pod-projected-configmaps-271b1e59-bc06-4e36-9376-ee36cbfad339 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:45:14.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7479" for this suite.
Nov 27 05:45:20.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:45:20.551: INFO: namespace projected-7479 deletion completed in 6.296704654s

• [SLOW TEST:12.765 seconds]
[sig-storage] Projected configMap
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:45:20.552: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-21
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 05:45:20.929: INFO: Create a RollingUpdate DaemonSet
Nov 27 05:45:20.944: INFO: Check that daemon pods launch on every node of the cluster
Nov 27 05:45:20.965: INFO: Number of nodes with available pods: 0
Nov 27 05:45:20.965: INFO: Node master1 is running more than one daemon pod
Nov 27 05:45:21.995: INFO: Number of nodes with available pods: 0
Nov 27 05:45:21.995: INFO: Node master1 is running more than one daemon pod
Nov 27 05:45:22.989: INFO: Number of nodes with available pods: 0
Nov 27 05:45:22.989: INFO: Node master1 is running more than one daemon pod
Nov 27 05:45:23.987: INFO: Number of nodes with available pods: 0
Nov 27 05:45:23.987: INFO: Node master1 is running more than one daemon pod
Nov 27 05:45:25.025: INFO: Number of nodes with available pods: 1
Nov 27 05:45:25.025: INFO: Node slave1 is running more than one daemon pod
Nov 27 05:45:25.996: INFO: Number of nodes with available pods: 2
Nov 27 05:45:25.996: INFO: Number of running nodes: 2, number of available pods: 2
Nov 27 05:45:25.996: INFO: Update the DaemonSet to trigger a rollout
Nov 27 05:45:26.019: INFO: Updating DaemonSet daemon-set
Nov 27 05:45:34.079: INFO: Roll back the DaemonSet before rollout is complete
Nov 27 05:45:34.098: INFO: Updating DaemonSet daemon-set
Nov 27 05:45:34.098: INFO: Make sure DaemonSet rollback is complete
Nov 27 05:45:34.109: INFO: Wrong image for pod: daemon-set-sfjmd. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 27 05:45:34.109: INFO: Pod daemon-set-sfjmd is not available
Nov 27 05:45:35.135: INFO: Wrong image for pod: daemon-set-sfjmd. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 27 05:45:35.135: INFO: Pod daemon-set-sfjmd is not available
Nov 27 05:45:36.135: INFO: Wrong image for pod: daemon-set-sfjmd. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 27 05:45:36.135: INFO: Pod daemon-set-sfjmd is not available
Nov 27 05:45:37.139: INFO: Pod daemon-set-hshv5 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-21, will wait for the garbage collector to delete the pods
I1127 05:45:37.178098      19 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 05:45:37.178397      19 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Nov 27 05:45:37.252: INFO: Deleting DaemonSet.extensions daemon-set took: 23.940021ms
Nov 27 05:45:37.753: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.896834ms
I1127 05:45:37.752782      19 controller_utils.go:810] Ignoring inactive pod daemonsets-21/daemon-set-vld7r in state Running, deletion time 2019-11-27 05:46:07 +0000 UTC
I1127 05:45:37.752983      19 controller_utils.go:810] Ignoring inactive pod daemonsets-21/daemon-set-hshv5 in state Pending, deletion time 2019-11-27 05:46:07 +0000 UTC
Nov 27 05:45:49.167: INFO: Number of nodes with available pods: 0
Nov 27 05:45:49.167: INFO: Number of running nodes: 0, number of available pods: 0
Nov 27 05:45:49.178: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-21/daemonsets","resourceVersion":"963198"},"items":null}

Nov 27 05:45:49.185: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-21/pods","resourceVersion":"963198"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:45:49.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-21" for this suite.
Nov 27 05:45:55.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:45:55.499: INFO: namespace daemonsets-21 deletion completed in 6.271070626s

• [SLOW TEST:34.947 seconds]
[sig-apps] Daemon set [Serial]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:45:55.501: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-193
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-255ac0d4-4876-4606-a3b6-82a296e7b323
STEP: Creating a pod to test consume configMaps
Nov 27 05:45:55.861: INFO: Waiting up to 5m0s for pod "pod-configmaps-5f2385a1-0cae-4625-88d9-386190d7bb39" in namespace "configmap-193" to be "success or failure"
Nov 27 05:45:55.870: INFO: Pod "pod-configmaps-5f2385a1-0cae-4625-88d9-386190d7bb39": Phase="Pending", Reason="", readiness=false. Elapsed: 8.43435ms
Nov 27 05:45:57.879: INFO: Pod "pod-configmaps-5f2385a1-0cae-4625-88d9-386190d7bb39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017629756s
Nov 27 05:45:59.891: INFO: Pod "pod-configmaps-5f2385a1-0cae-4625-88d9-386190d7bb39": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029972643s
Nov 27 05:46:01.901: INFO: Pod "pod-configmaps-5f2385a1-0cae-4625-88d9-386190d7bb39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039457962s
STEP: Saw pod success
Nov 27 05:46:01.901: INFO: Pod "pod-configmaps-5f2385a1-0cae-4625-88d9-386190d7bb39" satisfied condition "success or failure"
Nov 27 05:46:01.909: INFO: Trying to get logs from node slave1 pod pod-configmaps-5f2385a1-0cae-4625-88d9-386190d7bb39 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 27 05:46:01.978: INFO: Waiting for pod pod-configmaps-5f2385a1-0cae-4625-88d9-386190d7bb39 to disappear
Nov 27 05:46:01.986: INFO: Pod pod-configmaps-5f2385a1-0cae-4625-88d9-386190d7bb39 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:46:01.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-193" for this suite.
Nov 27 05:46:08.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:46:08.251: INFO: namespace configmap-193 deletion completed in 6.253076987s

• [SLOW TEST:12.751 seconds]
[sig-storage] ConfigMap
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:46:08.252: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-9316
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Nov 27 05:46:15.130: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9316 pod-service-account-060c4534-e78e-4f3e-bc34-37ea44f4eea6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Nov 27 05:46:16.675: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9316 pod-service-account-060c4534-e78e-4f3e-bc34-37ea44f4eea6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Nov 27 05:46:18.062: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9316 pod-service-account-060c4534-e78e-4f3e-bc34-37ea44f4eea6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:46:19.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9316" for this suite.
Nov 27 05:46:25.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:46:25.747: INFO: namespace svcaccounts-9316 deletion completed in 6.314018733s

• [SLOW TEST:17.495 seconds]
[sig-auth] ServiceAccounts
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:46:25.749: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4737
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 27 05:47:16.969: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Nov 27 05:47:19.000: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710430437, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710430437, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710430437, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710430436, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 05:47:21.009: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710430437, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710430437, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710430437, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710430436, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 27 05:47:24.035: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:47:25.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4737" for this suite.
Nov 27 05:47:33.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:47:33.691: INFO: namespace webhook-4737 deletion completed in 8.285916146s
STEP: Destroying namespace "webhook-4737-markers" for this suite.
Nov 27 05:47:39.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:47:39.975: INFO: namespace webhook-4737-markers deletion completed in 6.283526594s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:74.265 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:47:40.016: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3791
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 27 05:47:51.992: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Nov 27 05:47:54.028: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710430472, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710430472, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710430472, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710430471, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 05:47:56.046: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710430472, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710430472, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710430472, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710430471, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 27 05:47:59.081: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 05:47:59.091: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1432-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:48:02.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3791" for this suite.
Nov 27 05:48:10.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:48:11.150: INFO: namespace webhook-3791 deletion completed in 8.360475467s
STEP: Destroying namespace "webhook-3791-markers" for this suite.
Nov 27 05:48:17.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:48:17.434: INFO: namespace webhook-3791-markers deletion completed in 6.282957524s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:37.457 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:48:17.474: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1461
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:49:17.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1461" for this suite.
Nov 27 05:49:45.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:49:46.119: INFO: namespace container-probe-1461 deletion completed in 28.263061457s

• [SLOW TEST:88.645 seconds]
[k8s.io] Probing container
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:49:46.119: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7824
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Nov 27 05:49:46.414: INFO: namespace kubectl-7824
Nov 27 05:49:46.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 create -f - --namespace=kubectl-7824'
Nov 27 05:49:47.906: INFO: stderr: ""
Nov 27 05:49:47.907: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Nov 27 05:49:48.928: INFO: Selector matched 1 pods for map[app:redis]
Nov 27 05:49:48.928: INFO: Found 0 / 1
Nov 27 05:49:49.920: INFO: Selector matched 1 pods for map[app:redis]
Nov 27 05:49:49.920: INFO: Found 0 / 1
Nov 27 05:49:50.919: INFO: Selector matched 1 pods for map[app:redis]
Nov 27 05:49:50.919: INFO: Found 0 / 1
Nov 27 05:49:51.919: INFO: Selector matched 1 pods for map[app:redis]
Nov 27 05:49:51.919: INFO: Found 0 / 1
Nov 27 05:49:52.918: INFO: Selector matched 1 pods for map[app:redis]
Nov 27 05:49:52.919: INFO: Found 1 / 1
Nov 27 05:49:52.919: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 27 05:49:52.927: INFO: Selector matched 1 pods for map[app:redis]
Nov 27 05:49:52.928: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 27 05:49:52.928: INFO: wait on redis-master startup in kubectl-7824 
Nov 27 05:49:52.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:49:53.929: INFO: stderr: ""
Nov 27 05:49:53.929: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:49:55.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:49:56.703: INFO: stderr: ""
Nov 27 05:49:56.703: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:49:58.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:49:59.497: INFO: stderr: ""
Nov 27 05:49:59.497: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:50:01.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:50:02.294: INFO: stderr: ""
Nov 27 05:50:02.295: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:50:04.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:50:05.232: INFO: stderr: ""
Nov 27 05:50:05.232: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:50:07.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:50:08.044: INFO: stderr: ""
Nov 27 05:50:08.044: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:50:10.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:50:10.811: INFO: stderr: ""
Nov 27 05:50:10.811: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:50:12.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:50:13.610: INFO: stderr: ""
Nov 27 05:50:13.611: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:50:15.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:50:16.414: INFO: stderr: ""
Nov 27 05:50:16.415: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:50:18.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:50:19.222: INFO: stderr: ""
Nov 27 05:50:19.223: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:50:21.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:50:22.021: INFO: stderr: ""
Nov 27 05:50:22.022: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:50:24.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:50:24.787: INFO: stderr: ""
Nov 27 05:50:24.787: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:50:26.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:50:27.659: INFO: stderr: ""
Nov 27 05:50:27.659: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:50:29.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:50:30.443: INFO: stderr: ""
Nov 27 05:50:30.444: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:50:32.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:50:33.274: INFO: stderr: ""
Nov 27 05:50:33.275: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:50:35.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:50:36.065: INFO: stderr: ""
Nov 27 05:50:36.066: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:50:38.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:50:38.867: INFO: stderr: ""
Nov 27 05:50:38.867: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:50:40.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:50:41.669: INFO: stderr: ""
Nov 27 05:50:41.669: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:50:43.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:50:44.511: INFO: stderr: ""
Nov 27 05:50:44.511: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:50:46.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:50:47.252: INFO: stderr: ""
Nov 27 05:50:47.253: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:50:49.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:50:49.968: INFO: stderr: ""
Nov 27 05:50:49.968: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:50:51.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:50:52.764: INFO: stderr: ""
Nov 27 05:50:52.764: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:50:54.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:50:55.623: INFO: stderr: ""
Nov 27 05:50:55.624: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:50:57.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:50:58.363: INFO: stderr: ""
Nov 27 05:50:58.363: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:51:00.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:51:01.158: INFO: stderr: ""
Nov 27 05:51:01.159: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:51:03.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:51:03.968: INFO: stderr: ""
Nov 27 05:51:03.968: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:51:05.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:51:06.784: INFO: stderr: ""
Nov 27 05:51:06.784: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:51:08.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:51:09.604: INFO: stderr: ""
Nov 27 05:51:09.605: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:51:11.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:51:12.359: INFO: stderr: ""
Nov 27 05:51:12.360: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:51:14.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:51:15.163: INFO: stderr: ""
Nov 27 05:51:15.164: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:51:17.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:51:17.956: INFO: stderr: ""
Nov 27 05:51:17.957: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:51:19.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:51:20.743: INFO: stderr: ""
Nov 27 05:51:20.744: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:51:22.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:51:23.559: INFO: stderr: ""
Nov 27 05:51:23.560: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:51:25.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:51:26.391: INFO: stderr: ""
Nov 27 05:51:26.392: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:51:28.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:51:29.170: INFO: stderr: ""
Nov 27 05:51:29.170: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:51:31.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:51:31.980: INFO: stderr: ""
Nov 27 05:51:31.981: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:51:33.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:51:34.800: INFO: stderr: ""
Nov 27 05:51:34.800: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:51:36.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:51:37.608: INFO: stderr: ""
Nov 27 05:51:37.609: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:51:39.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:51:40.367: INFO: stderr: ""
Nov 27 05:51:40.367: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:51:42.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:51:43.219: INFO: stderr: ""
Nov 27 05:51:43.219: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:51:45.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:51:46.016: INFO: stderr: ""
Nov 27 05:51:46.017: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:51:48.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:51:48.801: INFO: stderr: ""
Nov 27 05:51:48.802: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:51:50.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:51:51.609: INFO: stderr: ""
Nov 27 05:51:51.610: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:51:53.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:51:54.378: INFO: stderr: ""
Nov 27 05:51:54.379: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:51:56.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:51:57.169: INFO: stderr: ""
Nov 27 05:51:57.170: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:51:59.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:52:00.010: INFO: stderr: ""
Nov 27 05:52:00.011: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:52:02.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:52:02.776: INFO: stderr: ""
Nov 27 05:52:02.777: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:52:04.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:52:06.013: INFO: stderr: ""
Nov 27 05:52:06.013: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:52:08.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:52:08.777: INFO: stderr: ""
Nov 27 05:52:08.778: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:52:10.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:52:11.633: INFO: stderr: ""
Nov 27 05:52:11.633: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:52:13.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:52:14.390: INFO: stderr: ""
Nov 27 05:52:14.391: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:52:16.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:52:17.168: INFO: stderr: ""
Nov 27 05:52:17.169: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:52:19.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:52:19.987: INFO: stderr: ""
Nov 27 05:52:19.987: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:52:21.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:52:22.780: INFO: stderr: ""
Nov 27 05:52:22.780: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:52:24.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:52:25.640: INFO: stderr: ""
Nov 27 05:52:25.641: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:52:27.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:52:28.451: INFO: stderr: ""
Nov 27 05:52:28.452: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:52:30.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:52:31.225: INFO: stderr: ""
Nov 27 05:52:31.226: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:52:33.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:52:34.025: INFO: stderr: ""
Nov 27 05:52:34.025: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:52:36.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:52:36.808: INFO: stderr: ""
Nov 27 05:52:36.808: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:52:38.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:52:39.631: INFO: stderr: ""
Nov 27 05:52:39.631: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:52:41.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:52:42.404: INFO: stderr: ""
Nov 27 05:52:42.405: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:52:44.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:52:45.203: INFO: stderr: ""
Nov 27 05:52:45.204: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:52:47.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:52:47.970: INFO: stderr: ""
Nov 27 05:52:47.970: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:52:49.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:52:50.749: INFO: stderr: ""
Nov 27 05:52:50.750: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:52:52.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:52:53.543: INFO: stderr: ""
Nov 27 05:52:53.543: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:52:55.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:52:56.373: INFO: stderr: ""
Nov 27 05:52:56.373: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:52:58.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:52:59.135: INFO: stderr: ""
Nov 27 05:52:59.135: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:53:01.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:53:01.929: INFO: stderr: ""
Nov 27 05:53:01.929: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:53:03.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:53:04.765: INFO: stderr: ""
Nov 27 05:53:04.766: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:53:06.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:53:07.554: INFO: stderr: ""
Nov 27 05:53:07.554: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:53:09.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:53:10.325: INFO: stderr: ""
Nov 27 05:53:10.325: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:53:12.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:53:13.134: INFO: stderr: ""
Nov 27 05:53:13.134: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:53:15.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:53:15.918: INFO: stderr: ""
Nov 27 05:53:15.919: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:53:17.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:53:18.716: INFO: stderr: ""
Nov 27 05:53:18.717: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:53:20.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:53:21.538: INFO: stderr: ""
Nov 27 05:53:21.539: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:53:23.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:53:24.320: INFO: stderr: ""
Nov 27 05:53:24.321: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:53:26.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:53:27.127: INFO: stderr: ""
Nov 27 05:53:27.128: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:53:29.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:53:30.010: INFO: stderr: ""
Nov 27 05:53:30.010: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:53:32.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:53:32.788: INFO: stderr: ""
Nov 27 05:53:32.789: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:53:34.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:53:35.635: INFO: stderr: ""
Nov 27 05:53:35.636: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:53:37.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:53:38.403: INFO: stderr: ""
Nov 27 05:53:38.404: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:53:40.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:53:41.189: INFO: stderr: ""
Nov 27 05:53:41.189: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:53:43.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:53:43.948: INFO: stderr: ""
Nov 27 05:53:43.949: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:53:45.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:53:46.690: INFO: stderr: ""
Nov 27 05:53:46.691: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:53:48.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:53:49.514: INFO: stderr: ""
Nov 27 05:53:49.514: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:53:51.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:53:52.347: INFO: stderr: ""
Nov 27 05:53:52.347: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:53:54.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:53:55.159: INFO: stderr: ""
Nov 27 05:53:55.159: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:53:57.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:53:57.989: INFO: stderr: ""
Nov 27 05:53:57.989: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:53:59.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:54:00.722: INFO: stderr: ""
Nov 27 05:54:00.723: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:54:02.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:54:03.509: INFO: stderr: ""
Nov 27 05:54:03.510: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:54:05.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:54:06.894: INFO: stderr: ""
Nov 27 05:54:06.895: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:54:08.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:54:09.669: INFO: stderr: ""
Nov 27 05:54:09.670: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:54:11.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:54:12.360: INFO: stderr: ""
Nov 27 05:54:12.360: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:54:14.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:54:15.145: INFO: stderr: ""
Nov 27 05:54:15.145: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:54:17.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:54:17.950: INFO: stderr: ""
Nov 27 05:54:17.951: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:54:19.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:54:20.691: INFO: stderr: ""
Nov 27 05:54:20.691: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:54:22.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:54:23.520: INFO: stderr: ""
Nov 27 05:54:23.521: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:54:25.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:54:26.383: INFO: stderr: ""
Nov 27 05:54:26.384: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:54:28.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:54:29.229: INFO: stderr: ""
Nov 27 05:54:29.229: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:54:31.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:54:32.009: INFO: stderr: ""
Nov 27 05:54:32.009: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:54:34.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:54:34.761: INFO: stderr: ""
Nov 27 05:54:34.762: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:54:36.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:54:37.598: INFO: stderr: ""
Nov 27 05:54:37.598: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:54:39.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:54:40.382: INFO: stderr: ""
Nov 27 05:54:40.383: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:54:42.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:54:43.146: INFO: stderr: ""
Nov 27 05:54:43.147: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:54:45.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:54:45.904: INFO: stderr: ""
Nov 27 05:54:45.905: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:54:47.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:54:48.727: INFO: stderr: ""
Nov 27 05:54:48.727: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
Nov 27 05:54:50.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs redis-master-pkplr redis-master --namespace=kubectl-7824'
Nov 27 05:54:51.519: INFO: stderr: ""
Nov 27 05:54:51.519: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.0.6 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Nov 05:49:51.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Nov 05:49:51.282 # Server started, Redis version 3.0.6\n1:M 27 Nov 05:49:51.282 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Nov 05:49:51.282 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Nov 27 05:54:53.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-7824'
Nov 27 05:54:54.381: INFO: stderr: ""
Nov 27 05:54:54.381: INFO: stdout: "service/rm2 exposed\n"
Nov 27 05:54:54.405: INFO: Service rm2 in namespace kubectl-7824 found.
STEP: exposing service
Nov 27 05:54:56.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-7824'
Nov 27 05:54:57.165: INFO: stderr: ""
Nov 27 05:54:57.166: INFO: stdout: "service/rm3 exposed\n"
Nov 27 05:54:57.181: INFO: Service rm3 in namespace kubectl-7824 found.
[AfterEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:54:59.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7824" for this suite.
Nov 27 05:55:27.245: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:55:27.556: INFO: namespace kubectl-7824 deletion completed in 28.343860006s

• [SLOW TEST:341.437 seconds]
[sig-cli] Kubectl client
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:55:27.557: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3191
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov 27 05:55:27.888: INFO: Waiting up to 5m0s for pod "pod-88214a85-7d48-49cd-8638-12a00cc5cb14" in namespace "emptydir-3191" to be "success or failure"
Nov 27 05:55:27.897: INFO: Pod "pod-88214a85-7d48-49cd-8638-12a00cc5cb14": Phase="Pending", Reason="", readiness=false. Elapsed: 8.846485ms
Nov 27 05:55:29.907: INFO: Pod "pod-88214a85-7d48-49cd-8638-12a00cc5cb14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018476515s
Nov 27 05:55:31.953: INFO: Pod "pod-88214a85-7d48-49cd-8638-12a00cc5cb14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064985559s
STEP: Saw pod success
Nov 27 05:55:31.953: INFO: Pod "pod-88214a85-7d48-49cd-8638-12a00cc5cb14" satisfied condition "success or failure"
Nov 27 05:55:31.965: INFO: Trying to get logs from node slave1 pod pod-88214a85-7d48-49cd-8638-12a00cc5cb14 container test-container: <nil>
STEP: delete the pod
Nov 27 05:55:32.018: INFO: Waiting for pod pod-88214a85-7d48-49cd-8638-12a00cc5cb14 to disappear
Nov 27 05:55:32.026: INFO: Pod pod-88214a85-7d48-49cd-8638-12a00cc5cb14 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:55:32.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3191" for this suite.
Nov 27 05:55:38.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:55:38.290: INFO: namespace emptydir-3191 deletion completed in 6.248323987s

• [SLOW TEST:10.733 seconds]
[sig-storage] EmptyDir volumes
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:55:38.290: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2520
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov 27 05:55:46.944: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 27 05:55:46.953: INFO: Pod pod-with-poststart-http-hook still exists
Nov 27 05:55:48.954: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 27 05:55:48.975: INFO: Pod pod-with-poststart-http-hook still exists
Nov 27 05:55:50.954: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 27 05:55:50.966: INFO: Pod pod-with-poststart-http-hook still exists
Nov 27 05:55:52.954: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 27 05:55:52.971: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:55:52.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2520" for this suite.
Nov 27 05:56:21.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:56:21.316: INFO: namespace container-lifecycle-hook-2520 deletion completed in 28.329385407s

• [SLOW TEST:43.025 seconds]
[k8s.io] Container Lifecycle Hook
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:56:21.319: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-361
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-361
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 27 05:56:21.658: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Nov 27 05:56:45.873: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.31.161.38 8081 | grep -v '^\s*$'] Namespace:pod-network-test-361 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 27 05:56:45.873: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
Nov 27 05:56:47.498: INFO: Found all expected endpoints: [netserver-0]
Nov 27 05:56:47.508: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.31.51.171 8081 | grep -v '^\s*$'] Namespace:pod-network-test-361 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 27 05:56:47.508: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
Nov 27 05:56:49.174: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:56:49.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-361" for this suite.
Nov 27 05:57:01.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:57:01.497: INFO: namespace pod-network-test-361 deletion completed in 12.307103905s

• [SLOW TEST:40.178 seconds]
[sig-network] Networking
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:57:01.498: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9506
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W1127 05:57:02.942107      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov 27 05:57:02.942: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:57:02.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9506" for this suite.
Nov 27 05:57:08.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:57:09.247: INFO: namespace gc-9506 deletion completed in 6.291721387s

• [SLOW TEST:7.749 seconds]
[sig-api-machinery] Garbage collector
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:57:09.247: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-512
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Nov 27 05:57:09.564: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Nov 27 05:57:09.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 create -f - --namespace=kubectl-512'
Nov 27 05:57:10.739: INFO: stderr: ""
Nov 27 05:57:10.739: INFO: stdout: "service/redis-slave created\n"
Nov 27 05:57:10.741: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Nov 27 05:57:10.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 create -f - --namespace=kubectl-512'
Nov 27 05:57:11.865: INFO: stderr: ""
Nov 27 05:57:11.865: INFO: stdout: "service/redis-master created\n"
Nov 27 05:57:11.867: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Nov 27 05:57:11.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 create -f - --namespace=kubectl-512'
Nov 27 05:57:13.008: INFO: stderr: ""
Nov 27 05:57:13.008: INFO: stdout: "service/frontend created\n"
Nov 27 05:57:13.011: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Nov 27 05:57:13.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 create -f - --namespace=kubectl-512'
Nov 27 05:57:14.214: INFO: stderr: ""
Nov 27 05:57:14.215: INFO: stdout: "deployment.apps/frontend created\n"
Nov 27 05:57:14.220: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 27 05:57:14.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 create -f - --namespace=kubectl-512'
Nov 27 05:57:15.613: INFO: stderr: ""
Nov 27 05:57:15.613: INFO: stdout: "deployment.apps/redis-master created\n"
Nov 27 05:57:15.616: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Nov 27 05:57:15.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 create -f - --namespace=kubectl-512'
Nov 27 05:57:17.043: INFO: stderr: ""
Nov 27 05:57:17.043: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Nov 27 05:57:17.043: INFO: Waiting for all frontend pods to be Running.
I1127 05:57:17.045472      19 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 05:57:17.045726      19 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Nov 27 05:57:22.096: INFO: Waiting for frontend to serve content.
Nov 27 05:57:22.156: INFO: Trying to add a new entry to the guestbook.
Nov 27 05:57:22.337: INFO: Verifying that added entry can be retrieved.
Nov 27 05:57:22.374: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Nov 27 05:57:27.402: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Nov 27 05:57:32.481: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Nov 27 05:57:37.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 delete --grace-period=0 --force -f - --namespace=kubectl-512'
Nov 27 05:57:38.236: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 27 05:57:38.236: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Nov 27 05:57:38.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 delete --grace-period=0 --force -f - --namespace=kubectl-512'
Nov 27 05:57:38.987: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 27 05:57:38.987: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Nov 27 05:57:38.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 delete --grace-period=0 --force -f - --namespace=kubectl-512'
Nov 27 05:57:39.721: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 27 05:57:39.722: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov 27 05:57:39.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 delete --grace-period=0 --force -f - --namespace=kubectl-512'
Nov 27 05:57:40.466: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 27 05:57:40.467: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov 27 05:57:40.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 delete --grace-period=0 --force -f - --namespace=kubectl-512'
Nov 27 05:57:41.207: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 27 05:57:41.207: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Nov 27 05:57:41.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 delete --grace-period=0 --force -f - --namespace=kubectl-512'
Nov 27 05:57:41.985: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 27 05:57:41.985: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:57:41.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-512" for this suite.
Nov 27 05:58:10.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:58:10.284: INFO: namespace kubectl-512 deletion completed in 28.279721889s

• [SLOW TEST:61.037 seconds]
[sig-cli] Kubectl client
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:58:10.285: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5965
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:58:10.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5965" for this suite.
Nov 27 05:58:16.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:58:16.892: INFO: namespace custom-resource-definition-5965 deletion completed in 6.273678965s

• [SLOW TEST:6.607 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:58:16.895: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9519
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 27 05:58:17.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-9519'
Nov 27 05:58:18.063: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Nov 27 05:58:18.063: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Nov 27 05:58:18.088: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-lrkds]
Nov 27 05:58:18.088: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-lrkds" in namespace "kubectl-9519" to be "running and ready"
Nov 27 05:58:18.095: INFO: Pod "e2e-test-httpd-rc-lrkds": Phase="Pending", Reason="", readiness=false. Elapsed: 6.991171ms
Nov 27 05:58:20.108: INFO: Pod "e2e-test-httpd-rc-lrkds": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019808839s
Nov 27 05:58:22.118: INFO: Pod "e2e-test-httpd-rc-lrkds": Phase="Running", Reason="", readiness=true. Elapsed: 4.029853016s
Nov 27 05:58:22.118: INFO: Pod "e2e-test-httpd-rc-lrkds" satisfied condition "running and ready"
Nov 27 05:58:22.118: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-lrkds]
Nov 27 05:58:22.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs rc/e2e-test-httpd-rc --namespace=kubectl-9519'
Nov 27 05:58:23.161: INFO: stderr: ""
Nov 27 05:58:23.162: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.31.51.189. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.31.51.189. Set the 'ServerName' directive globally to suppress this message\n[Wed Nov 27 05:58:21.379486 2019] [mpm_event:notice] [pid 1:tid 1099278703296] AH00489: Apache/2.4.41 (Unix) configured -- resuming normal operations\n[Wed Nov 27 05:58:21.380356 2019] [core:notice] [pid 1:tid 1099278703296] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Nov 27 05:58:23.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 delete rc e2e-test-httpd-rc --namespace=kubectl-9519'
Nov 27 05:58:23.941: INFO: stderr: ""
Nov 27 05:58:23.941: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:58:23.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9519" for this suite.
Nov 27 05:58:35.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:58:36.242: INFO: namespace kubectl-9519 deletion completed in 12.283727322s

• [SLOW TEST:19.348 seconds]
[sig-cli] Kubectl client
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:58:36.246: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4118
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov 27 05:58:41.257: INFO: Successfully updated pod "pod-update-bc3a4f8e-f565-4349-b5f6-50756456f7f3"
STEP: verifying the updated pod is in kubernetes
Nov 27 05:58:41.283: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:58:41.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4118" for this suite.
Nov 27 05:59:09.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:59:09.541: INFO: namespace pods-4118 deletion completed in 28.245559376s

• [SLOW TEST:33.294 seconds]
[k8s.io] Pods
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:59:09.542: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9381
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Nov 27 05:59:09.875: INFO: Waiting up to 5m0s for pod "client-containers-92621ccc-6617-4f6c-92a6-2953b455cdc6" in namespace "containers-9381" to be "success or failure"
Nov 27 05:59:09.882: INFO: Pod "client-containers-92621ccc-6617-4f6c-92a6-2953b455cdc6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.510375ms
Nov 27 05:59:11.895: INFO: Pod "client-containers-92621ccc-6617-4f6c-92a6-2953b455cdc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020616979s
Nov 27 05:59:13.911: INFO: Pod "client-containers-92621ccc-6617-4f6c-92a6-2953b455cdc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03605267s
STEP: Saw pod success
Nov 27 05:59:13.911: INFO: Pod "client-containers-92621ccc-6617-4f6c-92a6-2953b455cdc6" satisfied condition "success or failure"
Nov 27 05:59:13.920: INFO: Trying to get logs from node master1 pod client-containers-92621ccc-6617-4f6c-92a6-2953b455cdc6 container test-container: <nil>
STEP: delete the pod
Nov 27 05:59:14.221: INFO: Waiting for pod client-containers-92621ccc-6617-4f6c-92a6-2953b455cdc6 to disappear
Nov 27 05:59:14.229: INFO: Pod client-containers-92621ccc-6617-4f6c-92a6-2953b455cdc6 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:59:14.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9381" for this suite.
Nov 27 05:59:20.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:59:20.554: INFO: namespace containers-9381 deletion completed in 6.310043851s

• [SLOW TEST:11.012 seconds]
[k8s.io] Docker Containers
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:59:20.556: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6946
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Nov 27 05:59:20.878: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 27 05:59:20.922: INFO: Waiting for terminating namespaces to be deleted...
Nov 27 05:59:20.933: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Nov 27 05:59:20.962: INFO: kube-apiserver-master1 from kube-system started at 2019-11-27 03:11:20 +0000 UTC (1 container statuses recorded)
Nov 27 05:59:20.962: INFO: 	Container kube-apiserver ready: true, restart count 6
Nov 27 05:59:20.962: INFO: kube-controller-manager-master1 from kube-system started at 2019-11-27 03:11:20 +0000 UTC (1 container statuses recorded)
Nov 27 05:59:20.962: INFO: 	Container kube-controller-manager ready: true, restart count 73
Nov 27 05:59:20.962: INFO: resource-reserver-master1 from kube-system started at 2019-11-27 03:11:20 +0000 UTC (1 container statuses recorded)
Nov 27 05:59:20.962: INFO: 	Container sleep-forever ready: true, restart count 7
Nov 27 05:59:20.962: INFO: coredns-lxl9z from kube-system started at 2019-11-14 04:51:49 +0000 UTC (1 container statuses recorded)
Nov 27 05:59:20.962: INFO: 	Container coredns ready: true, restart count 6
Nov 27 05:59:20.962: INFO: calico-node-cppjk from kube-system started at 2019-11-20 07:23:24 +0000 UTC (1 container statuses recorded)
Nov 27 05:59:20.962: INFO: 	Container calico-node ready: true, restart count 5
Nov 27 05:59:20.962: INFO: metrics-server-687c949fd7-pf9hm from kube-system started at 2019-11-22 00:26:19 +0000 UTC (1 container statuses recorded)
Nov 27 05:59:20.962: INFO: 	Container metrics-server ready: true, restart count 3
Nov 27 05:59:20.962: INFO: kube-proxy-master1 from kube-system started at 2019-11-27 03:11:20 +0000 UTC (1 container statuses recorded)
Nov 27 05:59:20.962: INFO: 	Container kube-proxy ready: true, restart count 14
Nov 27 05:59:20.962: INFO: kube-scheduler-master1 from kube-system started at 2019-11-27 03:11:20 +0000 UTC (1 container statuses recorded)
Nov 27 05:59:20.962: INFO: 	Container kube-scheduler ready: true, restart count 64
Nov 27 05:59:20.962: INFO: e2e-conformance-test from conformance started at 2019-11-27 04:04:03 +0000 UTC (1 container statuses recorded)
Nov 27 05:59:20.962: INFO: 	Container conformance-container ready: true, restart count 0
Nov 27 05:59:20.962: INFO: 
Logging pods the kubelet thinks is on node slave1 before test
Nov 27 05:59:20.988: INFO: calico-node-5nkgf from kube-system started at 2019-11-20 07:23:24 +0000 UTC (1 container statuses recorded)
Nov 27 05:59:20.988: INFO: 	Container calico-node ready: true, restart count 5
Nov 27 05:59:20.988: INFO: kube-proxy-slave1 from kube-system started at 2019-11-27 03:14:35 +0000 UTC (1 container statuses recorded)
Nov 27 05:59:20.988: INFO: 	Container kube-proxy ready: true, restart count 260
Nov 27 05:59:20.988: INFO: test-webserver-d1bd1012-16a8-4768-81ad-d85c5340fd42 from container-probe-4706 started at 2019-11-27 04:02:33 +0000 UTC (1 container statuses recorded)
Nov 27 05:59:20.988: INFO: 	Container test-webserver ready: false, restart count 0
Nov 27 05:59:20.988: INFO: nginx-proxy-slave1 from kube-system started at 2019-11-27 03:14:35 +0000 UTC (1 container statuses recorded)
Nov 27 05:59:20.988: INFO: 	Container nginx-proxy ready: true, restart count 15
Nov 27 05:59:20.988: INFO: dns-autoscaler-799d586fb4-mqr72 from kube-system started at 2019-11-08 02:43:45 +0000 UTC (1 container statuses recorded)
Nov 27 05:59:20.988: INFO: 	Container autoscaler ready: true, restart count 5
Nov 27 05:59:20.988: INFO: calico-kube-controllers-685dd5746c-8jjc9 from kube-system started at 2019-11-08 02:42:34 +0000 UTC (1 container statuses recorded)
Nov 27 05:59:20.988: INFO: 	Container calico-kube-controllers ready: true, restart count 5
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
I1127 05:59:21.017453      19 reflector.go:120] Starting reflector *v1.Event (0s) from k8s.io/kubernetes/test/e2e/common/events.go:136
I1127 05:59:21.017547      19 reflector.go:158] Listing and watching *v1.Event from k8s.io/kubernetes/test/e2e/common/events.go:136
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15daef81ef029d3f], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 05:59:22.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6946" for this suite.
Nov 27 05:59:28.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 05:59:28.356: INFO: namespace sched-pred-6946 deletion completed in 6.268523142s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:7.800 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSI1127 05:59:28.356305      19 request.go:706] Error in request: resource name may not be empty
SSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 05:59:28.358: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-8130
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Nov 27 05:59:54.557: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Nov 27 05:59:56.587: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710431194, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710431194, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710431194, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710431194, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 05:59:58.597: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710431194, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710431194, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710431194, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710431194, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 27 06:00:01.635: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 06:00:01.722: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:00:04.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8130" for this suite.
Nov 27 06:00:12.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:00:12.684: INFO: namespace crd-webhook-8130 deletion completed in 8.401759568s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:44.374 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:00:12.732: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9107
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-c90f69e5-9e55-4304-a6ac-5555891031e3
STEP: Creating a pod to test consume configMaps
Nov 27 06:00:13.128: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-145d144c-4c6b-405b-9ad7-3c1001f2efc9" in namespace "projected-9107" to be "success or failure"
Nov 27 06:00:13.156: INFO: Pod "pod-projected-configmaps-145d144c-4c6b-405b-9ad7-3c1001f2efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 28.206196ms
Nov 27 06:00:15.168: INFO: Pod "pod-projected-configmaps-145d144c-4c6b-405b-9ad7-3c1001f2efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040030433s
Nov 27 06:00:17.181: INFO: Pod "pod-projected-configmaps-145d144c-4c6b-405b-9ad7-3c1001f2efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052882324s
STEP: Saw pod success
Nov 27 06:00:17.181: INFO: Pod "pod-projected-configmaps-145d144c-4c6b-405b-9ad7-3c1001f2efc9" satisfied condition "success or failure"
Nov 27 06:00:17.189: INFO: Trying to get logs from node slave1 pod pod-projected-configmaps-145d144c-4c6b-405b-9ad7-3c1001f2efc9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 27 06:00:17.255: INFO: Waiting for pod pod-projected-configmaps-145d144c-4c6b-405b-9ad7-3c1001f2efc9 to disappear
Nov 27 06:00:17.263: INFO: Pod pod-projected-configmaps-145d144c-4c6b-405b-9ad7-3c1001f2efc9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:00:17.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9107" for this suite.
Nov 27 06:00:23.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:00:23.561: INFO: namespace projected-9107 deletion completed in 6.283979495s

• [SLOW TEST:10.829 seconds]
[sig-storage] Projected configMap
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:00:23.565: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6135
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:00:40.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6135" for this suite.
Nov 27 06:00:46.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:00:46.445: INFO: namespace resourcequota-6135 deletion completed in 6.286607784s

• [SLOW TEST:22.880 seconds]
[sig-api-machinery] ResourceQuota
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:00:46.446: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6920
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Nov 27 06:00:46.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 --namespace=kubectl-6920 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Nov 27 06:00:52.927: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Nov 27 06:00:52.927: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:00:54.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6920" for this suite.
Nov 27 06:01:00.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:01:01.277: INFO: namespace kubectl-6920 deletion completed in 6.313201611s

• [SLOW TEST:14.830 seconds]
[sig-cli] Kubectl client
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:01:01.279: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4613
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-4613, will wait for the garbage collector to delete the pods
I1127 06:01:07.643123      19 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 06:01:07.643313      19 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Nov 27 06:01:07.712: INFO: Deleting Job.batch foo took: 19.149452ms
I1127 06:01:07.812741      19 controller_utils.go:810] Ignoring inactive pod job-4613/foo-j7b4n in state Running, deletion time 2019-11-27 06:01:37 +0000 UTC
I1127 06:01:07.812856      19 controller_utils.go:810] Ignoring inactive pod job-4613/foo-kg4p8 in state Running, deletion time 2019-11-27 06:01:37 +0000 UTC
Nov 27 06:01:07.812: INFO: Terminating Job.batch foo pods took: 100.556012ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:01:43.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4613" for this suite.
Nov 27 06:01:49.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:01:49.948: INFO: namespace job-4613 deletion completed in 6.310762034s

• [SLOW TEST:48.669 seconds]
[sig-apps] Job
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:01:49.949: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9080
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-86n8
STEP: Creating a pod to test atomic-volume-subpath
Nov 27 06:01:50.310: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-86n8" in namespace "subpath-9080" to be "success or failure"
Nov 27 06:01:50.320: INFO: Pod "pod-subpath-test-configmap-86n8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.234976ms
Nov 27 06:01:52.330: INFO: Pod "pod-subpath-test-configmap-86n8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019775594s
Nov 27 06:01:54.341: INFO: Pod "pod-subpath-test-configmap-86n8": Phase="Running", Reason="", readiness=true. Elapsed: 4.030849381s
Nov 27 06:01:56.350: INFO: Pod "pod-subpath-test-configmap-86n8": Phase="Running", Reason="", readiness=true. Elapsed: 6.039624882s
Nov 27 06:01:58.363: INFO: Pod "pod-subpath-test-configmap-86n8": Phase="Running", Reason="", readiness=true. Elapsed: 8.053201801s
Nov 27 06:02:00.376: INFO: Pod "pod-subpath-test-configmap-86n8": Phase="Running", Reason="", readiness=true. Elapsed: 10.06550711s
Nov 27 06:02:02.385: INFO: Pod "pod-subpath-test-configmap-86n8": Phase="Running", Reason="", readiness=true. Elapsed: 12.07497635s
Nov 27 06:02:04.401: INFO: Pod "pod-subpath-test-configmap-86n8": Phase="Running", Reason="", readiness=true. Elapsed: 14.090638887s
Nov 27 06:02:06.411: INFO: Pod "pod-subpath-test-configmap-86n8": Phase="Running", Reason="", readiness=true. Elapsed: 16.101088357s
Nov 27 06:02:08.424: INFO: Pod "pod-subpath-test-configmap-86n8": Phase="Running", Reason="", readiness=true. Elapsed: 18.113765224s
Nov 27 06:02:10.434: INFO: Pod "pod-subpath-test-configmap-86n8": Phase="Running", Reason="", readiness=true. Elapsed: 20.124294873s
Nov 27 06:02:12.444: INFO: Pod "pod-subpath-test-configmap-86n8": Phase="Running", Reason="", readiness=true. Elapsed: 22.133989493s
Nov 27 06:02:14.458: INFO: Pod "pod-subpath-test-configmap-86n8": Phase="Running", Reason="", readiness=true. Elapsed: 24.147519479s
Nov 27 06:02:16.467: INFO: Pod "pod-subpath-test-configmap-86n8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.157142231s
STEP: Saw pod success
Nov 27 06:02:16.468: INFO: Pod "pod-subpath-test-configmap-86n8" satisfied condition "success or failure"
Nov 27 06:02:16.476: INFO: Trying to get logs from node slave1 pod pod-subpath-test-configmap-86n8 container test-container-subpath-configmap-86n8: <nil>
STEP: delete the pod
Nov 27 06:02:16.756: INFO: Waiting for pod pod-subpath-test-configmap-86n8 to disappear
Nov 27 06:02:16.764: INFO: Pod pod-subpath-test-configmap-86n8 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-86n8
Nov 27 06:02:16.764: INFO: Deleting pod "pod-subpath-test-configmap-86n8" in namespace "subpath-9080"
[AfterEach] [sig-storage] Subpath
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:02:16.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9080" for this suite.
Nov 27 06:02:22.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:02:23.079: INFO: namespace subpath-9080 deletion completed in 6.293061662s

• [SLOW TEST:33.131 seconds]
[sig-storage] Subpath
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:02:23.081: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-407
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 27 06:02:23.428: INFO: Waiting up to 5m0s for pod "downwardapi-volume-758a9dee-4599-4afa-b0f1-722f3017dcc2" in namespace "downward-api-407" to be "success or failure"
Nov 27 06:02:23.436: INFO: Pod "downwardapi-volume-758a9dee-4599-4afa-b0f1-722f3017dcc2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006112ms
Nov 27 06:02:25.450: INFO: Pod "downwardapi-volume-758a9dee-4599-4afa-b0f1-722f3017dcc2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022449839s
Nov 27 06:02:27.460: INFO: Pod "downwardapi-volume-758a9dee-4599-4afa-b0f1-722f3017dcc2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032139081s
Nov 27 06:02:29.469: INFO: Pod "downwardapi-volume-758a9dee-4599-4afa-b0f1-722f3017dcc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041249962s
STEP: Saw pod success
Nov 27 06:02:29.469: INFO: Pod "downwardapi-volume-758a9dee-4599-4afa-b0f1-722f3017dcc2" satisfied condition "success or failure"
Nov 27 06:02:29.477: INFO: Trying to get logs from node slave1 pod downwardapi-volume-758a9dee-4599-4afa-b0f1-722f3017dcc2 container client-container: <nil>
STEP: delete the pod
Nov 27 06:02:29.538: INFO: Waiting for pod downwardapi-volume-758a9dee-4599-4afa-b0f1-722f3017dcc2 to disappear
Nov 27 06:02:29.547: INFO: Pod downwardapi-volume-758a9dee-4599-4afa-b0f1-722f3017dcc2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:02:29.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-407" for this suite.
Nov 27 06:02:35.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:02:35.818: INFO: namespace downward-api-407 deletion completed in 6.256538906s

• [SLOW TEST:12.738 seconds]
[sig-storage] Downward API volume
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:02:35.823: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6779
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov 27 06:02:36.164: INFO: Waiting up to 5m0s for pod "pod-88e73643-c940-4ba9-9749-22283c586c29" in namespace "emptydir-6779" to be "success or failure"
Nov 27 06:02:36.173: INFO: Pod "pod-88e73643-c940-4ba9-9749-22283c586c29": Phase="Pending", Reason="", readiness=false. Elapsed: 9.255235ms
Nov 27 06:02:38.188: INFO: Pod "pod-88e73643-c940-4ba9-9749-22283c586c29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023800651s
Nov 27 06:02:40.197: INFO: Pod "pod-88e73643-c940-4ba9-9749-22283c586c29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033026422s
STEP: Saw pod success
Nov 27 06:02:40.197: INFO: Pod "pod-88e73643-c940-4ba9-9749-22283c586c29" satisfied condition "success or failure"
Nov 27 06:02:40.206: INFO: Trying to get logs from node slave1 pod pod-88e73643-c940-4ba9-9749-22283c586c29 container test-container: <nil>
STEP: delete the pod
Nov 27 06:02:40.266: INFO: Waiting for pod pod-88e73643-c940-4ba9-9749-22283c586c29 to disappear
Nov 27 06:02:40.282: INFO: Pod pod-88e73643-c940-4ba9-9749-22283c586c29 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:02:40.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6779" for this suite.
Nov 27 06:02:46.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:02:46.569: INFO: namespace emptydir-6779 deletion completed in 6.272292996s

• [SLOW TEST:10.747 seconds]
[sig-storage] EmptyDir volumes
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:02:46.570: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4747
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-4747
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-4747
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4747
Nov 27 06:02:46.939: INFO: Found 0 stateful pods, waiting for 1
Nov 27 06:02:56.953: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Nov 27 06:02:56.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-4747 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 27 06:02:58.547: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 27 06:02:58.548: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 27 06:02:58.548: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 27 06:02:58.562: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 27 06:03:08.579: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 27 06:03:08.579: INFO: Waiting for statefulset status.replicas updated to 0
Nov 27 06:03:08.615: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999994133s
Nov 27 06:03:09.627: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.990451006s
Nov 27 06:03:10.639: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.978232656s
Nov 27 06:03:11.648: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.966975469s
Nov 27 06:03:12.659: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.957239717s
Nov 27 06:03:13.668: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.94642889s
Nov 27 06:03:14.679: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.937397634s
Nov 27 06:03:15.691: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.926758408s
Nov 27 06:03:16.701: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.914416857s
Nov 27 06:03:17.712: INFO: Verifying statefulset ss doesn't scale past 1 for another 904.287057ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4747
Nov 27 06:03:18.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-4747 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:03:20.082: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 27 06:03:20.082: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 27 06:03:20.082: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 27 06:03:20.097: INFO: Found 1 stateful pods, waiting for 3
Nov 27 06:03:30.115: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 27 06:03:30.115: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 27 06:03:30.115: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Nov 27 06:03:30.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-4747 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 27 06:03:31.522: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 27 06:03:31.522: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 27 06:03:31.522: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 27 06:03:31.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-4747 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 27 06:03:32.929: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 27 06:03:32.929: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 27 06:03:32.929: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 27 06:03:32.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-4747 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 27 06:03:34.332: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 27 06:03:34.332: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 27 06:03:34.332: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 27 06:03:34.332: INFO: Waiting for statefulset status.replicas updated to 0
Nov 27 06:03:34.345: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Nov 27 06:03:44.372: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 27 06:03:44.372: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 27 06:03:44.372: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 27 06:03:44.427: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999993645s
Nov 27 06:03:45.441: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.981979956s
Nov 27 06:03:46.454: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.968207415s
Nov 27 06:03:47.466: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.955669817s
Nov 27 06:03:48.478: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.942996796s
Nov 27 06:03:49.491: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.931049782s
Nov 27 06:03:50.502: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.918890587s
Nov 27 06:03:51.514: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.907846069s
Nov 27 06:03:52.528: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.895256649s
Nov 27 06:03:53.541: INFO: Verifying statefulset ss doesn't scale past 3 for another 881.173661ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4747
Nov 27 06:03:54.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-4747 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:03:55.982: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 27 06:03:55.983: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 27 06:03:55.983: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 27 06:03:55.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-4747 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:03:57.349: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 27 06:03:57.349: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 27 06:03:57.349: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 27 06:03:57.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-4747 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:03:58.836: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 27 06:03:58.836: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 27 06:03:58.836: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 27 06:03:58.836: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Nov 27 06:04:48.899: INFO: Deleting all statefulset in ns statefulset-4747
Nov 27 06:04:48.911: INFO: Scaling statefulset ss to 0
Nov 27 06:04:48.936: INFO: Waiting for statefulset status.replicas updated to 0
Nov 27 06:04:48.943: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:04:48.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4747" for this suite.
Nov 27 06:04:55.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:04:55.277: INFO: namespace statefulset-4747 deletion completed in 6.290493995s

• [SLOW TEST:128.707 seconds]
[sig-apps] StatefulSet
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:04:55.279: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4962
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
I1127 06:04:55.599438      19 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/client-go/tools/watch/informerwatcher.go:146
I1127 06:04:55.599749      19 reflector.go:158] Listing and watching *v1.Pod from k8s.io/client-go/tools/watch/informerwatcher.go:146
Nov 27 06:04:55.605: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:05:13.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4962" for this suite.
Nov 27 06:05:19.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:05:19.895: INFO: namespace pods-4962 deletion completed in 6.259906891s

• [SLOW TEST:24.617 seconds]
[k8s.io] Pods
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:05:19.905: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4368
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-4368/configmap-test-4bc6e1c2-bcae-466f-b405-016dc32af3c5
STEP: Creating a pod to test consume configMaps
Nov 27 06:05:20.246: INFO: Waiting up to 5m0s for pod "pod-configmaps-e7e0ab88-3182-4349-b135-99b0767da17f" in namespace "configmap-4368" to be "success or failure"
Nov 27 06:05:20.256: INFO: Pod "pod-configmaps-e7e0ab88-3182-4349-b135-99b0767da17f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.750972ms
Nov 27 06:05:22.267: INFO: Pod "pod-configmaps-e7e0ab88-3182-4349-b135-99b0767da17f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020275732s
Nov 27 06:05:24.281: INFO: Pod "pod-configmaps-e7e0ab88-3182-4349-b135-99b0767da17f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034777637s
STEP: Saw pod success
Nov 27 06:05:24.281: INFO: Pod "pod-configmaps-e7e0ab88-3182-4349-b135-99b0767da17f" satisfied condition "success or failure"
Nov 27 06:05:24.287: INFO: Trying to get logs from node slave1 pod pod-configmaps-e7e0ab88-3182-4349-b135-99b0767da17f container env-test: <nil>
STEP: delete the pod
Nov 27 06:05:24.347: INFO: Waiting for pod pod-configmaps-e7e0ab88-3182-4349-b135-99b0767da17f to disappear
Nov 27 06:05:24.357: INFO: Pod pod-configmaps-e7e0ab88-3182-4349-b135-99b0767da17f no longer exists
[AfterEach] [sig-node] ConfigMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:05:24.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4368" for this suite.
Nov 27 06:05:30.394: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:05:30.607: INFO: namespace configmap-4368 deletion completed in 6.238741733s

• [SLOW TEST:10.703 seconds]
[sig-node] ConfigMap
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:05:30.608: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5228
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 27 06:05:30.950: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9eb3a3b5-2c7a-4922-a8ad-83e73dd62a19" in namespace "downward-api-5228" to be "success or failure"
Nov 27 06:05:30.958: INFO: Pod "downwardapi-volume-9eb3a3b5-2c7a-4922-a8ad-83e73dd62a19": Phase="Pending", Reason="", readiness=false. Elapsed: 8.046202ms
Nov 27 06:05:32.971: INFO: Pod "downwardapi-volume-9eb3a3b5-2c7a-4922-a8ad-83e73dd62a19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021105383s
Nov 27 06:05:35.000: INFO: Pod "downwardapi-volume-9eb3a3b5-2c7a-4922-a8ad-83e73dd62a19": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050603105s
Nov 27 06:05:37.012: INFO: Pod "downwardapi-volume-9eb3a3b5-2c7a-4922-a8ad-83e73dd62a19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.062564721s
STEP: Saw pod success
Nov 27 06:05:37.012: INFO: Pod "downwardapi-volume-9eb3a3b5-2c7a-4922-a8ad-83e73dd62a19" satisfied condition "success or failure"
Nov 27 06:05:37.022: INFO: Trying to get logs from node slave1 pod downwardapi-volume-9eb3a3b5-2c7a-4922-a8ad-83e73dd62a19 container client-container: <nil>
STEP: delete the pod
Nov 27 06:05:37.110: INFO: Waiting for pod downwardapi-volume-9eb3a3b5-2c7a-4922-a8ad-83e73dd62a19 to disappear
Nov 27 06:05:37.120: INFO: Pod downwardapi-volume-9eb3a3b5-2c7a-4922-a8ad-83e73dd62a19 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:05:37.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5228" for this suite.
Nov 27 06:05:43.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:05:43.405: INFO: namespace downward-api-5228 deletion completed in 6.272652333s

• [SLOW TEST:12.798 seconds]
[sig-storage] Downward API volume
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:05:43.407: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2623
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-2623
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-2623
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2623
Nov 27 06:05:43.760: INFO: Found 0 stateful pods, waiting for 1
Nov 27 06:05:53.774: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Nov 27 06:05:53.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 27 06:05:55.206: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 27 06:05:55.207: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 27 06:05:55.207: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 27 06:05:55.228: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 27 06:05:55.228: INFO: Waiting for statefulset status.replicas updated to 0
Nov 27 06:05:55.277: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Nov 27 06:05:55.277: INFO: ss-0  slave1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:43 +0000 UTC  }]
Nov 27 06:05:55.277: INFO: 
Nov 27 06:05:55.277: INFO: StatefulSet ss has not reached scale 3, at 1
Nov 27 06:05:56.289: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.987580226s
Nov 27 06:05:57.306: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.975281208s
Nov 27 06:05:58.320: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.957034807s
Nov 27 06:05:59.333: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.944763656s
Nov 27 06:06:00.345: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.931424763s
Nov 27 06:06:01.356: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.919250902s
Nov 27 06:06:02.368: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.908285095s
Nov 27 06:06:03.383: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.896085812s
Nov 27 06:06:04.399: INFO: Verifying statefulset ss doesn't scale past 3 for another 881.126238ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2623
Nov 27 06:06:05.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:06:06.766: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 27 06:06:06.766: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 27 06:06:06.766: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 27 06:06:06.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:06:08.182: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: cannot stat '/tmp/index.html': No such file or directory\n+ true\n"
Nov 27 06:06:08.183: INFO: stdout: ""
Nov 27 06:06:08.183: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
Nov 27 06:06:08.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:06:09.668: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: cannot stat '/tmp/index.html': No such file or directory\n+ true\n"
Nov 27 06:06:09.669: INFO: stdout: ""
Nov 27 06:06:09.669: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Nov 27 06:06:09.684: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 27 06:06:09.684: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 27 06:06:09.684: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Nov 27 06:06:09.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 27 06:06:11.104: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 27 06:06:11.104: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 27 06:06:11.104: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 27 06:06:11.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 27 06:06:12.523: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 27 06:06:12.523: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 27 06:06:12.523: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 27 06:06:12.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 27 06:06:14.007: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 27 06:06:14.007: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 27 06:06:14.007: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 27 06:06:14.007: INFO: Waiting for statefulset status.replicas updated to 0
Nov 27 06:06:14.020: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Nov 27 06:06:24.045: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 27 06:06:24.045: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 27 06:06:24.045: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 27 06:06:24.091: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Nov 27 06:06:24.091: INFO: ss-0  slave1   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:43 +0000 UTC  }]
Nov 27 06:06:24.091: INFO: ss-1  master1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  }]
Nov 27 06:06:24.091: INFO: ss-2  slave1   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  }]
Nov 27 06:06:24.091: INFO: 
Nov 27 06:06:24.091: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 27 06:06:25.105: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Nov 27 06:06:25.106: INFO: ss-0  slave1   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:43 +0000 UTC  }]
Nov 27 06:06:25.106: INFO: ss-1  master1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  }]
Nov 27 06:06:25.106: INFO: ss-2  slave1   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  }]
Nov 27 06:06:25.106: INFO: 
Nov 27 06:06:25.106: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 27 06:06:26.130: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Nov 27 06:06:26.130: INFO: ss-0  slave1   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:43 +0000 UTC  }]
Nov 27 06:06:26.130: INFO: ss-1  master1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  }]
Nov 27 06:06:26.131: INFO: ss-2  slave1   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  }]
Nov 27 06:06:26.131: INFO: 
Nov 27 06:06:26.131: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 27 06:06:27.147: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Nov 27 06:06:27.147: INFO: ss-0  slave1   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:43 +0000 UTC  }]
Nov 27 06:06:27.148: INFO: ss-1  master1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  }]
Nov 27 06:06:27.148: INFO: ss-2  slave1   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  }]
Nov 27 06:06:27.148: INFO: 
Nov 27 06:06:27.148: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 27 06:06:28.169: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Nov 27 06:06:28.169: INFO: ss-0  slave1   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:43 +0000 UTC  }]
Nov 27 06:06:28.170: INFO: ss-1  master1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  }]
Nov 27 06:06:28.170: INFO: ss-2  slave1   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  }]
Nov 27 06:06:28.170: INFO: 
Nov 27 06:06:28.170: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 27 06:06:29.186: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Nov 27 06:06:29.186: INFO: ss-0  slave1   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:43 +0000 UTC  }]
Nov 27 06:06:29.186: INFO: ss-1  master1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  }]
Nov 27 06:06:29.186: INFO: ss-2  slave1   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  }]
Nov 27 06:06:29.186: INFO: 
Nov 27 06:06:29.186: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 27 06:06:30.198: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Nov 27 06:06:30.199: INFO: ss-0  slave1   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:43 +0000 UTC  }]
Nov 27 06:06:30.199: INFO: ss-1  master1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  }]
Nov 27 06:06:30.200: INFO: ss-2  slave1   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  }]
Nov 27 06:06:30.200: INFO: 
Nov 27 06:06:30.200: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 27 06:06:31.217: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Nov 27 06:06:31.217: INFO: ss-0  slave1   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:43 +0000 UTC  }]
Nov 27 06:06:31.217: INFO: ss-1  master1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  }]
Nov 27 06:06:31.217: INFO: ss-2  slave1   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  }]
Nov 27 06:06:31.218: INFO: 
Nov 27 06:06:31.218: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 27 06:06:32.230: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Nov 27 06:06:32.230: INFO: ss-0  slave1   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:43 +0000 UTC  }]
Nov 27 06:06:32.231: INFO: ss-1  master1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  }]
Nov 27 06:06:32.231: INFO: ss-2  slave1   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  }]
Nov 27 06:06:32.231: INFO: 
Nov 27 06:06:32.231: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 27 06:06:33.243: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Nov 27 06:06:33.243: INFO: ss-0  slave1   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:43 +0000 UTC  }]
Nov 27 06:06:33.243: INFO: ss-1  master1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  }]
Nov 27 06:06:33.244: INFO: ss-2  slave1   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-27 06:05:55 +0000 UTC  }]
Nov 27 06:06:33.244: INFO: 
Nov 27 06:06:33.244: INFO: StatefulSet ss has not reached scale 0, at 3
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2623
Nov 27 06:06:34.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:06:35.399: INFO: rc: 1
Nov 27 06:06:35.400: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  error: unable to upgrade connection: container not found ("webserver")
 [] <nil> 0xc0026f2d80 exit status 1 <nil> <nil> true [0xc002e8f278 0xc002e8f2c0 0xc002e8f310] [0xc002e8f278 0xc002e8f2c0 0xc002e8f310] [0xc002e8f2a0 0xc002e8f2f8] [0x120e534d8 0x120e534d8] 0xc001c10540 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 27 06:06:45.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:06:46.126: INFO: rc: 1
Nov 27 06:06:46.127: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003069380 exit status 1 <nil> <nil> true [0xc0028c9148 0xc0028c9168 0xc0028c9180] [0xc0028c9148 0xc0028c9168 0xc0028c9180] [0xc0028c9160 0xc0028c9178] [0x120e534d8 0x120e534d8] 0xc0064239e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:06:56.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:06:56.895: INFO: rc: 1
Nov 27 06:06:56.896: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0030695f0 exit status 1 <nil> <nil> true [0xc0028c9188 0xc0028c91a8 0xc0028c91c0] [0xc0028c9188 0xc0028c91a8 0xc0028c91c0] [0xc0028c91a0 0xc0028c91b8] [0x120e534d8 0x120e534d8] 0xc00228a5a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:07:06.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:07:07.670: INFO: rc: 1
Nov 27 06:07:07.671: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002fe3f80 exit status 1 <nil> <nil> true [0xc004aea010 0xc004aea028 0xc004aea040] [0xc004aea010 0xc004aea028 0xc004aea040] [0xc004aea020 0xc004aea038] [0x120e534d8 0x120e534d8] 0xc0043b7a40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:07:17.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:07:18.592: INFO: rc: 1
Nov 27 06:07:18.593: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0053d61e0 exit status 1 <nil> <nil> true [0xc004aea048 0xc004aea060 0xc004aea078] [0xc004aea048 0xc004aea060 0xc004aea078] [0xc004aea058 0xc004aea070] [0x120e534d8 0x120e534d8] 0xc0043b7da0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:07:28.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:07:29.376: INFO: rc: 1
Nov 27 06:07:29.377: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0053d6480 exit status 1 <nil> <nil> true [0xc004aea080 0xc004aea098 0xc004aea0b0] [0xc004aea080 0xc004aea098 0xc004aea0b0] [0xc004aea090 0xc004aea0a8] [0x120e534d8 0x120e534d8] 0xc002e824e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:07:39.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:07:40.154: INFO: rc: 1
Nov 27 06:07:40.157: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0064141e0 exit status 1 <nil> <nil> true [0xc00017c370 0xc00017c998 0xc00017cc10] [0xc00017c370 0xc00017c998 0xc00017cc10] [0xc00017c5c8 0xc00017cba0] [0x120e534d8 0x120e534d8] 0xc0064225a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:07:50.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:07:50.914: INFO: rc: 1
Nov 27 06:07:50.914: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00388a210 exit status 1 <nil> <nil> true [0xc00000e058 0xc00000f0f8 0xc00000f6f0] [0xc00000e058 0xc00000f0f8 0xc00000f6f0] [0xc00000ecf0 0xc00000f588] [0x120e534d8 0x120e534d8] 0xc0025f1020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:08:00.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:08:01.684: INFO: rc: 1
Nov 27 06:08:01.685: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0064143f0 exit status 1 <nil> <nil> true [0xc00017cda8 0xc00017cfc8 0xc00017d418] [0xc00017cda8 0xc00017cfc8 0xc00017d418] [0xc00017cf78 0xc00017d238] [0x120e534d8 0x120e534d8] 0xc006422de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:08:11.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:08:12.391: INFO: rc: 1
Nov 27 06:08:12.392: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc004fc01e0 exit status 1 <nil> <nil> true [0xc0004c8278 0xc0004c85e0 0xc0004c8918] [0xc0004c8278 0xc0004c85e0 0xc0004c8918] [0xc0004c85a0 0xc0004c87d8] [0x120e534d8 0x120e534d8] 0xc002e1c480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:08:22.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:08:23.147: INFO: rc: 1
Nov 27 06:08:23.148: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc004fc03f0 exit status 1 <nil> <nil> true [0xc0004c8948 0xc0004c8c00 0xc0004c8e48] [0xc0004c8948 0xc0004c8c00 0xc0004c8e48] [0xc0004c8b80 0xc0004c8c70] [0x120e534d8 0x120e534d8] 0xc002e1c9c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:08:33.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:08:33.910: INFO: rc: 1
Nov 27 06:08:33.911: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00564c210 exit status 1 <nil> <nil> true [0xc005c6c000 0xc005c6c018 0xc005c6c030] [0xc005c6c000 0xc005c6c018 0xc005c6c030] [0xc005c6c010 0xc005c6c028] [0x120e534d8 0x120e534d8] 0xc002bd48a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:08:43.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:08:44.670: INFO: rc: 1
Nov 27 06:08:44.671: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc004fc0630 exit status 1 <nil> <nil> true [0xc0004c8eb0 0xc0004c90e0 0xc0004c9360] [0xc0004c8eb0 0xc0004c90e0 0xc0004c9360] [0xc0004c9020 0xc0004c9308] [0x120e534d8 0x120e534d8] 0xc002e1cd80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:08:54.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:08:55.362: INFO: rc: 1
Nov 27 06:08:55.363: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00564c4b0 exit status 1 <nil> <nil> true [0xc005c6c038 0xc005c6c050 0xc005c6c068] [0xc005c6c038 0xc005c6c050 0xc005c6c068] [0xc005c6c048 0xc005c6c060] [0x120e534d8 0x120e534d8] 0xc002bd5620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:09:05.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:09:06.079: INFO: rc: 1
Nov 27 06:09:06.080: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00388a480 exit status 1 <nil> <nil> true [0xc00000fd68 0xc0028c8038 0xc0028c80b8] [0xc00000fd68 0xc0028c8038 0xc0028c80b8] [0xc0028c8028 0xc0028c8078] [0x120e534d8 0x120e534d8] 0xc0025f1e00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:09:16.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:09:16.846: INFO: rc: 1
Nov 27 06:09:16.847: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc006414630 exit status 1 <nil> <nil> true [0xc00017d450 0xc00017d670 0xc00017d780] [0xc00017d450 0xc00017d670 0xc00017d780] [0xc00017d548 0xc00017d748] [0x120e534d8 0x120e534d8] 0xc006423b60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:09:26.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:09:27.641: INFO: rc: 1
Nov 27 06:09:27.642: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00388a6c0 exit status 1 <nil> <nil> true [0xc0028c80e0 0xc0028c8168 0xc0028c8208] [0xc0028c80e0 0xc0028c8168 0xc0028c8208] [0xc0028c8158 0xc0028c81e8] [0x120e534d8 0x120e534d8] 0xc002fa4600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:09:37.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:09:38.402: INFO: rc: 1
Nov 27 06:09:38.403: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00564c1e0 exit status 1 <nil> <nil> true [0xc00000e340 0xc00000f130 0xc00000fd68] [0xc00000e340 0xc00000f130 0xc00000fd68] [0xc00000f0f8 0xc00000f6f0] [0x120e534d8 0x120e534d8] 0xc0025f1020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:09:48.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:09:49.107: INFO: rc: 1
Nov 27 06:09:49.109: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc006414210 exit status 1 <nil> <nil> true [0xc005c6c000 0xc005c6c018 0xc005c6c030] [0xc005c6c000 0xc005c6c018 0xc005c6c030] [0xc005c6c010 0xc005c6c028] [0x120e534d8 0x120e534d8] 0xc002bd48a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:09:59.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:09:59.821: INFO: rc: 1
Nov 27 06:09:59.823: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc006414450 exit status 1 <nil> <nil> true [0xc005c6c038 0xc005c6c050 0xc005c6c068] [0xc005c6c038 0xc005c6c050 0xc005c6c068] [0xc005c6c048 0xc005c6c060] [0x120e534d8 0x120e534d8] 0xc002bd5620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:10:09.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:10:10.650: INFO: rc: 1
Nov 27 06:10:10.650: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0064146c0 exit status 1 <nil> <nil> true [0xc005c6c070 0xc005c6c088 0xc005c6c0a0] [0xc005c6c070 0xc005c6c088 0xc005c6c0a0] [0xc005c6c080 0xc005c6c098] [0x120e534d8 0x120e534d8] 0xc002bd5d40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:10:20.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:10:21.398: INFO: rc: 1
Nov 27 06:10:21.402: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00388a270 exit status 1 <nil> <nil> true [0xc0028c8018 0xc0028c8068 0xc0028c80e0] [0xc0028c8018 0xc0028c8068 0xc0028c80e0] [0xc0028c8038 0xc0028c80b8] [0x120e534d8 0x120e534d8] 0xc002fa4540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:10:31.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:10:32.150: INFO: rc: 1
Nov 27 06:10:32.150: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00388a4e0 exit status 1 <nil> <nil> true [0xc0028c8120 0xc0028c8190 0xc0028c8218] [0xc0028c8120 0xc0028c8190 0xc0028c8218] [0xc0028c8168 0xc0028c8208] [0x120e534d8 0x120e534d8] 0xc002fa4c60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:10:42.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:10:42.895: INFO: rc: 1
Nov 27 06:10:42.896: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00388a720 exit status 1 <nil> <nil> true [0xc0028c8228 0xc0028c82a8 0xc0028c8330] [0xc0028c8228 0xc0028c82a8 0xc0028c8330] [0xc0028c8298 0xc0028c82f8] [0x120e534d8 0x120e534d8] 0xc002fa5320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:10:52.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:10:53.621: INFO: rc: 1
Nov 27 06:10:53.622: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc004fc0210 exit status 1 <nil> <nil> true [0xc00017c250 0xc00017c5c8 0xc00017cba0] [0xc00017c250 0xc00017c5c8 0xc00017cba0] [0xc00017c520 0xc00017ca30] [0x120e534d8 0x120e534d8] 0xc0064225a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:11:03.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:11:04.307: INFO: rc: 1
Nov 27 06:11:04.308: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc004fc0450 exit status 1 <nil> <nil> true [0xc00017cc10 0xc00017cf78 0xc00017d238] [0xc00017cc10 0xc00017cf78 0xc00017d238] [0xc00017ce20 0xc00017d0a8] [0x120e534d8 0x120e534d8] 0xc006422de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:11:14.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:11:15.047: INFO: rc: 1
Nov 27 06:11:15.048: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc006414990 exit status 1 <nil> <nil> true [0xc005c6c0a8 0xc005c6c0c0 0xc005c6c0d8] [0xc005c6c0a8 0xc005c6c0c0 0xc005c6c0d8] [0xc005c6c0b8 0xc005c6c0d0] [0x120e534d8 0x120e534d8] 0xc002e1c420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:11:25.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:11:25.789: INFO: rc: 1
Nov 27 06:11:25.790: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00388a960 exit status 1 <nil> <nil> true [0xc0028c8370 0xc0028c83a8 0xc0028c8450] [0xc0028c8370 0xc0028c83a8 0xc0028c8450] [0xc0028c8398 0xc0028c8438] [0x120e534d8 0x120e534d8] 0xc0025381e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 27 06:11:35.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 exec --namespace=statefulset-2623 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 27 06:11:36.509: INFO: rc: 1
Nov 27 06:11:36.510: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
Nov 27 06:11:36.510: INFO: Scaling statefulset ss to 0
Nov 27 06:11:36.551: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Nov 27 06:11:36.560: INFO: Deleting all statefulset in ns statefulset-2623
Nov 27 06:11:36.569: INFO: Scaling statefulset ss to 0
Nov 27 06:11:36.606: INFO: Waiting for statefulset status.replicas updated to 0
Nov 27 06:11:36.623: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:11:36.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2623" for this suite.
Nov 27 06:11:42.760: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:11:43.049: INFO: namespace statefulset-2623 deletion completed in 6.327682134s

• [SLOW TEST:359.642 seconds]
[sig-apps] StatefulSet
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:11:43.052: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7364
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Nov 27 06:11:43.365: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-192585042 proxy --unix-socket=/tmp/kubectl-proxy-unix255762689/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:11:43.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7364" for this suite.
Nov 27 06:11:49.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:11:50.149: INFO: namespace kubectl-7364 deletion completed in 6.299512116s

• [SLOW TEST:7.097 seconds]
[sig-cli] Kubectl client
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:11:50.149: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8807
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 27 06:11:50.473: INFO: Waiting up to 5m0s for pod "downwardapi-volume-19dce878-7c49-4b5a-b46b-783bb4dca91d" in namespace "projected-8807" to be "success or failure"
Nov 27 06:11:50.483: INFO: Pod "downwardapi-volume-19dce878-7c49-4b5a-b46b-783bb4dca91d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.473066ms
Nov 27 06:11:52.493: INFO: Pod "downwardapi-volume-19dce878-7c49-4b5a-b46b-783bb4dca91d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019858706s
Nov 27 06:11:54.502: INFO: Pod "downwardapi-volume-19dce878-7c49-4b5a-b46b-783bb4dca91d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029513236s
STEP: Saw pod success
Nov 27 06:11:54.503: INFO: Pod "downwardapi-volume-19dce878-7c49-4b5a-b46b-783bb4dca91d" satisfied condition "success or failure"
Nov 27 06:11:54.510: INFO: Trying to get logs from node slave1 pod downwardapi-volume-19dce878-7c49-4b5a-b46b-783bb4dca91d container client-container: <nil>
STEP: delete the pod
Nov 27 06:11:54.827: INFO: Waiting for pod downwardapi-volume-19dce878-7c49-4b5a-b46b-783bb4dca91d to disappear
Nov 27 06:11:54.835: INFO: Pod downwardapi-volume-19dce878-7c49-4b5a-b46b-783bb4dca91d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:11:54.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8807" for this suite.
Nov 27 06:12:00.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:12:01.156: INFO: namespace projected-8807 deletion completed in 6.306375198s

• [SLOW TEST:11.007 seconds]
[sig-storage] Projected downwardAPI
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:12:01.159: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-9824
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Nov 27 06:12:02.086: INFO: Pod name wrapped-volume-race-fcdea591-d0bc-4218-a7d0-7f9c4453a447: Found 0 pods out of 5
Nov 27 06:12:07.119: INFO: Pod name wrapped-volume-race-fcdea591-d0bc-4218-a7d0-7f9c4453a447: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-fcdea591-d0bc-4218-a7d0-7f9c4453a447 in namespace emptydir-wrapper-9824, will wait for the garbage collector to delete the pods
I1127 06:12:21.296102      19 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 06:12:21.296500      19 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Nov 27 06:12:21.371: INFO: Deleting ReplicationController wrapped-volume-race-fcdea591-d0bc-4218-a7d0-7f9c4453a447 took: 25.587684ms
I1127 06:12:21.872617      19 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-9824/wrapped-volume-race-fcdea591-d0bc-4218-a7d0-7f9c4453a447-zqkz9 in state Running, deletion time 2019-11-27 06:12:51 +0000 UTC
I1127 06:12:21.872823      19 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-9824/wrapped-volume-race-fcdea591-d0bc-4218-a7d0-7f9c4453a447-v5zvz in state Running, deletion time 2019-11-27 06:12:51 +0000 UTC
I1127 06:12:21.872863      19 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-9824/wrapped-volume-race-fcdea591-d0bc-4218-a7d0-7f9c4453a447-fx5ld in state Running, deletion time 2019-11-27 06:12:51 +0000 UTC
I1127 06:12:21.872902      19 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-9824/wrapped-volume-race-fcdea591-d0bc-4218-a7d0-7f9c4453a447-jd95x in state Running, deletion time 2019-11-27 06:12:51 +0000 UTC
I1127 06:12:21.972602      19 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-9824/wrapped-volume-race-fcdea591-d0bc-4218-a7d0-7f9c4453a447-fx5ld in state Running, deletion time 2019-11-27 06:12:51 +0000 UTC
I1127 06:12:21.972819      19 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-9824/wrapped-volume-race-fcdea591-d0bc-4218-a7d0-7f9c4453a447-jd95x in state Running, deletion time 2019-11-27 06:12:51 +0000 UTC
I1127 06:12:21.972863      19 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-9824/wrapped-volume-race-fcdea591-d0bc-4218-a7d0-7f9c4453a447-4w8lh in state Running, deletion time 2019-11-27 06:12:51 +0000 UTC
I1127 06:12:21.972905      19 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-9824/wrapped-volume-race-fcdea591-d0bc-4218-a7d0-7f9c4453a447-zqkz9 in state Running, deletion time 2019-11-27 06:12:51 +0000 UTC
I1127 06:12:21.972941      19 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-9824/wrapped-volume-race-fcdea591-d0bc-4218-a7d0-7f9c4453a447-v5zvz in state Running, deletion time 2019-11-27 06:12:51 +0000 UTC
Nov 27 06:12:21.973: INFO: Terminating ReplicationController wrapped-volume-race-fcdea591-d0bc-4218-a7d0-7f9c4453a447 pods took: 601.243118ms
STEP: Creating RC which spawns configmap-volume pods
Nov 27 06:12:59.438: INFO: Pod name wrapped-volume-race-50fa662d-ce42-4f43-92e5-b9818563d0a0: Found 0 pods out of 5
Nov 27 06:13:04.466: INFO: Pod name wrapped-volume-race-50fa662d-ce42-4f43-92e5-b9818563d0a0: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-50fa662d-ce42-4f43-92e5-b9818563d0a0 in namespace emptydir-wrapper-9824, will wait for the garbage collector to delete the pods
I1127 06:13:18.548317      19 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 06:13:18.548412      19 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Nov 27 06:13:18.621: INFO: Deleting ReplicationController wrapped-volume-race-50fa662d-ce42-4f43-92e5-b9818563d0a0 took: 23.542645ms
I1127 06:13:19.124112      19 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-9824/wrapped-volume-race-50fa662d-ce42-4f43-92e5-b9818563d0a0-pbntw in state Running, deletion time 2019-11-27 06:13:49 +0000 UTC
I1127 06:13:19.124237      19 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-9824/wrapped-volume-race-50fa662d-ce42-4f43-92e5-b9818563d0a0-7kbn7 in state Running, deletion time 2019-11-27 06:13:49 +0000 UTC
Nov 27 06:13:19.225: INFO: Terminating ReplicationController wrapped-volume-race-50fa662d-ce42-4f43-92e5-b9818563d0a0 pods took: 603.249047ms
I1127 06:13:19.224834      19 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-9824/wrapped-volume-race-50fa662d-ce42-4f43-92e5-b9818563d0a0-pbntw in state Running, deletion time 2019-11-27 06:13:49 +0000 UTC
I1127 06:13:19.225005      19 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-9824/wrapped-volume-race-50fa662d-ce42-4f43-92e5-b9818563d0a0-jlc4j in state Running, deletion time 2019-11-27 06:13:49 +0000 UTC
I1127 06:13:19.225047      19 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-9824/wrapped-volume-race-50fa662d-ce42-4f43-92e5-b9818563d0a0-25dt9 in state Running, deletion time 2019-11-27 06:13:49 +0000 UTC
I1127 06:13:19.225086      19 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-9824/wrapped-volume-race-50fa662d-ce42-4f43-92e5-b9818563d0a0-7kbn7 in state Running, deletion time 2019-11-27 06:13:49 +0000 UTC
I1127 06:13:19.225119      19 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-9824/wrapped-volume-race-50fa662d-ce42-4f43-92e5-b9818563d0a0-mgnbq in state Running, deletion time 2019-11-27 06:13:49 +0000 UTC
STEP: Creating RC which spawns configmap-volume pods
Nov 27 06:13:59.686: INFO: Pod name wrapped-volume-race-ddda2fe7-6547-487a-b36d-ec7546a541c9: Found 0 pods out of 5
Nov 27 06:14:04.714: INFO: Pod name wrapped-volume-race-ddda2fe7-6547-487a-b36d-ec7546a541c9: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ddda2fe7-6547-487a-b36d-ec7546a541c9 in namespace emptydir-wrapper-9824, will wait for the garbage collector to delete the pods
I1127 06:14:18.796125      19 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 06:14:18.796227      19 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Nov 27 06:14:18.867: INFO: Deleting ReplicationController wrapped-volume-race-ddda2fe7-6547-487a-b36d-ec7546a541c9 took: 21.245336ms
I1127 06:14:19.370868      19 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-9824/wrapped-volume-race-ddda2fe7-6547-487a-b36d-ec7546a541c9-ljc22 in state Running, deletion time 2019-11-27 06:14:49 +0000 UTC
I1127 06:14:19.370990      19 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-9824/wrapped-volume-race-ddda2fe7-6547-487a-b36d-ec7546a541c9-9n6xn in state Running, deletion time 2019-11-27 06:14:49 +0000 UTC
I1127 06:14:19.371034      19 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-9824/wrapped-volume-race-ddda2fe7-6547-487a-b36d-ec7546a541c9-qxgx8 in state Running, deletion time 2019-11-27 06:14:49 +0000 UTC
I1127 06:14:19.469683      19 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-9824/wrapped-volume-race-ddda2fe7-6547-487a-b36d-ec7546a541c9-5pkmg in state Running, deletion time 2019-11-27 06:14:49 +0000 UTC
I1127 06:14:19.469935      19 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-9824/wrapped-volume-race-ddda2fe7-6547-487a-b36d-ec7546a541c9-fjpgc in state Running, deletion time 2019-11-27 06:14:49 +0000 UTC
I1127 06:14:19.470010      19 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-9824/wrapped-volume-race-ddda2fe7-6547-487a-b36d-ec7546a541c9-9n6xn in state Running, deletion time 2019-11-27 06:14:49 +0000 UTC
I1127 06:14:19.470051      19 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-9824/wrapped-volume-race-ddda2fe7-6547-487a-b36d-ec7546a541c9-qxgx8 in state Running, deletion time 2019-11-27 06:14:49 +0000 UTC
Nov 27 06:14:19.470: INFO: Terminating ReplicationController wrapped-volume-race-ddda2fe7-6547-487a-b36d-ec7546a541c9 pods took: 602.626686ms
I1127 06:14:19.470095      19 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-9824/wrapped-volume-race-ddda2fe7-6547-487a-b36d-ec7546a541c9-ljc22 in state Running, deletion time 2019-11-27 06:14:49 +0000 UTC
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:15:00.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9824" for this suite.
Nov 27 06:15:11.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:15:11.297: INFO: namespace emptydir-wrapper-9824 deletion completed in 10.295136264s

• [SLOW TEST:190.138 seconds]
[sig-storage] EmptyDir wrapper volumes
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:15:11.300: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-11
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-ae60980e-19fd-482f-be4a-59a31133bde1
STEP: Creating a pod to test consume configMaps
Nov 27 06:15:11.656: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5ed9e4be-3969-489a-93c8-5abe7f4eb71d" in namespace "projected-11" to be "success or failure"
Nov 27 06:15:11.665: INFO: Pod "pod-projected-configmaps-5ed9e4be-3969-489a-93c8-5abe7f4eb71d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.128611ms
Nov 27 06:15:13.675: INFO: Pod "pod-projected-configmaps-5ed9e4be-3969-489a-93c8-5abe7f4eb71d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019014877s
Nov 27 06:15:15.688: INFO: Pod "pod-projected-configmaps-5ed9e4be-3969-489a-93c8-5abe7f4eb71d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031986057s
STEP: Saw pod success
Nov 27 06:15:15.688: INFO: Pod "pod-projected-configmaps-5ed9e4be-3969-489a-93c8-5abe7f4eb71d" satisfied condition "success or failure"
Nov 27 06:15:15.696: INFO: Trying to get logs from node slave1 pod pod-projected-configmaps-5ed9e4be-3969-489a-93c8-5abe7f4eb71d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 27 06:15:15.995: INFO: Waiting for pod pod-projected-configmaps-5ed9e4be-3969-489a-93c8-5abe7f4eb71d to disappear
Nov 27 06:15:16.014: INFO: Pod pod-projected-configmaps-5ed9e4be-3969-489a-93c8-5abe7f4eb71d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:15:16.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-11" for this suite.
Nov 27 06:15:22.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:15:22.320: INFO: namespace projected-11 deletion completed in 6.292630947s

• [SLOW TEST:11.020 seconds]
[sig-storage] Projected configMap
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:15:22.323: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9479
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 06:15:22.623: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 27 06:15:36.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 --namespace=crd-publish-openapi-9479 create -f -'
Nov 27 06:15:38.323: INFO: stderr: ""
Nov 27 06:15:38.324: INFO: stdout: "e2e-test-crd-publish-openapi-1292-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 27 06:15:38.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 --namespace=crd-publish-openapi-9479 delete e2e-test-crd-publish-openapi-1292-crds test-cr'
Nov 27 06:15:39.042: INFO: stderr: ""
Nov 27 06:15:39.043: INFO: stdout: "e2e-test-crd-publish-openapi-1292-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Nov 27 06:15:39.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 --namespace=crd-publish-openapi-9479 apply -f -'
Nov 27 06:15:40.176: INFO: stderr: ""
Nov 27 06:15:40.177: INFO: stdout: "e2e-test-crd-publish-openapi-1292-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 27 06:15:40.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 --namespace=crd-publish-openapi-9479 delete e2e-test-crd-publish-openapi-1292-crds test-cr'
Nov 27 06:15:40.948: INFO: stderr: ""
Nov 27 06:15:40.948: INFO: stdout: "e2e-test-crd-publish-openapi-1292-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Nov 27 06:15:40.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 explain e2e-test-crd-publish-openapi-1292-crds'
Nov 27 06:15:42.175: INFO: stderr: ""
Nov 27 06:15:42.176: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1292-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:15:56.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9479" for this suite.
Nov 27 06:16:02.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:16:02.890: INFO: namespace crd-publish-openapi-9479 deletion completed in 6.34927303s

• [SLOW TEST:40.568 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:16:02.898: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9191
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-129f79e1-c5c5-4807-b311-f96a00b8ace4
STEP: Creating a pod to test consume configMaps
Nov 27 06:16:03.245: INFO: Waiting up to 5m0s for pod "pod-configmaps-457bb58b-23cf-46c4-8c96-72e36138498c" in namespace "configmap-9191" to be "success or failure"
Nov 27 06:16:03.253: INFO: Pod "pod-configmaps-457bb58b-23cf-46c4-8c96-72e36138498c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.050568ms
Nov 27 06:16:05.263: INFO: Pod "pod-configmaps-457bb58b-23cf-46c4-8c96-72e36138498c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017075669s
Nov 27 06:16:07.286: INFO: Pod "pod-configmaps-457bb58b-23cf-46c4-8c96-72e36138498c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040495186s
STEP: Saw pod success
Nov 27 06:16:07.286: INFO: Pod "pod-configmaps-457bb58b-23cf-46c4-8c96-72e36138498c" satisfied condition "success or failure"
Nov 27 06:16:07.302: INFO: Trying to get logs from node slave1 pod pod-configmaps-457bb58b-23cf-46c4-8c96-72e36138498c container configmap-volume-test: <nil>
STEP: delete the pod
Nov 27 06:16:07.363: INFO: Waiting for pod pod-configmaps-457bb58b-23cf-46c4-8c96-72e36138498c to disappear
Nov 27 06:16:07.369: INFO: Pod pod-configmaps-457bb58b-23cf-46c4-8c96-72e36138498c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:16:07.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9191" for this suite.
Nov 27 06:16:13.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:16:13.642: INFO: namespace configmap-9191 deletion completed in 6.260589673s

• [SLOW TEST:10.744 seconds]
[sig-storage] ConfigMap
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:16:13.644: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8037
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:16:14.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8037" for this suite.
Nov 27 06:16:20.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:16:20.332: INFO: namespace resourcequota-8037 deletion completed in 6.289357973s

• [SLOW TEST:6.689 seconds]
[sig-api-machinery] ResourceQuota
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:16:20.336: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7077
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:16:26.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7077" for this suite.
Nov 27 06:17:14.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:17:15.019: INFO: namespace kubelet-test-7077 deletion completed in 48.267553632s

• [SLOW TEST:54.684 seconds]
[k8s.io] Kubelet
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:17:15.020: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1518
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Nov 27 06:17:15.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-1518 -- logs-generator --log-lines-total 100 --run-duration 20s'
Nov 27 06:17:16.132: INFO: stderr: ""
Nov 27 06:17:16.133: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Nov 27 06:17:16.133: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Nov 27 06:17:16.133: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-1518" to be "running and ready, or succeeded"
Nov 27 06:17:16.146: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 12.996677ms
Nov 27 06:17:18.158: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025036767s
Nov 27 06:17:20.172: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.038726865s
Nov 27 06:17:20.172: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Nov 27 06:17:20.172: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Nov 27 06:17:20.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs logs-generator logs-generator --namespace=kubectl-1518'
Nov 27 06:17:21.157: INFO: stderr: ""
Nov 27 06:17:21.158: INFO: stdout: "I1127 06:17:18.776612       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/z5gb 391\nI1127 06:17:18.976828       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/n82c 522\nI1127 06:17:19.176869       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/6gk 438\nI1127 06:17:19.376885       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/rxs 524\nI1127 06:17:19.577156       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/blp 483\nI1127 06:17:19.777063       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/fbp 341\nI1127 06:17:19.976942       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/trh 338\nI1127 06:17:20.176931       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/bfvf 224\nI1127 06:17:20.376889       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/plv 307\nI1127 06:17:20.576873       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/pxr7 260\nI1127 06:17:20.777060       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/bx8 397\nI1127 06:17:20.976972       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/ldh 495\n"
STEP: limiting log lines
Nov 27 06:17:21.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs logs-generator logs-generator --namespace=kubectl-1518 --tail=1'
Nov 27 06:17:21.956: INFO: stderr: ""
Nov 27 06:17:21.957: INFO: stdout: "I1127 06:17:21.777054       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/zdg 279\n"
STEP: limiting log bytes
Nov 27 06:17:21.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs logs-generator logs-generator --namespace=kubectl-1518 --limit-bytes=1'
Nov 27 06:17:22.735: INFO: stderr: ""
Nov 27 06:17:22.736: INFO: stdout: "I"
STEP: exposing timestamps
Nov 27 06:17:22.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs logs-generator logs-generator --namespace=kubectl-1518 --tail=1 --timestamps'
Nov 27 06:17:23.504: INFO: stderr: ""
Nov 27 06:17:23.504: INFO: stdout: "2019-11-27T06:17:23.37719054Z I1127 06:17:23.376835       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/ns/pods/d7f6 253\n"
STEP: restricting to a time range
Nov 27 06:17:26.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs logs-generator logs-generator --namespace=kubectl-1518 --since=1s'
Nov 27 06:17:26.807: INFO: stderr: ""
Nov 27 06:17:26.807: INFO: stdout: "I1127 06:17:25.976855       1 logs_generator.go:76] 36 PUT /api/v1/namespaces/ns/pods/7rgt 345\nI1127 06:17:26.177019       1 logs_generator.go:76] 37 GET /api/v1/namespaces/ns/pods/jgv 381\nI1127 06:17:26.376944       1 logs_generator.go:76] 38 PUT /api/v1/namespaces/kube-system/pods/q9cw 555\nI1127 06:17:26.576854       1 logs_generator.go:76] 39 POST /api/v1/namespaces/ns/pods/486n 476\nI1127 06:17:26.776888       1 logs_generator.go:76] 40 GET /api/v1/namespaces/ns/pods/rvm6 554\n"
Nov 27 06:17:26.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 logs logs-generator logs-generator --namespace=kubectl-1518 --since=24h'
Nov 27 06:17:27.606: INFO: stderr: ""
Nov 27 06:17:27.607: INFO: stdout: "I1127 06:17:18.776612       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/z5gb 391\nI1127 06:17:18.976828       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/n82c 522\nI1127 06:17:19.176869       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/6gk 438\nI1127 06:17:19.376885       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/rxs 524\nI1127 06:17:19.577156       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/blp 483\nI1127 06:17:19.777063       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/fbp 341\nI1127 06:17:19.976942       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/trh 338\nI1127 06:17:20.176931       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/bfvf 224\nI1127 06:17:20.376889       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/plv 307\nI1127 06:17:20.576873       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/pxr7 260\nI1127 06:17:20.777060       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/bx8 397\nI1127 06:17:20.976972       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/ldh 495\nI1127 06:17:21.176867       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/r46v 398\nI1127 06:17:21.376868       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/flz 350\nI1127 06:17:21.576825       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/ddmh 352\nI1127 06:17:21.777054       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/zdg 279\nI1127 06:17:21.976859       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/wqcq 336\nI1127 06:17:22.176875       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/9fm8 214\nI1127 06:17:22.376809       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/wdbs 532\nI1127 06:17:22.576800       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/xm56 381\nI1127 06:17:22.776905       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/n4t 253\nI1127 06:17:22.976889       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/2hb 367\nI1127 06:17:23.176891       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/sdlk 382\nI1127 06:17:23.376835       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/ns/pods/d7f6 253\nI1127 06:17:23.576916       1 logs_generator.go:76] 24 POST /api/v1/namespaces/default/pods/6vw 261\nI1127 06:17:23.776788       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/default/pods/zkdc 328\nI1127 06:17:23.976853       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/6gw 231\nI1127 06:17:24.176926       1 logs_generator.go:76] 27 POST /api/v1/namespaces/kube-system/pods/n689 300\nI1127 06:17:24.376800       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/kube-system/pods/5dg 403\nI1127 06:17:24.576779       1 logs_generator.go:76] 29 POST /api/v1/namespaces/kube-system/pods/t8dj 283\nI1127 06:17:24.776802       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/ns/pods/2s8f 352\nI1127 06:17:24.976747       1 logs_generator.go:76] 31 GET /api/v1/namespaces/kube-system/pods/9kp 504\nI1127 06:17:25.179542       1 logs_generator.go:76] 32 GET /api/v1/namespaces/kube-system/pods/9jrx 200\nI1127 06:17:25.376979       1 logs_generator.go:76] 33 PUT /api/v1/namespaces/ns/pods/ldx 432\nI1127 06:17:25.577159       1 logs_generator.go:76] 34 GET /api/v1/namespaces/default/pods/9frb 298\nI1127 06:17:25.776926       1 logs_generator.go:76] 35 POST /api/v1/namespaces/ns/pods/zbx 332\nI1127 06:17:25.976855       1 logs_generator.go:76] 36 PUT /api/v1/namespaces/ns/pods/7rgt 345\nI1127 06:17:26.177019       1 logs_generator.go:76] 37 GET /api/v1/namespaces/ns/pods/jgv 381\nI1127 06:17:26.376944       1 logs_generator.go:76] 38 PUT /api/v1/namespaces/kube-system/pods/q9cw 555\nI1127 06:17:26.576854       1 logs_generator.go:76] 39 POST /api/v1/namespaces/ns/pods/486n 476\nI1127 06:17:26.776888       1 logs_generator.go:76] 40 GET /api/v1/namespaces/ns/pods/rvm6 554\nI1127 06:17:26.976830       1 logs_generator.go:76] 41 POST /api/v1/namespaces/ns/pods/zp9 576\nI1127 06:17:27.176933       1 logs_generator.go:76] 42 POST /api/v1/namespaces/kube-system/pods/xtk 274\nI1127 06:17:27.378823       1 logs_generator.go:76] 43 PUT /api/v1/namespaces/default/pods/r96 377\nI1127 06:17:27.576831       1 logs_generator.go:76] 44 GET /api/v1/namespaces/kube-system/pods/p8j 274\n"
[AfterEach] Kubectl logs
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Nov 27 06:17:27.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 delete pod logs-generator --namespace=kubectl-1518'
Nov 27 06:17:39.120: INFO: stderr: ""
Nov 27 06:17:39.120: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:17:39.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1518" for this suite.
Nov 27 06:17:45.171: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:17:45.407: INFO: namespace kubectl-1518 deletion completed in 6.264379556s

• [SLOW TEST:30.387 seconds]
[sig-cli] Kubectl client
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:17:45.414: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6414
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 27 06:17:45.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-6414'
Nov 27 06:17:46.492: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Nov 27 06:17:46.492: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Nov 27 06:17:46.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 delete jobs e2e-test-httpd-job --namespace=kubectl-6414'
Nov 27 06:17:47.276: INFO: stderr: ""
Nov 27 06:17:47.277: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:17:47.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6414" for this suite.
Nov 27 06:17:53.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:17:53.614: INFO: namespace kubectl-6414 deletion completed in 6.320646995s

• [SLOW TEST:8.199 seconds]
[sig-cli] Kubectl client
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:17:53.616: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5580
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 06:17:53.942: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Nov 27 06:17:58.954: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 27 06:17:58.954: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Nov 27 06:17:59.008: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5580 /apis/apps/v1/namespaces/deployment-5580/deployments/test-cleanup-deployment 5b5c7583-1236-439b-9f19-b77ba4593dd9 969187 1 2019-11-27 06:17:58 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00731d738 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Nov 27 06:17:59.040: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-5580 /apis/apps/v1/namespaces/deployment-5580/replicasets/test-cleanup-deployment-65db99849b db35b722-51b9-4471-a5f1-c231a48b5252 969189 1 2019-11-27 06:17:59 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 5b5c7583-1236-439b-9f19-b77ba4593dd9 0xc00405fb17 0xc00405fb18}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00405fb78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 27 06:17:59.041: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Nov 27 06:17:59.041: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-5580 /apis/apps/v1/namespaces/deployment-5580/replicasets/test-cleanup-controller 2bf974eb-d29c-4a67-88c8-c481f2cd0ffc 969188 1 2019-11-27 06:17:53 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 5b5c7583-1236-439b-9f19-b77ba4593dd9 0xc00405fa47 0xc00405fa48}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00405faa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 27 06:17:59.121: INFO: Pod "test-cleanup-controller-bglsh" is available:
&Pod{ObjectMeta:{test-cleanup-controller-bglsh test-cleanup-controller- deployment-5580 /api/v1/namespaces/deployment-5580/pods/test-cleanup-controller-bglsh 81c5adea-f19a-4b73-920e-3462e23b3d73 969183 0 2019-11-27 06:17:53 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 2bf974eb-d29c-4a67-88c8-c481f2cd0ffc 0xc0085d00c7 0xc0085d00c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2xnx5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2xnx5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2xnx5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 06:17:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 06:17:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 06:17:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-11-27 06:17:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.200.72.17,PodIP:172.31.51.209,StartTime:2019-11-27 06:17:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-11-27 06:17:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c2b0d33a386966885dd604f38d06db8c78942dbee8e1f5a8db17ad0d8a368da,ContainerID:docker://08d8a02adf2a26fbe3b93b64e39fb5f075d289313ea503b72599d8b46ffae72b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.31.51.209,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 27 06:17:59.122: INFO: Pod "test-cleanup-deployment-65db99849b-grgkb" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-65db99849b-grgkb test-cleanup-deployment-65db99849b- deployment-5580 /api/v1/namespaces/deployment-5580/pods/test-cleanup-deployment-65db99849b-grgkb a86d44a1-2b93-4c2d-8f69-7eda124018c2 969192 0 2019-11-27 06:17:59 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-65db99849b db35b722-51b9-4471-a5f1-c231a48b5252 0xc0085d0257 0xc0085d0258}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2xnx5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2xnx5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2xnx5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:17:59.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5580" for this suite.
Nov 27 06:18:05.197: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:18:05.438: INFO: namespace deployment-5580 deletion completed in 6.281388073s

• [SLOW TEST:11.823 seconds]
[sig-apps] Deployment
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:18:05.441: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6066
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov 27 06:18:05.783: INFO: Waiting up to 5m0s for pod "pod-943561d8-979a-4dd7-88a1-2704ea691733" in namespace "emptydir-6066" to be "success or failure"
Nov 27 06:18:05.790: INFO: Pod "pod-943561d8-979a-4dd7-88a1-2704ea691733": Phase="Pending", Reason="", readiness=false. Elapsed: 7.443365ms
Nov 27 06:18:07.800: INFO: Pod "pod-943561d8-979a-4dd7-88a1-2704ea691733": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016713399s
Nov 27 06:18:09.808: INFO: Pod "pod-943561d8-979a-4dd7-88a1-2704ea691733": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024827695s
STEP: Saw pod success
Nov 27 06:18:09.808: INFO: Pod "pod-943561d8-979a-4dd7-88a1-2704ea691733" satisfied condition "success or failure"
Nov 27 06:18:09.816: INFO: Trying to get logs from node slave1 pod pod-943561d8-979a-4dd7-88a1-2704ea691733 container test-container: <nil>
STEP: delete the pod
Nov 27 06:18:10.108: INFO: Waiting for pod pod-943561d8-979a-4dd7-88a1-2704ea691733 to disappear
Nov 27 06:18:10.119: INFO: Pod pod-943561d8-979a-4dd7-88a1-2704ea691733 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:18:10.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6066" for this suite.
Nov 27 06:18:16.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:18:16.412: INFO: namespace emptydir-6066 deletion completed in 6.275627959s

• [SLOW TEST:10.971 seconds]
[sig-storage] EmptyDir volumes
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:18:16.413: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4380
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Nov 27 06:18:16.753: INFO: Waiting up to 5m0s for pod "downward-api-4ef2b5f0-c646-406d-be7d-db28307140db" in namespace "downward-api-4380" to be "success or failure"
Nov 27 06:18:16.764: INFO: Pod "downward-api-4ef2b5f0-c646-406d-be7d-db28307140db": Phase="Pending", Reason="", readiness=false. Elapsed: 10.629957ms
Nov 27 06:18:18.773: INFO: Pod "downward-api-4ef2b5f0-c646-406d-be7d-db28307140db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019450211s
Nov 27 06:18:20.783: INFO: Pod "downward-api-4ef2b5f0-c646-406d-be7d-db28307140db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029237003s
STEP: Saw pod success
Nov 27 06:18:20.783: INFO: Pod "downward-api-4ef2b5f0-c646-406d-be7d-db28307140db" satisfied condition "success or failure"
Nov 27 06:18:20.790: INFO: Trying to get logs from node slave1 pod downward-api-4ef2b5f0-c646-406d-be7d-db28307140db container dapi-container: <nil>
STEP: delete the pod
Nov 27 06:18:20.848: INFO: Waiting for pod downward-api-4ef2b5f0-c646-406d-be7d-db28307140db to disappear
Nov 27 06:18:20.863: INFO: Pod downward-api-4ef2b5f0-c646-406d-be7d-db28307140db no longer exists
[AfterEach] [sig-node] Downward API
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:18:20.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4380" for this suite.
Nov 27 06:18:26.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:18:27.156: INFO: namespace downward-api-4380 deletion completed in 6.277709168s

• [SLOW TEST:10.744 seconds]
[sig-node] Downward API
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:18:27.157: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8360
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1127 06:18:58.074171      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov 27 06:18:58.074: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:18:58.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8360" for this suite.
Nov 27 06:19:06.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:19:06.407: INFO: namespace gc-8360 deletion completed in 8.317012264s

• [SLOW TEST:39.250 seconds]
[sig-api-machinery] Garbage collector
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:19:06.409: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5027
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 27 06:19:06.754: INFO: Waiting up to 5m0s for pod "downwardapi-volume-22c30bb8-9986-4f1e-9230-b69a3d51c192" in namespace "projected-5027" to be "success or failure"
Nov 27 06:19:06.761: INFO: Pod "downwardapi-volume-22c30bb8-9986-4f1e-9230-b69a3d51c192": Phase="Pending", Reason="", readiness=false. Elapsed: 7.295231ms
Nov 27 06:19:08.783: INFO: Pod "downwardapi-volume-22c30bb8-9986-4f1e-9230-b69a3d51c192": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028438471s
Nov 27 06:19:10.799: INFO: Pod "downwardapi-volume-22c30bb8-9986-4f1e-9230-b69a3d51c192": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045130359s
STEP: Saw pod success
Nov 27 06:19:10.799: INFO: Pod "downwardapi-volume-22c30bb8-9986-4f1e-9230-b69a3d51c192" satisfied condition "success or failure"
Nov 27 06:19:10.813: INFO: Trying to get logs from node slave1 pod downwardapi-volume-22c30bb8-9986-4f1e-9230-b69a3d51c192 container client-container: <nil>
STEP: delete the pod
Nov 27 06:19:10.873: INFO: Waiting for pod downwardapi-volume-22c30bb8-9986-4f1e-9230-b69a3d51c192 to disappear
Nov 27 06:19:10.880: INFO: Pod downwardapi-volume-22c30bb8-9986-4f1e-9230-b69a3d51c192 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:19:10.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5027" for this suite.
Nov 27 06:19:16.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:19:17.165: INFO: namespace projected-5027 deletion completed in 6.273569239s

• [SLOW TEST:10.757 seconds]
[sig-storage] Projected downwardAPI
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:19:17.166: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4587
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 06:19:17.481: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Nov 27 06:19:19.565: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:19:19.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4587" for this suite.
Nov 27 06:19:25.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:19:25.932: INFO: namespace replication-controller-4587 deletion completed in 6.323975854s

• [SLOW TEST:8.767 seconds]
[sig-apps] ReplicationController
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:19:25.937: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8213
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Nov 27 06:19:26.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 create -f - --namespace=kubectl-8213'
Nov 27 06:19:28.032: INFO: stderr: ""
Nov 27 06:19:28.039: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 27 06:19:28.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8213'
Nov 27 06:19:28.930: INFO: stderr: ""
Nov 27 06:19:28.931: INFO: stdout: "update-demo-nautilus-st5sc update-demo-nautilus-vmzv7 "
Nov 27 06:19:28.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods update-demo-nautilus-st5sc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8213'
Nov 27 06:19:29.757: INFO: stderr: ""
Nov 27 06:19:29.757: INFO: stdout: ""
Nov 27 06:19:29.758: INFO: update-demo-nautilus-st5sc is created but not running
Nov 27 06:19:34.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8213'
Nov 27 06:19:35.500: INFO: stderr: ""
Nov 27 06:19:35.500: INFO: stdout: "update-demo-nautilus-st5sc update-demo-nautilus-vmzv7 "
Nov 27 06:19:35.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods update-demo-nautilus-st5sc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8213'
Nov 27 06:19:36.236: INFO: stderr: ""
Nov 27 06:19:36.237: INFO: stdout: "true"
Nov 27 06:19:36.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods update-demo-nautilus-st5sc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8213'
Nov 27 06:19:36.954: INFO: stderr: ""
Nov 27 06:19:36.954: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 27 06:19:36.955: INFO: validating pod update-demo-nautilus-st5sc
Nov 27 06:19:36.972: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 27 06:19:36.973: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 27 06:19:36.973: INFO: update-demo-nautilus-st5sc is verified up and running
Nov 27 06:19:36.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods update-demo-nautilus-vmzv7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8213'
Nov 27 06:19:37.659: INFO: stderr: ""
Nov 27 06:19:37.660: INFO: stdout: "true"
Nov 27 06:19:37.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods update-demo-nautilus-vmzv7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8213'
Nov 27 06:19:38.404: INFO: stderr: ""
Nov 27 06:19:38.404: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 27 06:19:38.404: INFO: validating pod update-demo-nautilus-vmzv7
Nov 27 06:19:38.423: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 27 06:19:38.423: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 27 06:19:38.423: INFO: update-demo-nautilus-vmzv7 is verified up and running
STEP: scaling down the replication controller
Nov 27 06:19:38.434: INFO: scanned /root for discovery docs: <nil>
Nov 27 06:19:38.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-8213'
Nov 27 06:19:39.156: INFO: stderr: ""
Nov 27 06:19:39.157: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 27 06:19:39.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8213'
Nov 27 06:19:39.855: INFO: stderr: ""
Nov 27 06:19:39.855: INFO: stdout: "update-demo-nautilus-st5sc update-demo-nautilus-vmzv7 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Nov 27 06:19:44.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8213'
Nov 27 06:19:45.617: INFO: stderr: ""
Nov 27 06:19:45.618: INFO: stdout: "update-demo-nautilus-st5sc update-demo-nautilus-vmzv7 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Nov 27 06:19:50.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8213'
Nov 27 06:19:51.377: INFO: stderr: ""
Nov 27 06:19:51.377: INFO: stdout: "update-demo-nautilus-st5sc update-demo-nautilus-vmzv7 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Nov 27 06:19:56.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8213'
Nov 27 06:19:57.116: INFO: stderr: ""
Nov 27 06:19:57.117: INFO: stdout: "update-demo-nautilus-st5sc "
Nov 27 06:19:57.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods update-demo-nautilus-st5sc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8213'
Nov 27 06:19:57.907: INFO: stderr: ""
Nov 27 06:19:57.908: INFO: stdout: "true"
Nov 27 06:19:57.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods update-demo-nautilus-st5sc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8213'
Nov 27 06:19:58.695: INFO: stderr: ""
Nov 27 06:19:58.696: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 27 06:19:58.696: INFO: validating pod update-demo-nautilus-st5sc
Nov 27 06:19:58.711: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 27 06:19:58.711: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 27 06:19:58.711: INFO: update-demo-nautilus-st5sc is verified up and running
STEP: scaling up the replication controller
Nov 27 06:19:58.721: INFO: scanned /root for discovery docs: <nil>
Nov 27 06:19:58.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-8213'
Nov 27 06:20:00.504: INFO: stderr: ""
Nov 27 06:20:00.505: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 27 06:20:00.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8213'
Nov 27 06:20:01.291: INFO: stderr: ""
Nov 27 06:20:01.292: INFO: stdout: "update-demo-nautilus-d9229 update-demo-nautilus-st5sc "
Nov 27 06:20:01.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods update-demo-nautilus-d9229 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8213'
Nov 27 06:20:02.141: INFO: stderr: ""
Nov 27 06:20:02.141: INFO: stdout: ""
Nov 27 06:20:02.141: INFO: update-demo-nautilus-d9229 is created but not running
Nov 27 06:20:07.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8213'
Nov 27 06:20:07.820: INFO: stderr: ""
Nov 27 06:20:07.820: INFO: stdout: "update-demo-nautilus-d9229 update-demo-nautilus-st5sc "
Nov 27 06:20:07.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods update-demo-nautilus-d9229 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8213'
Nov 27 06:20:08.542: INFO: stderr: ""
Nov 27 06:20:08.543: INFO: stdout: "true"
Nov 27 06:20:08.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods update-demo-nautilus-d9229 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8213'
Nov 27 06:20:09.298: INFO: stderr: ""
Nov 27 06:20:09.298: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 27 06:20:09.298: INFO: validating pod update-demo-nautilus-d9229
Nov 27 06:20:09.316: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 27 06:20:09.316: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 27 06:20:09.316: INFO: update-demo-nautilus-d9229 is verified up and running
Nov 27 06:20:09.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods update-demo-nautilus-st5sc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8213'
Nov 27 06:20:09.966: INFO: stderr: ""
Nov 27 06:20:09.966: INFO: stdout: "true"
Nov 27 06:20:09.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods update-demo-nautilus-st5sc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8213'
Nov 27 06:20:10.742: INFO: stderr: ""
Nov 27 06:20:10.742: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 27 06:20:10.743: INFO: validating pod update-demo-nautilus-st5sc
Nov 27 06:20:10.757: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 27 06:20:10.757: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 27 06:20:10.757: INFO: update-demo-nautilus-st5sc is verified up and running
STEP: using delete to clean up resources
Nov 27 06:20:10.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 delete --grace-period=0 --force -f - --namespace=kubectl-8213'
Nov 27 06:20:11.519: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 27 06:20:11.520: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 27 06:20:11.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8213'
Nov 27 06:20:12.374: INFO: stderr: "No resources found in kubectl-8213 namespace.\n"
Nov 27 06:20:12.374: INFO: stdout: ""
Nov 27 06:20:12.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pods -l name=update-demo --namespace=kubectl-8213 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 27 06:20:13.249: INFO: stderr: ""
Nov 27 06:20:13.249: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:20:13.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8213" for this suite.
Nov 27 06:20:25.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:20:25.561: INFO: namespace kubectl-8213 deletion completed in 12.291658233s

• [SLOW TEST:59.625 seconds]
[sig-cli] Kubectl client
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:20:25.563: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1965
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 06:20:25.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 create -f - --namespace=kubectl-1965'
Nov 27 06:20:26.970: INFO: stderr: ""
Nov 27 06:20:26.970: INFO: stdout: "replicationcontroller/redis-master created\n"
Nov 27 06:20:26.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 create -f - --namespace=kubectl-1965'
Nov 27 06:20:28.151: INFO: stderr: ""
Nov 27 06:20:28.151: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Nov 27 06:20:29.170: INFO: Selector matched 1 pods for map[app:redis]
Nov 27 06:20:29.170: INFO: Found 0 / 1
Nov 27 06:20:30.162: INFO: Selector matched 1 pods for map[app:redis]
Nov 27 06:20:30.163: INFO: Found 0 / 1
Nov 27 06:20:31.164: INFO: Selector matched 1 pods for map[app:redis]
Nov 27 06:20:31.164: INFO: Found 0 / 1
Nov 27 06:20:32.164: INFO: Selector matched 1 pods for map[app:redis]
Nov 27 06:20:32.164: INFO: Found 1 / 1
Nov 27 06:20:32.164: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 27 06:20:32.173: INFO: Selector matched 1 pods for map[app:redis]
Nov 27 06:20:32.173: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 27 06:20:32.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 describe pod redis-master-8sfxd --namespace=kubectl-1965'
Nov 27 06:20:32.926: INFO: stderr: ""
Nov 27 06:20:32.927: INFO: stdout: "Name:         redis-master-8sfxd\nNamespace:    kubectl-1965\nPriority:     0\nNode:         slave1/10.200.72.17\nStart Time:   Wed, 27 Nov 2019 06:20:27 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nStatus:       Running\nIP:           172.31.51.220\nIPs:\n  IP:           172.31.51.220\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://3a229ee5e3ffc4df35ef22b586f4039dff2f74a1c0ec206213f73329b20424b7\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker://sha256:0819d392499b83e17f70aecae0173f2bd403a637c12598d5e0cb23a04f7490f3\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 27 Nov 2019 06:20:29 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-czg8z (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-czg8z:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-czg8z\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From               Message\n  ----    ------     ----       ----               -------\n  Normal  Scheduled  <unknown>  default-scheduler  Successfully assigned kubectl-1965/redis-master-8sfxd to slave1\n  Normal  Pulled     3s         kubelet, slave1    Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    3s         kubelet, slave1    Created container redis-master\n  Normal  Started    2s         kubelet, slave1    Started container redis-master\n"
Nov 27 06:20:32.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 describe rc redis-master --namespace=kubectl-1965'
Nov 27 06:20:33.773: INFO: stderr: ""
Nov 27 06:20:33.774: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-1965\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  7s    replication-controller  Created pod: redis-master-8sfxd\n"
Nov 27 06:20:33.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 describe service redis-master --namespace=kubectl-1965'
Nov 27 06:20:34.521: INFO: stderr: ""
Nov 27 06:20:34.521: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-1965\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.30.56.84\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.31.51.220:6379\nSession Affinity:  None\nEvents:            <none>\n"
Nov 27 06:20:34.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 describe node master1'
Nov 27 06:20:35.405: INFO: stderr: ""
Nov 27 06:20:35.406: INFO: stdout: "Name:               master1\nRoles:              master,monitor,node\nLabels:             beta.kubernetes.io/arch=mips64le\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=mips64le\n                    kubernetes.io/hostname=master1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=true\n                    node-role.kubernetes.io/monitor=true\n                    node-role.kubernetes.io/node=true\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 05 Nov 2019 07:37:47 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Wed, 27 Nov 2019 06:20:26 +0000   Mon, 25 Nov 2019 05:02:57 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 27 Nov 2019 06:20:26 +0000   Mon, 25 Nov 2019 05:02:57 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Wed, 27 Nov 2019 06:20:26 +0000   Mon, 25 Nov 2019 05:02:57 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Wed, 27 Nov 2019 06:20:26 +0000   Mon, 25 Nov 2019 05:03:07 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.200.72.1\n  Hostname:    master1\nCapacity:\n cpu:                8\n ephemeral-storage:  212485676Ki\n hugepages-32Mi:     0\n memory:             32749152Ki\n pods:               60\nAllocatable:\n cpu:                7\n ephemeral-storage:  201999916Ki\n hugepages-32Mi:     0\n memory:             31224864Ki\n pods:               60\nSystem Info:\n Machine ID:                 14b02505658d4b43811f0eeb870f8fde\n System UUID:                14b02505658d4b43811f0eeb870f8fde\n Boot ID:                    6465c130-8673-4b73-b52e-97e82cc9d90e\n Kernel Version:             3.10.0-862.9.1.ns7_4.37.mips64el\n OS Image:                   NeoKylin Linux Server 7.0 (loongson)\n Operating System:           linux\n Architecture:               mips64le\n Container Runtime Version:  docker://18.9.8\n Kubelet Version:            v1.16.2\n Kube-Proxy Version:         v1.16.2\nNon-terminated Pods:         (9 in total)\n  Namespace                  Name                               CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                               ------------  ----------  ---------------  -------------  ---\n  conformance                e2e-conformance-test               0 (0%)        0 (0%)      0 (0%)           0 (0%)         136m\n  kube-system                calico-node-cppjk                  150m (2%)     1 (14%)     64Mi (0%)        4Gi (13%)      6d22h\n  kube-system                coredns-lxl9z                      100m (1%)     0 (0%)      70Mi (0%)        170Mi (0%)     13d\n  kube-system                kube-apiserver-master1             100m (1%)     2 (28%)     256Mi (0%)       6Gi (20%)      21d\n  kube-system                kube-controller-manager-master1    100m (1%)     2 (28%)     100Mi (0%)       6Gi (20%)      21d\n  kube-system                kube-proxy-master1                 150m (2%)     500m (7%)   64M (0%)         2G (6%)        14d\n  kube-system                kube-scheduler-master1             80m (1%)      2 (28%)     170Mi (0%)       6Gi (20%)      21d\n  kube-system                metrics-server-687c949fd7-pf9hm    0 (0%)        0 (0%)      0 (0%)           0 (0%)         5d5h\n  kube-system                resource-reserver-master1          800m (11%)    800m (11%)  512Mi (1%)       512Mi (1%)     21d\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                1480m (21%)     8300m (118%)\n  memory             1262628Ki (4%)  25720165Ki (82%)\n  ephemeral-storage  0 (0%)          0 (0%)\nEvents:              <none>\n"
Nov 27 06:20:35.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 describe namespace kubectl-1965'
Nov 27 06:20:36.168: INFO: stderr: ""
Nov 27 06:20:36.169: INFO: stdout: "Name:         kubectl-1965\nLabels:       e2e-framework=kubectl\n              e2e-run=9e3dca65-22db-46ff-bf22-fcae64da58bd\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:20:36.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1965" for this suite.
Nov 27 06:21:04.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:21:04.474: INFO: namespace kubectl-1965 deletion completed in 28.287293815s

• [SLOW TEST:38.911 seconds]
[sig-cli] Kubectl client
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:21:04.476: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1099
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-bc9eb992-586f-4c7e-8640-c439c8010247
STEP: Creating a pod to test consume secrets
Nov 27 06:21:04.839: INFO: Waiting up to 5m0s for pod "pod-secrets-afa7424a-8b1f-4a50-b037-7451f2bee379" in namespace "secrets-1099" to be "success or failure"
Nov 27 06:21:04.871: INFO: Pod "pod-secrets-afa7424a-8b1f-4a50-b037-7451f2bee379": Phase="Pending", Reason="", readiness=false. Elapsed: 32.328893ms
Nov 27 06:21:06.882: INFO: Pod "pod-secrets-afa7424a-8b1f-4a50-b037-7451f2bee379": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043512935s
Nov 27 06:21:08.891: INFO: Pod "pod-secrets-afa7424a-8b1f-4a50-b037-7451f2bee379": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052567857s
STEP: Saw pod success
Nov 27 06:21:08.891: INFO: Pod "pod-secrets-afa7424a-8b1f-4a50-b037-7451f2bee379" satisfied condition "success or failure"
Nov 27 06:21:08.904: INFO: Trying to get logs from node slave1 pod pod-secrets-afa7424a-8b1f-4a50-b037-7451f2bee379 container secret-volume-test: <nil>
STEP: delete the pod
Nov 27 06:21:09.210: INFO: Waiting for pod pod-secrets-afa7424a-8b1f-4a50-b037-7451f2bee379 to disappear
Nov 27 06:21:09.217: INFO: Pod pod-secrets-afa7424a-8b1f-4a50-b037-7451f2bee379 no longer exists
[AfterEach] [sig-storage] Secrets
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:21:09.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1099" for this suite.
Nov 27 06:21:15.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:21:15.508: INFO: namespace secrets-1099 deletion completed in 6.276144717s

• [SLOW TEST:11.032 seconds]
[sig-storage] Secrets
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:21:15.510: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5096
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Nov 27 06:21:15.864: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5096 /api/v1/namespaces/watch-5096/configmaps/e2e-watch-test-watch-closed b7343172-ceec-4925-b9f5-e25d365db095 970024 0 2019-11-27 06:21:15 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov 27 06:21:15.864: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5096 /api/v1/namespaces/watch-5096/configmaps/e2e-watch-test-watch-closed b7343172-ceec-4925-b9f5-e25d365db095 970025 0 2019-11-27 06:21:15 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Nov 27 06:21:15.902: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5096 /api/v1/namespaces/watch-5096/configmaps/e2e-watch-test-watch-closed b7343172-ceec-4925-b9f5-e25d365db095 970026 0 2019-11-27 06:21:15 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov 27 06:21:15.902: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5096 /api/v1/namespaces/watch-5096/configmaps/e2e-watch-test-watch-closed b7343172-ceec-4925-b9f5-e25d365db095 970027 0 2019-11-27 06:21:15 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:21:15.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5096" for this suite.
Nov 27 06:21:21.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:21:22.198: INFO: namespace watch-5096 deletion completed in 6.282024608s

• [SLOW TEST:6.688 seconds]
[sig-api-machinery] Watchers
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:21:22.199: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9519
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:21:28.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9519" for this suite.
Nov 27 06:22:14.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:22:14.922: INFO: namespace kubelet-test-9519 deletion completed in 46.307696298s

• [SLOW TEST:52.723 seconds]
[k8s.io] Kubelet
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:22:14.923: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-7460
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-20
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1311
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:22:21.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7460" for this suite.
Nov 27 06:22:27.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:22:28.244: INFO: namespace namespaces-7460 deletion completed in 6.325243549s
STEP: Destroying namespace "nsdeletetest-20" for this suite.
Nov 27 06:22:28.251: INFO: Namespace nsdeletetest-20 was already deleted
STEP: Destroying namespace "nsdeletetest-1311" for this suite.
Nov 27 06:22:34.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:22:34.537: INFO: namespace nsdeletetest-1311 deletion completed in 6.285197022s

• [SLOW TEST:19.614 seconds]
[sig-api-machinery] Namespaces [Serial]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:22:34.537: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4927
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-4ea11eb6-f84b-482d-80c2-9c4d98963a5e
STEP: Creating a pod to test consume secrets
Nov 27 06:22:34.873: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-37fbc091-65dc-4c96-a539-de06f27f3dce" in namespace "projected-4927" to be "success or failure"
Nov 27 06:22:34.883: INFO: Pod "pod-projected-secrets-37fbc091-65dc-4c96-a539-de06f27f3dce": Phase="Pending", Reason="", readiness=false. Elapsed: 9.058661ms
Nov 27 06:22:36.895: INFO: Pod "pod-projected-secrets-37fbc091-65dc-4c96-a539-de06f27f3dce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021115862s
Nov 27 06:22:38.903: INFO: Pod "pod-projected-secrets-37fbc091-65dc-4c96-a539-de06f27f3dce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02954647s
STEP: Saw pod success
Nov 27 06:22:38.903: INFO: Pod "pod-projected-secrets-37fbc091-65dc-4c96-a539-de06f27f3dce" satisfied condition "success or failure"
Nov 27 06:22:38.911: INFO: Trying to get logs from node slave1 pod pod-projected-secrets-37fbc091-65dc-4c96-a539-de06f27f3dce container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 27 06:22:38.984: INFO: Waiting for pod pod-projected-secrets-37fbc091-65dc-4c96-a539-de06f27f3dce to disappear
Nov 27 06:22:38.992: INFO: Pod pod-projected-secrets-37fbc091-65dc-4c96-a539-de06f27f3dce no longer exists
[AfterEach] [sig-storage] Projected secret
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:22:38.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4927" for this suite.
Nov 27 06:22:45.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:22:45.350: INFO: namespace projected-4927 deletion completed in 6.341575441s

• [SLOW TEST:10.813 seconds]
[sig-storage] Projected secret
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:22:45.351: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2596
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 27 06:23:11.174: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 27 06:23:13.203: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710432591, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710432591, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710432591, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710432591, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 06:23:15.213: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710432591, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710432591, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710432591, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710432591, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 27 06:23:18.243: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
Nov 27 06:23:28.313: INFO: Waiting for webhook configuration to be ready...
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:23:41.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2596" for this suite.
Nov 27 06:23:47.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:23:47.399: INFO: namespace webhook-2596 deletion completed in 6.353735626s
STEP: Destroying namespace "webhook-2596-markers" for this suite.
Nov 27 06:23:53.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:23:53.665: INFO: namespace webhook-2596-markers deletion completed in 6.266119029s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:68.351 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:23:53.707: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4534
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-4534
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4534 to expose endpoints map[]
Nov 27 06:23:54.068: INFO: Get endpoints failed (12.913566ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Nov 27 06:23:55.077: INFO: successfully validated that service endpoint-test2 in namespace services-4534 exposes endpoints map[] (1.022030002s elapsed)
STEP: Creating pod pod1 in namespace services-4534
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4534 to expose endpoints map[pod1:[80]]
Nov 27 06:23:59.201: INFO: successfully validated that service endpoint-test2 in namespace services-4534 exposes endpoints map[pod1:[80]] (4.099456413s elapsed)
STEP: Creating pod pod2 in namespace services-4534
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4534 to expose endpoints map[pod1:[80] pod2:[80]]
Nov 27 06:24:03.353: INFO: Unexpected endpoints: found map[3d5452d7-e9fa-45a8-a7f3-abd8d4b4d3dd:[80]], expected map[pod1:[80] pod2:[80]] (4.136979285s elapsed, will retry)
Nov 27 06:24:04.375: INFO: successfully validated that service endpoint-test2 in namespace services-4534 exposes endpoints map[pod1:[80] pod2:[80]] (5.158736975s elapsed)
STEP: Deleting pod pod1 in namespace services-4534
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4534 to expose endpoints map[pod2:[80]]
Nov 27 06:24:05.428: INFO: successfully validated that service endpoint-test2 in namespace services-4534 exposes endpoints map[pod2:[80]] (1.03823185s elapsed)
STEP: Deleting pod pod2 in namespace services-4534
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4534 to expose endpoints map[]
Nov 27 06:24:06.468: INFO: successfully validated that service endpoint-test2 in namespace services-4534 exposes endpoints map[] (1.023399386s elapsed)
[AfterEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:24:06.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4534" for this suite.
Nov 27 06:24:18.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:24:18.832: INFO: namespace services-4534 deletion completed in 12.302279879s
[AfterEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:25.125 seconds]
[sig-network] Services
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:24:18.833: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2628
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov 27 06:24:19.146: INFO: Waiting up to 5m0s for pod "pod-d107f249-4fd6-436a-ae69-67bd2bdeb61d" in namespace "emptydir-2628" to be "success or failure"
Nov 27 06:24:19.154: INFO: Pod "pod-d107f249-4fd6-436a-ae69-67bd2bdeb61d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.556298ms
Nov 27 06:24:21.164: INFO: Pod "pod-d107f249-4fd6-436a-ae69-67bd2bdeb61d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017658425s
Nov 27 06:24:23.180: INFO: Pod "pod-d107f249-4fd6-436a-ae69-67bd2bdeb61d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033892222s
Nov 27 06:24:25.189: INFO: Pod "pod-d107f249-4fd6-436a-ae69-67bd2bdeb61d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042648432s
STEP: Saw pod success
Nov 27 06:24:25.189: INFO: Pod "pod-d107f249-4fd6-436a-ae69-67bd2bdeb61d" satisfied condition "success or failure"
Nov 27 06:24:25.196: INFO: Trying to get logs from node slave1 pod pod-d107f249-4fd6-436a-ae69-67bd2bdeb61d container test-container: <nil>
STEP: delete the pod
Nov 27 06:24:25.457: INFO: Waiting for pod pod-d107f249-4fd6-436a-ae69-67bd2bdeb61d to disappear
Nov 27 06:24:25.464: INFO: Pod pod-d107f249-4fd6-436a-ae69-67bd2bdeb61d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:24:25.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2628" for this suite.
Nov 27 06:24:31.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:24:31.740: INFO: namespace emptydir-2628 deletion completed in 6.264311599s

• [SLOW TEST:12.907 seconds]
[sig-storage] EmptyDir volumes
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:24:31.741: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7339
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7339.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7339.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7339.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7339.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 27 06:24:36.138: INFO: DNS probes using dns-test-51bb610f-9659-446b-b4ee-58f2589f2cca succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7339.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7339.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7339.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7339.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 27 06:24:42.277: INFO: DNS probes using dns-test-0118bc97-1d38-4208-9132-44c5eebec843 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7339.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7339.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7339.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7339.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 27 06:24:48.430: INFO: DNS probes using dns-test-a736d42c-295b-4224-bcac-70e665ffdf86 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:24:48.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7339" for this suite.
Nov 27 06:24:56.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:24:56.751: INFO: namespace dns-7339 deletion completed in 8.241073806s

• [SLOW TEST:25.010 seconds]
[sig-network] DNS
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:24:56.752: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9872
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 27 06:24:57.098: INFO: Waiting up to 5m0s for pod "downwardapi-volume-22fd358b-4e46-4b80-939b-e85cfdd3faeb" in namespace "downward-api-9872" to be "success or failure"
Nov 27 06:24:57.107: INFO: Pod "downwardapi-volume-22fd358b-4e46-4b80-939b-e85cfdd3faeb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.127812ms
Nov 27 06:24:59.116: INFO: Pod "downwardapi-volume-22fd358b-4e46-4b80-939b-e85cfdd3faeb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017286379s
Nov 27 06:25:01.128: INFO: Pod "downwardapi-volume-22fd358b-4e46-4b80-939b-e85cfdd3faeb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029979628s
Nov 27 06:25:03.138: INFO: Pod "downwardapi-volume-22fd358b-4e46-4b80-939b-e85cfdd3faeb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039961487s
STEP: Saw pod success
Nov 27 06:25:03.139: INFO: Pod "downwardapi-volume-22fd358b-4e46-4b80-939b-e85cfdd3faeb" satisfied condition "success or failure"
Nov 27 06:25:03.149: INFO: Trying to get logs from node slave1 pod downwardapi-volume-22fd358b-4e46-4b80-939b-e85cfdd3faeb container client-container: <nil>
STEP: delete the pod
Nov 27 06:25:03.207: INFO: Waiting for pod downwardapi-volume-22fd358b-4e46-4b80-939b-e85cfdd3faeb to disappear
Nov 27 06:25:03.220: INFO: Pod downwardapi-volume-22fd358b-4e46-4b80-939b-e85cfdd3faeb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:25:03.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9872" for this suite.
Nov 27 06:25:09.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:25:09.537: INFO: namespace downward-api-9872 deletion completed in 6.304420704s

• [SLOW TEST:12.785 seconds]
[sig-storage] Downward API volume
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:25:09.539: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8650
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1127 06:25:19.929797      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov 27 06:25:19.929: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:25:19.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8650" for this suite.
Nov 27 06:25:25.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:25:26.245: INFO: namespace gc-8650 deletion completed in 6.30281323s

• [SLOW TEST:16.706 seconds]
[sig-api-machinery] Garbage collector
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:25:26.246: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1404
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov 27 06:25:26.574: INFO: Waiting up to 5m0s for pod "pod-84cba911-e49f-4eaa-b6ff-a2a45c907233" in namespace "emptydir-1404" to be "success or failure"
Nov 27 06:25:26.581: INFO: Pod "pod-84cba911-e49f-4eaa-b6ff-a2a45c907233": Phase="Pending", Reason="", readiness=false. Elapsed: 6.995052ms
Nov 27 06:25:28.590: INFO: Pod "pod-84cba911-e49f-4eaa-b6ff-a2a45c907233": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016577976s
Nov 27 06:25:30.602: INFO: Pod "pod-84cba911-e49f-4eaa-b6ff-a2a45c907233": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02855011s
Nov 27 06:25:32.614: INFO: Pod "pod-84cba911-e49f-4eaa-b6ff-a2a45c907233": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03993362s
STEP: Saw pod success
Nov 27 06:25:32.614: INFO: Pod "pod-84cba911-e49f-4eaa-b6ff-a2a45c907233" satisfied condition "success or failure"
Nov 27 06:25:32.623: INFO: Trying to get logs from node slave1 pod pod-84cba911-e49f-4eaa-b6ff-a2a45c907233 container test-container: <nil>
STEP: delete the pod
Nov 27 06:25:32.694: INFO: Waiting for pod pod-84cba911-e49f-4eaa-b6ff-a2a45c907233 to disappear
Nov 27 06:25:32.702: INFO: Pod pod-84cba911-e49f-4eaa-b6ff-a2a45c907233 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:25:32.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1404" for this suite.
Nov 27 06:25:38.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:25:38.992: INFO: namespace emptydir-1404 deletion completed in 6.276973387s

• [SLOW TEST:12.746 seconds]
[sig-storage] EmptyDir volumes
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:25:38.993: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8451
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1127 06:26:19.388698      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov 27 06:26:19.388: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:26:19.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8451" for this suite.
Nov 27 06:26:27.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:26:27.781: INFO: namespace gc-8451 deletion completed in 8.381210982s

• [SLOW TEST:48.788 seconds]
[sig-api-machinery] Garbage collector
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:26:27.782: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7795
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Nov 27 06:26:28.133: INFO: Waiting up to 5m0s for pod "var-expansion-6aefd707-f0a6-4afe-abf3-1e17d3c33eca" in namespace "var-expansion-7795" to be "success or failure"
Nov 27 06:26:28.144: INFO: Pod "var-expansion-6aefd707-f0a6-4afe-abf3-1e17d3c33eca": Phase="Pending", Reason="", readiness=false. Elapsed: 11.204404ms
Nov 27 06:26:30.155: INFO: Pod "var-expansion-6aefd707-f0a6-4afe-abf3-1e17d3c33eca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02241289s
Nov 27 06:26:32.166: INFO: Pod "var-expansion-6aefd707-f0a6-4afe-abf3-1e17d3c33eca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033022975s
Nov 27 06:26:34.175: INFO: Pod "var-expansion-6aefd707-f0a6-4afe-abf3-1e17d3c33eca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042397631s
STEP: Saw pod success
Nov 27 06:26:34.176: INFO: Pod "var-expansion-6aefd707-f0a6-4afe-abf3-1e17d3c33eca" satisfied condition "success or failure"
Nov 27 06:26:34.183: INFO: Trying to get logs from node slave1 pod var-expansion-6aefd707-f0a6-4afe-abf3-1e17d3c33eca container dapi-container: <nil>
STEP: delete the pod
Nov 27 06:26:34.242: INFO: Waiting for pod var-expansion-6aefd707-f0a6-4afe-abf3-1e17d3c33eca to disappear
Nov 27 06:26:34.254: INFO: Pod var-expansion-6aefd707-f0a6-4afe-abf3-1e17d3c33eca no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:26:34.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7795" for this suite.
Nov 27 06:26:40.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:26:40.551: INFO: namespace var-expansion-7795 deletion completed in 6.285610623s

• [SLOW TEST:12.769 seconds]
[k8s.io] Variable Expansion
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:26:40.552: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1081
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 27 06:27:23.202: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Nov 27 06:27:25.230: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710432843, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710432843, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710432843, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710432843, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 06:27:27.239: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710432843, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710432843, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710432843, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710432843, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 27 06:27:30.267: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:27:30.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1081" for this suite.
Nov 27 06:27:36.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:27:36.888: INFO: namespace webhook-1081 deletion completed in 6.302618651s
STEP: Destroying namespace "webhook-1081-markers" for this suite.
Nov 27 06:27:42.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:27:43.204: INFO: namespace webhook-1081-markers deletion completed in 6.316164843s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:62.711 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:27:43.263: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4593
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:27:43.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4593" for this suite.
Nov 27 06:27:49.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:27:49.955: INFO: namespace kubelet-test-4593 deletion completed in 6.285829158s

• [SLOW TEST:6.692 seconds]
[k8s.io] Kubelet
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:27:49.957: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7883
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 06:27:50.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 version'
Nov 27 06:27:51.014: INFO: stderr: ""
Nov 27 06:27:51.014: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.2\", GitCommit:\"c97fe5036ef3df2967d086711e6c0c405941e14b\", GitTreeState:\"archive\", BuildDate:\"2019-10-31T07:30:40Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/mips64le\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.2\", GitCommit:\"c97fe5036ef3df2967d086711e6c0c405941e14b\", GitTreeState:\"archive\", BuildDate:\"2019-11-06T03:00:40Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/mips64le\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:27:51.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7883" for this suite.
Nov 27 06:27:57.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:27:57.331: INFO: namespace kubectl-7883 deletion completed in 6.298787214s

• [SLOW TEST:7.374 seconds]
[sig-cli] Kubectl client
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:27:57.335: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6166
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-x4x9
STEP: Creating a pod to test atomic-volume-subpath
Nov 27 06:27:57.700: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-x4x9" in namespace "subpath-6166" to be "success or failure"
Nov 27 06:27:57.712: INFO: Pod "pod-subpath-test-secret-x4x9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.450587ms
Nov 27 06:27:59.735: INFO: Pod "pod-subpath-test-secret-x4x9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035077122s
Nov 27 06:28:01.746: INFO: Pod "pod-subpath-test-secret-x4x9": Phase="Running", Reason="", readiness=true. Elapsed: 4.045955607s
Nov 27 06:28:03.756: INFO: Pod "pod-subpath-test-secret-x4x9": Phase="Running", Reason="", readiness=true. Elapsed: 6.0558216s
Nov 27 06:28:05.765: INFO: Pod "pod-subpath-test-secret-x4x9": Phase="Running", Reason="", readiness=true. Elapsed: 8.065687103s
Nov 27 06:28:07.778: INFO: Pod "pod-subpath-test-secret-x4x9": Phase="Running", Reason="", readiness=true. Elapsed: 10.07780786s
Nov 27 06:28:09.788: INFO: Pod "pod-subpath-test-secret-x4x9": Phase="Running", Reason="", readiness=true. Elapsed: 12.088026832s
Nov 27 06:28:11.800: INFO: Pod "pod-subpath-test-secret-x4x9": Phase="Running", Reason="", readiness=true. Elapsed: 14.099817381s
Nov 27 06:28:13.809: INFO: Pod "pod-subpath-test-secret-x4x9": Phase="Running", Reason="", readiness=true. Elapsed: 16.109713306s
Nov 27 06:28:15.818: INFO: Pod "pod-subpath-test-secret-x4x9": Phase="Running", Reason="", readiness=true. Elapsed: 18.118225679s
Nov 27 06:28:17.833: INFO: Pod "pod-subpath-test-secret-x4x9": Phase="Running", Reason="", readiness=true. Elapsed: 20.132762615s
Nov 27 06:28:19.846: INFO: Pod "pod-subpath-test-secret-x4x9": Phase="Running", Reason="", readiness=true. Elapsed: 22.145828976s
Nov 27 06:28:21.855: INFO: Pod "pod-subpath-test-secret-x4x9": Phase="Running", Reason="", readiness=true. Elapsed: 24.155379746s
Nov 27 06:28:23.864: INFO: Pod "pod-subpath-test-secret-x4x9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.164384918s
STEP: Saw pod success
Nov 27 06:28:23.864: INFO: Pod "pod-subpath-test-secret-x4x9" satisfied condition "success or failure"
Nov 27 06:28:23.871: INFO: Trying to get logs from node slave1 pod pod-subpath-test-secret-x4x9 container test-container-subpath-secret-x4x9: <nil>
STEP: delete the pod
Nov 27 06:28:24.128: INFO: Waiting for pod pod-subpath-test-secret-x4x9 to disappear
Nov 27 06:28:24.134: INFO: Pod pod-subpath-test-secret-x4x9 no longer exists
STEP: Deleting pod pod-subpath-test-secret-x4x9
Nov 27 06:28:24.134: INFO: Deleting pod "pod-subpath-test-secret-x4x9" in namespace "subpath-6166"
[AfterEach] [sig-storage] Subpath
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:28:24.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6166" for this suite.
Nov 27 06:28:30.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:28:30.429: INFO: namespace subpath-6166 deletion completed in 6.275544983s

• [SLOW TEST:33.095 seconds]
[sig-storage] Subpath
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:28:30.433: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3769
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Nov 27 06:28:36.852: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:28:36.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1127 06:28:36.852606      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-3769" for this suite.
Nov 27 06:28:42.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:28:43.190: INFO: namespace gc-3769 deletion completed in 6.322402421s

• [SLOW TEST:12.758 seconds]
[sig-api-machinery] Garbage collector
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:28:43.191: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9353
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Nov 27 06:28:50.131: INFO: Successfully updated pod "labelsupdate5f457c95-d1ed-496a-9880-5ea42c9b5731"
[AfterEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:28:52.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9353" for this suite.
Nov 27 06:29:06.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:29:06.488: INFO: namespace projected-9353 deletion completed in 14.250698663s

• [SLOW TEST:23.298 seconds]
[sig-storage] Projected downwardAPI
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:29:06.490: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-207
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Nov 27 06:29:06.801: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
Nov 27 06:29:20.852: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:30:17.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-207" for this suite.
Nov 27 06:30:23.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:30:24.202: INFO: namespace crd-publish-openapi-207 deletion completed in 6.31485644s

• [SLOW TEST:77.712 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:30:24.203: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8951
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-8951
[It] Should recreate evicted statefulset [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-8951
STEP: Creating statefulset with conflicting port in namespace statefulset-8951
STEP: Waiting until pod test-pod will start running in namespace statefulset-8951
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8951
Nov 27 06:30:30.598: INFO: Observed stateful pod in namespace: statefulset-8951, name: ss-0, uid: b5482117-b16b-454d-8409-925e9e7dd649, status phase: Failed. Waiting for statefulset controller to delete.
Nov 27 06:30:30.603: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8951
STEP: Removing pod with conflicting port in namespace statefulset-8951
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8951 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Nov 27 06:30:36.713: INFO: Deleting all statefulset in ns statefulset-8951
Nov 27 06:30:36.723: INFO: Scaling statefulset ss to 0
Nov 27 06:30:56.768: INFO: Waiting for statefulset status.replicas updated to 0
Nov 27 06:30:56.779: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:30:56.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8951" for this suite.
Nov 27 06:31:02.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:31:03.111: INFO: namespace statefulset-8951 deletion completed in 6.280246126s

• [SLOW TEST:38.908 seconds]
[sig-apps] StatefulSet
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:31:03.112: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-7033
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Nov 27 06:31:03.424: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Nov 27 06:31:16.033: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Nov 27 06:31:18.207: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433076, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433076, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433076, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433076, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
I1127 06:31:22.220885      19 request.go:538] Throttling request took 76.305098ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:22.421606      19 request.go:538] Throttling request took 77.123497ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:22.620903      19 request.go:538] Throttling request took 74.161815ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:22.820790      19 request.go:538] Throttling request took 76.454698ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:23.020813      19 request.go:538] Throttling request took 176.653204ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:23.222294      19 request.go:538] Throttling request took 178.050934ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:23.420883      19 request.go:538] Throttling request took 76.517276ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:24.023277      19 request.go:538] Throttling request took 78.956337ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:24.220791      19 request.go:538] Throttling request took 76.483543ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:24.820848      19 request.go:538] Throttling request took 75.545368ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:25.020712      19 request.go:538] Throttling request took 76.600387ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:25.220888      19 request.go:538] Throttling request took 176.805248ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:25.420837      19 request.go:538] Throttling request took 73.279861ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:25.620780      19 request.go:538] Throttling request took 176.713826ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:25.833571      19 request.go:538] Throttling request took 183.376876ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:26.020893      19 request.go:538] Throttling request took 76.555898ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:26.220917      19 request.go:538] Throttling request took 76.32221ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:26.420914      19 request.go:538] Throttling request took 176.763693ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:26.620826      19 request.go:538] Throttling request took 175.565918ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:26.820773      19 request.go:538] Throttling request took 175.898362ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:27.020781      19 request.go:538] Throttling request took 176.612137ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:27.220884      19 request.go:538] Throttling request took 76.524121ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:27.421540      19 request.go:538] Throttling request took 177.431025ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:27.620860      19 request.go:538] Throttling request took 76.520209ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:27.820905      19 request.go:538] Throttling request took 176.488938ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:28.021021      19 request.go:538] Throttling request took 176.801337ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:28.222416      19 request.go:538] Throttling request took 77.779584ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:28.420876      19 request.go:538] Throttling request took 76.508476ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:28.620793      19 request.go:538] Throttling request took 175.646584ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:28.820793      19 request.go:538] Throttling request took 176.097338ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:29.020842      19 request.go:538] Throttling request took 76.350565ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:29.220860      19 request.go:538] Throttling request took 76.328076ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:29.420943      19 request.go:538] Throttling request took 176.575471ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:29.620937      19 request.go:538] Throttling request took 176.613115ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:29.820831      19 request.go:538] Throttling request took 176.542715ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:30.020802      19 request.go:538] Throttling request took 176.618493ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:30.220793      19 request.go:538] Throttling request took 176.58476ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:30.420866      19 request.go:538] Throttling request took 176.586716ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:30.621037      19 request.go:538] Throttling request took 176.579871ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:30.820850      19 request.go:538] Throttling request took 176.284094ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:31.020928      19 request.go:538] Throttling request took 76.377454ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:31.221091      19 request.go:538] Throttling request took 176.665427ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:31.421998      19 request.go:538] Throttling request took 77.635851ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:31.620766      19 request.go:538] Throttling request took 76.497231ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:31.820823      19 request.go:538] Throttling request took 76.576431ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:32.023131      19 request.go:538] Throttling request took 178.949999ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:32.220740      19 request.go:538] Throttling request took 76.35301ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:32.420813      19 request.go:538] Throttling request took 176.662493ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:32.620785      19 request.go:538] Throttling request took 176.723604ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:32.823109      19 request.go:538] Throttling request took 178.389734ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:33.021046      19 request.go:538] Throttling request took 176.55396ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:33.220889      19 request.go:538] Throttling request took 176.569115ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:33.420789      19 request.go:538] Throttling request took 176.18485ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:33.620909      19 request.go:538] Throttling request took 176.706004ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:33.820923      19 request.go:538] Throttling request took 175.447118ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:34.020805      19 request.go:538] Throttling request took 76.642431ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:34.222516      19 request.go:538] Throttling request took 74.03226ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
I1127 06:31:34.425709      19 request.go:538] Throttling request took 78.014739ms, request: GET:https://172.30.0.1:443/apis/apiregistration.k8s.io/v1/apiservices/v1alpha1.wardle.k8s.io
Nov 27 06:31:35.237: INFO: Waited 14.994149654s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:31:35.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7033" for this suite.
Nov 27 06:31:41.979: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:31:42.208: INFO: namespace aggregator-7033 deletion completed in 6.276137514s

• [SLOW TEST:39.096 seconds]
[sig-api-machinery] Aggregator
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:31:42.210: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7681
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 27 06:32:12.540: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Nov 27 06:32:14.569: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433132, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433132, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433132, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433132, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 06:32:16.577: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433132, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433132, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433132, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433132, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 27 06:32:19.605: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
Nov 27 06:32:29.709: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource definition that should be denied by the webhook
Nov 27 06:32:29.935: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:32:29.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7681" for this suite.
Nov 27 06:32:36.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:32:36.274: INFO: namespace webhook-7681 deletion completed in 6.284157717s
STEP: Destroying namespace "webhook-7681-markers" for this suite.
Nov 27 06:32:42.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:32:42.505: INFO: namespace webhook-7681-markers deletion completed in 6.23054825s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:60.333 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:32:42.548: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3055
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-966dbc59-1f3a-4ee3-bf3c-b76d69473d54
[AfterEach] [sig-node] ConfigMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:32:42.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3055" for this suite.
Nov 27 06:32:48.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:32:49.190: INFO: namespace configmap-3055 deletion completed in 6.32157669s

• [SLOW TEST:6.643 seconds]
[sig-node] ConfigMap
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:32:49.193: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7314
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 27 06:32:49.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-7314'
Nov 27 06:32:50.514: INFO: stderr: ""
Nov 27 06:32:50.514: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
I1127 06:32:50.517410      19 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 06:32:50.517701      19 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
STEP: verifying the pod e2e-test-httpd-pod was created
Nov 27 06:32:55.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 get pod e2e-test-httpd-pod --namespace=kubectl-7314 -o json'
Nov 27 06:32:56.302: INFO: stderr: ""
Nov 27 06:32:56.303: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-11-27T06:32:50Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7314\",\n        \"resourceVersion\": \"972797\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-7314/pods/e2e-test-httpd-pod\",\n        \"uid\": \"eb843fbe-4c55-4eb1-b282-71d8abf5001b\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-b9wxr\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"slave1\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-b9wxr\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-b9wxr\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-11-27T06:32:50Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-11-27T06:32:54Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-11-27T06:32:54Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-11-27T06:32:50Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://f14eea13fcc7dd101eaf159c39157d9ec96e6cfed449b2a78b971c07c42a8176\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker://sha256:0c2b0d33a386966885dd604f38d06db8c78942dbee8e1f5a8db17ad0d8a368da\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-11-27T06:32:53Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.200.72.17\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.31.51.239\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.31.51.239\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-11-27T06:32:50Z\"\n    }\n}\n"
STEP: replace the image in the pod
Nov 27 06:32:56.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 replace -f - --namespace=kubectl-7314'
Nov 27 06:32:57.861: INFO: stderr: ""
Nov 27 06:32:57.861: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Nov 27 06:32:57.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 delete pods e2e-test-httpd-pod --namespace=kubectl-7314'
Nov 27 06:33:03.259: INFO: stderr: ""
Nov 27 06:33:03.259: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:33:03.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7314" for this suite.
Nov 27 06:33:09.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:33:09.582: INFO: namespace kubectl-7314 deletion completed in 6.303113358s

• [SLOW TEST:20.389 seconds]
[sig-cli] Kubectl client
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:33:09.583: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1428
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 06:33:09.963: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Nov 27 06:33:09.994: INFO: Number of nodes with available pods: 0
Nov 27 06:33:09.994: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Nov 27 06:33:10.056: INFO: Number of nodes with available pods: 0
Nov 27 06:33:10.056: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:11.070: INFO: Number of nodes with available pods: 0
Nov 27 06:33:11.070: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:12.068: INFO: Number of nodes with available pods: 0
Nov 27 06:33:12.068: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:13.072: INFO: Number of nodes with available pods: 0
Nov 27 06:33:13.073: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:14.068: INFO: Number of nodes with available pods: 1
Nov 27 06:33:14.068: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Nov 27 06:33:14.110: INFO: Number of nodes with available pods: 1
Nov 27 06:33:14.110: INFO: Number of running nodes: 0, number of available pods: 1
Nov 27 06:33:15.119: INFO: Number of nodes with available pods: 0
Nov 27 06:33:15.119: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Nov 27 06:33:15.159: INFO: Number of nodes with available pods: 0
Nov 27 06:33:15.159: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:16.169: INFO: Number of nodes with available pods: 0
Nov 27 06:33:16.169: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:17.169: INFO: Number of nodes with available pods: 0
Nov 27 06:33:17.169: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:18.170: INFO: Number of nodes with available pods: 0
Nov 27 06:33:18.171: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:19.173: INFO: Number of nodes with available pods: 0
Nov 27 06:33:19.173: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:20.170: INFO: Number of nodes with available pods: 0
Nov 27 06:33:20.170: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:21.169: INFO: Number of nodes with available pods: 0
Nov 27 06:33:21.169: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:22.170: INFO: Number of nodes with available pods: 0
Nov 27 06:33:22.170: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:23.170: INFO: Number of nodes with available pods: 0
Nov 27 06:33:23.170: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:24.173: INFO: Number of nodes with available pods: 0
Nov 27 06:33:24.173: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:25.171: INFO: Number of nodes with available pods: 0
Nov 27 06:33:25.171: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:26.172: INFO: Number of nodes with available pods: 0
Nov 27 06:33:26.172: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:27.169: INFO: Number of nodes with available pods: 0
Nov 27 06:33:27.169: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:28.170: INFO: Number of nodes with available pods: 0
Nov 27 06:33:28.170: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:29.176: INFO: Number of nodes with available pods: 0
Nov 27 06:33:29.176: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:30.168: INFO: Number of nodes with available pods: 0
Nov 27 06:33:30.168: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:31.176: INFO: Number of nodes with available pods: 0
Nov 27 06:33:31.176: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:32.168: INFO: Number of nodes with available pods: 0
Nov 27 06:33:32.168: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:33.169: INFO: Number of nodes with available pods: 1
Nov 27 06:33:33.169: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1428, will wait for the garbage collector to delete the pods
I1127 06:33:33.191555      19 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 06:33:33.191754      19 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Nov 27 06:33:33.255: INFO: Deleting DaemonSet.extensions daemon-set took: 14.308764ms
Nov 27 06:33:33.756: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.492886ms
I1127 06:33:33.756292      19 controller_utils.go:810] Ignoring inactive pod daemonsets-1428/daemon-set-z9r2d in state Running, deletion time 2019-11-27 06:34:03 +0000 UTC
Nov 27 06:33:39.767: INFO: Number of nodes with available pods: 0
Nov 27 06:33:39.767: INFO: Number of running nodes: 0, number of available pods: 0
Nov 27 06:33:39.775: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1428/daemonsets","resourceVersion":"972952"},"items":null}

Nov 27 06:33:39.781: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1428/pods","resourceVersion":"972952"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:33:39.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1428" for this suite.
Nov 27 06:33:45.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:33:46.144: INFO: namespace daemonsets-1428 deletion completed in 6.306867527s

• [SLOW TEST:36.561 seconds]
[sig-apps] Daemon set [Serial]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:33:46.145: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4024
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 06:33:46.508: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Nov 27 06:33:46.543: INFO: Number of nodes with available pods: 0
Nov 27 06:33:46.543: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:47.572: INFO: Number of nodes with available pods: 0
Nov 27 06:33:47.572: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:48.565: INFO: Number of nodes with available pods: 0
Nov 27 06:33:48.565: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:49.565: INFO: Number of nodes with available pods: 0
Nov 27 06:33:49.565: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:50.569: INFO: Number of nodes with available pods: 0
Nov 27 06:33:50.569: INFO: Node master1 is running more than one daemon pod
Nov 27 06:33:51.568: INFO: Number of nodes with available pods: 2
Nov 27 06:33:51.568: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Nov 27 06:33:51.634: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:33:51.634: INFO: Wrong image for pod: daemon-set-clnfw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:33:52.658: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:33:52.659: INFO: Wrong image for pod: daemon-set-clnfw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:33:53.659: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:33:53.660: INFO: Wrong image for pod: daemon-set-clnfw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:33:54.659: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:33:54.659: INFO: Wrong image for pod: daemon-set-clnfw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:33:55.658: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:33:55.658: INFO: Wrong image for pod: daemon-set-clnfw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:33:56.657: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:33:56.657: INFO: Wrong image for pod: daemon-set-clnfw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:33:56.657: INFO: Pod daemon-set-clnfw is not available
Nov 27 06:33:57.658: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:33:57.658: INFO: Wrong image for pod: daemon-set-clnfw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:33:57.658: INFO: Pod daemon-set-clnfw is not available
Nov 27 06:33:58.659: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:33:58.659: INFO: Wrong image for pod: daemon-set-clnfw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:33:58.659: INFO: Pod daemon-set-clnfw is not available
Nov 27 06:33:59.658: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:33:59.659: INFO: Wrong image for pod: daemon-set-clnfw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:33:59.659: INFO: Pod daemon-set-clnfw is not available
Nov 27 06:34:00.661: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:34:00.661: INFO: Wrong image for pod: daemon-set-clnfw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:34:00.661: INFO: Pod daemon-set-clnfw is not available
Nov 27 06:34:01.658: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:34:01.658: INFO: Wrong image for pod: daemon-set-clnfw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:34:01.658: INFO: Pod daemon-set-clnfw is not available
Nov 27 06:34:02.659: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:34:02.659: INFO: Wrong image for pod: daemon-set-clnfw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:34:02.659: INFO: Pod daemon-set-clnfw is not available
Nov 27 06:34:03.665: INFO: Pod daemon-set-6nxmb is not available
Nov 27 06:34:03.666: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:34:04.658: INFO: Pod daemon-set-6nxmb is not available
Nov 27 06:34:04.658: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:34:05.658: INFO: Pod daemon-set-6nxmb is not available
Nov 27 06:34:05.658: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:34:06.657: INFO: Pod daemon-set-6nxmb is not available
Nov 27 06:34:06.657: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:34:07.658: INFO: Pod daemon-set-6nxmb is not available
Nov 27 06:34:07.658: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:34:08.658: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:34:09.662: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:34:10.704: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:34:11.660: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:34:12.659: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:34:12.659: INFO: Pod daemon-set-bnvn8 is not available
Nov 27 06:34:13.658: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:34:13.658: INFO: Pod daemon-set-bnvn8 is not available
Nov 27 06:34:14.659: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:34:14.659: INFO: Pod daemon-set-bnvn8 is not available
Nov 27 06:34:15.659: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:34:15.659: INFO: Pod daemon-set-bnvn8 is not available
Nov 27 06:34:16.659: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:34:16.659: INFO: Pod daemon-set-bnvn8 is not available
Nov 27 06:34:17.665: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:34:17.666: INFO: Pod daemon-set-bnvn8 is not available
Nov 27 06:34:18.658: INFO: Wrong image for pod: daemon-set-bnvn8. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 27 06:34:18.659: INFO: Pod daemon-set-bnvn8 is not available
Nov 27 06:34:19.660: INFO: Pod daemon-set-hgmg4 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Nov 27 06:34:19.696: INFO: Number of nodes with available pods: 1
Nov 27 06:34:19.696: INFO: Node master1 is running more than one daemon pod
Nov 27 06:34:20.739: INFO: Number of nodes with available pods: 1
Nov 27 06:34:20.739: INFO: Node master1 is running more than one daemon pod
Nov 27 06:34:21.727: INFO: Number of nodes with available pods: 1
Nov 27 06:34:21.727: INFO: Node master1 is running more than one daemon pod
Nov 27 06:34:22.728: INFO: Number of nodes with available pods: 1
Nov 27 06:34:22.728: INFO: Node master1 is running more than one daemon pod
Nov 27 06:34:23.720: INFO: Number of nodes with available pods: 2
Nov 27 06:34:23.720: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4024, will wait for the garbage collector to delete the pods
I1127 06:34:23.774660      19 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 06:34:23.774751      19 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Nov 27 06:34:23.838: INFO: Deleting DaemonSet.extensions daemon-set took: 14.168453ms
I1127 06:34:24.339755      19 controller_utils.go:810] Ignoring inactive pod daemonsets-4024/daemon-set-hgmg4 in state Running, deletion time 2019-11-27 06:34:54 +0000 UTC
I1127 06:34:24.339970      19 controller_utils.go:810] Ignoring inactive pod daemonsets-4024/daemon-set-6nxmb in state Running, deletion time 2019-11-27 06:34:54 +0000 UTC
Nov 27 06:34:24.340: INFO: Terminating DaemonSet.extensions daemon-set pods took: 501.126485ms
Nov 27 06:34:39.152: INFO: Number of nodes with available pods: 0
Nov 27 06:34:39.152: INFO: Number of running nodes: 0, number of available pods: 0
Nov 27 06:34:39.159: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4024/daemonsets","resourceVersion":"973164"},"items":null}

Nov 27 06:34:39.165: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4024/pods","resourceVersion":"973164"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:34:39.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4024" for this suite.
Nov 27 06:34:45.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:34:45.462: INFO: namespace daemonsets-4024 deletion completed in 6.24851145s

• [SLOW TEST:59.318 seconds]
[sig-apps] Daemon set [Serial]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:34:45.463: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7895
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:34:50.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7895" for this suite.
Nov 27 06:35:18.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:35:19.121: INFO: namespace replication-controller-7895 deletion completed in 28.269486613s

• [SLOW TEST:33.658 seconds]
[sig-apps] ReplicationController
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:35:19.148: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-8098
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9026
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1298
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:35:52.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8098" for this suite.
Nov 27 06:35:58.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:35:58.476: INFO: namespace namespaces-8098 deletion completed in 6.280511592s
STEP: Destroying namespace "nsdeletetest-9026" for this suite.
Nov 27 06:35:58.482: INFO: Namespace nsdeletetest-9026 was already deleted
STEP: Destroying namespace "nsdeletetest-1298" for this suite.
Nov 27 06:36:04.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:36:04.737: INFO: namespace nsdeletetest-1298 deletion completed in 6.25480099s

• [SLOW TEST:45.589 seconds]
[sig-api-machinery] Namespaces [Serial]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:36:04.739: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-9199
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-sjv49 in namespace proxy-9199
I1127 06:36:05.096714      19 runners.go:184] Created replication controller with name: proxy-service-sjv49, namespace: proxy-9199, replica count: 1
I1127 06:36:05.097061      19 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 06:36:05.098635      19 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 06:36:06.147537      19 runners.go:184] proxy-service-sjv49 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1127 06:36:07.148065      19 runners.go:184] proxy-service-sjv49 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1127 06:36:08.148489      19 runners.go:184] proxy-service-sjv49 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1127 06:36:09.148912      19 runners.go:184] proxy-service-sjv49 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1127 06:36:10.149305      19 runners.go:184] proxy-service-sjv49 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 27 06:36:10.162: INFO: setup took 5.115697117s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Nov 27 06:36:10.186: INFO: (0) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/rewriteme">test</a> (200; 23.708608ms)
Nov 27 06:36:10.189: INFO: (0) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 26.749489ms)
Nov 27 06:36:10.189: INFO: (0) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 27.11371ms)
Nov 27 06:36:10.203: INFO: (0) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname2/proxy/: bar (200; 40.785454ms)
Nov 27 06:36:10.211: INFO: (0) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">... (200; 47.901214ms)
Nov 27 06:36:10.211: INFO: (0) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 48.982145ms)
Nov 27 06:36:10.216: INFO: (0) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 54.265065ms)
Nov 27 06:36:10.217: INFO: (0) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">test<... (200; 54.313465ms)
Nov 27 06:36:10.222: INFO: (0) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname1/proxy/: foo (200; 58.753054ms)
Nov 27 06:36:10.241: INFO: (0) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname1/proxy/: foo (200; 78.901092ms)
Nov 27 06:36:10.241: INFO: (0) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname2/proxy/: bar (200; 78.982737ms)
Nov 27 06:36:10.337: INFO: (0) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:460/proxy/: tls baz (200; 174.866319ms)
Nov 27 06:36:10.342: INFO: (0) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/tlsrewritem... (200; 179.178309ms)
Nov 27 06:36:10.352: INFO: (0) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname1/proxy/: tls baz (200; 189.736327ms)
Nov 27 06:36:10.353: INFO: (0) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:462/proxy/: tls qux (200; 190.916013ms)
Nov 27 06:36:10.354: INFO: (0) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname2/proxy/: tls qux (200; 191.271924ms)
Nov 27 06:36:10.377: INFO: (1) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 20.111861ms)
Nov 27 06:36:10.377: INFO: (1) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/rewriteme">test</a> (200; 21.379058ms)
Nov 27 06:36:10.377: INFO: (1) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">test<... (200; 21.280302ms)
Nov 27 06:36:10.377: INFO: (1) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 23.442163ms)
Nov 27 06:36:10.379: INFO: (1) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/tlsrewritem... (200; 22.689277ms)
Nov 27 06:36:10.391: INFO: (1) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname2/proxy/: tls qux (200; 34.060315ms)
Nov 27 06:36:10.391: INFO: (1) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname1/proxy/: foo (200; 35.918577ms)
Nov 27 06:36:10.391: INFO: (1) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:462/proxy/: tls qux (200; 34.849868ms)
Nov 27 06:36:10.392: INFO: (1) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname1/proxy/: tls baz (200; 37.065996ms)
Nov 27 06:36:10.393: INFO: (1) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname1/proxy/: foo (200; 38.463726ms)
Nov 27 06:36:10.393: INFO: (1) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">... (200; 36.65142ms)
Nov 27 06:36:10.394: INFO: (1) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 38.033505ms)
Nov 27 06:36:10.395: INFO: (1) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 38.712081ms)
Nov 27 06:36:10.396: INFO: (1) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname2/proxy/: bar (200; 40.467188ms)
Nov 27 06:36:10.396: INFO: (1) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname2/proxy/: bar (200; 41.430297ms)
Nov 27 06:36:10.397: INFO: (1) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:460/proxy/: tls baz (200; 42.367983ms)
Nov 27 06:36:10.430: INFO: (2) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 30.9285ms)
Nov 27 06:36:10.430: INFO: (2) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">... (200; 29.875925ms)
Nov 27 06:36:10.430: INFO: (2) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 30.244058ms)
Nov 27 06:36:10.430: INFO: (2) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/tlsrewritem... (200; 31.410056ms)
Nov 27 06:36:10.432: INFO: (2) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:460/proxy/: tls baz (200; 33.046362ms)
Nov 27 06:36:10.435: INFO: (2) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname1/proxy/: foo (200; 35.278135ms)
Nov 27 06:36:10.437: INFO: (2) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname1/proxy/: tls baz (200; 38.651948ms)
Nov 27 06:36:10.438: INFO: (2) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">test<... (200; 37.834528ms)
Nov 27 06:36:10.438: INFO: (2) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname2/proxy/: tls qux (200; 39.419991ms)
Nov 27 06:36:10.438: INFO: (2) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/rewriteme">test</a> (200; 38.480349ms)
Nov 27 06:36:10.441: INFO: (2) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:462/proxy/: tls qux (200; 43.658158ms)
Nov 27 06:36:10.442: INFO: (2) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname2/proxy/: bar (200; 41.702119ms)
Nov 27 06:36:10.442: INFO: (2) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 43.998913ms)
Nov 27 06:36:10.447: INFO: (2) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 46.754283ms)
Nov 27 06:36:10.447: INFO: (2) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname2/proxy/: bar (200; 47.941791ms)
Nov 27 06:36:10.450: INFO: (2) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname1/proxy/: foo (200; 50.820851ms)
Nov 27 06:36:10.490: INFO: (3) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 35.391067ms)
Nov 27 06:36:10.493: INFO: (3) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/tlsrewritem... (200; 40.615321ms)
Nov 27 06:36:10.494: INFO: (3) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/rewriteme">test</a> (200; 42.765449ms)
Nov 27 06:36:10.494: INFO: (3) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 43.823891ms)
Nov 27 06:36:10.504: INFO: (3) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">... (200; 49.936453ms)
Nov 27 06:36:10.505: INFO: (3) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:460/proxy/: tls baz (200; 51.487694ms)
Nov 27 06:36:10.505: INFO: (3) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:462/proxy/: tls qux (200; 49.844543ms)
Nov 27 06:36:10.505: INFO: (3) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 50.886851ms)
Nov 27 06:36:10.505: INFO: (3) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname2/proxy/: tls qux (200; 49.623077ms)
Nov 27 06:36:10.505: INFO: (3) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">test<... (200; 53.939465ms)
Nov 27 06:36:10.505: INFO: (3) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname1/proxy/: foo (200; 51.875871ms)
Nov 27 06:36:10.505: INFO: (3) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname2/proxy/: bar (200; 50.191652ms)
Nov 27 06:36:10.508: INFO: (3) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname1/proxy/: tls baz (200; 56.98328ms)
Nov 27 06:36:10.508: INFO: (3) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 55.299551ms)
Nov 27 06:36:10.509: INFO: (3) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname2/proxy/: bar (200; 55.017951ms)
Nov 27 06:36:10.510: INFO: (3) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname1/proxy/: foo (200; 55.128929ms)
Nov 27 06:36:10.532: INFO: (4) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">test<... (200; 20.820259ms)
Nov 27 06:36:10.582: INFO: (4) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:460/proxy/: tls baz (200; 69.802893ms)
Nov 27 06:36:10.586: INFO: (4) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 73.923238ms)
Nov 27 06:36:10.587: INFO: (4) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 76.170654ms)
Nov 27 06:36:10.587: INFO: (4) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:462/proxy/: tls qux (200; 74.54657ms)
Nov 27 06:36:10.588: INFO: (4) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 76.0235ms)
Nov 27 06:36:10.588: INFO: (4) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">... (200; 74.861414ms)
Nov 27 06:36:10.588: INFO: (4) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/tlsrewritem... (200; 76.142299ms)
Nov 27 06:36:10.588: INFO: (4) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/rewriteme">test</a> (200; 77.387496ms)
Nov 27 06:36:10.598: INFO: (4) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname2/proxy/: tls qux (200; 84.591256ms)
Nov 27 06:36:10.598: INFO: (4) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 85.746008ms)
Nov 27 06:36:10.598: INFO: (4) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname2/proxy/: bar (200; 85.617431ms)
Nov 27 06:36:10.598: INFO: (4) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname1/proxy/: tls baz (200; 87.179916ms)
Nov 27 06:36:10.598: INFO: (4) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname1/proxy/: foo (200; 86.43974ms)
Nov 27 06:36:10.599: INFO: (4) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname1/proxy/: foo (200; 86.370318ms)
Nov 27 06:36:10.599: INFO: (4) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname2/proxy/: bar (200; 85.97383ms)
Nov 27 06:36:10.625: INFO: (5) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 25.791269ms)
Nov 27 06:36:10.627: INFO: (5) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 26.828688ms)
Nov 27 06:36:10.627: INFO: (5) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">... (200; 27.714553ms)
Nov 27 06:36:10.639: INFO: (5) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">test<... (200; 38.556127ms)
Nov 27 06:36:10.639: INFO: (5) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname2/proxy/: bar (200; 38.253994ms)
Nov 27 06:36:10.640: INFO: (5) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname1/proxy/: foo (200; 38.854348ms)
Nov 27 06:36:10.640: INFO: (5) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname1/proxy/: tls baz (200; 39.742657ms)
Nov 27 06:36:10.642: INFO: (5) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname2/proxy/: tls qux (200; 41.752474ms)
Nov 27 06:36:10.650: INFO: (5) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname2/proxy/: bar (200; 50.134453ms)
Nov 27 06:36:10.651: INFO: (5) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:460/proxy/: tls baz (200; 49.539965ms)
Nov 27 06:36:10.651: INFO: (5) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/tlsrewritem... (200; 50.969473ms)
Nov 27 06:36:10.651: INFO: (5) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname1/proxy/: foo (200; 51.594271ms)
Nov 27 06:36:10.651: INFO: (5) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 52.003959ms)
Nov 27 06:36:10.651: INFO: (5) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:462/proxy/: tls qux (200; 51.368894ms)
Nov 27 06:36:10.652: INFO: (5) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/rewriteme">test</a> (200; 51.122006ms)
Nov 27 06:36:10.657: INFO: (5) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 56.751058ms)
Nov 27 06:36:10.691: INFO: (6) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 32.972051ms)
Nov 27 06:36:10.692: INFO: (6) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:462/proxy/: tls qux (200; 34.489069ms)
Nov 27 06:36:10.694: INFO: (6) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname1/proxy/: tls baz (200; 35.148579ms)
Nov 27 06:36:10.703: INFO: (6) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/rewriteme">test</a> (200; 43.983757ms)
Nov 27 06:36:10.712: INFO: (6) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname2/proxy/: bar (200; 54.282176ms)
Nov 27 06:36:10.720: INFO: (6) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 59.805628ms)
Nov 27 06:36:10.721: INFO: (6) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:460/proxy/: tls baz (200; 59.857451ms)
Nov 27 06:36:10.722: INFO: (6) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 62.260334ms)
Nov 27 06:36:10.728: INFO: (6) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname1/proxy/: foo (200; 66.646634ms)
Nov 27 06:36:10.728: INFO: (6) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/tlsrewritem... (200; 67.164366ms)
Nov 27 06:36:10.728: INFO: (6) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 67.590187ms)
Nov 27 06:36:10.730: INFO: (6) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname2/proxy/: tls qux (200; 70.515691ms)
Nov 27 06:36:10.730: INFO: (6) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname2/proxy/: bar (200; 70.242891ms)
Nov 27 06:36:10.730: INFO: (6) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">... (200; 70.015559ms)
Nov 27 06:36:10.730: INFO: (6) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">test<... (200; 69.022139ms)
Nov 27 06:36:10.731: INFO: (6) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname1/proxy/: foo (200; 71.229956ms)
Nov 27 06:36:10.746: INFO: (7) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname2/proxy/: bar (200; 15.072896ms)
Nov 27 06:36:10.748: INFO: (7) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:460/proxy/: tls baz (200; 16.687203ms)
Nov 27 06:36:10.787: INFO: (7) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname2/proxy/: bar (200; 55.433506ms)
Nov 27 06:36:10.788: INFO: (7) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">... (200; 55.946349ms)
Nov 27 06:36:10.789: INFO: (7) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/tlsrewritem... (200; 57.014569ms)
Nov 27 06:36:10.789: INFO: (7) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:462/proxy/: tls qux (200; 56.07346ms)
Nov 27 06:36:10.789: INFO: (7) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname1/proxy/: foo (200; 56.921681ms)
Nov 27 06:36:10.791: INFO: (7) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 59.207719ms)
Nov 27 06:36:10.805: INFO: (7) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/rewriteme">test</a> (200; 71.09209ms)
Nov 27 06:36:10.805: INFO: (7) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 71.748666ms)
Nov 27 06:36:10.805: INFO: (7) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">test<... (200; 71.785821ms)
Nov 27 06:36:10.805: INFO: (7) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname1/proxy/: foo (200; 72.51133ms)
Nov 27 06:36:10.805: INFO: (7) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 72.84133ms)
Nov 27 06:36:10.805: INFO: (7) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 71.790221ms)
Nov 27 06:36:10.808: INFO: (7) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname2/proxy/: tls qux (200; 75.487679ms)
Nov 27 06:36:10.812: INFO: (7) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname1/proxy/: tls baz (200; 78.892292ms)
Nov 27 06:36:10.826: INFO: (8) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">test<... (200; 14.207075ms)
Nov 27 06:36:10.831: INFO: (8) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/tlsrewritem... (200; 18.704353ms)
Nov 27 06:36:10.836: INFO: (8) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 24.216073ms)
Nov 27 06:36:10.840: INFO: (8) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname1/proxy/: foo (200; 27.558598ms)
Nov 27 06:36:10.841: INFO: (8) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/rewriteme">test</a> (200; 29.221794ms)
Nov 27 06:36:10.841: INFO: (8) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 29.049705ms)
Nov 27 06:36:10.842: INFO: (8) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname1/proxy/: tls baz (200; 29.704815ms)
Nov 27 06:36:10.843: INFO: (8) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">... (200; 31.111344ms)
Nov 27 06:36:10.845: INFO: (8) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 32.948096ms)
Nov 27 06:36:10.847: INFO: (8) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname2/proxy/: bar (200; 34.902668ms)
Nov 27 06:36:10.880: INFO: (8) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 67.806276ms)
Nov 27 06:36:10.880: INFO: (8) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname1/proxy/: foo (200; 68.263874ms)
Nov 27 06:36:10.880: INFO: (8) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:462/proxy/: tls qux (200; 67.64201ms)
Nov 27 06:36:10.880: INFO: (8) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname2/proxy/: bar (200; 67.422498ms)
Nov 27 06:36:10.882: INFO: (8) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname2/proxy/: tls qux (200; 69.233339ms)
Nov 27 06:36:10.885: INFO: (8) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:460/proxy/: tls baz (200; 71.939821ms)
Nov 27 06:36:10.903: INFO: (9) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/tlsrewritem... (200; 17.097869ms)
Nov 27 06:36:10.913: INFO: (9) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">test<... (200; 26.444912ms)
Nov 27 06:36:10.913: INFO: (9) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 27.052599ms)
Nov 27 06:36:10.916: INFO: (9) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname2/proxy/: tls qux (200; 30.418591ms)
Nov 27 06:36:10.916: INFO: (9) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:462/proxy/: tls qux (200; 31.308855ms)
Nov 27 06:36:10.918: INFO: (9) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname2/proxy/: bar (200; 31.919965ms)
Nov 27 06:36:10.918: INFO: (9) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 32.452853ms)
Nov 27 06:36:10.919: INFO: (9) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/rewriteme">test</a> (200; 33.06934ms)
Nov 27 06:36:10.919: INFO: (9) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname1/proxy/: foo (200; 33.716138ms)
Nov 27 06:36:10.919: INFO: (9) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 32.938318ms)
Nov 27 06:36:10.919: INFO: (9) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">... (200; 32.936363ms)
Nov 27 06:36:10.920: INFO: (9) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 34.30867ms)
Nov 27 06:36:10.920: INFO: (9) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname1/proxy/: foo (200; 34.44947ms)
Nov 27 06:36:10.920: INFO: (9) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname2/proxy/: bar (200; 34.309159ms)
Nov 27 06:36:10.922: INFO: (9) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname1/proxy/: tls baz (200; 35.915643ms)
Nov 27 06:36:10.922: INFO: (9) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:460/proxy/: tls baz (200; 35.97431ms)
Nov 27 06:36:10.984: INFO: (10) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">... (200; 60.210428ms)
Nov 27 06:36:10.994: INFO: (10) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:460/proxy/: tls baz (200; 70.676535ms)
Nov 27 06:36:10.994: INFO: (10) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">test<... (200; 70.135336ms)
Nov 27 06:36:10.998: INFO: (10) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/tlsrewritem... (200; 74.604259ms)
Nov 27 06:36:10.998: INFO: (10) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 74.249815ms)
Nov 27 06:36:11.002: INFO: (10) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname1/proxy/: foo (200; 77.904739ms)
Nov 27 06:36:11.002: INFO: (10) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 78.878604ms)
Nov 27 06:36:11.005: INFO: (10) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname1/proxy/: foo (200; 80.332066ms)
Nov 27 06:36:11.005: INFO: (10) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 80.837088ms)
Nov 27 06:36:11.009: INFO: (10) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 85.722542ms)
Nov 27 06:36:11.009: INFO: (10) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname2/proxy/: tls qux (200; 86.65534ms)
Nov 27 06:36:11.010: INFO: (10) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname2/proxy/: bar (200; 85.469298ms)
Nov 27 06:36:11.010: INFO: (10) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname1/proxy/: tls baz (200; 86.385473ms)
Nov 27 06:36:11.010: INFO: (10) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname2/proxy/: bar (200; 87.396005ms)
Nov 27 06:36:11.011: INFO: (10) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/rewriteme">test</a> (200; 87.588137ms)
Nov 27 06:36:11.012: INFO: (10) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:462/proxy/: tls qux (200; 87.478626ms)
Nov 27 06:36:11.032: INFO: (11) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 19.632262ms)
Nov 27 06:36:11.034: INFO: (11) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">... (200; 21.137058ms)
Nov 27 06:36:11.047: INFO: (11) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:462/proxy/: tls qux (200; 34.286182ms)
Nov 27 06:36:11.047: INFO: (11) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/tlsrewritem... (200; 33.940048ms)
Nov 27 06:36:11.082: INFO: (11) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname1/proxy/: tls baz (200; 69.442583ms)
Nov 27 06:36:11.085: INFO: (11) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 72.442886ms)
Nov 27 06:36:11.087: INFO: (11) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">test<... (200; 74.104126ms)
Nov 27 06:36:11.087: INFO: (11) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 74.26546ms)
Nov 27 06:36:11.088: INFO: (11) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname1/proxy/: foo (200; 75.28039ms)
Nov 27 06:36:11.088: INFO: (11) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname2/proxy/: bar (200; 76.000522ms)
Nov 27 06:36:11.094: INFO: (11) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:460/proxy/: tls baz (200; 80.600955ms)
Nov 27 06:36:11.094: INFO: (11) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname2/proxy/: tls qux (200; 80.804821ms)
Nov 27 06:36:11.096: INFO: (11) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 83.597347ms)
Nov 27 06:36:11.097: INFO: (11) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/rewriteme">test</a> (200; 84.13219ms)
Nov 27 06:36:11.097: INFO: (11) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname2/proxy/: bar (200; 84.173746ms)
Nov 27 06:36:11.098: INFO: (11) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname1/proxy/: foo (200; 85.042988ms)
Nov 27 06:36:11.112: INFO: (12) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:462/proxy/: tls qux (200; 13.271345ms)
Nov 27 06:36:11.114: INFO: (12) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/rewriteme">test</a> (200; 14.478408ms)
Nov 27 06:36:11.118: INFO: (12) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 17.381423ms)
Nov 27 06:36:11.127: INFO: (12) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 25.580558ms)
Nov 27 06:36:11.127: INFO: (12) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 27.712597ms)
Nov 27 06:36:11.128: INFO: (12) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">test<... (200; 27.969752ms)
Nov 27 06:36:11.134: INFO: (12) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname2/proxy/: tls qux (200; 34.345825ms)
Nov 27 06:36:11.134: INFO: (12) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">... (200; 32.698274ms)
Nov 27 06:36:11.134: INFO: (12) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname2/proxy/: bar (200; 33.491739ms)
Nov 27 06:36:11.137: INFO: (12) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:460/proxy/: tls baz (200; 25.273537ms)
Nov 27 06:36:11.137: INFO: (12) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 37.334396ms)
Nov 27 06:36:11.138: INFO: (12) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname1/proxy/: foo (200; 35.539689ms)
Nov 27 06:36:11.139: INFO: (12) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname1/proxy/: foo (200; 38.827459ms)
Nov 27 06:36:11.140: INFO: (12) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname2/proxy/: bar (200; 27.757087ms)
Nov 27 06:36:11.149: INFO: (12) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/tlsrewritem... (200; 32.918763ms)
Nov 27 06:36:11.149: INFO: (12) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname1/proxy/: tls baz (200; 36.341954ms)
Nov 27 06:36:11.187: INFO: (13) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/rewriteme">test</a> (200; 37.16964ms)
Nov 27 06:36:11.212: INFO: (13) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 61.199936ms)
Nov 27 06:36:11.213: INFO: (13) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">... (200; 61.547535ms)
Nov 27 06:36:11.214: INFO: (13) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:462/proxy/: tls qux (200; 64.066773ms)
Nov 27 06:36:11.225: INFO: (13) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname1/proxy/: foo (200; 71.579022ms)
Nov 27 06:36:11.225: INFO: (13) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 72.599819ms)
Nov 27 06:36:11.225: INFO: (13) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname2/proxy/: tls qux (200; 74.627236ms)
Nov 27 06:36:11.225: INFO: (13) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname1/proxy/: foo (200; 74.044971ms)
Nov 27 06:36:11.225: INFO: (13) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname2/proxy/: bar (200; 72.378353ms)
Nov 27 06:36:11.225: INFO: (13) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname1/proxy/: tls baz (200; 71.986265ms)
Nov 27 06:36:11.225: INFO: (13) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/tlsrewritem... (200; 72.873107ms)
Nov 27 06:36:11.225: INFO: (13) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 74.740658ms)
Nov 27 06:36:11.225: INFO: (13) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 74.698614ms)
Nov 27 06:36:11.226: INFO: (13) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">test<... (200; 75.214391ms)
Nov 27 06:36:11.226: INFO: (13) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname2/proxy/: bar (200; 74.730392ms)
Nov 27 06:36:11.226: INFO: (13) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:460/proxy/: tls baz (200; 73.996571ms)
Nov 27 06:36:11.251: INFO: (14) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 24.521628ms)
Nov 27 06:36:11.251: INFO: (14) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">... (200; 23.156165ms)
Nov 27 06:36:11.260: INFO: (14) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:460/proxy/: tls baz (200; 30.441569ms)
Nov 27 06:36:11.260: INFO: (14) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 31.120145ms)
Nov 27 06:36:11.281: INFO: (14) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname1/proxy/: foo (200; 50.568585ms)
Nov 27 06:36:11.281: INFO: (14) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 52.94849ms)
Nov 27 06:36:11.281: INFO: (14) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname1/proxy/: foo (200; 54.092488ms)
Nov 27 06:36:11.281: INFO: (14) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/tlsrewritem... (200; 51.680804ms)
Nov 27 06:36:11.281: INFO: (14) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:462/proxy/: tls qux (200; 52.874668ms)
Nov 27 06:36:11.281: INFO: (14) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 52.273337ms)
Nov 27 06:36:11.281: INFO: (14) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname2/proxy/: bar (200; 50.405297ms)
Nov 27 06:36:11.281: INFO: (14) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/rewriteme">test</a> (200; 51.527294ms)
Nov 27 06:36:11.281: INFO: (14) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">test<... (200; 51.55125ms)
Nov 27 06:36:11.283: INFO: (14) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname2/proxy/: tls qux (200; 54.273864ms)
Nov 27 06:36:11.284: INFO: (14) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname2/proxy/: bar (200; 57.19839ms)
Nov 27 06:36:11.289: INFO: (14) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname1/proxy/: tls baz (200; 59.010698ms)
Nov 27 06:36:11.383: INFO: (15) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">test<... (200; 88.443202ms)
Nov 27 06:36:11.422: INFO: (15) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 132.017271ms)
Nov 27 06:36:11.426: INFO: (15) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 131.582161ms)
Nov 27 06:36:11.427: INFO: (15) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/rewriteme">test</a> (200; 131.681406ms)
Nov 27 06:36:11.427: INFO: (15) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:462/proxy/: tls qux (200; 134.945708ms)
Nov 27 06:36:11.427: INFO: (15) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 136.745303ms)
Nov 27 06:36:11.427: INFO: (15) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:460/proxy/: tls baz (200; 131.937093ms)
Nov 27 06:36:11.427: INFO: (15) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname2/proxy/: bar (200; 135.721573ms)
Nov 27 06:36:11.500: INFO: (15) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname2/proxy/: bar (200; 208.437259ms)
Nov 27 06:36:11.500: INFO: (15) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 204.987178ms)
Nov 27 06:36:11.501: INFO: (15) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">... (200; 209.138812ms)
Nov 27 06:36:11.501: INFO: (15) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname1/proxy/: foo (200; 209.784633ms)
Nov 27 06:36:11.501: INFO: (15) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname1/proxy/: foo (200; 205.732732ms)
Nov 27 06:36:11.501: INFO: (15) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname2/proxy/: tls qux (200; 209.198946ms)
Nov 27 06:36:11.501: INFO: (15) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname1/proxy/: tls baz (200; 205.919486ms)
Nov 27 06:36:11.501: INFO: (15) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/tlsrewritem... (200; 206.085709ms)
Nov 27 06:36:11.595: INFO: (16) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 93.303234ms)
Nov 27 06:36:11.604: INFO: (16) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 98.357843ms)
Nov 27 06:36:11.606: INFO: (16) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">test<... (200; 102.199523ms)
Nov 27 06:36:11.608: INFO: (16) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/rewriteme">test</a> (200; 105.301516ms)
Nov 27 06:36:11.610: INFO: (16) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname1/proxy/: foo (200; 106.628846ms)
Nov 27 06:36:11.610: INFO: (16) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:460/proxy/: tls baz (200; 104.579428ms)
Nov 27 06:36:11.613: INFO: (16) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/tlsrewritem... (200; 108.476352ms)
Nov 27 06:36:11.680: INFO: (16) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 176.007383ms)
Nov 27 06:36:11.681: INFO: (16) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 174.882453ms)
Nov 27 06:36:11.689: INFO: (16) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname2/proxy/: bar (200; 186.314113ms)
Nov 27 06:36:11.691: INFO: (16) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname2/proxy/: tls qux (200; 183.298654ms)
Nov 27 06:36:11.691: INFO: (16) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname1/proxy/: tls baz (200; 188.203176ms)
Nov 27 06:36:11.695: INFO: (16) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname1/proxy/: foo (200; 192.716587ms)
Nov 27 06:36:11.695: INFO: (16) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname2/proxy/: bar (200; 188.413886ms)
Nov 27 06:36:11.701: INFO: (16) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">... (200; 194.889692ms)
Nov 27 06:36:11.714: INFO: (16) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:462/proxy/: tls qux (200; 206.958373ms)
Nov 27 06:36:11.801: INFO: (17) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 85.968942ms)
Nov 27 06:36:11.802: INFO: (17) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 87.209249ms)
Nov 27 06:36:11.804: INFO: (17) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:462/proxy/: tls qux (200; 88.661246ms)
Nov 27 06:36:11.810: INFO: (17) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 95.728606ms)
Nov 27 06:36:11.810: INFO: (17) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 94.951763ms)
Nov 27 06:36:11.810: INFO: (17) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:460/proxy/: tls baz (200; 94.417898ms)
Nov 27 06:36:11.813: INFO: (17) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/tlsrewritem... (200; 97.364913ms)
Nov 27 06:36:11.813: INFO: (17) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">test<... (200; 97.12878ms)
Nov 27 06:36:11.816: INFO: (17) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/rewriteme">test</a> (200; 100.447838ms)
Nov 27 06:36:11.882: INFO: (17) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">... (200; 167.016739ms)
Nov 27 06:36:11.883: INFO: (17) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname2/proxy/: bar (200; 168.556247ms)
Nov 27 06:36:11.887: INFO: (17) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname1/proxy/: tls baz (200; 170.785086ms)
Nov 27 06:36:11.891: INFO: (17) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname2/proxy/: tls qux (200; 175.21783ms)
Nov 27 06:36:11.891: INFO: (17) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname2/proxy/: bar (200; 175.540007ms)
Nov 27 06:36:11.894: INFO: (17) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname1/proxy/: foo (200; 179.575286ms)
Nov 27 06:36:11.896: INFO: (17) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname1/proxy/: foo (200; 180.447462ms)
Nov 27 06:36:11.918: INFO: (18) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 20.07715ms)
Nov 27 06:36:11.988: INFO: (18) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 91.212262ms)
Nov 27 06:36:11.990: INFO: (18) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:462/proxy/: tls qux (200; 92.297592ms)
Nov 27 06:36:12.079: INFO: (18) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">... (200; 182.169813ms)
Nov 27 06:36:12.084: INFO: (18) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/rewriteme">test</a> (200; 184.420652ms)
Nov 27 06:36:12.084: INFO: (18) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">test<... (200; 184.321896ms)
Nov 27 06:36:12.084: INFO: (18) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 185.141761ms)
Nov 27 06:36:12.084: INFO: (18) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 184.178164ms)
Nov 27 06:36:12.084: INFO: (18) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname2/proxy/: bar (200; 186.042292ms)
Nov 27 06:36:12.085: INFO: (18) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname1/proxy/: foo (200; 183.557275ms)
Nov 27 06:36:12.087: INFO: (18) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:460/proxy/: tls baz (200; 185.983137ms)
Nov 27 06:36:12.087: INFO: (18) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname1/proxy/: tls baz (200; 187.462511ms)
Nov 27 06:36:12.087: INFO: (18) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/tlsrewritem... (200; 186.483268ms)
Nov 27 06:36:12.087: INFO: (18) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname1/proxy/: foo (200; 188.823085ms)
Nov 27 06:36:12.087: INFO: (18) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname2/proxy/: bar (200; 185.795403ms)
Nov 27 06:36:12.093: INFO: (18) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname2/proxy/: tls qux (200; 194.765026ms)
Nov 27 06:36:12.192: INFO: (19) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:462/proxy/: tls qux (200; 96.599315ms)
Nov 27 06:36:12.192: INFO: (19) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 96.085983ms)
Nov 27 06:36:12.195: INFO: (19) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 101.112726ms)
Nov 27 06:36:12.217: INFO: (19) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname1/proxy/: tls baz (200; 119.270992ms)
Nov 27 06:36:12.217: INFO: (19) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:160/proxy/: foo (200; 122.354895ms)
Nov 27 06:36:12.219: INFO: (19) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname2/proxy/: bar (200; 124.145691ms)
Nov 27 06:36:12.219: INFO: (19) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl/proxy/rewriteme">test</a> (200; 120.940543ms)
Nov 27 06:36:12.219: INFO: (19) /api/v1/namespaces/proxy-9199/services/proxy-service-sjv49:portname1/proxy/: foo (200; 124.802267ms)
Nov 27 06:36:12.219: INFO: (19) /api/v1/namespaces/proxy-9199/services/https:proxy-service-sjv49:tlsportname2/proxy/: tls qux (200; 123.522359ms)
Nov 27 06:36:12.219: INFO: (19) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname2/proxy/: bar (200; 121.937385ms)
Nov 27 06:36:12.219: INFO: (19) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">test<... (200; 121.21432ms)
Nov 27 06:36:12.220: INFO: (19) /api/v1/namespaces/proxy-9199/services/http:proxy-service-sjv49:portname1/proxy/: foo (200; 122.48934ms)
Nov 27 06:36:12.220: INFO: (19) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:460/proxy/: tls baz (200; 123.680759ms)
Nov 27 06:36:12.206: INFO: (19) /api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/http:proxy-service-sjv49-q6wvl:1080/proxy/rewriteme">... (200; 111.176478ms)
Nov 27 06:36:12.218: INFO: (19) /api/v1/namespaces/proxy-9199/pods/proxy-service-sjv49-q6wvl:162/proxy/: bar (200; 119.232369ms)
Nov 27 06:36:12.226: INFO: (19) /api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/: <a href="/api/v1/namespaces/proxy-9199/pods/https:proxy-service-sjv49-q6wvl:443/proxy/tlsrewritem... (200; 127.176305ms)
STEP: deleting ReplicationController proxy-service-sjv49 in namespace proxy-9199, will wait for the garbage collector to delete the pods
I1127 06:36:12.387266      19 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 06:36:12.387441      19 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Nov 27 06:36:12.583: INFO: Deleting ReplicationController proxy-service-sjv49 took: 145.889458ms
Nov 27 06:36:13.090: INFO: Terminating ReplicationController proxy-service-sjv49 pods took: 507.067936ms
I1127 06:36:13.090227      19 controller_utils.go:810] Ignoring inactive pod proxy-9199/proxy-service-sjv49-q6wvl in state Running, deletion time 2019-11-27 06:36:14 +0000 UTC
[AfterEach] version v1
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:36:19.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9199" for this suite.
Nov 27 06:36:25.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:36:25.498: INFO: namespace proxy-9199 deletion completed in 6.294108536s

• [SLOW TEST:20.759 seconds]
[sig-network] Proxy
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:36:25.499: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9068
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Nov 27 06:36:30.609: INFO: Successfully updated pod "annotationupdate32536348-e873-4a26-b94d-c50a3fa0c564"
[AfterEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:36:34.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9068" for this suite.
Nov 27 06:37:02.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:37:02.990: INFO: namespace downward-api-9068 deletion completed in 28.271253336s

• [SLOW TEST:37.490 seconds]
[sig-storage] Downward API volume
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:37:02.993: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5134
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 27 06:37:03.389: INFO: Number of nodes with available pods: 0
Nov 27 06:37:03.389: INFO: Node master1 is running more than one daemon pod
Nov 27 06:37:04.417: INFO: Number of nodes with available pods: 0
Nov 27 06:37:04.417: INFO: Node master1 is running more than one daemon pod
Nov 27 06:37:05.417: INFO: Number of nodes with available pods: 0
Nov 27 06:37:05.417: INFO: Node master1 is running more than one daemon pod
Nov 27 06:37:06.417: INFO: Number of nodes with available pods: 0
Nov 27 06:37:06.417: INFO: Node master1 is running more than one daemon pod
Nov 27 06:37:07.411: INFO: Number of nodes with available pods: 2
Nov 27 06:37:07.412: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Nov 27 06:37:07.476: INFO: Number of nodes with available pods: 1
Nov 27 06:37:07.477: INFO: Node master1 is running more than one daemon pod
Nov 27 06:37:08.500: INFO: Number of nodes with available pods: 1
Nov 27 06:37:08.501: INFO: Node master1 is running more than one daemon pod
Nov 27 06:37:09.500: INFO: Number of nodes with available pods: 1
Nov 27 06:37:09.500: INFO: Node master1 is running more than one daemon pod
Nov 27 06:37:10.498: INFO: Number of nodes with available pods: 1
Nov 27 06:37:10.498: INFO: Node master1 is running more than one daemon pod
Nov 27 06:37:11.503: INFO: Number of nodes with available pods: 1
Nov 27 06:37:11.503: INFO: Node master1 is running more than one daemon pod
Nov 27 06:37:12.501: INFO: Number of nodes with available pods: 1
Nov 27 06:37:12.502: INFO: Node master1 is running more than one daemon pod
Nov 27 06:37:13.500: INFO: Number of nodes with available pods: 1
Nov 27 06:37:13.500: INFO: Node master1 is running more than one daemon pod
Nov 27 06:37:14.499: INFO: Number of nodes with available pods: 1
Nov 27 06:37:14.499: INFO: Node master1 is running more than one daemon pod
Nov 27 06:37:15.501: INFO: Number of nodes with available pods: 1
Nov 27 06:37:15.502: INFO: Node master1 is running more than one daemon pod
Nov 27 06:37:16.501: INFO: Number of nodes with available pods: 1
Nov 27 06:37:16.502: INFO: Node master1 is running more than one daemon pod
Nov 27 06:37:17.499: INFO: Number of nodes with available pods: 1
Nov 27 06:37:17.499: INFO: Node master1 is running more than one daemon pod
Nov 27 06:37:18.503: INFO: Number of nodes with available pods: 1
Nov 27 06:37:18.503: INFO: Node master1 is running more than one daemon pod
Nov 27 06:37:19.502: INFO: Number of nodes with available pods: 1
Nov 27 06:37:19.502: INFO: Node master1 is running more than one daemon pod
Nov 27 06:37:20.498: INFO: Number of nodes with available pods: 1
Nov 27 06:37:20.498: INFO: Node master1 is running more than one daemon pod
Nov 27 06:37:21.517: INFO: Number of nodes with available pods: 1
Nov 27 06:37:21.517: INFO: Node master1 is running more than one daemon pod
Nov 27 06:37:22.503: INFO: Number of nodes with available pods: 1
Nov 27 06:37:22.504: INFO: Node master1 is running more than one daemon pod
Nov 27 06:37:23.503: INFO: Number of nodes with available pods: 2
Nov 27 06:37:23.503: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5134, will wait for the garbage collector to delete the pods
I1127 06:37:23.520450      19 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I1127 06:37:23.520855      19 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Nov 27 06:37:23.588: INFO: Deleting DaemonSet.extensions daemon-set took: 17.567765ms
Nov 27 06:37:24.089: INFO: Terminating DaemonSet.extensions daemon-set pods took: 501.118393ms
I1127 06:37:24.089005      19 controller_utils.go:810] Ignoring inactive pod daemonsets-5134/daemon-set-6r5bt in state Running, deletion time 2019-11-27 06:37:53 +0000 UTC
I1127 06:37:24.089139      19 controller_utils.go:810] Ignoring inactive pod daemonsets-5134/daemon-set-x2r4z in state Running, deletion time 2019-11-27 06:37:53 +0000 UTC
Nov 27 06:37:39.201: INFO: Number of nodes with available pods: 0
Nov 27 06:37:39.201: INFO: Number of running nodes: 0, number of available pods: 0
Nov 27 06:37:39.208: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5134/daemonsets","resourceVersion":"973764"},"items":null}

Nov 27 06:37:39.214: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5134/pods","resourceVersion":"973764"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:37:39.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5134" for this suite.
Nov 27 06:37:45.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:37:45.535: INFO: namespace daemonsets-5134 deletion completed in 6.277077505s

• [SLOW TEST:42.543 seconds]
[sig-apps] Daemon set [Serial]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:37:45.536: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-7076
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
Nov 27 06:37:45.861: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 27 06:38:45.930: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 06:38:45.940: INFO: Starting informer...
STEP: Starting pod...
I1127 06:38:45.942845      19 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/e2e/scheduling/taints.go:146
I1127 06:38:45.942946      19 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/e2e/scheduling/taints.go:146
Nov 27 06:38:46.178: INFO: Pod is running on slave1. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Nov 27 06:38:46.237: INFO: Pod wasn't evicted. Proceeding
Nov 27 06:38:46.237: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Nov 27 06:40:01.408: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:40:01.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-7076" for this suite.
Nov 27 06:40:29.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:40:29.722: INFO: namespace taint-single-pod-7076 deletion completed in 28.294903468s

• [SLOW TEST:164.186 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:40:29.724: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3837
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-ed195135-9306-4416-9a6d-00d6f077207c
STEP: Creating a pod to test consume secrets
Nov 27 06:40:30.055: INFO: Waiting up to 5m0s for pod "pod-secrets-feb7b352-8808-4842-aac4-0b43bf8b679a" in namespace "secrets-3837" to be "success or failure"
Nov 27 06:40:30.062: INFO: Pod "pod-secrets-feb7b352-8808-4842-aac4-0b43bf8b679a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.670192ms
Nov 27 06:40:32.071: INFO: Pod "pod-secrets-feb7b352-8808-4842-aac4-0b43bf8b679a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016109147s
Nov 27 06:40:34.083: INFO: Pod "pod-secrets-feb7b352-8808-4842-aac4-0b43bf8b679a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027909219s
STEP: Saw pod success
Nov 27 06:40:34.083: INFO: Pod "pod-secrets-feb7b352-8808-4842-aac4-0b43bf8b679a" satisfied condition "success or failure"
Nov 27 06:40:34.090: INFO: Trying to get logs from node slave1 pod pod-secrets-feb7b352-8808-4842-aac4-0b43bf8b679a container secret-volume-test: <nil>
STEP: delete the pod
Nov 27 06:40:34.383: INFO: Waiting for pod pod-secrets-feb7b352-8808-4842-aac4-0b43bf8b679a to disappear
Nov 27 06:40:34.391: INFO: Pod pod-secrets-feb7b352-8808-4842-aac4-0b43bf8b679a no longer exists
[AfterEach] [sig-storage] Secrets
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:40:34.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3837" for this suite.
Nov 27 06:40:40.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:40:40.680: INFO: namespace secrets-3837 deletion completed in 6.277422663s

• [SLOW TEST:10.957 seconds]
[sig-storage] Secrets
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:40:40.681: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9295
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 27 06:41:24.297: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 27 06:41:26.330: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433684, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433684, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433684, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433684, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 06:41:28.340: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433684, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433684, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433684, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433684, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 27 06:41:31.369: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:41:32.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9295" for this suite.
Nov 27 06:41:38.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:41:39.054: INFO: namespace webhook-9295 deletion completed in 6.3456795s
STEP: Destroying namespace "webhook-9295-markers" for this suite.
Nov 27 06:41:45.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:41:45.341: INFO: namespace webhook-9295-markers deletion completed in 6.286267656s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:64.700 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:41:45.387: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7484
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-737f765e-65da-40d3-8006-bfa239426ad2
STEP: Creating secret with name secret-projected-all-test-volume-59384ea4-a0ce-49c8-99fd-36e35182226c
STEP: Creating a pod to test Check all projections for projected volume plugin
Nov 27 06:41:45.735: INFO: Waiting up to 5m0s for pod "projected-volume-9a8bffce-d284-4031-89be-603324796746" in namespace "projected-7484" to be "success or failure"
Nov 27 06:41:45.748: INFO: Pod "projected-volume-9a8bffce-d284-4031-89be-603324796746": Phase="Pending", Reason="", readiness=false. Elapsed: 12.744868ms
Nov 27 06:41:47.757: INFO: Pod "projected-volume-9a8bffce-d284-4031-89be-603324796746": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02231658s
Nov 27 06:41:49.767: INFO: Pod "projected-volume-9a8bffce-d284-4031-89be-603324796746": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032474471s
STEP: Saw pod success
Nov 27 06:41:49.768: INFO: Pod "projected-volume-9a8bffce-d284-4031-89be-603324796746" satisfied condition "success or failure"
Nov 27 06:41:49.774: INFO: Trying to get logs from node slave1 pod projected-volume-9a8bffce-d284-4031-89be-603324796746 container projected-all-volume-test: <nil>
STEP: delete the pod
Nov 27 06:41:49.853: INFO: Waiting for pod projected-volume-9a8bffce-d284-4031-89be-603324796746 to disappear
Nov 27 06:41:49.860: INFO: Pod projected-volume-9a8bffce-d284-4031-89be-603324796746 no longer exists
[AfterEach] [sig-storage] Projected combined
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:41:49.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7484" for this suite.
Nov 27 06:41:55.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:41:56.160: INFO: namespace projected-7484 deletion completed in 6.285161299s

• [SLOW TEST:10.774 seconds]
[sig-storage] Projected combined
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:41:56.161: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4554
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4554.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4554.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 27 06:42:00.611: INFO: DNS probes using dns-4554/dns-test-53da186a-36de-407e-972c-472f8fe0f310 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:42:00.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4554" for this suite.
Nov 27 06:42:06.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:42:06.924: INFO: namespace dns-4554 deletion completed in 6.269680604s

• [SLOW TEST:10.763 seconds]
[sig-network] DNS
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:42:06.927: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6295
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Nov 27 06:42:07.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 api-versions'
Nov 27 06:42:07.957: INFO: stderr: ""
Nov 27 06:42:07.958: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd-publish-openapi-test-multi-ver.example.com/v2\ncrd-publish-openapi-test-multi-ver.example.com/v3\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:42:07.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6295" for this suite.
Nov 27 06:42:14.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:42:14.256: INFO: namespace kubectl-6295 deletion completed in 6.277774175s

• [SLOW TEST:7.329 seconds]
[sig-cli] Kubectl client
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:42:14.263: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5646
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-37036480-4267-4761-b307-8e8b142d276c
STEP: Creating a pod to test consume secrets
Nov 27 06:42:14.673: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e986ff92-81c4-4229-9213-24eb0fc73cec" in namespace "projected-5646" to be "success or failure"
Nov 27 06:42:14.686: INFO: Pod "pod-projected-secrets-e986ff92-81c4-4229-9213-24eb0fc73cec": Phase="Pending", Reason="", readiness=false. Elapsed: 12.377712ms
Nov 27 06:42:16.698: INFO: Pod "pod-projected-secrets-e986ff92-81c4-4229-9213-24eb0fc73cec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02496783s
Nov 27 06:42:18.707: INFO: Pod "pod-projected-secrets-e986ff92-81c4-4229-9213-24eb0fc73cec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033145228s
STEP: Saw pod success
Nov 27 06:42:18.707: INFO: Pod "pod-projected-secrets-e986ff92-81c4-4229-9213-24eb0fc73cec" satisfied condition "success or failure"
Nov 27 06:42:18.714: INFO: Trying to get logs from node slave1 pod pod-projected-secrets-e986ff92-81c4-4229-9213-24eb0fc73cec container secret-volume-test: <nil>
STEP: delete the pod
Nov 27 06:42:18.764: INFO: Waiting for pod pod-projected-secrets-e986ff92-81c4-4229-9213-24eb0fc73cec to disappear
Nov 27 06:42:18.772: INFO: Pod pod-projected-secrets-e986ff92-81c4-4229-9213-24eb0fc73cec no longer exists
[AfterEach] [sig-storage] Projected secret
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:42:18.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5646" for this suite.
Nov 27 06:42:24.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:42:25.073: INFO: namespace projected-5646 deletion completed in 6.288409972s

• [SLOW TEST:10.811 seconds]
[sig-storage] Projected secret
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:42:25.079: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-732
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:42:36.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-732" for this suite.
Nov 27 06:42:42.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:42:42.801: INFO: namespace resourcequota-732 deletion completed in 6.292457002s

• [SLOW TEST:17.723 seconds]
[sig-api-machinery] ResourceQuota
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:42:42.805: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3084
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 27 06:42:57.764: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 27 06:42:59.801: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433777, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433777, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433777, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433777, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 27 06:43:01.816: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433777, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433777, loc:(*time.Location)(0x1283e4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433777, loc:(*time.Location)(0x1283e4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710433777, loc:(*time.Location)(0x1283e4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 27 06:43:04.829: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:43:05.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3084" for this suite.
Nov 27 06:43:13.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:43:13.678: INFO: namespace webhook-3084 deletion completed in 8.257575432s
STEP: Destroying namespace "webhook-3084-markers" for this suite.
Nov 27 06:43:19.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:43:19.967: INFO: namespace webhook-3084-markers deletion completed in 6.288866595s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:37.203 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:43:20.008: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-8944
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
Nov 27 06:43:20.318: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 27 06:44:20.407: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 06:44:20.415: INFO: Starting informer...
STEP: Starting pods...
I1127 06:44:20.417287      19 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/e2e/scheduling/taints.go:146
I1127 06:44:20.417386      19 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/e2e/scheduling/taints.go:146
Nov 27 06:44:20.663: INFO: Pod1 is running on slave1. Tainting Node
Nov 27 06:44:26.915: INFO: Pod2 is running on slave1. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Nov 27 06:44:43.587: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Nov 27 06:45:03.594: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:45:03.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-8944" for this suite.
Nov 27 06:45:09.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:45:09.944: INFO: namespace taint-multiple-pods-8944 deletion completed in 6.278915244s

• [SLOW TEST:109.937 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:45:09.945: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3034
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Nov 27 06:45:10.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 create -f - --namespace=kubectl-3034'
Nov 27 06:45:11.575: INFO: stderr: ""
Nov 27 06:45:11.575: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Nov 27 06:45:12.592: INFO: Selector matched 1 pods for map[app:redis]
Nov 27 06:45:12.592: INFO: Found 0 / 1
Nov 27 06:45:13.594: INFO: Selector matched 1 pods for map[app:redis]
Nov 27 06:45:13.594: INFO: Found 0 / 1
Nov 27 06:45:14.591: INFO: Selector matched 1 pods for map[app:redis]
Nov 27 06:45:14.592: INFO: Found 0 / 1
Nov 27 06:45:15.602: INFO: Selector matched 1 pods for map[app:redis]
Nov 27 06:45:15.602: INFO: Found 1 / 1
Nov 27 06:45:15.602: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Nov 27 06:45:15.610: INFO: Selector matched 1 pods for map[app:redis]
Nov 27 06:45:15.610: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 27 06:45:15.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192585042 patch pod redis-master-94vtf --namespace=kubectl-3034 -p {"metadata":{"annotations":{"x":"y"}}}'
Nov 27 06:45:16.386: INFO: stderr: ""
Nov 27 06:45:16.386: INFO: stdout: "pod/redis-master-94vtf patched\n"
STEP: checking annotations
Nov 27 06:45:16.402: INFO: Selector matched 1 pods for map[app:redis]
Nov 27 06:45:16.402: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:45:16.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3034" for this suite.
Nov 27 06:45:44.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:45:44.713: INFO: namespace kubectl-3034 deletion completed in 28.296174651s

• [SLOW TEST:34.768 seconds]
[sig-cli] Kubectl client
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:45:44.719: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1007
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 27 06:45:49.145: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:45:49.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1007" for this suite.
Nov 27 06:45:55.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:45:55.520: INFO: namespace container-runtime-1007 deletion completed in 6.317805029s

• [SLOW TEST:10.801 seconds]
[k8s.io] Container Runtime
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:45:55.521: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6039
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-14849c0b-c55a-4fa9-a221-f11fa501e774
Nov 27 06:45:55.863: INFO: Pod name my-hostname-basic-14849c0b-c55a-4fa9-a221-f11fa501e774: Found 0 pods out of 1
Nov 27 06:46:00.879: INFO: Pod name my-hostname-basic-14849c0b-c55a-4fa9-a221-f11fa501e774: Found 1 pods out of 1
Nov 27 06:46:00.879: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-14849c0b-c55a-4fa9-a221-f11fa501e774" are running
Nov 27 06:46:00.888: INFO: Pod "my-hostname-basic-14849c0b-c55a-4fa9-a221-f11fa501e774-xqkls" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-27 06:45:55 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-27 06:45:59 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-27 06:45:59 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-27 06:45:55 +0000 UTC Reason: Message:}])
Nov 27 06:46:00.888: INFO: Trying to dial the pod
I1127 06:46:03.954775      19 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 10 items received
Nov 27 06:46:05.917: INFO: Controller my-hostname-basic-14849c0b-c55a-4fa9-a221-f11fa501e774: Got expected result from replica 1 [my-hostname-basic-14849c0b-c55a-4fa9-a221-f11fa501e774-xqkls]: "my-hostname-basic-14849c0b-c55a-4fa9-a221-f11fa501e774-xqkls", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:46:05.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6039" for this suite.
Nov 27 06:46:11.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:46:12.193: INFO: namespace replication-controller-6039 deletion completed in 6.262722739s

• [SLOW TEST:16.672 seconds]
[sig-apps] ReplicationController
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:46:12.199: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-593
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Nov 27 06:46:12.530: INFO: Waiting up to 5m0s for pod "var-expansion-e040baa9-e24f-4985-8a62-d4465a2e18da" in namespace "var-expansion-593" to be "success or failure"
Nov 27 06:46:12.539: INFO: Pod "var-expansion-e040baa9-e24f-4985-8a62-d4465a2e18da": Phase="Pending", Reason="", readiness=false. Elapsed: 9.15007ms
Nov 27 06:46:14.549: INFO: Pod "var-expansion-e040baa9-e24f-4985-8a62-d4465a2e18da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019196095s
Nov 27 06:46:16.558: INFO: Pod "var-expansion-e040baa9-e24f-4985-8a62-d4465a2e18da": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027977848s
Nov 27 06:46:18.570: INFO: Pod "var-expansion-e040baa9-e24f-4985-8a62-d4465a2e18da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039751611s
STEP: Saw pod success
Nov 27 06:46:18.571: INFO: Pod "var-expansion-e040baa9-e24f-4985-8a62-d4465a2e18da" satisfied condition "success or failure"
Nov 27 06:46:18.579: INFO: Trying to get logs from node slave1 pod var-expansion-e040baa9-e24f-4985-8a62-d4465a2e18da container dapi-container: <nil>
STEP: delete the pod
Nov 27 06:46:18.851: INFO: Waiting for pod var-expansion-e040baa9-e24f-4985-8a62-d4465a2e18da to disappear
Nov 27 06:46:18.857: INFO: Pod var-expansion-e040baa9-e24f-4985-8a62-d4465a2e18da no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:46:18.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-593" for this suite.
Nov 27 06:46:24.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:46:25.132: INFO: namespace var-expansion-593 deletion completed in 6.263284962s

• [SLOW TEST:12.934 seconds]
[k8s.io] Variable Expansion
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:46:25.137: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-426
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-426
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-426 to expose endpoints map[]
Nov 27 06:46:25.495: INFO: Get endpoints failed (20.843348ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Nov 27 06:46:26.508: INFO: successfully validated that service multi-endpoint-test in namespace services-426 exposes endpoints map[] (1.034578874s elapsed)
STEP: Creating pod pod1 in namespace services-426
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-426 to expose endpoints map[pod1:[100]]
Nov 27 06:46:30.632: INFO: successfully validated that service multi-endpoint-test in namespace services-426 exposes endpoints map[pod1:[100]] (4.102549751s elapsed)
STEP: Creating pod pod2 in namespace services-426
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-426 to expose endpoints map[pod1:[100] pod2:[101]]
Nov 27 06:46:34.779: INFO: Unexpected endpoints: found map[e8736d04-5ba8-49e5-a8d9-0a1a38b07274:[100]], expected map[pod1:[100] pod2:[101]] (4.133135706s elapsed, will retry)
Nov 27 06:46:35.802: INFO: successfully validated that service multi-endpoint-test in namespace services-426 exposes endpoints map[pod1:[100] pod2:[101]] (5.156228592s elapsed)
STEP: Deleting pod pod1 in namespace services-426
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-426 to expose endpoints map[pod2:[101]]
Nov 27 06:46:36.897: INFO: successfully validated that service multi-endpoint-test in namespace services-426 exposes endpoints map[pod2:[101]] (1.078095977s elapsed)
STEP: Deleting pod pod2 in namespace services-426
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-426 to expose endpoints map[]
Nov 27 06:46:37.930: INFO: successfully validated that service multi-endpoint-test in namespace services-426 exposes endpoints map[] (1.018881585s elapsed)
[AfterEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:46:37.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-426" for this suite.
Nov 27 06:47:06.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:47:06.262: INFO: namespace services-426 deletion completed in 28.26739208s
[AfterEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:41.125 seconds]
[sig-network] Services
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:47:06.263: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9736
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 27 06:47:06.565: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:47:07.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9736" for this suite.
Nov 27 06:47:13.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:47:13.451: INFO: namespace custom-resource-definition-9736 deletion completed in 6.270554762s

• [SLOW TEST:7.189 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:47:13.452: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-813
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:47:26.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-813" for this suite.
Nov 27 06:47:32.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:47:33.223: INFO: namespace resourcequota-813 deletion completed in 6.280179055s

• [SLOW TEST:19.771 seconds]
[sig-api-machinery] ResourceQuota
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:47:33.223: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4464
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Nov 27 06:47:33.519: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:47:38.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4464" for this suite.
Nov 27 06:47:44.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:47:44.873: INFO: namespace init-container-4464 deletion completed in 6.278744162s

• [SLOW TEST:11.649 seconds]
[k8s.io] InitContainer [NodeConformance]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:47:44.875: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7945
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-266fb053-f61d-43ca-8ede-bac84a4f47c9
STEP: Creating configMap with name cm-test-opt-upd-7bedb959-ddbd-482e-a9ad-cb3a2de1bd0f
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-266fb053-f61d-43ca-8ede-bac84a4f47c9
STEP: Updating configmap cm-test-opt-upd-7bedb959-ddbd-482e-a9ad-cb3a2de1bd0f
STEP: Creating configMap with name cm-test-opt-create-25bafacd-db0c-4dd2-a004-4b52d68d691b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:47:53.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7945" for this suite.
Nov 27 06:48:21.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:48:22.047: INFO: namespace projected-7945 deletion completed in 28.284741817s

• [SLOW TEST:37.172 seconds]
[sig-storage] Projected configMap
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:48:22.047: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1583
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 27 06:48:22.375: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ba92b1e5-5287-40b1-9741-1cbeeb659654" in namespace "projected-1583" to be "success or failure"
Nov 27 06:48:22.383: INFO: Pod "downwardapi-volume-ba92b1e5-5287-40b1-9741-1cbeeb659654": Phase="Pending", Reason="", readiness=false. Elapsed: 7.992867ms
Nov 27 06:48:24.392: INFO: Pod "downwardapi-volume-ba92b1e5-5287-40b1-9741-1cbeeb659654": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016238797s
Nov 27 06:48:26.404: INFO: Pod "downwardapi-volume-ba92b1e5-5287-40b1-9741-1cbeeb659654": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028253583s
Nov 27 06:48:28.413: INFO: Pod "downwardapi-volume-ba92b1e5-5287-40b1-9741-1cbeeb659654": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03708667s
STEP: Saw pod success
Nov 27 06:48:28.413: INFO: Pod "downwardapi-volume-ba92b1e5-5287-40b1-9741-1cbeeb659654" satisfied condition "success or failure"
Nov 27 06:48:28.420: INFO: Trying to get logs from node slave1 pod downwardapi-volume-ba92b1e5-5287-40b1-9741-1cbeeb659654 container client-container: <nil>
STEP: delete the pod
Nov 27 06:48:28.476: INFO: Waiting for pod downwardapi-volume-ba92b1e5-5287-40b1-9741-1cbeeb659654 to disappear
Nov 27 06:48:28.485: INFO: Pod downwardapi-volume-ba92b1e5-5287-40b1-9741-1cbeeb659654 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:48:28.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1583" for this suite.
Nov 27 06:48:34.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:48:34.785: INFO: namespace projected-1583 deletion completed in 6.284947202s

• [SLOW TEST:12.738 seconds]
[sig-storage] Projected downwardAPI
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:48:34.791: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9661
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Nov 27 06:48:45.313: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:48:45.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1127 06:48:45.313564      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-9661" for this suite.
Nov 27 06:48:53.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:48:53.647: INFO: namespace gc-9661 deletion completed in 8.320761167s

• [SLOW TEST:18.857 seconds]
[sig-api-machinery] Garbage collector
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:48:53.652: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5820
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:48:53.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5820" for this suite.
Nov 27 06:49:00.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:49:00.313: INFO: namespace services-5820 deletion completed in 6.30233703s
[AfterEach] [sig-network] Services
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:6.661 seconds]
[sig-network] Services
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:49:00.314: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8568
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov 27 06:49:00.653: INFO: Waiting up to 5m0s for pod "pod-1dcb1b5b-fa30-484e-b28d-80a45053c54d" in namespace "emptydir-8568" to be "success or failure"
Nov 27 06:49:00.662: INFO: Pod "pod-1dcb1b5b-fa30-484e-b28d-80a45053c54d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.584914ms
Nov 27 06:49:02.676: INFO: Pod "pod-1dcb1b5b-fa30-484e-b28d-80a45053c54d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022465305s
Nov 27 06:49:04.687: INFO: Pod "pod-1dcb1b5b-fa30-484e-b28d-80a45053c54d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033504265s
Nov 27 06:49:06.700: INFO: Pod "pod-1dcb1b5b-fa30-484e-b28d-80a45053c54d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046482164s
STEP: Saw pod success
Nov 27 06:49:06.700: INFO: Pod "pod-1dcb1b5b-fa30-484e-b28d-80a45053c54d" satisfied condition "success or failure"
Nov 27 06:49:06.709: INFO: Trying to get logs from node slave1 pod pod-1dcb1b5b-fa30-484e-b28d-80a45053c54d container test-container: <nil>
STEP: delete the pod
Nov 27 06:49:06.786: INFO: Waiting for pod pod-1dcb1b5b-fa30-484e-b28d-80a45053c54d to disappear
Nov 27 06:49:06.797: INFO: Pod pod-1dcb1b5b-fa30-484e-b28d-80a45053c54d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:49:06.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8568" for this suite.
Nov 27 06:49:12.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:49:13.110: INFO: namespace emptydir-8568 deletion completed in 6.299846623s

• [SLOW TEST:12.797 seconds]
[sig-storage] EmptyDir volumes
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:49:13.114: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2687
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-4ea0919a-4ac2-4671-ad85-fb6704aef63a
STEP: Creating a pod to test consume secrets
Nov 27 06:49:13.446: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a041cd12-837f-4aea-bba2-e6b5c5c02136" in namespace "projected-2687" to be "success or failure"
Nov 27 06:49:13.457: INFO: Pod "pod-projected-secrets-a041cd12-837f-4aea-bba2-e6b5c5c02136": Phase="Pending", Reason="", readiness=false. Elapsed: 10.541941ms
Nov 27 06:49:15.468: INFO: Pod "pod-projected-secrets-a041cd12-837f-4aea-bba2-e6b5c5c02136": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021535435s
Nov 27 06:49:17.482: INFO: Pod "pod-projected-secrets-a041cd12-837f-4aea-bba2-e6b5c5c02136": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03627725s
STEP: Saw pod success
Nov 27 06:49:17.482: INFO: Pod "pod-projected-secrets-a041cd12-837f-4aea-bba2-e6b5c5c02136" satisfied condition "success or failure"
Nov 27 06:49:17.492: INFO: Trying to get logs from node slave1 pod pod-projected-secrets-a041cd12-837f-4aea-bba2-e6b5c5c02136 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 27 06:49:17.552: INFO: Waiting for pod pod-projected-secrets-a041cd12-837f-4aea-bba2-e6b5c5c02136 to disappear
Nov 27 06:49:17.562: INFO: Pod pod-projected-secrets-a041cd12-837f-4aea-bba2-e6b5c5c02136 no longer exists
[AfterEach] [sig-storage] Projected secret
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:49:17.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2687" for this suite.
Nov 27 06:49:23.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:49:23.860: INFO: namespace projected-2687 deletion completed in 6.280626879s

• [SLOW TEST:10.746 seconds]
[sig-storage] Projected secret
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:49:23.862: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7687
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7687.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7687.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7687.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7687.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7687.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7687.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 27 06:49:28.277: INFO: Unable to read wheezy_hosts@dns-querier-2 from pod dns-7687/dns-test-d8d44d8d-53a9-49b4-b3c1-d010388d2e96: the server could not find the requested resource (get pods dns-test-d8d44d8d-53a9-49b4-b3c1-d010388d2e96)
Nov 27 06:49:28.288: INFO: Unable to read wheezy_udp@PodARecord from pod dns-7687/dns-test-d8d44d8d-53a9-49b4-b3c1-d010388d2e96: the server could not find the requested resource (get pods dns-test-d8d44d8d-53a9-49b4-b3c1-d010388d2e96)
Nov 27 06:49:28.299: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-7687/dns-test-d8d44d8d-53a9-49b4-b3c1-d010388d2e96: the server could not find the requested resource (get pods dns-test-d8d44d8d-53a9-49b4-b3c1-d010388d2e96)
Nov 27 06:49:28.348: INFO: Lookups using dns-7687/dns-test-d8d44d8d-53a9-49b4-b3c1-d010388d2e96 failed for: [wheezy_hosts@dns-querier-2 wheezy_udp@PodARecord wheezy_tcp@PodARecord]

Nov 27 06:49:33.442: INFO: DNS probes using dns-7687/dns-test-d8d44d8d-53a9-49b4-b3c1-d010388d2e96 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:49:33.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7687" for this suite.
Nov 27 06:49:39.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:49:39.851: INFO: namespace dns-7687 deletion completed in 6.310124075s

• [SLOW TEST:15.989 seconds]
[sig-network] DNS
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:49:39.852: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6168
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Nov 27 06:49:40.212: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6168 /api/v1/namespaces/watch-6168/configmaps/e2e-watch-test-label-changed 23692007-d1b5-4827-ae8b-c766bd2081cc 976096 0 2019-11-27 06:49:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov 27 06:49:40.213: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6168 /api/v1/namespaces/watch-6168/configmaps/e2e-watch-test-label-changed 23692007-d1b5-4827-ae8b-c766bd2081cc 976097 0 2019-11-27 06:49:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Nov 27 06:49:40.213: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6168 /api/v1/namespaces/watch-6168/configmaps/e2e-watch-test-label-changed 23692007-d1b5-4827-ae8b-c766bd2081cc 976098 0 2019-11-27 06:49:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Nov 27 06:49:50.289: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6168 /api/v1/namespaces/watch-6168/configmaps/e2e-watch-test-label-changed 23692007-d1b5-4827-ae8b-c766bd2081cc 976113 0 2019-11-27 06:49:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov 27 06:49:50.290: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6168 /api/v1/namespaces/watch-6168/configmaps/e2e-watch-test-label-changed 23692007-d1b5-4827-ae8b-c766bd2081cc 976114 0 2019-11-27 06:49:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Nov 27 06:49:50.290: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6168 /api/v1/namespaces/watch-6168/configmaps/e2e-watch-test-label-changed 23692007-d1b5-4827-ae8b-c766bd2081cc 976115 0 2019-11-27 06:49:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:49:50.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6168" for this suite.
Nov 27 06:49:56.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:49:56.619: INFO: namespace watch-6168 deletion completed in 6.317551785s

• [SLOW TEST:16.768 seconds]
[sig-api-machinery] Watchers
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:49:56.621: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8345
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-d36a5b58-47dc-4950-9b62-7a379d46f525
STEP: Creating a pod to test consume secrets
Nov 27 06:49:56.960: INFO: Waiting up to 5m0s for pod "pod-secrets-c09aa670-aae4-41e7-8010-dfbfc4ba8473" in namespace "secrets-8345" to be "success or failure"
Nov 27 06:49:56.970: INFO: Pod "pod-secrets-c09aa670-aae4-41e7-8010-dfbfc4ba8473": Phase="Pending", Reason="", readiness=false. Elapsed: 10.490119ms
Nov 27 06:49:58.981: INFO: Pod "pod-secrets-c09aa670-aae4-41e7-8010-dfbfc4ba8473": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021174145s
Nov 27 06:50:00.993: INFO: Pod "pod-secrets-c09aa670-aae4-41e7-8010-dfbfc4ba8473": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032963552s
Nov 27 06:50:03.002: INFO: Pod "pod-secrets-c09aa670-aae4-41e7-8010-dfbfc4ba8473": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042565175s
STEP: Saw pod success
Nov 27 06:50:03.002: INFO: Pod "pod-secrets-c09aa670-aae4-41e7-8010-dfbfc4ba8473" satisfied condition "success or failure"
Nov 27 06:50:03.010: INFO: Trying to get logs from node slave1 pod pod-secrets-c09aa670-aae4-41e7-8010-dfbfc4ba8473 container secret-volume-test: <nil>
STEP: delete the pod
Nov 27 06:50:03.078: INFO: Waiting for pod pod-secrets-c09aa670-aae4-41e7-8010-dfbfc4ba8473 to disappear
Nov 27 06:50:03.088: INFO: Pod pod-secrets-c09aa670-aae4-41e7-8010-dfbfc4ba8473 no longer exists
[AfterEach] [sig-storage] Secrets
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:50:03.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8345" for this suite.
Nov 27 06:50:09.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:50:09.380: INFO: namespace secrets-8345 deletion completed in 6.277397269s

• [SLOW TEST:12.759 seconds]
[sig-storage] Secrets
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:50:09.382: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1108
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-a9cd6971-ca09-4997-9f07-117d0ee5f33a
[AfterEach] [sig-api-machinery] Secrets
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:50:09.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1108" for this suite.
I1127 06:50:12.431185      19 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 19 items received
Nov 27 06:50:17.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:50:18.037: INFO: namespace secrets-1108 deletion completed in 8.309428202s

• [SLOW TEST:8.655 seconds]
[sig-api-machinery] Secrets
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:50:18.041: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3822
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 27 06:50:18.388: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d189196-bc35-4f1e-8104-a17bbf2a1dbc" in namespace "downward-api-3822" to be "success or failure"
Nov 27 06:50:18.398: INFO: Pod "downwardapi-volume-6d189196-bc35-4f1e-8104-a17bbf2a1dbc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.499627ms
Nov 27 06:50:20.408: INFO: Pod "downwardapi-volume-6d189196-bc35-4f1e-8104-a17bbf2a1dbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019895697s
Nov 27 06:50:22.427: INFO: Pod "downwardapi-volume-6d189196-bc35-4f1e-8104-a17bbf2a1dbc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038602901s
Nov 27 06:50:24.437: INFO: Pod "downwardapi-volume-6d189196-bc35-4f1e-8104-a17bbf2a1dbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048627414s
STEP: Saw pod success
Nov 27 06:50:24.437: INFO: Pod "downwardapi-volume-6d189196-bc35-4f1e-8104-a17bbf2a1dbc" satisfied condition "success or failure"
Nov 27 06:50:24.446: INFO: Trying to get logs from node slave1 pod downwardapi-volume-6d189196-bc35-4f1e-8104-a17bbf2a1dbc container client-container: <nil>
STEP: delete the pod
Nov 27 06:50:24.518: INFO: Waiting for pod downwardapi-volume-6d189196-bc35-4f1e-8104-a17bbf2a1dbc to disappear
Nov 27 06:50:24.527: INFO: Pod downwardapi-volume-6d189196-bc35-4f1e-8104-a17bbf2a1dbc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:50:24.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3822" for this suite.
Nov 27 06:50:30.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:50:30.812: INFO: namespace downward-api-3822 deletion completed in 6.274020994s

• [SLOW TEST:12.772 seconds]
[sig-storage] Downward API volume
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:50:30.815: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2127
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-e5164348-2879-464e-a154-46f0511ac386
STEP: Creating a pod to test consume secrets
Nov 27 06:50:31.154: INFO: Waiting up to 5m0s for pod "pod-secrets-eb85995f-facf-4c69-bda1-066b9cdcbde2" in namespace "secrets-2127" to be "success or failure"
Nov 27 06:50:31.166: INFO: Pod "pod-secrets-eb85995f-facf-4c69-bda1-066b9cdcbde2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.261099ms
Nov 27 06:50:33.179: INFO: Pod "pod-secrets-eb85995f-facf-4c69-bda1-066b9cdcbde2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024918556s
Nov 27 06:50:35.187: INFO: Pod "pod-secrets-eb85995f-facf-4c69-bda1-066b9cdcbde2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032924929s
Nov 27 06:50:37.196: INFO: Pod "pod-secrets-eb85995f-facf-4c69-bda1-066b9cdcbde2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041996595s
STEP: Saw pod success
Nov 27 06:50:37.197: INFO: Pod "pod-secrets-eb85995f-facf-4c69-bda1-066b9cdcbde2" satisfied condition "success or failure"
Nov 27 06:50:37.203: INFO: Trying to get logs from node slave1 pod pod-secrets-eb85995f-facf-4c69-bda1-066b9cdcbde2 container secret-volume-test: <nil>
STEP: delete the pod
Nov 27 06:50:37.253: INFO: Waiting for pod pod-secrets-eb85995f-facf-4c69-bda1-066b9cdcbde2 to disappear
Nov 27 06:50:37.261: INFO: Pod pod-secrets-eb85995f-facf-4c69-bda1-066b9cdcbde2 no longer exists
[AfterEach] [sig-storage] Secrets
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:50:37.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2127" for this suite.
Nov 27 06:50:43.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:50:43.549: INFO: namespace secrets-2127 deletion completed in 6.277548826s

• [SLOW TEST:12.734 seconds]
[sig-storage] Secrets
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:50:43.551: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5197
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Nov 27 06:50:43.844: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 27 06:50:43.877: INFO: Waiting for terminating namespaces to be deleted...
Nov 27 06:50:43.886: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Nov 27 06:50:44.136: INFO: kube-controller-manager-master1 from kube-system started at 2019-11-27 03:11:20 +0000 UTC (1 container statuses recorded)
Nov 27 06:50:44.136: INFO: 	Container kube-controller-manager ready: true, restart count 73
Nov 27 06:50:44.136: INFO: resource-reserver-master1 from kube-system started at 2019-11-27 03:11:20 +0000 UTC (1 container statuses recorded)
Nov 27 06:50:44.136: INFO: 	Container sleep-forever ready: true, restart count 7
Nov 27 06:50:44.136: INFO: dns-autoscaler-799d586fb4-rhnwf from kube-system started at 2019-11-27 06:38:46 +0000 UTC (1 container statuses recorded)
Nov 27 06:50:44.136: INFO: 	Container autoscaler ready: false, restart count 0
Nov 27 06:50:44.136: INFO: kube-apiserver-master1 from kube-system started at 2019-11-27 03:11:20 +0000 UTC (1 container statuses recorded)
Nov 27 06:50:44.137: INFO: 	Container kube-apiserver ready: true, restart count 6
Nov 27 06:50:44.137: INFO: calico-node-cppjk from kube-system started at 2019-11-20 07:23:24 +0000 UTC (1 container statuses recorded)
Nov 27 06:50:44.137: INFO: 	Container calico-node ready: true, restart count 5
Nov 27 06:50:44.137: INFO: coredns-lxl9z from kube-system started at 2019-11-14 04:51:49 +0000 UTC (1 container statuses recorded)
Nov 27 06:50:44.137: INFO: 	Container coredns ready: true, restart count 6
Nov 27 06:50:44.137: INFO: metrics-server-687c949fd7-pf9hm from kube-system started at 2019-11-22 00:26:19 +0000 UTC (1 container statuses recorded)
Nov 27 06:50:44.137: INFO: 	Container metrics-server ready: true, restart count 3
Nov 27 06:50:44.137: INFO: kube-scheduler-master1 from kube-system started at 2019-11-27 03:11:20 +0000 UTC (1 container statuses recorded)
Nov 27 06:50:44.137: INFO: 	Container kube-scheduler ready: true, restart count 64
Nov 27 06:50:44.137: INFO: e2e-conformance-test from conformance started at 2019-11-27 04:04:03 +0000 UTC (1 container statuses recorded)
Nov 27 06:50:44.137: INFO: 	Container conformance-container ready: true, restart count 0
Nov 27 06:50:44.137: INFO: calico-kube-controllers-685dd5746c-7hl8t from kube-system started at 2019-11-27 06:38:46 +0000 UTC (1 container statuses recorded)
Nov 27 06:50:44.137: INFO: 	Container calico-kube-controllers ready: false, restart count 0
Nov 27 06:50:44.137: INFO: kube-proxy-master1 from kube-system started at 2019-11-27 03:11:20 +0000 UTC (1 container statuses recorded)
Nov 27 06:50:44.138: INFO: 	Container kube-proxy ready: true, restart count 14
Nov 27 06:50:44.138: INFO: 
Logging pods the kubelet thinks is on node slave1 before test
Nov 27 06:50:44.162: INFO: calico-node-5nkgf from kube-system started at 2019-11-20 07:23:24 +0000 UTC (1 container statuses recorded)
Nov 27 06:50:44.162: INFO: 	Container calico-node ready: true, restart count 5
Nov 27 06:50:44.162: INFO: kube-proxy-slave1 from kube-system started at 2019-11-27 03:14:35 +0000 UTC (1 container statuses recorded)
Nov 27 06:50:44.162: INFO: 	Container kube-proxy ready: true, restart count 260
Nov 27 06:50:44.162: INFO: nginx-proxy-slave1 from kube-system started at 2019-11-27 03:14:35 +0000 UTC (1 container statuses recorded)
Nov 27 06:50:44.162: INFO: 	Container nginx-proxy ready: true, restart count 15
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node master1
STEP: verifying the node has the label node slave1
Nov 27 06:50:44.290: INFO: Pod e2e-conformance-test requesting resource cpu=0m on Node master1
Nov 27 06:50:44.290: INFO: Pod calico-kube-controllers-685dd5746c-7hl8t requesting resource cpu=30m on Node master1
Nov 27 06:50:44.290: INFO: Pod calico-node-5nkgf requesting resource cpu=150m on Node slave1
Nov 27 06:50:44.290: INFO: Pod calico-node-cppjk requesting resource cpu=150m on Node master1
Nov 27 06:50:44.290: INFO: Pod coredns-lxl9z requesting resource cpu=100m on Node master1
Nov 27 06:50:44.291: INFO: Pod dns-autoscaler-799d586fb4-rhnwf requesting resource cpu=20m on Node master1
Nov 27 06:50:44.291: INFO: Pod kube-apiserver-master1 requesting resource cpu=100m on Node master1
Nov 27 06:50:44.291: INFO: Pod kube-controller-manager-master1 requesting resource cpu=100m on Node master1
Nov 27 06:50:44.291: INFO: Pod kube-proxy-master1 requesting resource cpu=150m on Node master1
Nov 27 06:50:44.291: INFO: Pod kube-proxy-slave1 requesting resource cpu=150m on Node slave1
Nov 27 06:50:44.291: INFO: Pod kube-scheduler-master1 requesting resource cpu=80m on Node master1
Nov 27 06:50:44.291: INFO: Pod metrics-server-687c949fd7-pf9hm requesting resource cpu=0m on Node master1
Nov 27 06:50:44.291: INFO: Pod nginx-proxy-slave1 requesting resource cpu=25m on Node slave1
Nov 27 06:50:44.291: INFO: Pod resource-reserver-master1 requesting resource cpu=800m on Node master1
STEP: Starting Pods to consume most of the cluster CPU.
Nov 27 06:50:44.291: INFO: Creating a pod which consumes cpu=3829m on Node master1
Nov 27 06:50:44.318: INFO: Creating a pod which consumes cpu=4952m on Node slave1
STEP: Creating another pod that requires unavailable amount of CPU.
I1127 06:50:50.373395      19 reflector.go:120] Starting reflector *v1.Event (0s) from k8s.io/kubernetes/test/e2e/common/events.go:136
I1127 06:50:50.373478      19 reflector.go:158] Listing and watching *v1.Event from k8s.io/kubernetes/test/e2e/common/events.go:136
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0e3c7448-abcd-492e-af06-de6a4b9ce439.15daf24fd1bd0993], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5197/filler-pod-0e3c7448-abcd-492e-af06-de6a4b9ce439 to slave1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0e3c7448-abcd-492e-af06-de6a4b9ce439.15daf2507a283260], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0e3c7448-abcd-492e-af06-de6a4b9ce439.15daf2508664ffed], Reason = [Created], Message = [Created container filler-pod-0e3c7448-abcd-492e-af06-de6a4b9ce439]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0e3c7448-abcd-492e-af06-de6a4b9ce439.15daf250a3da4145], Reason = [Started], Message = [Started container filler-pod-0e3c7448-abcd-492e-af06-de6a4b9ce439]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4fa62d20-e240-4c14-b83a-fe1f3e50bacf.15daf24fd04a035c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5197/filler-pod-4fa62d20-e240-4c14-b83a-fe1f3e50bacf to master1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4fa62d20-e240-4c14-b83a-fe1f3e50bacf.15daf25053b2a653], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4fa62d20-e240-4c14-b83a-fe1f3e50bacf.15daf2505a479a9c], Reason = [Created], Message = [Created container filler-pod-4fa62d20-e240-4c14-b83a-fe1f3e50bacf]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4fa62d20-e240-4c14-b83a-fe1f3e50bacf.15daf2506f4c31d0], Reason = [Started], Message = [Started container filler-pod-4fa62d20-e240-4c14-b83a-fe1f3e50bacf]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15daf2513a5990a3], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node master1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node slave1
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:50:51.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5197" for this suite.
Nov 27 06:50:57.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:50:57.904: INFO: namespace sched-pred-5197 deletion completed in 6.377268267s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
I1127 06:50:57.905319      19 request.go:706] Error in request: resource name may not be empty

• [SLOW TEST:14.355 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:50:57.911: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5103
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-8aba6e36-45d1-4c07-a1f1-da81b4ab5172
STEP: Creating a pod to test consume configMaps
Nov 27 06:50:58.274: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-59433626-5d43-4704-8498-c31e28cbc2dd" in namespace "projected-5103" to be "success or failure"
Nov 27 06:50:58.285: INFO: Pod "pod-projected-configmaps-59433626-5d43-4704-8498-c31e28cbc2dd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.920831ms
Nov 27 06:51:00.297: INFO: Pod "pod-projected-configmaps-59433626-5d43-4704-8498-c31e28cbc2dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022265348s
Nov 27 06:51:02.305: INFO: Pod "pod-projected-configmaps-59433626-5d43-4704-8498-c31e28cbc2dd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031063724s
Nov 27 06:51:04.319: INFO: Pod "pod-projected-configmaps-59433626-5d43-4704-8498-c31e28cbc2dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045213004s
STEP: Saw pod success
Nov 27 06:51:04.320: INFO: Pod "pod-projected-configmaps-59433626-5d43-4704-8498-c31e28cbc2dd" satisfied condition "success or failure"
Nov 27 06:51:04.330: INFO: Trying to get logs from node slave1 pod pod-projected-configmaps-59433626-5d43-4704-8498-c31e28cbc2dd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 27 06:51:04.398: INFO: Waiting for pod pod-projected-configmaps-59433626-5d43-4704-8498-c31e28cbc2dd to disappear
Nov 27 06:51:04.406: INFO: Pod pod-projected-configmaps-59433626-5d43-4704-8498-c31e28cbc2dd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:51:04.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5103" for this suite.
Nov 27 06:51:10.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:51:10.789: INFO: namespace projected-5103 deletion completed in 6.366889126s

• [SLOW TEST:12.879 seconds]
[sig-storage] Projected configMap
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 27 06:51:10.790: INFO: >>> kubeConfig: /tmp/kubeconfig-192585042
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6280
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Nov 27 06:51:11.125: INFO: Waiting up to 5m0s for pod "pod-0b55dbd9-00de-48b4-8b0f-8143f92fd020" in namespace "emptydir-6280" to be "success or failure"
Nov 27 06:51:11.134: INFO: Pod "pod-0b55dbd9-00de-48b4-8b0f-8143f92fd020": Phase="Pending", Reason="", readiness=false. Elapsed: 8.717892ms
Nov 27 06:51:13.143: INFO: Pod "pod-0b55dbd9-00de-48b4-8b0f-8143f92fd020": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017677113s
Nov 27 06:51:15.157: INFO: Pod "pod-0b55dbd9-00de-48b4-8b0f-8143f92fd020": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031733504s
STEP: Saw pod success
Nov 27 06:51:15.157: INFO: Pod "pod-0b55dbd9-00de-48b4-8b0f-8143f92fd020" satisfied condition "success or failure"
Nov 27 06:51:15.165: INFO: Trying to get logs from node slave1 pod pod-0b55dbd9-00de-48b4-8b0f-8143f92fd020 container test-container: <nil>
STEP: delete the pod
Nov 27 06:51:15.232: INFO: Waiting for pod pod-0b55dbd9-00de-48b4-8b0f-8143f92fd020 to disappear
Nov 27 06:51:15.238: INFO: Pod pod-0b55dbd9-00de-48b4-8b0f-8143f92fd020 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 27 06:51:15.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6280" for this suite.
I1127 06:51:15.960559      19 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
Nov 27 06:51:21.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 27 06:51:21.529: INFO: namespace emptydir-6280 deletion completed in 6.27754687s

• [SLOW TEST:10.739 seconds]
[sig-storage] EmptyDir volumes
/root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /root/zhanwang/hyperkube/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSNov 27 06:51:21.533: INFO: Running AfterSuite actions on all nodes
Nov 27 06:51:21.533: INFO: Running AfterSuite actions on node 1
Nov 27 06:51:21.533: INFO: Skipping dumping logs from cluster

Ran 276 of 4897 Specs in 10022.273 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Pending | 4621 Skipped
PASS

Ginkgo ran 1 suite in 2h47m15.17249399s
Test Suite Passed
