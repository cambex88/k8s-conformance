I0821 15:39:42.969579      24 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-192619208
I0821 15:39:42.970130      24 e2e.go:92] Starting e2e run "89df089f-3ac2-455a-b770-fd122f035cbd" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1598024381 - Will randomize all specs
Will run 276 of 4897 specs

Aug 21 15:39:42.995: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 15:39:42.999: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 21 15:39:43.060: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 21 15:39:43.131: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 21 15:39:43.131: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Aug 21 15:39:43.131: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 21 15:39:43.155: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Aug 21 15:39:43.156: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
Aug 21 15:39:43.156: INFO: e2e test version: v1.16.2
Aug 21 15:39:43.160: INFO: kube-apiserver version: v1.16.2+554af56
Aug 21 15:39:43.161: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 15:39:43.175: INFO: Cluster IP family: ipv4
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:39:43.177: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename replication-controller
Aug 21 15:39:43.372: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 15:39:43.379: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Aug 21 15:39:44.510: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:39:45.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7490" for this suite.
Aug 21 15:39:53.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:39:55.771: INFO: namespace replication-controller-7490 deletion completed in 10.217019286s

• [SLOW TEST:12.594 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:39:55.772: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-bdd05750-59cd-49c5-9f6d-35c90b1feee2
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:39:55.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2389" for this suite.
Aug 21 15:40:04.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:40:06.215: INFO: namespace secrets-2389 deletion completed in 10.228089445s

• [SLOW TEST:10.443 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:40:06.218: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-6681/secret-test-8e892d88-0e04-472e-a0f9-24d5638fd3a4
STEP: Creating a pod to test consume secrets
Aug 21 15:40:06.528: INFO: Waiting up to 5m0s for pod "pod-configmaps-e9fab79b-9ee3-4a69-8020-00b83e664ddd" in namespace "secrets-6681" to be "success or failure"
Aug 21 15:40:06.538: INFO: Pod "pod-configmaps-e9fab79b-9ee3-4a69-8020-00b83e664ddd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.652831ms
Aug 21 15:40:08.551: INFO: Pod "pod-configmaps-e9fab79b-9ee3-4a69-8020-00b83e664ddd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02358444s
Aug 21 15:40:10.565: INFO: Pod "pod-configmaps-e9fab79b-9ee3-4a69-8020-00b83e664ddd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037435587s
STEP: Saw pod success
Aug 21 15:40:10.565: INFO: Pod "pod-configmaps-e9fab79b-9ee3-4a69-8020-00b83e664ddd" satisfied condition "success or failure"
Aug 21 15:40:10.576: INFO: Trying to get logs from node 10.188.240.202 pod pod-configmaps-e9fab79b-9ee3-4a69-8020-00b83e664ddd container env-test: <nil>
STEP: delete the pod
Aug 21 15:40:10.696: INFO: Waiting for pod pod-configmaps-e9fab79b-9ee3-4a69-8020-00b83e664ddd to disappear
Aug 21 15:40:10.712: INFO: Pod pod-configmaps-e9fab79b-9ee3-4a69-8020-00b83e664ddd no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:40:10.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6681" for this suite.
Aug 21 15:40:18.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:40:20.929: INFO: namespace secrets-6681 deletion completed in 10.194069989s

• [SLOW TEST:14.711 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:40:20.929: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-be25c75d-47ba-48c1-a9be-bc08a9fc4a3c
STEP: Creating a pod to test consume secrets
Aug 21 15:40:21.208: INFO: Waiting up to 5m0s for pod "pod-secrets-a3a67af8-34d6-4abf-a521-5d1a8448fb73" in namespace "secrets-618" to be "success or failure"
Aug 21 15:40:21.219: INFO: Pod "pod-secrets-a3a67af8-34d6-4abf-a521-5d1a8448fb73": Phase="Pending", Reason="", readiness=false. Elapsed: 11.004459ms
Aug 21 15:40:23.231: INFO: Pod "pod-secrets-a3a67af8-34d6-4abf-a521-5d1a8448fb73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022564897s
STEP: Saw pod success
Aug 21 15:40:23.231: INFO: Pod "pod-secrets-a3a67af8-34d6-4abf-a521-5d1a8448fb73" satisfied condition "success or failure"
Aug 21 15:40:23.244: INFO: Trying to get logs from node 10.188.240.202 pod pod-secrets-a3a67af8-34d6-4abf-a521-5d1a8448fb73 container secret-env-test: <nil>
STEP: delete the pod
Aug 21 15:40:23.319: INFO: Waiting for pod pod-secrets-a3a67af8-34d6-4abf-a521-5d1a8448fb73 to disappear
Aug 21 15:40:23.336: INFO: Pod pod-secrets-a3a67af8-34d6-4abf-a521-5d1a8448fb73 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:40:23.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-618" for this suite.
Aug 21 15:40:31.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:40:33.489: INFO: namespace secrets-618 deletion completed in 10.124526542s

• [SLOW TEST:12.560 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:40:33.489: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-4f71e011-bc7e-4c40-998f-2b57287a410b
STEP: Creating a pod to test consume configMaps
Aug 21 15:40:33.738: INFO: Waiting up to 5m0s for pod "pod-configmaps-b7a00e69-57ce-4ed5-9d4b-1cbaf39d7c68" in namespace "configmap-4372" to be "success or failure"
Aug 21 15:40:33.753: INFO: Pod "pod-configmaps-b7a00e69-57ce-4ed5-9d4b-1cbaf39d7c68": Phase="Pending", Reason="", readiness=false. Elapsed: 14.999724ms
Aug 21 15:40:35.765: INFO: Pod "pod-configmaps-b7a00e69-57ce-4ed5-9d4b-1cbaf39d7c68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02747747s
Aug 21 15:40:37.777: INFO: Pod "pod-configmaps-b7a00e69-57ce-4ed5-9d4b-1cbaf39d7c68": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038864482s
Aug 21 15:40:39.790: INFO: Pod "pod-configmaps-b7a00e69-57ce-4ed5-9d4b-1cbaf39d7c68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052287589s
STEP: Saw pod success
Aug 21 15:40:39.790: INFO: Pod "pod-configmaps-b7a00e69-57ce-4ed5-9d4b-1cbaf39d7c68" satisfied condition "success or failure"
Aug 21 15:40:39.803: INFO: Trying to get logs from node 10.188.240.202 pod pod-configmaps-b7a00e69-57ce-4ed5-9d4b-1cbaf39d7c68 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 15:40:39.873: INFO: Waiting for pod pod-configmaps-b7a00e69-57ce-4ed5-9d4b-1cbaf39d7c68 to disappear
Aug 21 15:40:39.886: INFO: Pod pod-configmaps-b7a00e69-57ce-4ed5-9d4b-1cbaf39d7c68 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:40:39.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4372" for this suite.
Aug 21 15:40:47.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:40:49.959: INFO: namespace configmap-4372 deletion completed in 10.049372506s

• [SLOW TEST:16.470 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:40:49.960: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Aug 21 15:40:50.181: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:41:11.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5387" for this suite.
Aug 21 15:41:19.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:41:21.464: INFO: namespace pods-5387 deletion completed in 10.207283672s

• [SLOW TEST:31.505 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:41:21.464: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7142
I0821 15:41:21.751553      24 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7142, replica count: 1
I0821 15:41:22.802087      24 runners.go:184] svc-latency-rc Pods: 0 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0821 15:41:23.802323      24 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0821 15:41:24.802568      24 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0821 15:41:25.802996      24 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 21 15:41:25.957: INFO: Created: latency-svc-rzczk
Aug 21 15:41:25.976: INFO: Got endpoints: latency-svc-rzczk [73.04765ms]
Aug 21 15:41:26.003: INFO: Created: latency-svc-pd78q
Aug 21 15:41:26.020: INFO: Got endpoints: latency-svc-pd78q [43.420611ms]
Aug 21 15:41:26.020: INFO: Created: latency-svc-hmn8d
Aug 21 15:41:26.038: INFO: Got endpoints: latency-svc-hmn8d [61.218445ms]
Aug 21 15:41:26.038: INFO: Created: latency-svc-49snd
Aug 21 15:41:26.051: INFO: Created: latency-svc-2j2j2
Aug 21 15:41:26.056: INFO: Got endpoints: latency-svc-49snd [78.954984ms]
Aug 21 15:41:26.067: INFO: Created: latency-svc-xw9d4
Aug 21 15:41:26.077: INFO: Got endpoints: latency-svc-2j2j2 [99.464083ms]
Aug 21 15:41:26.078: INFO: Created: latency-svc-tl7v4
Aug 21 15:41:26.093: INFO: Created: latency-svc-mrtgz
Aug 21 15:41:26.105: INFO: Got endpoints: latency-svc-xw9d4 [127.246821ms]
Aug 21 15:41:26.113: INFO: Created: latency-svc-ljvrr
Aug 21 15:41:26.124: INFO: Got endpoints: latency-svc-mrtgz [146.286054ms]
Aug 21 15:41:26.125: INFO: Got endpoints: latency-svc-tl7v4 [146.326876ms]
Aug 21 15:41:26.129: INFO: Got endpoints: latency-svc-ljvrr [150.495178ms]
Aug 21 15:41:26.169: INFO: Created: latency-svc-pt5fz
Aug 21 15:41:26.208: INFO: Got endpoints: latency-svc-pt5fz [230.85416ms]
Aug 21 15:41:26.230: INFO: Created: latency-svc-m54w8
Aug 21 15:41:26.256: INFO: Created: latency-svc-r2dhx
Aug 21 15:41:26.280: INFO: Created: latency-svc-ztrgl
Aug 21 15:41:26.281: INFO: Got endpoints: latency-svc-m54w8 [301.826736ms]
Aug 21 15:41:26.295: INFO: Created: latency-svc-6j69j
Aug 21 15:41:26.299: INFO: Got endpoints: latency-svc-ztrgl [319.954652ms]
Aug 21 15:41:26.299: INFO: Got endpoints: latency-svc-r2dhx [320.679481ms]
Aug 21 15:41:26.316: INFO: Created: latency-svc-mlflj
Aug 21 15:41:26.327: INFO: Got endpoints: latency-svc-6j69j [348.707166ms]
Aug 21 15:41:26.334: INFO: Got endpoints: latency-svc-mlflj [354.670494ms]
Aug 21 15:41:26.336: INFO: Created: latency-svc-c6s4c
Aug 21 15:41:26.390: INFO: Got endpoints: latency-svc-c6s4c [411.399744ms]
Aug 21 15:41:26.395: INFO: Created: latency-svc-5vsbg
Aug 21 15:41:26.409: INFO: Got endpoints: latency-svc-5vsbg [389.554847ms]
Aug 21 15:41:26.421: INFO: Created: latency-svc-5s4dt
Aug 21 15:41:26.438: INFO: Created: latency-svc-v9xqh
Aug 21 15:41:26.439: INFO: Got endpoints: latency-svc-5s4dt [400.859644ms]
Aug 21 15:41:26.451: INFO: Created: latency-svc-wk8mh
Aug 21 15:41:26.461: INFO: Got endpoints: latency-svc-v9xqh [405.020147ms]
Aug 21 15:41:26.468: INFO: Created: latency-svc-qwr6j
Aug 21 15:41:26.469: INFO: Got endpoints: latency-svc-wk8mh [391.521949ms]
Aug 21 15:41:26.485: INFO: Created: latency-svc-rz5t4
Aug 21 15:41:26.513: INFO: Created: latency-svc-b54sj
Aug 21 15:41:26.514: INFO: Got endpoints: latency-svc-rz5t4 [389.08788ms]
Aug 21 15:41:26.514: INFO: Got endpoints: latency-svc-qwr6j [408.55289ms]
Aug 21 15:41:26.516: INFO: Created: latency-svc-j8szw
Aug 21 15:41:26.517: INFO: Got endpoints: latency-svc-b54sj [392.655164ms]
Aug 21 15:41:26.535: INFO: Got endpoints: latency-svc-j8szw [406.504276ms]
Aug 21 15:41:26.535: INFO: Created: latency-svc-xcrtm
Aug 21 15:41:26.548: INFO: Created: latency-svc-86gv8
Aug 21 15:41:26.553: INFO: Got endpoints: latency-svc-xcrtm [344.115076ms]
Aug 21 15:41:26.566: INFO: Created: latency-svc-jtjv5
Aug 21 15:41:26.572: INFO: Got endpoints: latency-svc-86gv8 [290.516966ms]
Aug 21 15:41:26.588: INFO: Got endpoints: latency-svc-jtjv5 [288.777749ms]
Aug 21 15:41:26.589: INFO: Created: latency-svc-ltvp4
Aug 21 15:41:26.604: INFO: Got endpoints: latency-svc-ltvp4 [304.309412ms]
Aug 21 15:41:26.607: INFO: Created: latency-svc-8v4nq
Aug 21 15:41:26.622: INFO: Got endpoints: latency-svc-8v4nq [294.341323ms]
Aug 21 15:41:26.625: INFO: Created: latency-svc-dfhwr
Aug 21 15:41:26.646: INFO: Got endpoints: latency-svc-dfhwr [311.922083ms]
Aug 21 15:41:26.647: INFO: Created: latency-svc-6v6gc
Aug 21 15:41:26.665: INFO: Created: latency-svc-wtmvd
Aug 21 15:41:26.668: INFO: Got endpoints: latency-svc-6v6gc [277.278049ms]
Aug 21 15:41:26.678: INFO: Created: latency-svc-vvdjq
Aug 21 15:41:26.683: INFO: Got endpoints: latency-svc-wtmvd [273.392951ms]
Aug 21 15:41:26.701: INFO: Created: latency-svc-bdt9m
Aug 21 15:41:26.716: INFO: Created: latency-svc-h5bnf
Aug 21 15:41:26.733: INFO: Created: latency-svc-6626s
Aug 21 15:41:26.735: INFO: Got endpoints: latency-svc-vvdjq [296.857652ms]
Aug 21 15:41:26.749: INFO: Created: latency-svc-nwzsd
Aug 21 15:41:26.767: INFO: Created: latency-svc-plxc2
Aug 21 15:41:26.777: INFO: Got endpoints: latency-svc-bdt9m [315.591512ms]
Aug 21 15:41:26.778: INFO: Got endpoints: latency-svc-h5bnf [308.42699ms]
Aug 21 15:41:26.783: INFO: Created: latency-svc-sxskj
Aug 21 15:41:26.799: INFO: Created: latency-svc-wzqv9
Aug 21 15:41:26.800: INFO: Got endpoints: latency-svc-6626s [285.53267ms]
Aug 21 15:41:26.822: INFO: Got endpoints: latency-svc-nwzsd [304.979544ms]
Aug 21 15:41:26.824: INFO: Created: latency-svc-4h4rd
Aug 21 15:41:26.826: INFO: Got endpoints: latency-svc-plxc2 [311.13782ms]
Aug 21 15:41:26.840: INFO: Got endpoints: latency-svc-sxskj [296.455041ms]
Aug 21 15:41:26.856: INFO: Created: latency-svc-z8v9j
Aug 21 15:41:26.856: INFO: Got endpoints: latency-svc-wzqv9 [303.306368ms]
Aug 21 15:41:26.857: INFO: Got endpoints: latency-svc-4h4rd [285.095791ms]
Aug 21 15:41:26.871: INFO: Got endpoints: latency-svc-z8v9j [282.982739ms]
Aug 21 15:41:26.871: INFO: Created: latency-svc-lmt2c
Aug 21 15:41:26.885: INFO: Got endpoints: latency-svc-lmt2c [281.538535ms]
Aug 21 15:41:26.887: INFO: Created: latency-svc-lpssw
Aug 21 15:41:26.907: INFO: Created: latency-svc-z9jbg
Aug 21 15:41:26.922: INFO: Created: latency-svc-z2ql7
Aug 21 15:41:26.940: INFO: Got endpoints: latency-svc-lpssw [318.076933ms]
Aug 21 15:41:26.949: INFO: Created: latency-svc-qvhtr
Aug 21 15:41:26.976: INFO: Created: latency-svc-ghn9x
Aug 21 15:41:26.982: INFO: Got endpoints: latency-svc-z2ql7 [314.480928ms]
Aug 21 15:41:26.983: INFO: Got endpoints: latency-svc-z9jbg [336.368058ms]
Aug 21 15:41:26.996: INFO: Created: latency-svc-mc9f8
Aug 21 15:41:27.012: INFO: Got endpoints: latency-svc-qvhtr [329.465917ms]
Aug 21 15:41:27.016: INFO: Created: latency-svc-rtgkg
Aug 21 15:41:27.030: INFO: Created: latency-svc-mpwjv
Aug 21 15:41:27.033: INFO: Got endpoints: latency-svc-ghn9x [297.177183ms]
Aug 21 15:41:27.049: INFO: Created: latency-svc-m8xq4
Aug 21 15:41:27.054: INFO: Got endpoints: latency-svc-mc9f8 [275.872891ms]
Aug 21 15:41:27.065: INFO: Created: latency-svc-d96zz
Aug 21 15:41:27.074: INFO: Got endpoints: latency-svc-rtgkg [297.005365ms]
Aug 21 15:41:27.099: INFO: Got endpoints: latency-svc-mpwjv [299.303818ms]
Aug 21 15:41:27.100: INFO: Created: latency-svc-rxvd4
Aug 21 15:41:27.118: INFO: Got endpoints: latency-svc-m8xq4 [296.262748ms]
Aug 21 15:41:27.120: INFO: Got endpoints: latency-svc-d96zz [294.139052ms]
Aug 21 15:41:27.125: INFO: Created: latency-svc-mdg55
Aug 21 15:41:27.137: INFO: Got endpoints: latency-svc-rxvd4 [297.014841ms]
Aug 21 15:41:27.147: INFO: Created: latency-svc-h96fz
Aug 21 15:41:27.149: INFO: Got endpoints: latency-svc-mdg55 [292.261492ms]
Aug 21 15:41:27.179: INFO: Created: latency-svc-7zqlp
Aug 21 15:41:27.179: INFO: Got endpoints: latency-svc-h96fz [322.305867ms]
Aug 21 15:41:27.187: INFO: Created: latency-svc-lfd7l
Aug 21 15:41:27.191: INFO: Got endpoints: latency-svc-7zqlp [319.394704ms]
Aug 21 15:41:27.210: INFO: Got endpoints: latency-svc-lfd7l [324.687568ms]
Aug 21 15:41:27.215: INFO: Created: latency-svc-npbw9
Aug 21 15:41:27.240: INFO: Created: latency-svc-6ctmx
Aug 21 15:41:27.244: INFO: Got endpoints: latency-svc-npbw9 [304.134754ms]
Aug 21 15:41:27.264: INFO: Got endpoints: latency-svc-6ctmx [281.106348ms]
Aug 21 15:41:27.271: INFO: Created: latency-svc-rrfhm
Aug 21 15:41:27.288: INFO: Got endpoints: latency-svc-rrfhm [305.655822ms]
Aug 21 15:41:27.288: INFO: Created: latency-svc-gzc5r
Aug 21 15:41:27.305: INFO: Got endpoints: latency-svc-gzc5r [292.580647ms]
Aug 21 15:41:27.310: INFO: Created: latency-svc-w4zwz
Aug 21 15:41:27.323: INFO: Created: latency-svc-pwt57
Aug 21 15:41:27.332: INFO: Got endpoints: latency-svc-w4zwz [299.481912ms]
Aug 21 15:41:27.343: INFO: Got endpoints: latency-svc-pwt57 [288.839558ms]
Aug 21 15:41:27.343: INFO: Created: latency-svc-2vnc5
Aug 21 15:41:27.358: INFO: Got endpoints: latency-svc-2vnc5 [283.825322ms]
Aug 21 15:41:27.370: INFO: Created: latency-svc-rb9ld
Aug 21 15:41:27.384: INFO: Got endpoints: latency-svc-rb9ld [284.496371ms]
Aug 21 15:41:27.393: INFO: Created: latency-svc-59gdl
Aug 21 15:41:27.410: INFO: Got endpoints: latency-svc-59gdl [291.560499ms]
Aug 21 15:41:27.410: INFO: Created: latency-svc-t9cb2
Aug 21 15:41:27.433: INFO: Got endpoints: latency-svc-t9cb2 [312.899422ms]
Aug 21 15:41:27.434: INFO: Created: latency-svc-frzb5
Aug 21 15:41:27.445: INFO: Got endpoints: latency-svc-frzb5 [307.866307ms]
Aug 21 15:41:27.449: INFO: Created: latency-svc-w65pn
Aug 21 15:41:27.465: INFO: Created: latency-svc-rtkpw
Aug 21 15:41:27.477: INFO: Got endpoints: latency-svc-w65pn [328.649637ms]
Aug 21 15:41:27.481: INFO: Created: latency-svc-pn8vh
Aug 21 15:41:27.481: INFO: Got endpoints: latency-svc-rtkpw [301.304912ms]
Aug 21 15:41:27.508: INFO: Got endpoints: latency-svc-pn8vh [316.977606ms]
Aug 21 15:41:27.522: INFO: Created: latency-svc-gds45
Aug 21 15:41:27.545: INFO: Got endpoints: latency-svc-gds45 [334.450967ms]
Aug 21 15:41:27.546: INFO: Created: latency-svc-nb8p7
Aug 21 15:41:27.559: INFO: Created: latency-svc-qq5kz
Aug 21 15:41:27.565: INFO: Got endpoints: latency-svc-nb8p7 [320.6708ms]
Aug 21 15:41:27.575: INFO: Got endpoints: latency-svc-qq5kz [310.703602ms]
Aug 21 15:41:27.578: INFO: Created: latency-svc-bq4xh
Aug 21 15:41:27.599: INFO: Created: latency-svc-h7f8h
Aug 21 15:41:27.599: INFO: Got endpoints: latency-svc-bq4xh [54.21311ms]
Aug 21 15:41:27.614: INFO: Created: latency-svc-hwxnf
Aug 21 15:41:27.617: INFO: Got endpoints: latency-svc-h7f8h [328.604204ms]
Aug 21 15:41:27.627: INFO: Got endpoints: latency-svc-hwxnf [322.499953ms]
Aug 21 15:41:27.629: INFO: Created: latency-svc-t848p
Aug 21 15:41:27.647: INFO: Created: latency-svc-vkhvz
Aug 21 15:41:27.662: INFO: Created: latency-svc-stlfd
Aug 21 15:41:27.669: INFO: Got endpoints: latency-svc-vkhvz [325.859487ms]
Aug 21 15:41:27.669: INFO: Got endpoints: latency-svc-t848p [336.482003ms]
Aug 21 15:41:27.676: INFO: Created: latency-svc-gbsjr
Aug 21 15:41:27.683: INFO: Got endpoints: latency-svc-stlfd [324.324787ms]
Aug 21 15:41:27.689: INFO: Got endpoints: latency-svc-gbsjr [304.45548ms]
Aug 21 15:41:27.692: INFO: Created: latency-svc-25bv9
Aug 21 15:41:27.709: INFO: Got endpoints: latency-svc-25bv9 [298.513494ms]
Aug 21 15:41:27.715: INFO: Created: latency-svc-wln4n
Aug 21 15:41:27.733: INFO: Got endpoints: latency-svc-wln4n [299.676063ms]
Aug 21 15:41:27.733: INFO: Created: latency-svc-zgmdb
Aug 21 15:41:27.761: INFO: Got endpoints: latency-svc-zgmdb [315.475366ms]
Aug 21 15:41:27.763: INFO: Created: latency-svc-kn8k7
Aug 21 15:41:27.786: INFO: Created: latency-svc-k5qlf
Aug 21 15:41:27.802: INFO: Got endpoints: latency-svc-kn8k7 [324.477384ms]
Aug 21 15:41:27.821: INFO: Got endpoints: latency-svc-k5qlf [339.659696ms]
Aug 21 15:41:27.822: INFO: Created: latency-svc-hq89c
Aug 21 15:41:27.831: INFO: Created: latency-svc-dqlk6
Aug 21 15:41:27.831: INFO: Got endpoints: latency-svc-hq89c [321.107744ms]
Aug 21 15:41:27.849: INFO: Created: latency-svc-qlrmh
Aug 21 15:41:27.852: INFO: Got endpoints: latency-svc-dqlk6 [287.142717ms]
Aug 21 15:41:27.872: INFO: Got endpoints: latency-svc-qlrmh [297.093992ms]
Aug 21 15:41:27.873: INFO: Created: latency-svc-bvgm5
Aug 21 15:41:27.882: INFO: Created: latency-svc-s7946
Aug 21 15:41:27.887: INFO: Got endpoints: latency-svc-bvgm5 [287.802757ms]
Aug 21 15:41:27.900: INFO: Created: latency-svc-qhckj
Aug 21 15:41:27.908: INFO: Got endpoints: latency-svc-s7946 [290.815235ms]
Aug 21 15:41:27.914: INFO: Created: latency-svc-nhbzk
Aug 21 15:41:27.919: INFO: Got endpoints: latency-svc-qhckj [290.858874ms]
Aug 21 15:41:27.936: INFO: Got endpoints: latency-svc-nhbzk [266.94041ms]
Aug 21 15:41:27.936: INFO: Created: latency-svc-t5k78
Aug 21 15:41:27.947: INFO: Created: latency-svc-tzxbf
Aug 21 15:41:27.951: INFO: Got endpoints: latency-svc-t5k78 [281.946515ms]
Aug 21 15:41:27.964: INFO: Got endpoints: latency-svc-tzxbf [281.554761ms]
Aug 21 15:41:27.965: INFO: Created: latency-svc-sl7kj
Aug 21 15:41:27.980: INFO: Got endpoints: latency-svc-sl7kj [291.568424ms]
Aug 21 15:41:27.981: INFO: Created: latency-svc-95m8p
Aug 21 15:41:28.006: INFO: Got endpoints: latency-svc-95m8p [297.153421ms]
Aug 21 15:41:28.007: INFO: Created: latency-svc-tjn7v
Aug 21 15:41:28.011: INFO: Created: latency-svc-lhwx9
Aug 21 15:41:28.018: INFO: Got endpoints: latency-svc-tjn7v [284.697748ms]
Aug 21 15:41:28.026: INFO: Created: latency-svc-mlhp4
Aug 21 15:41:28.027: INFO: Got endpoints: latency-svc-lhwx9 [266.626049ms]
Aug 21 15:41:28.044: INFO: Got endpoints: latency-svc-mlhp4 [242.125614ms]
Aug 21 15:41:28.047: INFO: Created: latency-svc-22d4c
Aug 21 15:41:28.069: INFO: Got endpoints: latency-svc-22d4c [247.574823ms]
Aug 21 15:41:28.069: INFO: Created: latency-svc-vzlvx
Aug 21 15:41:28.080: INFO: Got endpoints: latency-svc-vzlvx [248.234466ms]
Aug 21 15:41:28.087: INFO: Created: latency-svc-7bmrc
Aug 21 15:41:28.098: INFO: Created: latency-svc-964cv
Aug 21 15:41:28.112: INFO: Got endpoints: latency-svc-7bmrc [259.14097ms]
Aug 21 15:41:28.126: INFO: Got endpoints: latency-svc-964cv [253.470356ms]
Aug 21 15:41:28.134: INFO: Created: latency-svc-8wcnr
Aug 21 15:41:28.146: INFO: Created: latency-svc-k7swj
Aug 21 15:41:28.146: INFO: Got endpoints: latency-svc-8wcnr [258.924254ms]
Aug 21 15:41:28.165: INFO: Got endpoints: latency-svc-k7swj [257.354857ms]
Aug 21 15:41:28.166: INFO: Created: latency-svc-8fzwb
Aug 21 15:41:28.185: INFO: Created: latency-svc-4tp86
Aug 21 15:41:28.187: INFO: Got endpoints: latency-svc-8fzwb [268.154937ms]
Aug 21 15:41:28.210: INFO: Created: latency-svc-rfr26
Aug 21 15:41:28.220: INFO: Got endpoints: latency-svc-4tp86 [284.262707ms]
Aug 21 15:41:28.229: INFO: Created: latency-svc-2krmm
Aug 21 15:41:28.230: INFO: Got endpoints: latency-svc-rfr26 [278.925513ms]
Aug 21 15:41:28.255: INFO: Got endpoints: latency-svc-2krmm [291.030964ms]
Aug 21 15:41:28.256: INFO: Created: latency-svc-fqt8d
Aug 21 15:41:28.265: INFO: Created: latency-svc-xw8g6
Aug 21 15:41:28.272: INFO: Got endpoints: latency-svc-fqt8d [291.279471ms]
Aug 21 15:41:28.287: INFO: Created: latency-svc-qm2gm
Aug 21 15:41:28.288: INFO: Got endpoints: latency-svc-xw8g6 [282.169808ms]
Aug 21 15:41:28.304: INFO: Created: latency-svc-mmkzl
Aug 21 15:41:28.304: INFO: Got endpoints: latency-svc-qm2gm [286.505965ms]
Aug 21 15:41:28.321: INFO: Created: latency-svc-px59r
Aug 21 15:41:28.325: INFO: Got endpoints: latency-svc-mmkzl [297.519233ms]
Aug 21 15:41:28.336: INFO: Created: latency-svc-pwhpz
Aug 21 15:41:28.340: INFO: Got endpoints: latency-svc-px59r [295.868513ms]
Aug 21 15:41:28.357: INFO: Got endpoints: latency-svc-pwhpz [287.270774ms]
Aug 21 15:41:28.363: INFO: Created: latency-svc-clcsw
Aug 21 15:41:28.380: INFO: Got endpoints: latency-svc-clcsw [299.92361ms]
Aug 21 15:41:28.380: INFO: Created: latency-svc-4gvrd
Aug 21 15:41:28.400: INFO: Got endpoints: latency-svc-4gvrd [288.343332ms]
Aug 21 15:41:28.401: INFO: Created: latency-svc-bvt6s
Aug 21 15:41:28.417: INFO: Created: latency-svc-j4krr
Aug 21 15:41:28.418: INFO: Got endpoints: latency-svc-bvt6s [284.43224ms]
Aug 21 15:41:28.437: INFO: Got endpoints: latency-svc-j4krr [290.80992ms]
Aug 21 15:41:28.438: INFO: Created: latency-svc-vcz5b
Aug 21 15:41:28.460: INFO: Got endpoints: latency-svc-vcz5b [295.014817ms]
Aug 21 15:41:28.463: INFO: Created: latency-svc-ntzl8
Aug 21 15:41:28.484: INFO: Created: latency-svc-jzm6k
Aug 21 15:41:28.484: INFO: Got endpoints: latency-svc-ntzl8 [296.950229ms]
Aug 21 15:41:28.499: INFO: Created: latency-svc-ktxrm
Aug 21 15:41:28.500: INFO: Got endpoints: latency-svc-jzm6k [263.064457ms]
Aug 21 15:41:28.535: INFO: Created: latency-svc-n9f2b
Aug 21 15:41:28.536: INFO: Got endpoints: latency-svc-ktxrm [298.247602ms]
Aug 21 15:41:28.571: INFO: Got endpoints: latency-svc-n9f2b [312.955913ms]
Aug 21 15:41:28.576: INFO: Created: latency-svc-ktt2z
Aug 21 15:41:28.598: INFO: Got endpoints: latency-svc-ktt2z [326.482455ms]
Aug 21 15:41:28.640: INFO: Created: latency-svc-nrbkt
Aug 21 15:41:28.679: INFO: Got endpoints: latency-svc-nrbkt [390.475244ms]
Aug 21 15:41:28.709: INFO: Created: latency-svc-gbdbj
Aug 21 15:41:28.742: INFO: Got endpoints: latency-svc-gbdbj [437.856099ms]
Aug 21 15:41:28.782: INFO: Created: latency-svc-824tc
Aug 21 15:41:28.790: INFO: Got endpoints: latency-svc-824tc [464.607749ms]
Aug 21 15:41:28.807: INFO: Created: latency-svc-h85bp
Aug 21 15:41:28.813: INFO: Created: latency-svc-lrxxg
Aug 21 15:41:28.814: INFO: Got endpoints: latency-svc-h85bp [473.422632ms]
Aug 21 15:41:28.834: INFO: Got endpoints: latency-svc-lrxxg [476.811074ms]
Aug 21 15:41:28.834: INFO: Created: latency-svc-xz77r
Aug 21 15:41:28.859: INFO: Created: latency-svc-x6sls
Aug 21 15:41:28.871: INFO: Got endpoints: latency-svc-xz77r [490.589381ms]
Aug 21 15:41:28.879: INFO: Created: latency-svc-v4lc5
Aug 21 15:41:28.880: INFO: Got endpoints: latency-svc-x6sls [479.132403ms]
Aug 21 15:41:28.899: INFO: Got endpoints: latency-svc-v4lc5 [480.583813ms]
Aug 21 15:41:28.904: INFO: Created: latency-svc-j8pqr
Aug 21 15:41:28.921: INFO: Created: latency-svc-spcpb
Aug 21 15:41:28.923: INFO: Got endpoints: latency-svc-j8pqr [485.398633ms]
Aug 21 15:41:28.942: INFO: Got endpoints: latency-svc-spcpb [481.588456ms]
Aug 21 15:41:28.946: INFO: Created: latency-svc-92l8n
Aug 21 15:41:28.966: INFO: Got endpoints: latency-svc-92l8n [480.995458ms]
Aug 21 15:41:28.966: INFO: Created: latency-svc-v8v2g
Aug 21 15:41:28.981: INFO: Got endpoints: latency-svc-v8v2g [480.873843ms]
Aug 21 15:41:28.997: INFO: Created: latency-svc-2z82w
Aug 21 15:41:29.031: INFO: Got endpoints: latency-svc-2z82w [495.45181ms]
Aug 21 15:41:29.049: INFO: Created: latency-svc-rkm55
Aug 21 15:41:29.106: INFO: Got endpoints: latency-svc-rkm55 [534.897108ms]
Aug 21 15:41:29.161: INFO: Created: latency-svc-m8458
Aug 21 15:41:29.204: INFO: Got endpoints: latency-svc-m8458 [605.363071ms]
Aug 21 15:41:29.218: INFO: Created: latency-svc-rgjf5
Aug 21 15:41:29.257: INFO: Got endpoints: latency-svc-rgjf5 [577.874514ms]
Aug 21 15:41:29.265: INFO: Created: latency-svc-gttln
Aug 21 15:41:29.299: INFO: Got endpoints: latency-svc-gttln [556.266029ms]
Aug 21 15:41:29.313: INFO: Created: latency-svc-clwh7
Aug 21 15:41:29.330: INFO: Got endpoints: latency-svc-clwh7 [540.361679ms]
Aug 21 15:41:29.332: INFO: Created: latency-svc-jmnrt
Aug 21 15:41:29.365: INFO: Created: latency-svc-6sqkp
Aug 21 15:41:29.376: INFO: Got endpoints: latency-svc-jmnrt [561.752596ms]
Aug 21 15:41:29.377: INFO: Got endpoints: latency-svc-6sqkp [542.812721ms]
Aug 21 15:41:29.399: INFO: Created: latency-svc-2m7tn
Aug 21 15:41:29.424: INFO: Created: latency-svc-2n8vs
Aug 21 15:41:29.424: INFO: Got endpoints: latency-svc-2m7tn [553.657375ms]
Aug 21 15:41:29.436: INFO: Created: latency-svc-jdpbn
Aug 21 15:41:29.444: INFO: Got endpoints: latency-svc-2n8vs [563.994569ms]
Aug 21 15:41:29.452: INFO: Created: latency-svc-sxchx
Aug 21 15:41:29.456: INFO: Got endpoints: latency-svc-jdpbn [557.368125ms]
Aug 21 15:41:29.474: INFO: Created: latency-svc-qd6w5
Aug 21 15:41:29.480: INFO: Got endpoints: latency-svc-sxchx [556.487486ms]
Aug 21 15:41:29.488: INFO: Created: latency-svc-9svq8
Aug 21 15:41:29.488: INFO: Got endpoints: latency-svc-qd6w5 [545.811062ms]
Aug 21 15:41:29.497: INFO: Created: latency-svc-hgrbq
Aug 21 15:41:29.501: INFO: Got endpoints: latency-svc-9svq8 [535.454397ms]
Aug 21 15:41:29.528: INFO: Created: latency-svc-g2fc8
Aug 21 15:41:29.528: INFO: Got endpoints: latency-svc-hgrbq [546.936085ms]
Aug 21 15:41:29.545: INFO: Created: latency-svc-xpntp
Aug 21 15:41:29.554: INFO: Got endpoints: latency-svc-g2fc8 [522.45332ms]
Aug 21 15:41:29.564: INFO: Got endpoints: latency-svc-xpntp [457.937784ms]
Aug 21 15:41:29.569: INFO: Created: latency-svc-g4xtz
Aug 21 15:41:29.592: INFO: Got endpoints: latency-svc-g4xtz [387.716992ms]
Aug 21 15:41:29.592: INFO: Created: latency-svc-9kj5q
Aug 21 15:41:29.615: INFO: Created: latency-svc-svzxv
Aug 21 15:41:29.615: INFO: Got endpoints: latency-svc-9kj5q [357.670741ms]
Aug 21 15:41:29.628: INFO: Created: latency-svc-27q7v
Aug 21 15:41:29.631: INFO: Got endpoints: latency-svc-svzxv [325.123076ms]
Aug 21 15:41:29.652: INFO: Created: latency-svc-mnmnw
Aug 21 15:41:29.659: INFO: Got endpoints: latency-svc-27q7v [328.631502ms]
Aug 21 15:41:29.671: INFO: Created: latency-svc-svkhh
Aug 21 15:41:29.672: INFO: Got endpoints: latency-svc-mnmnw [295.330394ms]
Aug 21 15:41:29.687: INFO: Got endpoints: latency-svc-svkhh [309.626889ms]
Aug 21 15:41:29.687: INFO: Created: latency-svc-gs472
Aug 21 15:41:29.702: INFO: Created: latency-svc-m6r6b
Aug 21 15:41:29.705: INFO: Got endpoints: latency-svc-gs472 [280.451634ms]
Aug 21 15:41:29.728: INFO: Got endpoints: latency-svc-m6r6b [283.904472ms]
Aug 21 15:41:29.728: INFO: Created: latency-svc-ddfpj
Aug 21 15:41:29.738: INFO: Created: latency-svc-z8445
Aug 21 15:41:29.741: INFO: Got endpoints: latency-svc-ddfpj [284.602168ms]
Aug 21 15:41:29.752: INFO: Created: latency-svc-p7pj9
Aug 21 15:41:29.757: INFO: Got endpoints: latency-svc-z8445 [276.87891ms]
Aug 21 15:41:29.768: INFO: Got endpoints: latency-svc-p7pj9 [279.747246ms]
Aug 21 15:41:29.769: INFO: Created: latency-svc-kwwts
Aug 21 15:41:29.783: INFO: Created: latency-svc-psvsz
Aug 21 15:41:29.785: INFO: Got endpoints: latency-svc-kwwts [283.362703ms]
Aug 21 15:41:29.799: INFO: Got endpoints: latency-svc-psvsz [270.737415ms]
Aug 21 15:41:29.803: INFO: Created: latency-svc-trfbz
Aug 21 15:41:29.830: INFO: Created: latency-svc-sltv4
Aug 21 15:41:29.830: INFO: Got endpoints: latency-svc-trfbz [276.05285ms]
Aug 21 15:41:29.842: INFO: Got endpoints: latency-svc-sltv4 [277.793701ms]
Aug 21 15:41:29.845: INFO: Created: latency-svc-6qz9z
Aug 21 15:41:29.865: INFO: Created: latency-svc-sbqxx
Aug 21 15:41:29.872: INFO: Got endpoints: latency-svc-6qz9z [280.108256ms]
Aug 21 15:41:29.879: INFO: Got endpoints: latency-svc-sbqxx [264.613647ms]
Aug 21 15:41:29.880: INFO: Created: latency-svc-jzp2r
Aug 21 15:41:29.894: INFO: Got endpoints: latency-svc-jzp2r [263.149018ms]
Aug 21 15:41:29.897: INFO: Created: latency-svc-6lpl6
Aug 21 15:41:29.915: INFO: Created: latency-svc-db6xc
Aug 21 15:41:29.915: INFO: Got endpoints: latency-svc-6lpl6 [256.397828ms]
Aug 21 15:41:29.929: INFO: Got endpoints: latency-svc-db6xc [257.163593ms]
Aug 21 15:41:29.931: INFO: Created: latency-svc-ncmfk
Aug 21 15:41:29.948: INFO: Got endpoints: latency-svc-ncmfk [261.531344ms]
Aug 21 15:41:29.949: INFO: Created: latency-svc-kf8b9
Aug 21 15:41:29.965: INFO: Created: latency-svc-njfkl
Aug 21 15:41:29.970: INFO: Got endpoints: latency-svc-kf8b9 [265.025744ms]
Aug 21 15:41:29.980: INFO: Created: latency-svc-t668s
Aug 21 15:41:29.983: INFO: Got endpoints: latency-svc-njfkl [254.709872ms]
Aug 21 15:41:29.995: INFO: Created: latency-svc-8mpng
Aug 21 15:41:30.010: INFO: Created: latency-svc-285wm
Aug 21 15:41:30.011: INFO: Got endpoints: latency-svc-t668s [269.772117ms]
Aug 21 15:41:30.026: INFO: Got endpoints: latency-svc-8mpng [268.868324ms]
Aug 21 15:41:30.029: INFO: Got endpoints: latency-svc-285wm [260.886715ms]
Aug 21 15:41:30.037: INFO: Created: latency-svc-f2mrv
Aug 21 15:41:30.051: INFO: Created: latency-svc-zf254
Aug 21 15:41:30.057: INFO: Got endpoints: latency-svc-f2mrv [272.218585ms]
Aug 21 15:41:30.064: INFO: Created: latency-svc-7d8g2
Aug 21 15:41:30.065: INFO: Got endpoints: latency-svc-zf254 [266.149236ms]
Aug 21 15:41:30.078: INFO: Created: latency-svc-dqsgf
Aug 21 15:41:30.079: INFO: Got endpoints: latency-svc-7d8g2 [248.985957ms]
Aug 21 15:41:30.098: INFO: Created: latency-svc-vv5kg
Aug 21 15:41:30.099: INFO: Got endpoints: latency-svc-dqsgf [256.70457ms]
Aug 21 15:41:30.116: INFO: Created: latency-svc-qvbsr
Aug 21 15:41:30.117: INFO: Got endpoints: latency-svc-vv5kg [244.704682ms]
Aug 21 15:41:30.132: INFO: Created: latency-svc-4ktcz
Aug 21 15:41:30.136: INFO: Got endpoints: latency-svc-qvbsr [256.307022ms]
Aug 21 15:41:30.145: INFO: Created: latency-svc-vvw9s
Aug 21 15:41:30.149: INFO: Got endpoints: latency-svc-4ktcz [254.752075ms]
Aug 21 15:41:30.164: INFO: Created: latency-svc-g9bnk
Aug 21 15:41:30.165: INFO: Got endpoints: latency-svc-vvw9s [249.331621ms]
Aug 21 15:41:30.185: INFO: Created: latency-svc-dzxvc
Aug 21 15:41:30.185: INFO: Got endpoints: latency-svc-g9bnk [256.004296ms]
Aug 21 15:41:30.205: INFO: Created: latency-svc-k8p5g
Aug 21 15:41:30.210: INFO: Got endpoints: latency-svc-dzxvc [261.712989ms]
Aug 21 15:41:30.217: INFO: Created: latency-svc-nxb8b
Aug 21 15:41:30.219: INFO: Got endpoints: latency-svc-k8p5g [248.750708ms]
Aug 21 15:41:30.238: INFO: Got endpoints: latency-svc-nxb8b [255.203753ms]
Aug 21 15:41:30.238: INFO: Created: latency-svc-kxk46
Aug 21 15:41:30.255: INFO: Created: latency-svc-vjjfb
Aug 21 15:41:30.257: INFO: Got endpoints: latency-svc-kxk46 [245.684963ms]
Aug 21 15:41:30.269: INFO: Got endpoints: latency-svc-vjjfb [243.417963ms]
Aug 21 15:41:30.269: INFO: Created: latency-svc-74848
Aug 21 15:41:30.284: INFO: Created: latency-svc-trbz6
Aug 21 15:41:30.291: INFO: Got endpoints: latency-svc-74848 [261.321186ms]
Aug 21 15:41:30.298: INFO: Created: latency-svc-hkv8f
Aug 21 15:41:30.302: INFO: Got endpoints: latency-svc-trbz6 [244.699087ms]
Aug 21 15:41:30.312: INFO: Created: latency-svc-msl85
Aug 21 15:41:30.316: INFO: Got endpoints: latency-svc-hkv8f [251.298145ms]
Aug 21 15:41:30.330: INFO: Got endpoints: latency-svc-msl85 [250.636625ms]
Aug 21 15:41:30.330: INFO: Created: latency-svc-fd72j
Aug 21 15:41:30.344: INFO: Got endpoints: latency-svc-fd72j [245.321839ms]
Aug 21 15:41:30.344: INFO: Latencies: [43.420611ms 54.21311ms 61.218445ms 78.954984ms 99.464083ms 127.246821ms 146.286054ms 146.326876ms 150.495178ms 230.85416ms 242.125614ms 243.417963ms 244.699087ms 244.704682ms 245.321839ms 245.684963ms 247.574823ms 248.234466ms 248.750708ms 248.985957ms 249.331621ms 250.636625ms 251.298145ms 253.470356ms 254.709872ms 254.752075ms 255.203753ms 256.004296ms 256.307022ms 256.397828ms 256.70457ms 257.163593ms 257.354857ms 258.924254ms 259.14097ms 260.886715ms 261.321186ms 261.531344ms 261.712989ms 263.064457ms 263.149018ms 264.613647ms 265.025744ms 266.149236ms 266.626049ms 266.94041ms 268.154937ms 268.868324ms 269.772117ms 270.737415ms 272.218585ms 273.392951ms 275.872891ms 276.05285ms 276.87891ms 277.278049ms 277.793701ms 278.925513ms 279.747246ms 280.108256ms 280.451634ms 281.106348ms 281.538535ms 281.554761ms 281.946515ms 282.169808ms 282.982739ms 283.362703ms 283.825322ms 283.904472ms 284.262707ms 284.43224ms 284.496371ms 284.602168ms 284.697748ms 285.095791ms 285.53267ms 286.505965ms 287.142717ms 287.270774ms 287.802757ms 288.343332ms 288.777749ms 288.839558ms 290.516966ms 290.80992ms 290.815235ms 290.858874ms 291.030964ms 291.279471ms 291.560499ms 291.568424ms 292.261492ms 292.580647ms 294.139052ms 294.341323ms 295.014817ms 295.330394ms 295.868513ms 296.262748ms 296.455041ms 296.857652ms 296.950229ms 297.005365ms 297.014841ms 297.093992ms 297.153421ms 297.177183ms 297.519233ms 298.247602ms 298.513494ms 299.303818ms 299.481912ms 299.676063ms 299.92361ms 301.304912ms 301.826736ms 303.306368ms 304.134754ms 304.309412ms 304.45548ms 304.979544ms 305.655822ms 307.866307ms 308.42699ms 309.626889ms 310.703602ms 311.13782ms 311.922083ms 312.899422ms 312.955913ms 314.480928ms 315.475366ms 315.591512ms 316.977606ms 318.076933ms 319.394704ms 319.954652ms 320.6708ms 320.679481ms 321.107744ms 322.305867ms 322.499953ms 324.324787ms 324.477384ms 324.687568ms 325.123076ms 325.859487ms 326.482455ms 328.604204ms 328.631502ms 328.649637ms 329.465917ms 334.450967ms 336.368058ms 336.482003ms 339.659696ms 344.115076ms 348.707166ms 354.670494ms 357.670741ms 387.716992ms 389.08788ms 389.554847ms 390.475244ms 391.521949ms 392.655164ms 400.859644ms 405.020147ms 406.504276ms 408.55289ms 411.399744ms 437.856099ms 457.937784ms 464.607749ms 473.422632ms 476.811074ms 479.132403ms 480.583813ms 480.873843ms 480.995458ms 481.588456ms 485.398633ms 490.589381ms 495.45181ms 522.45332ms 534.897108ms 535.454397ms 540.361679ms 542.812721ms 545.811062ms 546.936085ms 553.657375ms 556.266029ms 556.487486ms 557.368125ms 561.752596ms 563.994569ms 577.874514ms 605.363071ms]
Aug 21 15:41:30.344: INFO: 50 %ile: 296.455041ms
Aug 21 15:41:30.344: INFO: 90 %ile: 480.995458ms
Aug 21 15:41:30.344: INFO: 99 %ile: 577.874514ms
Aug 21 15:41:30.344: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:41:30.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7142" for this suite.
Aug 21 15:42:00.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:42:02.812: INFO: namespace svc-latency-7142 deletion completed in 32.431382943s

• [SLOW TEST:41.348 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:42:02.817: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 21 15:42:03.062: INFO: Waiting up to 5m0s for pod "pod-769e70a6-691b-40b7-b0f3-38ae42939b6f" in namespace "emptydir-9831" to be "success or failure"
Aug 21 15:42:03.077: INFO: Pod "pod-769e70a6-691b-40b7-b0f3-38ae42939b6f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.934449ms
Aug 21 15:42:05.092: INFO: Pod "pod-769e70a6-691b-40b7-b0f3-38ae42939b6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029498047s
Aug 21 15:42:07.105: INFO: Pod "pod-769e70a6-691b-40b7-b0f3-38ae42939b6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042443902s
STEP: Saw pod success
Aug 21 15:42:07.105: INFO: Pod "pod-769e70a6-691b-40b7-b0f3-38ae42939b6f" satisfied condition "success or failure"
Aug 21 15:42:07.121: INFO: Trying to get logs from node 10.188.240.230 pod pod-769e70a6-691b-40b7-b0f3-38ae42939b6f container test-container: <nil>
STEP: delete the pod
Aug 21 15:42:07.219: INFO: Waiting for pod pod-769e70a6-691b-40b7-b0f3-38ae42939b6f to disappear
Aug 21 15:42:07.230: INFO: Pod pod-769e70a6-691b-40b7-b0f3-38ae42939b6f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:42:07.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9831" for this suite.
Aug 21 15:42:15.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:42:17.377: INFO: namespace emptydir-9831 deletion completed in 10.118016525s

• [SLOW TEST:14.560 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:42:17.377: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Aug 21 15:42:17.564: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 21 15:42:17.653: INFO: Waiting for terminating namespaces to be deleted...
Aug 21 15:42:17.681: INFO: 
Logging pods the kubelet thinks is on node 10.188.240.202 before test
Aug 21 15:42:17.785: INFO: tigera-operator-679798d94d-5dj78 from tigera-operator started at 2020-08-21 14:24:16 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.785: INFO: 	Container tigera-operator ready: true, restart count 1
Aug 21 15:42:17.785: INFO: node-exporter-vw6s8 from openshift-monitoring started at 2020-08-21 14:26:49 +0000 UTC (2 container statuses recorded)
Aug 21 15:42:17.785: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 15:42:17.785: INFO: 	Container node-exporter ready: true, restart count 0
Aug 21 15:42:17.785: INFO: thanos-querier-5bcc6bd6c4-w2z4d from openshift-monitoring started at 2020-08-21 14:35:05 +0000 UTC (4 container statuses recorded)
Aug 21 15:42:17.785: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 15:42:17.785: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 21 15:42:17.785: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 21 15:42:17.785: INFO: 	Container thanos-querier ready: true, restart count 0
Aug 21 15:42:17.785: INFO: packageserver-75777686bd-sxs4v from openshift-operator-lifecycle-manager started at 2020-08-21 14:31:38 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.785: INFO: 	Container packageserver ready: true, restart count 0
Aug 21 15:42:17.785: INFO: ibm-master-proxy-static-10.188.240.202 from kube-system started at 2020-08-21 14:24:11 +0000 UTC (2 container statuses recorded)
Aug 21 15:42:17.785: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 21 15:42:17.785: INFO: 	Container pause ready: true, restart count 0
Aug 21 15:42:17.785: INFO: calico-node-lfw9m from calico-system started at 2020-08-21 14:25:08 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.785: INFO: 	Container calico-node ready: true, restart count 0
Aug 21 15:42:17.785: INFO: calico-typha-6c986fbc8c-mtt9h from calico-system started at 2020-08-21 14:27:04 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.785: INFO: 	Container calico-typha ready: true, restart count 0
Aug 21 15:42:17.785: INFO: node-ca-2zlsz from openshift-image-registry started at 2020-08-21 14:27:50 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.785: INFO: 	Container node-ca ready: true, restart count 0
Aug 21 15:42:17.785: INFO: dns-default-zd27g from openshift-dns started at 2020-08-21 14:28:16 +0000 UTC (2 container statuses recorded)
Aug 21 15:42:17.785: INFO: 	Container dns ready: true, restart count 0
Aug 21 15:42:17.785: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 21 15:42:17.785: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-08-21 14:34:31 +0000 UTC (3 container statuses recorded)
Aug 21 15:42:17.785: INFO: 	Container alertmanager ready: true, restart count 0
Aug 21 15:42:17.785: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 21 15:42:17.785: INFO: 	Container config-reloader ready: true, restart count 0
Aug 21 15:42:17.785: INFO: sonobuoy-systemd-logs-daemon-set-5cea973cc7904f8c-nfmgc from sonobuoy started at 2020-08-21 15:39:10 +0000 UTC (2 container statuses recorded)
Aug 21 15:42:17.785: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 21 15:42:17.785: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 21 15:42:17.785: INFO: network-operator-7986644c85-778wc from openshift-network-operator started at 2020-08-21 14:24:16 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.786: INFO: 	Container network-operator ready: true, restart count 0
Aug 21 15:42:17.786: INFO: router-default-79bfbd48f7-bdqc5 from openshift-ingress started at 2020-08-21 14:28:24 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.786: INFO: 	Container router ready: true, restart count 0
Aug 21 15:42:17.786: INFO: redhat-operators-6d986fdd47-bvpx2 from openshift-marketplace started at 2020-08-21 14:28:43 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.786: INFO: 	Container redhat-operators ready: true, restart count 0
Aug 21 15:42:17.786: INFO: prometheus-adapter-5697b6dddd-ldgqf from openshift-monitoring started at 2020-08-21 14:34:09 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.786: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 21 15:42:17.786: INFO: service-serving-cert-signer-7879bf8d9f-db6v2 from openshift-service-ca started at 2020-08-21 14:26:58 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.786: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Aug 21 15:42:17.786: INFO: ibm-keepalived-watcher-hgfkl from kube-system started at 2020-08-21 14:24:13 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.786: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 21 15:42:17.786: INFO: openshift-kube-proxy-dqhxm from openshift-kube-proxy started at 2020-08-21 14:24:48 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.786: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 21 15:42:17.786: INFO: multus-admission-controller-7ccn4 from openshift-multus started at 2020-08-21 14:25:32 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.786: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 21 15:42:17.786: INFO: openshift-state-metrics-5849d797d8-h6klv from openshift-monitoring started at 2020-08-21 14:26:49 +0000 UTC (3 container statuses recorded)
Aug 21 15:42:17.786: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 21 15:42:17.786: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 21 15:42:17.786: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 21 15:42:17.786: INFO: image-registry-86f8b76dcb-jdpb9 from openshift-image-registry started at 2020-08-21 14:30:33 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.786: INFO: 	Container registry ready: true, restart count 0
Aug 21 15:42:17.786: INFO: prometheus-operator-56d9d699cb-drn75 from openshift-monitoring started at 2020-08-21 14:33:56 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.786: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 21 15:42:17.786: INFO: grafana-c9c7455d7-gcf82 from openshift-monitoring started at 2020-08-21 14:34:15 +0000 UTC (2 container statuses recorded)
Aug 21 15:42:17.786: INFO: 	Container grafana ready: true, restart count 0
Aug 21 15:42:17.786: INFO: 	Container grafana-proxy ready: true, restart count 0
Aug 21 15:42:17.786: INFO: ibmcloud-block-storage-driver-wmdvn from kube-system started at 2020-08-21 14:24:16 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.786: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 21 15:42:17.786: INFO: multus-59vjb from openshift-multus started at 2020-08-21 14:24:42 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.786: INFO: 	Container kube-multus ready: true, restart count 0
Aug 21 15:42:17.786: INFO: tuned-hfsdd from openshift-cluster-node-tuning-operator started at 2020-08-21 14:26:30 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.786: INFO: 	Container tuned ready: true, restart count 0
Aug 21 15:42:17.786: INFO: community-operators-68975cd7c8-sf6zn from openshift-marketplace started at 2020-08-21 14:28:43 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.786: INFO: 	Container community-operators ready: true, restart count 0
Aug 21 15:42:17.786: INFO: ibm-cloud-provider-ip-169-60-87-181-7df9c474cf-m5bdn from ibm-system started at 2020-08-21 14:30:31 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.786: INFO: 	Container ibm-cloud-provider-ip-169-60-87-181 ready: true, restart count 0
Aug 21 15:42:17.786: INFO: console-5bf7799b6-qfl8j from openshift-console started at 2020-08-21 14:29:21 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.786: INFO: 	Container console ready: true, restart count 0
Aug 21 15:42:17.786: INFO: apiservice-cabundle-injector-594fd4555f-pd5j2 from openshift-service-ca started at 2020-08-21 14:26:58 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.786: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Aug 21 15:42:17.786: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-08-21 14:35:11 +0000 UTC (7 container statuses recorded)
Aug 21 15:42:17.786: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 15:42:17.786: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 21 15:42:17.786: INFO: 	Container prometheus ready: true, restart count 1
Aug 21 15:42:17.786: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 21 15:42:17.786: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 21 15:42:17.786: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 21 15:42:17.786: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 21 15:42:17.786: INFO: 
Logging pods the kubelet thinks is on node 10.188.240.222 before test
Aug 21 15:42:17.917: INFO: ibmcloud-block-storage-driver-88f6v from kube-system started at 2020-08-21 14:24:22 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.917: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 21 15:42:17.917: INFO: kube-state-metrics-c5f65645-vgmjg from openshift-monitoring started at 2020-08-21 14:26:47 +0000 UTC (3 container statuses recorded)
Aug 21 15:42:17.917: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 21 15:42:17.917: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 21 15:42:17.917: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 21 15:42:17.917: INFO: cluster-samples-operator-55944b8f44-hpv4t from openshift-cluster-samples-operator started at 2020-08-21 14:27:28 +0000 UTC (2 container statuses recorded)
Aug 21 15:42:17.917: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Aug 21 15:42:17.917: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Aug 21 15:42:17.917: INFO: router-default-79bfbd48f7-wq29r from openshift-ingress started at 2020-08-21 14:28:25 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.917: INFO: 	Container router ready: true, restart count 0
Aug 21 15:42:17.917: INFO: ibm-master-proxy-static-10.188.240.222 from kube-system started at 2020-08-21 14:24:12 +0000 UTC (2 container statuses recorded)
Aug 21 15:42:17.917: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 21 15:42:17.917: INFO: 	Container pause ready: true, restart count 0
Aug 21 15:42:17.917: INFO: tuned-xffjb from openshift-cluster-node-tuning-operator started at 2020-08-21 14:26:30 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.917: INFO: 	Container tuned ready: true, restart count 0
Aug 21 15:42:17.917: INFO: node-ca-wn9jz from openshift-image-registry started at 2020-08-21 14:27:50 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.917: INFO: 	Container node-ca ready: true, restart count 0
Aug 21 15:42:17.917: INFO: openshift-kube-proxy-w7zk7 from openshift-kube-proxy started at 2020-08-21 14:24:48 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.917: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 21 15:42:17.917: INFO: configmap-cabundle-injector-8446d4b88f-lc4xk from openshift-service-ca started at 2020-08-21 14:26:59 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.917: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Aug 21 15:42:17.917: INFO: console-5bf7799b6-qbrjl from openshift-console started at 2020-08-21 14:29:07 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.917: INFO: 	Container console ready: true, restart count 0
Aug 21 15:42:17.917: INFO: calico-node-s8hvd from calico-system started at 2020-08-21 14:25:08 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.917: INFO: 	Container calico-node ready: true, restart count 0
Aug 21 15:42:17.917: INFO: multus-admission-controller-tmrh2 from openshift-multus started at 2020-08-21 14:25:45 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.917: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 21 15:42:17.917: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-08-21 14:35:32 +0000 UTC (7 container statuses recorded)
Aug 21 15:42:17.917: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 15:42:17.917: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 21 15:42:17.917: INFO: 	Container prometheus ready: true, restart count 1
Aug 21 15:42:17.917: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 21 15:42:17.917: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 21 15:42:17.917: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 21 15:42:17.917: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 21 15:42:17.917: INFO: vpn-69dd866c84-4s9m7 from kube-system started at 2020-08-21 14:31:55 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.917: INFO: 	Container vpn ready: true, restart count 0
Aug 21 15:42:17.917: INFO: ibm-keepalived-watcher-pqhzw from kube-system started at 2020-08-21 14:24:15 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.917: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 21 15:42:17.917: INFO: calico-typha-6c986fbc8c-d5w6g from calico-system started at 2020-08-21 14:25:07 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.917: INFO: 	Container calico-typha ready: true, restart count 0
Aug 21 15:42:17.917: INFO: node-exporter-67tp6 from openshift-monitoring started at 2020-08-21 14:26:50 +0000 UTC (2 container statuses recorded)
Aug 21 15:42:17.917: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 15:42:17.917: INFO: 	Container node-exporter ready: true, restart count 0
Aug 21 15:42:17.917: INFO: dns-default-xxclv from openshift-dns started at 2020-08-21 14:28:16 +0000 UTC (2 container statuses recorded)
Aug 21 15:42:17.917: INFO: 	Container dns ready: true, restart count 0
Aug 21 15:42:17.917: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 21 15:42:17.917: INFO: telemeter-client-6fdb57d68d-f82cq from openshift-monitoring started at 2020-08-21 14:34:07 +0000 UTC (3 container statuses recorded)
Aug 21 15:42:17.917: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 15:42:17.917: INFO: 	Container reload ready: true, restart count 0
Aug 21 15:42:17.917: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 21 15:42:17.917: INFO: sonobuoy-systemd-logs-daemon-set-5cea973cc7904f8c-m7w4l from sonobuoy started at 2020-08-21 15:39:10 +0000 UTC (2 container statuses recorded)
Aug 21 15:42:17.917: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 21 15:42:17.917: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 21 15:42:17.917: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-08-21 14:30:18 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.917: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Aug 21 15:42:17.917: INFO: registry-pvc-permissions-px8dj from openshift-image-registry started at 2020-08-21 14:30:33 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.917: INFO: 	Container pvc-permissions ready: false, restart count 0
Aug 21 15:42:17.917: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-08-21 14:34:42 +0000 UTC (3 container statuses recorded)
Aug 21 15:42:17.917: INFO: 	Container alertmanager ready: true, restart count 0
Aug 21 15:42:17.918: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 21 15:42:17.918: INFO: 	Container config-reloader ready: true, restart count 0
Aug 21 15:42:17.918: INFO: multus-4xs2x from openshift-multus started at 2020-08-21 14:24:42 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.918: INFO: 	Container kube-multus ready: true, restart count 0
Aug 21 15:42:17.918: INFO: thanos-querier-5bcc6bd6c4-6wnxk from openshift-monitoring started at 2020-08-21 14:34:55 +0000 UTC (4 container statuses recorded)
Aug 21 15:42:17.918: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 15:42:17.918: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 21 15:42:17.918: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 21 15:42:17.918: INFO: 	Container thanos-querier ready: true, restart count 0
Aug 21 15:42:17.918: INFO: 
Logging pods the kubelet thinks is on node 10.188.240.230 before test
Aug 21 15:42:17.994: INFO: ibm-keepalived-watcher-9tt65 from kube-system started at 2020-08-21 14:24:19 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 21 15:42:17.994: INFO: downloads-678f5d6564-sxpw2 from openshift-console started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container download-server ready: true, restart count 0
Aug 21 15:42:17.994: INFO: calico-typha-6c986fbc8c-rw4j9 from calico-system started at 2020-08-21 14:27:04 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container calico-typha ready: true, restart count 0
Aug 21 15:42:17.994: INFO: sonobuoy from sonobuoy started at 2020-08-21 15:39:03 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 21 15:42:17.994: INFO: console-operator-9878d4766-tfdhf from openshift-console-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container console-operator ready: true, restart count 1
Aug 21 15:42:17.994: INFO: openshift-service-catalog-controller-manager-operator-5496stt64 from openshift-service-catalog-controller-manager-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container operator ready: true, restart count 1
Aug 21 15:42:17.994: INFO: calico-kube-controllers-79d75767dd-bznnm from calico-system started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 21 15:42:17.994: INFO: node-ca-7vtw7 from openshift-image-registry started at 2020-08-21 14:27:50 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container node-ca ready: true, restart count 0
Aug 21 15:42:17.994: INFO: prometheus-adapter-5697b6dddd-hpnk5 from openshift-monitoring started at 2020-08-21 14:34:09 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 21 15:42:17.994: INFO: multus-5fbcs from openshift-multus started at 2020-08-21 14:24:42 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container kube-multus ready: true, restart count 0
Aug 21 15:42:17.994: INFO: cluster-monitoring-operator-5b5659466f-lbtcv from openshift-monitoring started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Aug 21 15:42:17.994: INFO: service-ca-operator-694cfbf5d5-vxbp2 from openshift-service-ca-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container operator ready: true, restart count 0
Aug 21 15:42:17.994: INFO: ingress-operator-695bc545b9-ps8wd from openshift-ingress-operator started at 2020-08-21 14:25:29 +0000 UTC (2 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container ingress-operator ready: true, restart count 0
Aug 21 15:42:17.994: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 15:42:17.994: INFO: openshift-kube-proxy-j677g from openshift-kube-proxy started at 2020-08-21 14:24:48 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 21 15:42:17.994: INFO: olm-operator-b5f57cdbb-nw9x4 from openshift-operator-lifecycle-manager started at 2020-08-21 14:25:30 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container olm-operator ready: true, restart count 0
Aug 21 15:42:17.994: INFO: dns-default-qhhzk from openshift-dns started at 2020-08-21 14:28:16 +0000 UTC (2 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container dns ready: true, restart count 0
Aug 21 15:42:17.994: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 21 15:42:17.994: INFO: ibm-cloud-provider-ip-169-60-87-181-7df9c474cf-v8lnf from ibm-system started at 2020-08-21 14:30:19 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container ibm-cloud-provider-ip-169-60-87-181 ready: true, restart count 0
Aug 21 15:42:17.994: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-08-21 14:34:17 +0000 UTC (3 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container alertmanager ready: true, restart count 0
Aug 21 15:42:17.994: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 21 15:42:17.994: INFO: 	Container config-reloader ready: true, restart count 0
Aug 21 15:42:17.994: INFO: openshift-service-catalog-apiserver-operator-78b9dddb6f-jxgbd from openshift-service-catalog-apiserver-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container operator ready: true, restart count 1
Aug 21 15:42:17.994: INFO: catalog-operator-85f6c659cc-dqh5m from openshift-operator-lifecycle-manager started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 21 15:42:17.994: INFO: sonobuoy-systemd-logs-daemon-set-5cea973cc7904f8c-rjxm6 from sonobuoy started at 2020-08-21 15:39:10 +0000 UTC (2 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 21 15:42:17.994: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 21 15:42:17.994: INFO: ibm-master-proxy-static-10.188.240.230 from kube-system started at 2020-08-21 14:24:17 +0000 UTC (2 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 21 15:42:17.994: INFO: 	Container pause ready: true, restart count 0
Aug 21 15:42:17.994: INFO: ibmcloud-block-storage-driver-jzb5f from kube-system started at 2020-08-21 14:24:22 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 21 15:42:17.994: INFO: marketplace-operator-6957767d58-5m7kk from openshift-marketplace started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container marketplace-operator ready: true, restart count 0
Aug 21 15:42:17.994: INFO: downloads-678f5d6564-78bw4 from openshift-console started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container download-server ready: true, restart count 0
Aug 21 15:42:17.994: INFO: certified-operators-5cbccf94f5-5kjdr from openshift-marketplace started at 2020-08-21 15:28:56 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container certified-operators ready: true, restart count 0
Aug 21 15:42:17.994: INFO: sonobuoy-e2e-job-3399d07d932e4f9d from sonobuoy started at 2020-08-21 15:39:10 +0000 UTC (2 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container e2e ready: true, restart count 0
Aug 21 15:42:17.994: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 21 15:42:17.994: INFO: ibm-file-plugin-6c96899f79-5r8lq from kube-system started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 21 15:42:17.994: INFO: dns-operator-6f9cf66db7-2qjt6 from openshift-dns-operator started at 2020-08-21 14:25:29 +0000 UTC (2 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container dns-operator ready: true, restart count 0
Aug 21 15:42:17.994: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 15:42:17.994: INFO: ibmcloud-block-storage-plugin-68d5c65db9-clx4d from kube-system started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Aug 21 15:42:17.994: INFO: multus-admission-controller-9x9x2 from openshift-multus started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 21 15:42:17.994: INFO: ibm-storage-watcher-68df9b45c4-9rvr8 from kube-system started at 2020-08-21 14:25:31 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 21 15:42:17.994: INFO: node-exporter-jkv5q from openshift-monitoring started at 2020-08-21 14:26:50 +0000 UTC (2 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 15:42:17.994: INFO: 	Container node-exporter ready: true, restart count 0
Aug 21 15:42:17.994: INFO: packageserver-75777686bd-6jlwz from openshift-operator-lifecycle-manager started at 2020-08-21 14:31:50 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container packageserver ready: true, restart count 0
Aug 21 15:42:17.994: INFO: calico-node-rw476 from calico-system started at 2020-08-21 14:25:08 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container calico-node ready: true, restart count 0
Aug 21 15:42:17.994: INFO: cluster-storage-operator-557b75f8d5-bpz57 from openshift-cluster-storage-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Aug 21 15:42:17.994: INFO: cluster-image-registry-operator-6cfd58b66c-rsxhh from openshift-image-registry started at 2020-08-21 14:25:29 +0000 UTC (2 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Aug 21 15:42:17.994: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Aug 21 15:42:17.994: INFO: cluster-node-tuning-operator-b5f884945-f92th from openshift-cluster-node-tuning-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Aug 21 15:42:17.994: INFO: tuned-bh9bt from openshift-cluster-node-tuning-operator started at 2020-08-21 14:26:30 +0000 UTC (1 container statuses recorded)
Aug 21 15:42:17.994: INFO: 	Container tuned ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-aa0b2fd0-a5a1-4b1d-a61f-3ad6605f18a0 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-aa0b2fd0-a5a1-4b1d-a61f-3ad6605f18a0 off the node 10.188.240.230
STEP: verifying the node doesn't have the label kubernetes.io/e2e-aa0b2fd0-a5a1-4b1d-a61f-3ad6605f18a0
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:42:26.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9827" for this suite.
Aug 21 15:42:36.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:42:38.454: INFO: namespace sched-pred-9827 deletion completed in 12.127845887s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:21.077 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:42:38.461: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-f263ed89-986d-4775-bba6-d85214c25082
STEP: Creating a pod to test consume secrets
Aug 21 15:42:38.688: INFO: Waiting up to 5m0s for pod "pod-secrets-524753bf-17c5-48d6-bf46-a9a9fc667754" in namespace "secrets-6899" to be "success or failure"
Aug 21 15:42:38.702: INFO: Pod "pod-secrets-524753bf-17c5-48d6-bf46-a9a9fc667754": Phase="Pending", Reason="", readiness=false. Elapsed: 13.849064ms
Aug 21 15:42:40.714: INFO: Pod "pod-secrets-524753bf-17c5-48d6-bf46-a9a9fc667754": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026468549s
Aug 21 15:42:42.726: INFO: Pod "pod-secrets-524753bf-17c5-48d6-bf46-a9a9fc667754": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037872286s
STEP: Saw pod success
Aug 21 15:42:42.726: INFO: Pod "pod-secrets-524753bf-17c5-48d6-bf46-a9a9fc667754" satisfied condition "success or failure"
Aug 21 15:42:42.736: INFO: Trying to get logs from node 10.188.240.230 pod pod-secrets-524753bf-17c5-48d6-bf46-a9a9fc667754 container secret-volume-test: <nil>
STEP: delete the pod
Aug 21 15:42:42.814: INFO: Waiting for pod pod-secrets-524753bf-17c5-48d6-bf46-a9a9fc667754 to disappear
Aug 21 15:42:42.832: INFO: Pod pod-secrets-524753bf-17c5-48d6-bf46-a9a9fc667754 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:42:42.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6899" for this suite.
Aug 21 15:42:50.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:42:52.937: INFO: namespace secrets-6899 deletion completed in 10.087455412s

• [SLOW TEST:14.476 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:42:52.938: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Aug 21 15:42:53.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 cluster-info'
Aug 21 15:42:53.553: INFO: stderr: ""
Aug 21 15:42:53.553: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:42:53.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3699" for this suite.
Aug 21 15:43:01.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:43:03.757: INFO: namespace kubectl-3699 deletion completed in 10.184937827s

• [SLOW TEST:10.819 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:43:03.757: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Aug 21 15:43:04.907: INFO: Pod name wrapped-volume-race-2e005bb6-7c29-4ade-b6f9-7cba8132f256: Found 0 pods out of 5
Aug 21 15:43:09.925: INFO: Pod name wrapped-volume-race-2e005bb6-7c29-4ade-b6f9-7cba8132f256: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2e005bb6-7c29-4ade-b6f9-7cba8132f256 in namespace emptydir-wrapper-8785, will wait for the garbage collector to delete the pods
Aug 21 15:43:10.074: INFO: Deleting ReplicationController wrapped-volume-race-2e005bb6-7c29-4ade-b6f9-7cba8132f256 took: 28.045859ms
Aug 21 15:43:10.174: INFO: Terminating ReplicationController wrapped-volume-race-2e005bb6-7c29-4ade-b6f9-7cba8132f256 pods took: 100.349009ms
STEP: Creating RC which spawns configmap-volume pods
Aug 21 15:43:51.640: INFO: Pod name wrapped-volume-race-130b8549-fafe-4b25-be25-433a19514cad: Found 0 pods out of 5
Aug 21 15:43:56.662: INFO: Pod name wrapped-volume-race-130b8549-fafe-4b25-be25-433a19514cad: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-130b8549-fafe-4b25-be25-433a19514cad in namespace emptydir-wrapper-8785, will wait for the garbage collector to delete the pods
Aug 21 15:43:56.839: INFO: Deleting ReplicationController wrapped-volume-race-130b8549-fafe-4b25-be25-433a19514cad took: 52.430156ms
Aug 21 15:43:57.439: INFO: Terminating ReplicationController wrapped-volume-race-130b8549-fafe-4b25-be25-433a19514cad pods took: 600.257328ms
STEP: Creating RC which spawns configmap-volume pods
Aug 21 15:44:41.806: INFO: Pod name wrapped-volume-race-18047d04-432c-4f7f-9511-81eefccbcf01: Found 0 pods out of 5
Aug 21 15:44:46.827: INFO: Pod name wrapped-volume-race-18047d04-432c-4f7f-9511-81eefccbcf01: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-18047d04-432c-4f7f-9511-81eefccbcf01 in namespace emptydir-wrapper-8785, will wait for the garbage collector to delete the pods
Aug 21 15:44:46.987: INFO: Deleting ReplicationController wrapped-volume-race-18047d04-432c-4f7f-9511-81eefccbcf01 took: 27.248151ms
Aug 21 15:44:47.587: INFO: Terminating ReplicationController wrapped-volume-race-18047d04-432c-4f7f-9511-81eefccbcf01 pods took: 600.291631ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:45:33.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8785" for this suite.
Aug 21 15:45:43.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:45:45.262: INFO: namespace emptydir-wrapper-8785 deletion completed in 12.144446343s

• [SLOW TEST:161.504 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:45:45.262: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:45:45.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7611" for this suite.
Aug 21 15:46:01.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:46:03.672: INFO: namespace pods-7611 deletion completed in 18.140730349s

• [SLOW TEST:18.410 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:46:03.672: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-f553534f-69e6-44c8-92f1-0af31d4a907f
STEP: Creating a pod to test consume secrets
Aug 21 15:46:03.936: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9377b54c-db2d-49a7-9393-cf3eaab3de26" in namespace "projected-8839" to be "success or failure"
Aug 21 15:46:03.946: INFO: Pod "pod-projected-secrets-9377b54c-db2d-49a7-9393-cf3eaab3de26": Phase="Pending", Reason="", readiness=false. Elapsed: 10.568216ms
Aug 21 15:46:05.961: INFO: Pod "pod-projected-secrets-9377b54c-db2d-49a7-9393-cf3eaab3de26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024659552s
Aug 21 15:46:07.976: INFO: Pod "pod-projected-secrets-9377b54c-db2d-49a7-9393-cf3eaab3de26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040195457s
STEP: Saw pod success
Aug 21 15:46:07.976: INFO: Pod "pod-projected-secrets-9377b54c-db2d-49a7-9393-cf3eaab3de26" satisfied condition "success or failure"
Aug 21 15:46:07.986: INFO: Trying to get logs from node 10.188.240.230 pod pod-projected-secrets-9377b54c-db2d-49a7-9393-cf3eaab3de26 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 21 15:46:08.113: INFO: Waiting for pod pod-projected-secrets-9377b54c-db2d-49a7-9393-cf3eaab3de26 to disappear
Aug 21 15:46:08.125: INFO: Pod pod-projected-secrets-9377b54c-db2d-49a7-9393-cf3eaab3de26 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:46:08.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8839" for this suite.
Aug 21 15:46:16.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:46:18.439: INFO: namespace projected-8839 deletion completed in 10.265298333s

• [SLOW TEST:14.766 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:46:18.442: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:46:20.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-477" for this suite.
Aug 21 15:47:04.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:47:07.128: INFO: namespace kubelet-test-477 deletion completed in 46.222051384s

• [SLOW TEST:48.687 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:47:07.128: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7580.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7580.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7580.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7580.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7580.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7580.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 21 15:47:21.566: INFO: DNS probes using dns-7580/dns-test-aabbfe8c-697d-4cb1-a274-17171898fe19 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:47:21.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7580" for this suite.
Aug 21 15:47:29.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:47:32.201: INFO: namespace dns-7580 deletion completed in 10.5709707s

• [SLOW TEST:25.072 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:47:32.202: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-193
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 21 15:47:32.398: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 21 15:47:58.876: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.245.92:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-193 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 15:47:58.876: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 15:47:59.149: INFO: Found all expected endpoints: [netserver-0]
Aug 21 15:47:59.160: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.174.125:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-193 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 15:47:59.160: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 15:47:59.457: INFO: Found all expected endpoints: [netserver-1]
Aug 21 15:47:59.472: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.172.47:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-193 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 15:47:59.472: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 15:47:59.718: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:47:59.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-193" for this suite.
Aug 21 15:48:09.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:48:12.041: INFO: namespace pod-network-test-193 deletion completed in 12.282595252s

• [SLOW TEST:39.839 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:48:12.041: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 21 15:48:13.299: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 21 15:48:15.343: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621693, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621693, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621693, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621693, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 21 15:48:18.386: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:48:18.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9947" for this suite.
Aug 21 15:48:26.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:48:28.730: INFO: namespace webhook-9947 deletion completed in 10.277153908s
STEP: Destroying namespace "webhook-9947-markers" for this suite.
Aug 21 15:48:36.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:48:39.143: INFO: namespace webhook-9947-markers deletion completed in 10.412788599s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:27.192 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:48:39.233: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Aug 21 15:48:39.596: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-47 /api/v1/namespaces/watch-47/configmaps/e2e-watch-test-resource-version fc6dd875-9e0c-4313-a520-892ec9047fbe 40973 0 2020-08-21 15:48:39 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 21 15:48:39.597: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-47 /api/v1/namespaces/watch-47/configmaps/e2e-watch-test-resource-version fc6dd875-9e0c-4313-a520-892ec9047fbe 40974 0 2020-08-21 15:48:39 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:48:39.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-47" for this suite.
Aug 21 15:48:47.699: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:48:49.835: INFO: namespace watch-47 deletion completed in 10.20094756s

• [SLOW TEST:10.601 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:48:49.837: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:49:01.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8106" for this suite.
Aug 21 15:49:09.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:49:11.701: INFO: namespace resourcequota-8106 deletion completed in 10.326116775s

• [SLOW TEST:21.865 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:49:11.702: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 15:49:11.920: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 21 15:49:21.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 --namespace=crd-publish-openapi-5376 create -f -'
Aug 21 15:49:22.449: INFO: stderr: ""
Aug 21 15:49:22.449: INFO: stdout: "e2e-test-crd-publish-openapi-527-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 21 15:49:22.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 --namespace=crd-publish-openapi-5376 delete e2e-test-crd-publish-openapi-527-crds test-cr'
Aug 21 15:49:22.629: INFO: stderr: ""
Aug 21 15:49:22.629: INFO: stdout: "e2e-test-crd-publish-openapi-527-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Aug 21 15:49:22.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 --namespace=crd-publish-openapi-5376 apply -f -'
Aug 21 15:49:23.409: INFO: stderr: ""
Aug 21 15:49:23.409: INFO: stdout: "e2e-test-crd-publish-openapi-527-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 21 15:49:23.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 --namespace=crd-publish-openapi-5376 delete e2e-test-crd-publish-openapi-527-crds test-cr'
Aug 21 15:49:23.583: INFO: stderr: ""
Aug 21 15:49:23.583: INFO: stdout: "e2e-test-crd-publish-openapi-527-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Aug 21 15:49:23.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 explain e2e-test-crd-publish-openapi-527-crds'
Aug 21 15:49:24.191: INFO: stderr: ""
Aug 21 15:49:24.191: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-527-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:49:39.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5376" for this suite.
Aug 21 15:49:47.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:49:50.144: INFO: namespace crd-publish-openapi-5376 deletion completed in 10.328168514s

• [SLOW TEST:38.443 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:49:50.147: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-6731e5bf-65a9-46d2-b7e4-98bda9c4a808
STEP: Creating a pod to test consume configMaps
Aug 21 15:49:50.404: INFO: Waiting up to 5m0s for pod "pod-configmaps-5f1a73b2-85e9-44a2-8727-2a8c08a68d5c" in namespace "configmap-5519" to be "success or failure"
Aug 21 15:49:50.419: INFO: Pod "pod-configmaps-5f1a73b2-85e9-44a2-8727-2a8c08a68d5c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.713936ms
Aug 21 15:49:52.435: INFO: Pod "pod-configmaps-5f1a73b2-85e9-44a2-8727-2a8c08a68d5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03151071s
STEP: Saw pod success
Aug 21 15:49:52.435: INFO: Pod "pod-configmaps-5f1a73b2-85e9-44a2-8727-2a8c08a68d5c" satisfied condition "success or failure"
Aug 21 15:49:52.447: INFO: Trying to get logs from node 10.188.240.202 pod pod-configmaps-5f1a73b2-85e9-44a2-8727-2a8c08a68d5c container configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 15:49:52.548: INFO: Waiting for pod pod-configmaps-5f1a73b2-85e9-44a2-8727-2a8c08a68d5c to disappear
Aug 21 15:49:52.557: INFO: Pod pod-configmaps-5f1a73b2-85e9-44a2-8727-2a8c08a68d5c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:49:52.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5519" for this suite.
Aug 21 15:50:00.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:50:02.913: INFO: namespace configmap-5519 deletion completed in 10.336226878s

• [SLOW TEST:12.767 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:50:02.915: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 15:50:03.367: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 21 15:50:08.382: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 21 15:50:08.382: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 21 15:50:10.396: INFO: Creating deployment "test-rollover-deployment"
Aug 21 15:50:10.447: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 21 15:50:12.476: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 21 15:50:12.510: INFO: Ensure that both replica sets have 1 created replica
Aug 21 15:50:12.534: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 21 15:50:12.572: INFO: Updating deployment test-rollover-deployment
Aug 21 15:50:12.572: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 21 15:50:14.603: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 21 15:50:14.630: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 21 15:50:14.659: INFO: all replica sets need to contain the pod-template-hash label
Aug 21 15:50:14.659: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621810, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621810, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621812, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621810, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 15:50:16.697: INFO: all replica sets need to contain the pod-template-hash label
Aug 21 15:50:16.697: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621810, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621810, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621812, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621810, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 15:50:18.689: INFO: all replica sets need to contain the pod-template-hash label
Aug 21 15:50:18.689: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621810, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621810, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621818, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621810, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 15:50:20.710: INFO: all replica sets need to contain the pod-template-hash label
Aug 21 15:50:20.710: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621810, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621810, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621818, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621810, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 15:50:22.686: INFO: all replica sets need to contain the pod-template-hash label
Aug 21 15:50:22.686: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621810, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621810, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621818, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621810, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 15:50:24.691: INFO: all replica sets need to contain the pod-template-hash label
Aug 21 15:50:24.691: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621810, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621810, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621818, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621810, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 15:50:26.690: INFO: all replica sets need to contain the pod-template-hash label
Aug 21 15:50:26.690: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621810, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621810, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621818, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733621810, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 15:50:28.693: INFO: 
Aug 21 15:50:28.693: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Aug 21 15:50:28.734: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-7420 /apis/apps/v1/namespaces/deployment-7420/deployments/test-rollover-deployment 29a1c041-4ad1-45a4-aa57-84165b65297e 41719 2 2020-08-21 15:50:10 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00d2b7568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-08-21 15:50:10 +0000 UTC,LastTransitionTime:2020-08-21 15:50:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2020-08-21 15:50:28 +0000 UTC,LastTransitionTime:2020-08-21 15:50:10 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 21 15:50:28.749: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-7420 /apis/apps/v1/namespaces/deployment-7420/replicasets/test-rollover-deployment-7d7dc6548c 4d28f55c-2821-4f8f-b067-ba60aa971800 41708 2 2020-08-21 15:50:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 29a1c041-4ad1-45a4-aa57-84165b65297e 0xc00d2f0117 0xc00d2f0118}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00d2f0188 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 21 15:50:28.749: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 21 15:50:28.749: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7420 /apis/apps/v1/namespaces/deployment-7420/replicasets/test-rollover-controller 17df9767-cf5c-4959-90b5-e71217b97859 41717 2 2020-08-21 15:50:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 29a1c041-4ad1-45a4-aa57-84165b65297e 0xc00d2c5fd7 0xc00d2c5fd8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00d2f0088 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 21 15:50:28.749: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-7420 /apis/apps/v1/namespaces/deployment-7420/replicasets/test-rollover-deployment-f6c94f66c ddca8192-4e66-41fc-8fcf-d36cab151597 41626 2 2020-08-21 15:50:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 29a1c041-4ad1-45a4-aa57-84165b65297e 0xc00d2f0220 0xc00d2f0221}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00d2f02a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 21 15:50:28.764: INFO: Pod "test-rollover-deployment-7d7dc6548c-6qltf" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-6qltf test-rollover-deployment-7d7dc6548c- deployment-7420 /api/v1/namespaces/deployment-7420/pods/test-rollover-deployment-7d7dc6548c-6qltf bdd367ba-c8a5-4636-99bb-6586c3cc850f 41664 0 2020-08-21 15:50:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[cni.projectcalico.org/podIP:172.30.174.67/32 cni.projectcalico.org/podIPs:172.30.174.67/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.174.67"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c 4d28f55c-2821-4f8f-b067-ba60aa971800 0xc00d2f0b17 0xc00d2f0b18}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kf6zh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kf6zh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kf6zh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.230,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-t564n,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 15:50:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 15:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 15:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 15:50:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.230,PodIP:172.30.174.67,StartTime:2020-08-21 15:50:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-21 15:50:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://899aa32e87a2d86fd1b83f0418488496e76a3bc5460dd45584f1809217a4a59f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.174.67,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:50:28.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7420" for this suite.
Aug 21 15:50:38.861: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:50:41.760: INFO: namespace deployment-7420 deletion completed in 12.969213502s

• [SLOW TEST:38.846 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:50:41.760: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Aug 21 15:50:42.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 --namespace=kubectl-4471 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Aug 21 15:50:46.521: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Aug 21 15:50:46.521: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:50:48.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4471" for this suite.
Aug 21 15:50:56.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:50:58.876: INFO: namespace kubectl-4471 deletion completed in 10.277229036s

• [SLOW TEST:17.116 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:50:58.876: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 21 15:51:00.238: INFO: Waiting up to 5m0s for pod "downwardapi-volume-35926ff4-3695-418a-adb5-2a3e8f85fa97" in namespace "projected-9856" to be "success or failure"
Aug 21 15:51:00.250: INFO: Pod "downwardapi-volume-35926ff4-3695-418a-adb5-2a3e8f85fa97": Phase="Pending", Reason="", readiness=false. Elapsed: 11.507159ms
Aug 21 15:51:02.270: INFO: Pod "downwardapi-volume-35926ff4-3695-418a-adb5-2a3e8f85fa97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031305018s
STEP: Saw pod success
Aug 21 15:51:02.270: INFO: Pod "downwardapi-volume-35926ff4-3695-418a-adb5-2a3e8f85fa97" satisfied condition "success or failure"
Aug 21 15:51:02.293: INFO: Trying to get logs from node 10.188.240.230 pod downwardapi-volume-35926ff4-3695-418a-adb5-2a3e8f85fa97 container client-container: <nil>
STEP: delete the pod
Aug 21 15:51:02.422: INFO: Waiting for pod downwardapi-volume-35926ff4-3695-418a-adb5-2a3e8f85fa97 to disappear
Aug 21 15:51:02.434: INFO: Pod downwardapi-volume-35926ff4-3695-418a-adb5-2a3e8f85fa97 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:51:02.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9856" for this suite.
Aug 21 15:51:12.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:51:14.744: INFO: namespace projected-9856 deletion completed in 12.261216899s

• [SLOW TEST:15.868 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:51:14.744: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-9bacc8bc-ee84-44f1-9488-2aed839b63ca
STEP: Creating a pod to test consume configMaps
Aug 21 15:51:15.041: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-68a9bf35-6966-469f-9e28-578d299b3911" in namespace "projected-2464" to be "success or failure"
Aug 21 15:51:15.052: INFO: Pod "pod-projected-configmaps-68a9bf35-6966-469f-9e28-578d299b3911": Phase="Pending", Reason="", readiness=false. Elapsed: 10.369737ms
Aug 21 15:51:17.067: INFO: Pod "pod-projected-configmaps-68a9bf35-6966-469f-9e28-578d299b3911": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025740232s
Aug 21 15:51:19.080: INFO: Pod "pod-projected-configmaps-68a9bf35-6966-469f-9e28-578d299b3911": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039165818s
STEP: Saw pod success
Aug 21 15:51:19.081: INFO: Pod "pod-projected-configmaps-68a9bf35-6966-469f-9e28-578d299b3911" satisfied condition "success or failure"
Aug 21 15:51:19.092: INFO: Trying to get logs from node 10.188.240.230 pod pod-projected-configmaps-68a9bf35-6966-469f-9e28-578d299b3911 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 15:51:19.180: INFO: Waiting for pod pod-projected-configmaps-68a9bf35-6966-469f-9e28-578d299b3911 to disappear
Aug 21 15:51:19.198: INFO: Pod pod-projected-configmaps-68a9bf35-6966-469f-9e28-578d299b3911 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:51:19.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2464" for this suite.
Aug 21 15:51:27.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:51:29.869: INFO: namespace projected-2464 deletion completed in 10.630224398s

• [SLOW TEST:15.125 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:51:29.869: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-16e0c327-dcf1-43c3-9fa0-4ee292cccdeb in namespace container-probe-2460
Aug 21 15:51:35.370: INFO: Started pod busybox-16e0c327-dcf1-43c3-9fa0-4ee292cccdeb in namespace container-probe-2460
STEP: checking the pod's current state and verifying that restartCount is present
Aug 21 15:51:35.385: INFO: Initial restart count of pod busybox-16e0c327-dcf1-43c3-9fa0-4ee292cccdeb is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:55:37.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2460" for this suite.
Aug 21 15:55:47.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:55:49.761: INFO: namespace container-probe-2460 deletion completed in 12.316840392s

• [SLOW TEST:259.892 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:55:49.765: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 21 15:55:50.038: INFO: Waiting up to 5m0s for pod "pod-bccab845-64d1-4119-a202-234aac42478c" in namespace "emptydir-4437" to be "success or failure"
Aug 21 15:55:50.049: INFO: Pod "pod-bccab845-64d1-4119-a202-234aac42478c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.294078ms
Aug 21 15:55:52.062: INFO: Pod "pod-bccab845-64d1-4119-a202-234aac42478c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023972326s
Aug 21 15:55:54.093: INFO: Pod "pod-bccab845-64d1-4119-a202-234aac42478c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055183842s
STEP: Saw pod success
Aug 21 15:55:54.093: INFO: Pod "pod-bccab845-64d1-4119-a202-234aac42478c" satisfied condition "success or failure"
Aug 21 15:55:54.105: INFO: Trying to get logs from node 10.188.240.202 pod pod-bccab845-64d1-4119-a202-234aac42478c container test-container: <nil>
STEP: delete the pod
Aug 21 15:55:54.227: INFO: Waiting for pod pod-bccab845-64d1-4119-a202-234aac42478c to disappear
Aug 21 15:55:54.259: INFO: Pod pod-bccab845-64d1-4119-a202-234aac42478c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:55:54.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4437" for this suite.
Aug 21 15:56:02.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:56:04.451: INFO: namespace emptydir-4437 deletion completed in 10.148295102s

• [SLOW TEST:14.686 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:56:04.451: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-9445
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 21 15:56:04.845: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 21 15:56:27.399: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.172.48 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9445 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 15:56:27.399: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 15:56:28.740: INFO: Found all expected endpoints: [netserver-0]
Aug 21 15:56:28.752: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.245.98 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9445 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 15:56:28.752: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 15:56:30.013: INFO: Found all expected endpoints: [netserver-1]
Aug 21 15:56:30.027: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.174.95 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9445 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 15:56:30.027: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 15:56:31.288: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:56:31.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9445" for this suite.
Aug 21 15:56:41.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:56:43.554: INFO: namespace pod-network-test-9445 deletion completed in 12.242561626s

• [SLOW TEST:39.103 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:56:43.556: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:56:45.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3740" for this suite.
Aug 21 15:57:28.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:57:30.174: INFO: namespace kubelet-test-3740 deletion completed in 44.215048552s

• [SLOW TEST:46.619 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:57:30.176: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:57:30.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1949" for this suite.
Aug 21 15:57:38.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:57:41.037: INFO: namespace kubelet-test-1949 deletion completed in 10.224809395s

• [SLOW TEST:10.860 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:57:41.037: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-be954b65-2907-4c59-9534-081c5c1e96a3
STEP: Creating a pod to test consume configMaps
Aug 21 15:57:41.304: INFO: Waiting up to 5m0s for pod "pod-configmaps-d6292565-bef1-4ed7-be94-4d20cf18bf3a" in namespace "configmap-6769" to be "success or failure"
Aug 21 15:57:41.315: INFO: Pod "pod-configmaps-d6292565-bef1-4ed7-be94-4d20cf18bf3a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.195671ms
Aug 21 15:57:43.327: INFO: Pod "pod-configmaps-d6292565-bef1-4ed7-be94-4d20cf18bf3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022770956s
STEP: Saw pod success
Aug 21 15:57:43.327: INFO: Pod "pod-configmaps-d6292565-bef1-4ed7-be94-4d20cf18bf3a" satisfied condition "success or failure"
Aug 21 15:57:43.337: INFO: Trying to get logs from node 10.188.240.202 pod pod-configmaps-d6292565-bef1-4ed7-be94-4d20cf18bf3a container configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 15:57:43.453: INFO: Waiting for pod pod-configmaps-d6292565-bef1-4ed7-be94-4d20cf18bf3a to disappear
Aug 21 15:57:43.463: INFO: Pod pod-configmaps-d6292565-bef1-4ed7-be94-4d20cf18bf3a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:57:43.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6769" for this suite.
Aug 21 15:57:51.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:57:53.639: INFO: namespace configmap-6769 deletion completed in 10.149639446s

• [SLOW TEST:12.603 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:57:53.640: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 21 15:57:53.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-6759'
Aug 21 15:57:53.990: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 21 15:57:53.990: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Aug 21 15:57:54.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 delete jobs e2e-test-httpd-job --namespace=kubectl-6759'
Aug 21 15:57:54.178: INFO: stderr: ""
Aug 21 15:57:54.178: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:57:54.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6759" for this suite.
Aug 21 15:58:02.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:58:04.395: INFO: namespace kubectl-6759 deletion completed in 10.174676923s

• [SLOW TEST:10.756 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:58:04.396: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 21 15:58:07.712: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:58:07.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5509" for this suite.
Aug 21 15:58:15.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:58:18.008: INFO: namespace container-runtime-5509 deletion completed in 10.21981869s

• [SLOW TEST:13.612 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:58:18.008: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-58fee738-12e8-46f0-aed9-da92916b3a64
STEP: Creating a pod to test consume secrets
Aug 21 15:58:19.309: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-157019e1-9dd8-4410-a45f-47b340e7b4dc" in namespace "projected-8760" to be "success or failure"
Aug 21 15:58:19.330: INFO: Pod "pod-projected-secrets-157019e1-9dd8-4410-a45f-47b340e7b4dc": Phase="Pending", Reason="", readiness=false. Elapsed: 21.253882ms
Aug 21 15:58:21.346: INFO: Pod "pod-projected-secrets-157019e1-9dd8-4410-a45f-47b340e7b4dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03718479s
Aug 21 15:58:23.360: INFO: Pod "pod-projected-secrets-157019e1-9dd8-4410-a45f-47b340e7b4dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05112331s
STEP: Saw pod success
Aug 21 15:58:23.360: INFO: Pod "pod-projected-secrets-157019e1-9dd8-4410-a45f-47b340e7b4dc" satisfied condition "success or failure"
Aug 21 15:58:23.371: INFO: Trying to get logs from node 10.188.240.202 pod pod-projected-secrets-157019e1-9dd8-4410-a45f-47b340e7b4dc container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 21 15:58:23.441: INFO: Waiting for pod pod-projected-secrets-157019e1-9dd8-4410-a45f-47b340e7b4dc to disappear
Aug 21 15:58:23.453: INFO: Pod pod-projected-secrets-157019e1-9dd8-4410-a45f-47b340e7b4dc no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:58:23.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8760" for this suite.
Aug 21 15:58:33.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:58:35.954: INFO: namespace projected-8760 deletion completed in 12.460142125s

• [SLOW TEST:17.945 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:58:35.955: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0821 15:59:16.346443      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 21 15:59:16.346: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:59:16.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3492" for this suite.
Aug 21 15:59:26.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:59:28.484: INFO: namespace gc-3492 deletion completed in 12.115787696s

• [SLOW TEST:52.530 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:59:28.485: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Aug 21 15:59:28.723: INFO: Pod name pod-release: Found 0 pods out of 1
Aug 21 15:59:33.737: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:59:33.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6105" for this suite.
Aug 21 15:59:43.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:59:46.082: INFO: namespace replication-controller-6105 deletion completed in 12.226388211s

• [SLOW TEST:17.597 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 15:59:46.082: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-b0f3e5fe-3700-4b1d-ad48-bc9ac8ddf9b0
STEP: Creating a pod to test consume secrets
Aug 21 15:59:46.593: INFO: Waiting up to 5m0s for pod "pod-secrets-25752f72-413a-41b5-b0b1-d3e3aa12e151" in namespace "secrets-9445" to be "success or failure"
Aug 21 15:59:46.605: INFO: Pod "pod-secrets-25752f72-413a-41b5-b0b1-d3e3aa12e151": Phase="Pending", Reason="", readiness=false. Elapsed: 11.856042ms
Aug 21 15:59:48.618: INFO: Pod "pod-secrets-25752f72-413a-41b5-b0b1-d3e3aa12e151": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025146561s
STEP: Saw pod success
Aug 21 15:59:48.618: INFO: Pod "pod-secrets-25752f72-413a-41b5-b0b1-d3e3aa12e151" satisfied condition "success or failure"
Aug 21 15:59:48.631: INFO: Trying to get logs from node 10.188.240.230 pod pod-secrets-25752f72-413a-41b5-b0b1-d3e3aa12e151 container secret-volume-test: <nil>
STEP: delete the pod
Aug 21 15:59:48.758: INFO: Waiting for pod pod-secrets-25752f72-413a-41b5-b0b1-d3e3aa12e151 to disappear
Aug 21 15:59:48.771: INFO: Pod pod-secrets-25752f72-413a-41b5-b0b1-d3e3aa12e151 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 15:59:48.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9445" for this suite.
Aug 21 15:59:56.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 15:59:59.012: INFO: namespace secrets-9445 deletion completed in 10.20529094s
STEP: Destroying namespace "secret-namespace-4351" for this suite.
Aug 21 16:00:07.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:00:09.448: INFO: namespace secret-namespace-4351 deletion completed in 10.435539928s

• [SLOW TEST:23.366 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:00:09.448: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Aug 21 16:00:09.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 create -f - --namespace=kubectl-7791'
Aug 21 16:00:10.465: INFO: stderr: ""
Aug 21 16:00:10.465: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 21 16:00:10.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7791'
Aug 21 16:00:10.647: INFO: stderr: ""
Aug 21 16:00:10.647: INFO: stdout: "update-demo-nautilus-2kmmw update-demo-nautilus-fdnkr "
Aug 21 16:00:10.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-nautilus-2kmmw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7791'
Aug 21 16:00:10.822: INFO: stderr: ""
Aug 21 16:00:10.822: INFO: stdout: ""
Aug 21 16:00:10.822: INFO: update-demo-nautilus-2kmmw is created but not running
Aug 21 16:00:15.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7791'
Aug 21 16:00:15.981: INFO: stderr: ""
Aug 21 16:00:15.981: INFO: stdout: "update-demo-nautilus-2kmmw update-demo-nautilus-fdnkr "
Aug 21 16:00:15.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-nautilus-2kmmw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7791'
Aug 21 16:00:16.171: INFO: stderr: ""
Aug 21 16:00:16.171: INFO: stdout: "true"
Aug 21 16:00:16.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-nautilus-2kmmw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7791'
Aug 21 16:00:16.303: INFO: stderr: ""
Aug 21 16:00:16.303: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 21 16:00:16.303: INFO: validating pod update-demo-nautilus-2kmmw
Aug 21 16:00:16.348: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 21 16:00:16.348: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 21 16:00:16.348: INFO: update-demo-nautilus-2kmmw is verified up and running
Aug 21 16:00:16.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-nautilus-fdnkr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7791'
Aug 21 16:00:16.486: INFO: stderr: ""
Aug 21 16:00:16.486: INFO: stdout: "true"
Aug 21 16:00:16.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-nautilus-fdnkr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7791'
Aug 21 16:00:16.639: INFO: stderr: ""
Aug 21 16:00:16.639: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 21 16:00:16.639: INFO: validating pod update-demo-nautilus-fdnkr
Aug 21 16:00:16.672: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 21 16:00:16.672: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 21 16:00:16.672: INFO: update-demo-nautilus-fdnkr is verified up and running
STEP: using delete to clean up resources
Aug 21 16:00:16.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 delete --grace-period=0 --force -f - --namespace=kubectl-7791'
Aug 21 16:00:16.864: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 21 16:00:16.864: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 21 16:00:16.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7791'
Aug 21 16:00:17.029: INFO: stderr: "No resources found in kubectl-7791 namespace.\n"
Aug 21 16:00:17.029: INFO: stdout: ""
Aug 21 16:00:17.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods -l name=update-demo --namespace=kubectl-7791 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 21 16:00:17.191: INFO: stderr: ""
Aug 21 16:00:17.191: INFO: stdout: "update-demo-nautilus-2kmmw\nupdate-demo-nautilus-fdnkr\n"
Aug 21 16:00:17.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7791'
Aug 21 16:00:17.885: INFO: stderr: "No resources found in kubectl-7791 namespace.\n"
Aug 21 16:00:17.885: INFO: stdout: ""
Aug 21 16:00:17.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods -l name=update-demo --namespace=kubectl-7791 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 21 16:00:18.027: INFO: stderr: ""
Aug 21 16:00:18.028: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:00:18.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7791" for this suite.
Aug 21 16:00:34.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:00:36.363: INFO: namespace kubectl-7791 deletion completed in 18.297815951s

• [SLOW TEST:26.915 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:00:36.363: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Aug 21 16:00:36.594: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Aug 21 16:00:37.189: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Aug 21 16:00:39.383: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622437, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622437, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622437, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622437, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 16:00:41.398: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622437, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622437, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622437, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622437, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 16:00:43.403: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622437, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622437, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622437, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622437, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 16:00:45.402: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622437, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622437, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622437, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622437, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 16:00:47.398: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622437, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622437, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622437, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622437, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 16:00:49.398: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622437, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622437, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622437, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622437, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 16:00:51.636: INFO: Waited 216.341236ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:00:53.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-4157" for this suite.
Aug 21 16:01:01.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:01:04.190: INFO: namespace aggregator-4157 deletion completed in 10.318427407s

• [SLOW TEST:27.827 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:01:04.190: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 16:01:04.420: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-837df6ac-4002-445b-8c1e-e69c28a8e861
STEP: Creating secret with name s-test-opt-upd-f1145653-a164-40a5-908c-598b79e95001
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-837df6ac-4002-445b-8c1e-e69c28a8e861
STEP: Updating secret s-test-opt-upd-f1145653-a164-40a5-908c-598b79e95001
STEP: Creating secret with name s-test-opt-create-042504f6-77f1-4d3e-a6c6-fa1d0841605b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:02:13.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4016" for this suite.
Aug 21 16:02:34.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:02:36.228: INFO: namespace secrets-4016 deletion completed in 22.268888626s

• [SLOW TEST:92.039 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:02:36.229: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 21 16:02:41.170: INFO: Successfully updated pod "pod-update-activedeadlineseconds-bc89344b-35d2-45bd-b945-b7e627b582ed"
Aug 21 16:02:41.170: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-bc89344b-35d2-45bd-b945-b7e627b582ed" in namespace "pods-3989" to be "terminated due to deadline exceeded"
Aug 21 16:02:41.205: INFO: Pod "pod-update-activedeadlineseconds-bc89344b-35d2-45bd-b945-b7e627b582ed": Phase="Running", Reason="", readiness=true. Elapsed: 34.416072ms
Aug 21 16:02:43.216: INFO: Pod "pod-update-activedeadlineseconds-bc89344b-35d2-45bd-b945-b7e627b582ed": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.045439664s
Aug 21 16:02:43.216: INFO: Pod "pod-update-activedeadlineseconds-bc89344b-35d2-45bd-b945-b7e627b582ed" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:02:43.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3989" for this suite.
Aug 21 16:02:51.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:02:53.534: INFO: namespace pods-3989 deletion completed in 10.268369822s

• [SLOW TEST:17.305 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:02:53.534: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Aug 21 16:02:53.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 create -f - --namespace=kubectl-2636'
Aug 21 16:02:54.308: INFO: stderr: ""
Aug 21 16:02:54.308: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 21 16:02:54.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2636'
Aug 21 16:02:54.468: INFO: stderr: ""
Aug 21 16:02:54.468: INFO: stdout: "update-demo-nautilus-6pff8 update-demo-nautilus-sct8h "
Aug 21 16:02:54.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-nautilus-6pff8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2636'
Aug 21 16:02:54.621: INFO: stderr: ""
Aug 21 16:02:54.621: INFO: stdout: ""
Aug 21 16:02:54.621: INFO: update-demo-nautilus-6pff8 is created but not running
Aug 21 16:02:59.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2636'
Aug 21 16:02:59.807: INFO: stderr: ""
Aug 21 16:02:59.807: INFO: stdout: "update-demo-nautilus-6pff8 update-demo-nautilus-sct8h "
Aug 21 16:02:59.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-nautilus-6pff8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2636'
Aug 21 16:02:59.953: INFO: stderr: ""
Aug 21 16:02:59.953: INFO: stdout: "true"
Aug 21 16:02:59.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-nautilus-6pff8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2636'
Aug 21 16:03:00.099: INFO: stderr: ""
Aug 21 16:03:00.100: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 21 16:03:00.100: INFO: validating pod update-demo-nautilus-6pff8
Aug 21 16:03:00.132: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 21 16:03:00.132: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 21 16:03:00.132: INFO: update-demo-nautilus-6pff8 is verified up and running
Aug 21 16:03:00.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-nautilus-sct8h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2636'
Aug 21 16:03:00.325: INFO: stderr: ""
Aug 21 16:03:00.325: INFO: stdout: "true"
Aug 21 16:03:00.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-nautilus-sct8h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2636'
Aug 21 16:03:00.892: INFO: stderr: ""
Aug 21 16:03:00.892: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 21 16:03:00.892: INFO: validating pod update-demo-nautilus-sct8h
Aug 21 16:03:00.928: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 21 16:03:00.928: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 21 16:03:00.928: INFO: update-demo-nautilus-sct8h is verified up and running
STEP: rolling-update to new replication controller
Aug 21 16:03:00.933: INFO: scanned /root for discovery docs: <nil>
Aug 21 16:03:00.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-2636'
Aug 21 16:03:24.317: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug 21 16:03:24.317: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 21 16:03:24.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2636'
Aug 21 16:03:24.480: INFO: stderr: ""
Aug 21 16:03:24.480: INFO: stdout: "update-demo-kitten-bsttr update-demo-kitten-sqrm4 "
Aug 21 16:03:24.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-kitten-bsttr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2636'
Aug 21 16:03:24.624: INFO: stderr: ""
Aug 21 16:03:24.624: INFO: stdout: "true"
Aug 21 16:03:24.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-kitten-bsttr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2636'
Aug 21 16:03:24.761: INFO: stderr: ""
Aug 21 16:03:24.761: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug 21 16:03:24.761: INFO: validating pod update-demo-kitten-bsttr
Aug 21 16:03:24.784: INFO: got data: {
  "image": "kitten.jpg"
}

Aug 21 16:03:24.784: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug 21 16:03:24.784: INFO: update-demo-kitten-bsttr is verified up and running
Aug 21 16:03:24.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-kitten-sqrm4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2636'
Aug 21 16:03:24.951: INFO: stderr: ""
Aug 21 16:03:24.951: INFO: stdout: "true"
Aug 21 16:03:24.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-kitten-sqrm4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2636'
Aug 21 16:03:25.097: INFO: stderr: ""
Aug 21 16:03:25.097: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug 21 16:03:25.097: INFO: validating pod update-demo-kitten-sqrm4
Aug 21 16:03:25.133: INFO: got data: {
  "image": "kitten.jpg"
}

Aug 21 16:03:25.133: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug 21 16:03:25.133: INFO: update-demo-kitten-sqrm4 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:03:25.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2636" for this suite.
Aug 21 16:03:49.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:03:51.462: INFO: namespace kubectl-2636 deletion completed in 26.280117073s

• [SLOW TEST:57.928 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:03:51.464: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Aug 21 16:03:55.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec pod-sharedvolume-8ede210c-5c89-4569-b92b-ae78e0c25e0c -c busybox-main-container --namespace=emptydir-2430 -- cat /usr/share/volumeshare/shareddata.txt'
Aug 21 16:03:56.206: INFO: stderr: ""
Aug 21 16:03:56.206: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:03:56.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2430" for this suite.
Aug 21 16:04:04.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:04:06.373: INFO: namespace emptydir-2430 deletion completed in 10.117078206s

• [SLOW TEST:14.910 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:04:06.374: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 21 16:04:09.269: INFO: Successfully updated pod "pod-update-0e91864d-8c7a-4804-82e3-2967fafd6d1f"
STEP: verifying the updated pod is in kubernetes
Aug 21 16:04:09.292: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:04:09.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2609" for this suite.
Aug 21 16:04:25.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:04:27.671: INFO: namespace pods-2609 deletion completed in 18.352167687s

• [SLOW TEST:21.297 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:04:27.671: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 21 16:04:31.002: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:04:31.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9478" for this suite.
Aug 21 16:04:39.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:04:41.212: INFO: namespace container-runtime-9478 deletion completed in 10.132932767s

• [SLOW TEST:13.540 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:04:41.215: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-5ae71a9a-9e77-40be-a0ca-ec01505f6fa6 in namespace container-probe-9747
Aug 21 16:04:43.542: INFO: Started pod liveness-5ae71a9a-9e77-40be-a0ca-ec01505f6fa6 in namespace container-probe-9747
STEP: checking the pod's current state and verifying that restartCount is present
Aug 21 16:04:43.558: INFO: Initial restart count of pod liveness-5ae71a9a-9e77-40be-a0ca-ec01505f6fa6 is 0
Aug 21 16:05:03.742: INFO: Restart count of pod container-probe-9747/liveness-5ae71a9a-9e77-40be-a0ca-ec01505f6fa6 is now 1 (20.183763448s elapsed)
Aug 21 16:05:23.908: INFO: Restart count of pod container-probe-9747/liveness-5ae71a9a-9e77-40be-a0ca-ec01505f6fa6 is now 2 (40.349704199s elapsed)
Aug 21 16:05:44.066: INFO: Restart count of pod container-probe-9747/liveness-5ae71a9a-9e77-40be-a0ca-ec01505f6fa6 is now 3 (1m0.507345505s elapsed)
Aug 21 16:06:04.209: INFO: Restart count of pod container-probe-9747/liveness-5ae71a9a-9e77-40be-a0ca-ec01505f6fa6 is now 4 (1m20.651092744s elapsed)
Aug 21 16:07:14.910: INFO: Restart count of pod container-probe-9747/liveness-5ae71a9a-9e77-40be-a0ca-ec01505f6fa6 is now 5 (2m31.351648749s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:07:14.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9747" for this suite.
Aug 21 16:07:23.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:07:25.278: INFO: namespace container-probe-9747 deletion completed in 10.286390485s

• [SLOW TEST:164.063 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:07:25.278: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 21 16:07:25.561: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3de11754-c7d9-43bd-917f-88a6c1819d55" in namespace "downward-api-4481" to be "success or failure"
Aug 21 16:07:25.585: INFO: Pod "downwardapi-volume-3de11754-c7d9-43bd-917f-88a6c1819d55": Phase="Pending", Reason="", readiness=false. Elapsed: 23.852121ms
Aug 21 16:07:27.599: INFO: Pod "downwardapi-volume-3de11754-c7d9-43bd-917f-88a6c1819d55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.037047919s
STEP: Saw pod success
Aug 21 16:07:27.599: INFO: Pod "downwardapi-volume-3de11754-c7d9-43bd-917f-88a6c1819d55" satisfied condition "success or failure"
Aug 21 16:07:27.633: INFO: Trying to get logs from node 10.188.240.202 pod downwardapi-volume-3de11754-c7d9-43bd-917f-88a6c1819d55 container client-container: <nil>
STEP: delete the pod
Aug 21 16:07:27.751: INFO: Waiting for pod downwardapi-volume-3de11754-c7d9-43bd-917f-88a6c1819d55 to disappear
Aug 21 16:07:27.764: INFO: Pod downwardapi-volume-3de11754-c7d9-43bd-917f-88a6c1819d55 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:07:27.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4481" for this suite.
Aug 21 16:07:35.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:07:38.078: INFO: namespace downward-api-4481 deletion completed in 10.282435892s

• [SLOW TEST:12.800 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:07:38.079: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 21 16:07:39.432: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 21 16:07:41.485: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622859, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622859, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622859, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733622859, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 21 16:07:44.535: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:07:45.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8361" for this suite.
Aug 21 16:07:53.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:07:55.571: INFO: namespace webhook-8361 deletion completed in 10.209107544s
STEP: Destroying namespace "webhook-8361-markers" for this suite.
Aug 21 16:08:03.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:08:05.790: INFO: namespace webhook-8361-markers deletion completed in 10.218795652s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:27.801 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:08:05.881: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-6036
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6036 to expose endpoints map[]
Aug 21 16:08:06.137: INFO: Get endpoints failed (23.44485ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Aug 21 16:08:07.153: INFO: successfully validated that service endpoint-test2 in namespace services-6036 exposes endpoints map[] (1.039147302s elapsed)
STEP: Creating pod pod1 in namespace services-6036
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6036 to expose endpoints map[pod1:[80]]
Aug 21 16:08:09.294: INFO: successfully validated that service endpoint-test2 in namespace services-6036 exposes endpoints map[pod1:[80]] (2.084470492s elapsed)
STEP: Creating pod pod2 in namespace services-6036
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6036 to expose endpoints map[pod1:[80] pod2:[80]]
Aug 21 16:08:12.483: INFO: successfully validated that service endpoint-test2 in namespace services-6036 exposes endpoints map[pod1:[80] pod2:[80]] (3.150340959s elapsed)
STEP: Deleting pod pod1 in namespace services-6036
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6036 to expose endpoints map[pod2:[80]]
Aug 21 16:08:13.586: INFO: successfully validated that service endpoint-test2 in namespace services-6036 exposes endpoints map[pod2:[80]] (1.075162155s elapsed)
STEP: Deleting pod pod2 in namespace services-6036
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6036 to expose endpoints map[]
Aug 21 16:08:14.644: INFO: successfully validated that service endpoint-test2 in namespace services-6036 exposes endpoints map[] (1.03456052s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:08:14.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6036" for this suite.
Aug 21 16:08:30.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:08:33.114: INFO: namespace services-6036 deletion completed in 18.368673826s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:27.233 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:08:33.114: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:08:44.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4917" for this suite.
Aug 21 16:08:52.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:08:54.937: INFO: namespace resourcequota-4917 deletion completed in 10.245447392s

• [SLOW TEST:21.823 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:08:54.940: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Aug 21 16:08:55.185: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:09:00.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7074" for this suite.
Aug 21 16:09:16.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:09:18.447: INFO: namespace init-container-7074 deletion completed in 18.123114148s

• [SLOW TEST:23.507 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:09:18.448: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 16:09:18.658: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-04fc908c-2a82-404b-a66f-774879c4c303
STEP: Creating configMap with name cm-test-opt-upd-63b9c03a-f689-4119-b988-65ff0f308b64
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-04fc908c-2a82-404b-a66f-774879c4c303
STEP: Updating configmap cm-test-opt-upd-63b9c03a-f689-4119-b988-65ff0f308b64
STEP: Creating configMap with name cm-test-opt-create-ca717456-8bb6-4eae-8779-45e01598e376
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:09:25.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7876" for this suite.
Aug 21 16:09:41.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:09:43.524: INFO: namespace projected-7876 deletion completed in 18.425268098s

• [SLOW TEST:25.076 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:09:43.526: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-8216, will wait for the garbage collector to delete the pods
Aug 21 16:09:47.804: INFO: Deleting Job.batch foo took: 28.879307ms
Aug 21 16:09:48.405: INFO: Terminating Job.batch foo pods took: 600.457541ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:10:31.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8216" for this suite.
Aug 21 16:10:39.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:10:41.550: INFO: namespace job-8216 deletion completed in 10.196450454s

• [SLOW TEST:58.023 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:10:41.551: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 16:10:42.204: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-fab14e1b-89bd-48cf-9e4f-d9a0b8d1755d" in namespace "security-context-test-5235" to be "success or failure"
Aug 21 16:10:42.218: INFO: Pod "alpine-nnp-false-fab14e1b-89bd-48cf-9e4f-d9a0b8d1755d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.388479ms
Aug 21 16:10:44.236: INFO: Pod "alpine-nnp-false-fab14e1b-89bd-48cf-9e4f-d9a0b8d1755d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032145461s
Aug 21 16:10:46.247: INFO: Pod "alpine-nnp-false-fab14e1b-89bd-48cf-9e4f-d9a0b8d1755d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04354871s
Aug 21 16:10:48.259: INFO: Pod "alpine-nnp-false-fab14e1b-89bd-48cf-9e4f-d9a0b8d1755d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055311497s
Aug 21 16:10:48.259: INFO: Pod "alpine-nnp-false-fab14e1b-89bd-48cf-9e4f-d9a0b8d1755d" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:10:48.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5235" for this suite.
Aug 21 16:10:56.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:10:58.496: INFO: namespace security-context-test-5235 deletion completed in 10.144126181s

• [SLOW TEST:16.945 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:10:58.496: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 16:10:58.692: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:10:59.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2379" for this suite.
Aug 21 16:11:07.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:11:10.080: INFO: namespace custom-resource-definition-2379 deletion completed in 10.247940913s

• [SLOW TEST:11.584 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:11:10.081: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7855
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Aug 21 16:11:10.344: INFO: Found 0 stateful pods, waiting for 3
Aug 21 16:11:20.357: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 16:11:20.357: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 16:11:20.357: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Aug 21 16:11:30.361: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 16:11:30.361: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 16:11:30.361: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 16:11:30.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-7855 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 21 16:11:31.011: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 21 16:11:31.012: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 21 16:11:31.012: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Aug 21 16:11:41.121: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Aug 21 16:11:51.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-7855 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 16:11:51.617: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 21 16:11:51.617: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 21 16:11:51.617: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 21 16:12:01.709: INFO: Waiting for StatefulSet statefulset-7855/ss2 to complete update
Aug 21 16:12:01.709: INFO: Waiting for Pod statefulset-7855/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 21 16:12:01.709: INFO: Waiting for Pod statefulset-7855/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 21 16:12:01.709: INFO: Waiting for Pod statefulset-7855/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 21 16:12:11.759: INFO: Waiting for StatefulSet statefulset-7855/ss2 to complete update
Aug 21 16:12:11.759: INFO: Waiting for Pod statefulset-7855/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 21 16:12:11.759: INFO: Waiting for Pod statefulset-7855/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 21 16:12:21.736: INFO: Waiting for StatefulSet statefulset-7855/ss2 to complete update
Aug 21 16:12:21.736: INFO: Waiting for Pod statefulset-7855/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 21 16:12:31.736: INFO: Waiting for StatefulSet statefulset-7855/ss2 to complete update
STEP: Rolling back to a previous revision
Aug 21 16:12:41.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-7855 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 21 16:12:42.100: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 21 16:12:42.100: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 21 16:12:42.100: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 21 16:12:52.196: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Aug 21 16:13:02.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-7855 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 16:13:02.671: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 21 16:13:02.671: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 21 16:13:02.671: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 21 16:13:12.752: INFO: Waiting for StatefulSet statefulset-7855/ss2 to complete update
Aug 21 16:13:12.752: INFO: Waiting for Pod statefulset-7855/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Aug 21 16:13:12.752: INFO: Waiting for Pod statefulset-7855/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Aug 21 16:13:22.786: INFO: Waiting for StatefulSet statefulset-7855/ss2 to complete update
Aug 21 16:13:22.786: INFO: Waiting for Pod statefulset-7855/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Aug 21 16:13:32.777: INFO: Deleting all statefulset in ns statefulset-7855
Aug 21 16:13:32.788: INFO: Scaling statefulset ss2 to 0
Aug 21 16:13:52.846: INFO: Waiting for statefulset status.replicas updated to 0
Aug 21 16:13:52.856: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:13:52.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7855" for this suite.
Aug 21 16:14:02.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:14:05.120: INFO: namespace statefulset-7855 deletion completed in 12.185765739s

• [SLOW TEST:175.039 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:14:05.120: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Aug 21 16:14:05.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 create -f - --namespace=kubectl-6417'
Aug 21 16:14:05.941: INFO: stderr: ""
Aug 21 16:14:05.941: INFO: stdout: "pod/pause created\n"
Aug 21 16:14:05.941: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 21 16:14:05.941: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6417" to be "running and ready"
Aug 21 16:14:05.955: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 13.236647ms
Aug 21 16:14:07.966: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025153151s
Aug 21 16:14:09.979: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.037291534s
Aug 21 16:14:09.979: INFO: Pod "pause" satisfied condition "running and ready"
Aug 21 16:14:09.979: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Aug 21 16:14:09.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 label pods pause testing-label=testing-label-value --namespace=kubectl-6417'
Aug 21 16:14:10.161: INFO: stderr: ""
Aug 21 16:14:10.161: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Aug 21 16:14:10.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pod pause -L testing-label --namespace=kubectl-6417'
Aug 21 16:14:10.304: INFO: stderr: ""
Aug 21 16:14:10.304: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Aug 21 16:14:10.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 label pods pause testing-label- --namespace=kubectl-6417'
Aug 21 16:14:10.492: INFO: stderr: ""
Aug 21 16:14:10.492: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Aug 21 16:14:10.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pod pause -L testing-label --namespace=kubectl-6417'
Aug 21 16:14:10.649: INFO: stderr: ""
Aug 21 16:14:10.649: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Aug 21 16:14:10.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 delete --grace-period=0 --force -f - --namespace=kubectl-6417'
Aug 21 16:14:10.812: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 21 16:14:10.812: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 21 16:14:10.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get rc,svc -l name=pause --no-headers --namespace=kubectl-6417'
Aug 21 16:14:10.984: INFO: stderr: "No resources found in kubectl-6417 namespace.\n"
Aug 21 16:14:10.984: INFO: stdout: ""
Aug 21 16:14:10.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods -l name=pause --namespace=kubectl-6417 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 21 16:14:11.145: INFO: stderr: ""
Aug 21 16:14:11.145: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:14:11.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6417" for this suite.
Aug 21 16:14:19.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:14:21.349: INFO: namespace kubectl-6417 deletion completed in 10.174946629s

• [SLOW TEST:16.229 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:14:21.350: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-9040
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 21 16:14:21.559: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 21 16:14:48.089: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.174.109:8080/dial?request=hostName&protocol=http&host=172.30.172.55&port=8080&tries=1'] Namespace:pod-network-test-9040 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 16:14:48.089: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 16:14:48.360: INFO: Waiting for endpoints: map[]
Aug 21 16:14:48.374: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.174.109:8080/dial?request=hostName&protocol=http&host=172.30.174.110&port=8080&tries=1'] Namespace:pod-network-test-9040 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 16:14:48.374: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 16:14:48.636: INFO: Waiting for endpoints: map[]
Aug 21 16:14:48.648: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.174.109:8080/dial?request=hostName&protocol=http&host=172.30.245.126&port=8080&tries=1'] Namespace:pod-network-test-9040 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 16:14:48.648: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 16:14:48.918: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:14:48.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9040" for this suite.
Aug 21 16:14:59.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:15:01.411: INFO: namespace pod-network-test-9040 deletion completed in 12.465685965s

• [SLOW TEST:40.062 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:15:01.412: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Aug 21 16:15:01.688: INFO: namespace kubectl-8906
Aug 21 16:15:01.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 create -f - --namespace=kubectl-8906'
Aug 21 16:15:02.202: INFO: stderr: ""
Aug 21 16:15:02.202: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 21 16:15:03.217: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 16:15:03.217: INFO: Found 0 / 1
Aug 21 16:15:04.247: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 16:15:04.247: INFO: Found 0 / 1
Aug 21 16:15:05.216: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 16:15:05.216: INFO: Found 0 / 1
Aug 21 16:15:06.221: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 16:15:06.221: INFO: Found 0 / 1
Aug 21 16:15:07.216: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 16:15:07.216: INFO: Found 0 / 1
Aug 21 16:15:08.215: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 16:15:08.215: INFO: Found 1 / 1
Aug 21 16:15:08.215: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 21 16:15:08.228: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 16:15:08.228: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 21 16:15:08.228: INFO: wait on redis-master startup in kubectl-8906 
Aug 21 16:15:08.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 logs redis-master-pwc2b redis-master --namespace=kubectl-8906'
Aug 21 16:15:08.456: INFO: stderr: ""
Aug 21 16:15:08.456: INFO: stdout: "1:C 21 Aug 2020 16:15:06.887 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 21 Aug 2020 16:15:06.887 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 21 Aug 2020 16:15:06.887 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 21 Aug 2020 16:15:06.890 * Running mode=standalone, port=6379.\n1:M 21 Aug 2020 16:15:06.890 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 21 Aug 2020 16:15:06.890 # Server initialized\n1:M 21 Aug 2020 16:15:06.890 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 21 Aug 2020 16:15:06.890 * Ready to accept connections\n"
STEP: exposing RC
Aug 21 16:15:08.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-8906'
Aug 21 16:15:08.667: INFO: stderr: ""
Aug 21 16:15:08.667: INFO: stdout: "service/rm2 exposed\n"
Aug 21 16:15:08.684: INFO: Service rm2 in namespace kubectl-8906 found.
STEP: exposing service
Aug 21 16:15:10.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-8906'
Aug 21 16:15:10.923: INFO: stderr: ""
Aug 21 16:15:10.923: INFO: stdout: "service/rm3 exposed\n"
Aug 21 16:15:10.934: INFO: Service rm3 in namespace kubectl-8906 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:15:12.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8906" for this suite.
Aug 21 16:15:47.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:15:49.271: INFO: namespace kubectl-8906 deletion completed in 36.257002692s

• [SLOW TEST:47.859 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:15:49.271: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Aug 21 16:15:49.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 create -f - --namespace=kubectl-4488'
Aug 21 16:15:50.233: INFO: stderr: ""
Aug 21 16:15:50.233: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 21 16:15:50.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4488'
Aug 21 16:15:50.395: INFO: stderr: ""
Aug 21 16:15:50.395: INFO: stdout: "update-demo-nautilus-tj4hv update-demo-nautilus-xs9mt "
Aug 21 16:15:50.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-nautilus-tj4hv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4488'
Aug 21 16:15:50.555: INFO: stderr: ""
Aug 21 16:15:50.555: INFO: stdout: ""
Aug 21 16:15:50.555: INFO: update-demo-nautilus-tj4hv is created but not running
Aug 21 16:15:55.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4488'
Aug 21 16:15:55.711: INFO: stderr: ""
Aug 21 16:15:55.711: INFO: stdout: "update-demo-nautilus-tj4hv update-demo-nautilus-xs9mt "
Aug 21 16:15:55.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-nautilus-tj4hv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4488'
Aug 21 16:15:55.848: INFO: stderr: ""
Aug 21 16:15:55.848: INFO: stdout: "true"
Aug 21 16:15:55.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-nautilus-tj4hv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4488'
Aug 21 16:15:55.995: INFO: stderr: ""
Aug 21 16:15:55.996: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 21 16:15:55.996: INFO: validating pod update-demo-nautilus-tj4hv
Aug 21 16:15:56.032: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 21 16:15:56.032: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 21 16:15:56.032: INFO: update-demo-nautilus-tj4hv is verified up and running
Aug 21 16:15:56.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-nautilus-xs9mt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4488'
Aug 21 16:15:56.183: INFO: stderr: ""
Aug 21 16:15:56.183: INFO: stdout: "true"
Aug 21 16:15:56.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-nautilus-xs9mt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4488'
Aug 21 16:15:56.325: INFO: stderr: ""
Aug 21 16:15:56.325: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 21 16:15:56.325: INFO: validating pod update-demo-nautilus-xs9mt
Aug 21 16:15:56.357: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 21 16:15:56.357: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 21 16:15:56.357: INFO: update-demo-nautilus-xs9mt is verified up and running
STEP: scaling down the replication controller
Aug 21 16:15:56.362: INFO: scanned /root for discovery docs: <nil>
Aug 21 16:15:56.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-4488'
Aug 21 16:15:57.569: INFO: stderr: ""
Aug 21 16:15:57.569: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 21 16:15:57.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4488'
Aug 21 16:15:57.732: INFO: stderr: ""
Aug 21 16:15:57.732: INFO: stdout: "update-demo-nautilus-tj4hv update-demo-nautilus-xs9mt "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug 21 16:16:02.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4488'
Aug 21 16:16:02.877: INFO: stderr: ""
Aug 21 16:16:02.877: INFO: stdout: "update-demo-nautilus-tj4hv "
Aug 21 16:16:02.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-nautilus-tj4hv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4488'
Aug 21 16:16:03.009: INFO: stderr: ""
Aug 21 16:16:03.009: INFO: stdout: "true"
Aug 21 16:16:03.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-nautilus-tj4hv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4488'
Aug 21 16:16:03.168: INFO: stderr: ""
Aug 21 16:16:03.168: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 21 16:16:03.168: INFO: validating pod update-demo-nautilus-tj4hv
Aug 21 16:16:03.186: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 21 16:16:03.186: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 21 16:16:03.186: INFO: update-demo-nautilus-tj4hv is verified up and running
STEP: scaling up the replication controller
Aug 21 16:16:03.192: INFO: scanned /root for discovery docs: <nil>
Aug 21 16:16:03.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-4488'
Aug 21 16:16:04.425: INFO: stderr: ""
Aug 21 16:16:04.425: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 21 16:16:04.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4488'
Aug 21 16:16:04.575: INFO: stderr: ""
Aug 21 16:16:04.575: INFO: stdout: "update-demo-nautilus-tj4hv update-demo-nautilus-w9k8h "
Aug 21 16:16:04.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-nautilus-tj4hv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4488'
Aug 21 16:16:04.744: INFO: stderr: ""
Aug 21 16:16:04.744: INFO: stdout: "true"
Aug 21 16:16:04.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-nautilus-tj4hv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4488'
Aug 21 16:16:04.903: INFO: stderr: ""
Aug 21 16:16:04.903: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 21 16:16:04.903: INFO: validating pod update-demo-nautilus-tj4hv
Aug 21 16:16:04.926: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 21 16:16:04.926: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 21 16:16:04.926: INFO: update-demo-nautilus-tj4hv is verified up and running
Aug 21 16:16:04.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-nautilus-w9k8h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4488'
Aug 21 16:16:05.074: INFO: stderr: ""
Aug 21 16:16:05.074: INFO: stdout: ""
Aug 21 16:16:05.074: INFO: update-demo-nautilus-w9k8h is created but not running
Aug 21 16:16:10.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4488'
Aug 21 16:16:10.246: INFO: stderr: ""
Aug 21 16:16:10.246: INFO: stdout: "update-demo-nautilus-tj4hv update-demo-nautilus-w9k8h "
Aug 21 16:16:10.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-nautilus-tj4hv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4488'
Aug 21 16:16:10.387: INFO: stderr: ""
Aug 21 16:16:10.387: INFO: stdout: "true"
Aug 21 16:16:10.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-nautilus-tj4hv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4488'
Aug 21 16:16:10.548: INFO: stderr: ""
Aug 21 16:16:10.548: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 21 16:16:10.548: INFO: validating pod update-demo-nautilus-tj4hv
Aug 21 16:16:10.576: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 21 16:16:10.576: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 21 16:16:10.576: INFO: update-demo-nautilus-tj4hv is verified up and running
Aug 21 16:16:10.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-nautilus-w9k8h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4488'
Aug 21 16:16:10.726: INFO: stderr: ""
Aug 21 16:16:10.726: INFO: stdout: "true"
Aug 21 16:16:10.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods update-demo-nautilus-w9k8h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4488'
Aug 21 16:16:10.865: INFO: stderr: ""
Aug 21 16:16:10.865: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 21 16:16:10.865: INFO: validating pod update-demo-nautilus-w9k8h
Aug 21 16:16:10.894: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 21 16:16:10.894: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 21 16:16:10.894: INFO: update-demo-nautilus-w9k8h is verified up and running
STEP: using delete to clean up resources
Aug 21 16:16:10.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 delete --grace-period=0 --force -f - --namespace=kubectl-4488'
Aug 21 16:16:11.079: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 21 16:16:11.079: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 21 16:16:11.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4488'
Aug 21 16:16:11.247: INFO: stderr: "No resources found in kubectl-4488 namespace.\n"
Aug 21 16:16:11.247: INFO: stdout: ""
Aug 21 16:16:11.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods -l name=update-demo --namespace=kubectl-4488 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 21 16:16:11.387: INFO: stderr: ""
Aug 21 16:16:11.387: INFO: stdout: "update-demo-nautilus-tj4hv\nupdate-demo-nautilus-w9k8h\n"
Aug 21 16:16:11.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4488'
Aug 21 16:16:12.056: INFO: stderr: "No resources found in kubectl-4488 namespace.\n"
Aug 21 16:16:12.056: INFO: stdout: ""
Aug 21 16:16:12.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods -l name=update-demo --namespace=kubectl-4488 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 21 16:16:12.217: INFO: stderr: ""
Aug 21 16:16:12.217: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:16:12.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4488" for this suite.
Aug 21 16:16:28.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:16:30.705: INFO: namespace kubectl-4488 deletion completed in 18.429583583s

• [SLOW TEST:41.434 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:16:30.705: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 16:16:31.096: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"12bf7940-2ae2-45c2-8a93-5eba83fabd74", Controller:(*bool)(0xc00275dd22), BlockOwnerDeletion:(*bool)(0xc00275dd23)}}
Aug 21 16:16:31.146: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"9c942249-3250-49ac-86ab-f2d50b840bc7", Controller:(*bool)(0xc0033b6d62), BlockOwnerDeletion:(*bool)(0xc0033b6d63)}}
Aug 21 16:16:31.162: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"b8837e21-1c74-4d0b-ad0b-fbda3742414f", Controller:(*bool)(0xc0033b6f96), BlockOwnerDeletion:(*bool)(0xc0033b6f97)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:16:36.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4880" for this suite.
Aug 21 16:16:44.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:16:46.434: INFO: namespace gc-4880 deletion completed in 10.198050739s

• [SLOW TEST:15.730 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:16:46.435: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 16:16:46.760: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-b1285551-dfc0-42fc-86bb-ffcae925afb4
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-b1285551-dfc0-42fc-86bb-ffcae925afb4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:16:52.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9528" for this suite.
Aug 21 16:17:08.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:17:10.268: INFO: namespace projected-9528 deletion completed in 18.215581091s

• [SLOW TEST:23.833 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:17:10.268: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 16:17:10.507: INFO: Creating deployment "test-recreate-deployment"
Aug 21 16:17:10.527: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 21 16:17:10.567: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Aug 21 16:17:12.598: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 21 16:17:12.611: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733623430, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733623430, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733623430, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733623430, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 21 16:17:14.627: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 21 16:17:14.660: INFO: Updating deployment test-recreate-deployment
Aug 21 16:17:14.661: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Aug 21 16:17:14.932: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-7226 /apis/apps/v1/namespaces/deployment-7226/deployments/test-recreate-deployment e4350891-8745-45f6-adba-0ca397859a9a 52492 2 2020-08-21 16:17:10 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc000715c28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-08-21 16:17:14 +0000 UTC,LastTransitionTime:2020-08-21 16:17:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-08-21 16:17:14 +0000 UTC,LastTransitionTime:2020-08-21 16:17:10 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Aug 21 16:17:14.953: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-7226 /apis/apps/v1/namespaces/deployment-7226/replicasets/test-recreate-deployment-5f94c574ff 0550ac42-ac94-4e47-a592-33f1f4c4480e 52490 1 2020-08-21 16:17:14 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment e4350891-8745-45f6-adba-0ca397859a9a 0xc003f1faf7 0xc003f1faf8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003f1fb58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 21 16:17:14.953: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 21 16:17:14.953: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-7226 /apis/apps/v1/namespaces/deployment-7226/replicasets/test-recreate-deployment-68fc85c7bb 7d18acfe-b746-44e4-8e58-908115970b78 52480 2 2020-08-21 16:17:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment e4350891-8745-45f6-adba-0ca397859a9a 0xc003f1fbc7 0xc003f1fbc8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003f1fc28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 21 16:17:14.965: INFO: Pod "test-recreate-deployment-5f94c574ff-jsgdm" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-jsgdm test-recreate-deployment-5f94c574ff- deployment-7226 /api/v1/namespaces/deployment-7226/pods/test-recreate-deployment-5f94c574ff-jsgdm c576f04e-2f73-4f63-8ec4-f1e476d627c9 52493 0 2020-08-21 16:17:14 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 0550ac42-ac94-4e47-a592-33f1f4c4480e 0xc0006723e7 0xc0006723e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pqrdf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pqrdf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pqrdf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c8mn4,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 16:17:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 16:17:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 16:17:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 16:17:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.202,PodIP:,StartTime:2020-08-21 16:17:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:17:14.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7226" for this suite.
Aug 21 16:17:25.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:17:27.336: INFO: namespace deployment-7226 deletion completed in 12.317492718s

• [SLOW TEST:17.068 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:17:27.336: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:17:32.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7704" for this suite.
Aug 21 16:17:54.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:17:56.971: INFO: namespace containers-7704 deletion completed in 24.247184168s

• [SLOW TEST:29.635 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:17:56.973: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-ce44e62a-3d81-4c83-887f-cb85eb43b184
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:17:57.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1010" for this suite.
Aug 21 16:18:05.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:18:07.514: INFO: namespace configmap-1010 deletion completed in 10.241368285s

• [SLOW TEST:10.541 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:18:07.514: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 21 16:18:07.944: INFO: Number of nodes with available pods: 0
Aug 21 16:18:07.944: INFO: Node 10.188.240.202 is running more than one daemon pod
Aug 21 16:18:08.982: INFO: Number of nodes with available pods: 0
Aug 21 16:18:08.982: INFO: Node 10.188.240.202 is running more than one daemon pod
Aug 21 16:18:09.980: INFO: Number of nodes with available pods: 1
Aug 21 16:18:09.980: INFO: Node 10.188.240.222 is running more than one daemon pod
Aug 21 16:18:10.982: INFO: Number of nodes with available pods: 3
Aug 21 16:18:10.982: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Aug 21 16:18:11.071: INFO: Number of nodes with available pods: 2
Aug 21 16:18:11.071: INFO: Node 10.188.240.230 is running more than one daemon pod
Aug 21 16:18:12.109: INFO: Number of nodes with available pods: 2
Aug 21 16:18:12.110: INFO: Node 10.188.240.230 is running more than one daemon pod
Aug 21 16:18:13.106: INFO: Number of nodes with available pods: 2
Aug 21 16:18:13.106: INFO: Node 10.188.240.230 is running more than one daemon pod
Aug 21 16:18:14.106: INFO: Number of nodes with available pods: 2
Aug 21 16:18:14.106: INFO: Node 10.188.240.230 is running more than one daemon pod
Aug 21 16:18:15.107: INFO: Number of nodes with available pods: 2
Aug 21 16:18:15.107: INFO: Node 10.188.240.230 is running more than one daemon pod
Aug 21 16:18:16.103: INFO: Number of nodes with available pods: 2
Aug 21 16:18:16.103: INFO: Node 10.188.240.230 is running more than one daemon pod
Aug 21 16:18:17.104: INFO: Number of nodes with available pods: 3
Aug 21 16:18:17.104: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6871, will wait for the garbage collector to delete the pods
Aug 21 16:18:17.226: INFO: Deleting DaemonSet.extensions daemon-set took: 39.631773ms
Aug 21 16:18:17.826: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.46177ms
Aug 21 16:18:27.541: INFO: Number of nodes with available pods: 0
Aug 21 16:18:27.541: INFO: Number of running nodes: 0, number of available pods: 0
Aug 21 16:18:27.562: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6871/daemonsets","resourceVersion":"53095"},"items":null}

Aug 21 16:18:27.574: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6871/pods","resourceVersion":"53095"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:18:27.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6871" for this suite.
Aug 21 16:18:37.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:18:39.894: INFO: namespace daemonsets-6871 deletion completed in 12.229032466s

• [SLOW TEST:32.380 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:18:39.897: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 21 16:18:40.326: INFO: Waiting up to 5m0s for pod "pod-ec6c5495-9081-4d76-83af-c5583cc1041b" in namespace "emptydir-7571" to be "success or failure"
Aug 21 16:18:40.338: INFO: Pod "pod-ec6c5495-9081-4d76-83af-c5583cc1041b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.886399ms
Aug 21 16:18:42.350: INFO: Pod "pod-ec6c5495-9081-4d76-83af-c5583cc1041b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023717293s
STEP: Saw pod success
Aug 21 16:18:42.350: INFO: Pod "pod-ec6c5495-9081-4d76-83af-c5583cc1041b" satisfied condition "success or failure"
Aug 21 16:18:42.361: INFO: Trying to get logs from node 10.188.240.202 pod pod-ec6c5495-9081-4d76-83af-c5583cc1041b container test-container: <nil>
STEP: delete the pod
Aug 21 16:18:42.428: INFO: Waiting for pod pod-ec6c5495-9081-4d76-83af-c5583cc1041b to disappear
Aug 21 16:18:42.440: INFO: Pod pod-ec6c5495-9081-4d76-83af-c5583cc1041b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:18:42.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7571" for this suite.
Aug 21 16:18:50.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:18:52.662: INFO: namespace emptydir-7571 deletion completed in 10.197887343s

• [SLOW TEST:12.765 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:18:52.662: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-ca72122c-8ccf-4c8b-8fab-2177b86349be
Aug 21 16:18:52.898: INFO: Pod name my-hostname-basic-ca72122c-8ccf-4c8b-8fab-2177b86349be: Found 0 pods out of 1
Aug 21 16:18:57.911: INFO: Pod name my-hostname-basic-ca72122c-8ccf-4c8b-8fab-2177b86349be: Found 1 pods out of 1
Aug 21 16:18:57.911: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-ca72122c-8ccf-4c8b-8fab-2177b86349be" are running
Aug 21 16:18:57.921: INFO: Pod "my-hostname-basic-ca72122c-8ccf-4c8b-8fab-2177b86349be-fx74l" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-21 16:18:54 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-21 16:18:55 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-21 16:18:55 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-21 16:18:53 +0000 UTC Reason: Message:}])
Aug 21 16:18:57.921: INFO: Trying to dial the pod
Aug 21 16:19:02.978: INFO: Controller my-hostname-basic-ca72122c-8ccf-4c8b-8fab-2177b86349be: Got expected result from replica 1 [my-hostname-basic-ca72122c-8ccf-4c8b-8fab-2177b86349be-fx74l]: "my-hostname-basic-ca72122c-8ccf-4c8b-8fab-2177b86349be-fx74l", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:19:02.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4467" for this suite.
Aug 21 16:19:13.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:19:15.196: INFO: namespace replication-controller-4467 deletion completed in 12.196600179s

• [SLOW TEST:22.533 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:19:15.198: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 21 16:19:15.469: INFO: Waiting up to 5m0s for pod "pod-42d28adf-88a4-4438-9ba6-b9d75195191c" in namespace "emptydir-4169" to be "success or failure"
Aug 21 16:19:15.482: INFO: Pod "pod-42d28adf-88a4-4438-9ba6-b9d75195191c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.423097ms
Aug 21 16:19:17.494: INFO: Pod "pod-42d28adf-88a4-4438-9ba6-b9d75195191c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024734121s
STEP: Saw pod success
Aug 21 16:19:17.494: INFO: Pod "pod-42d28adf-88a4-4438-9ba6-b9d75195191c" satisfied condition "success or failure"
Aug 21 16:19:17.521: INFO: Trying to get logs from node 10.188.240.230 pod pod-42d28adf-88a4-4438-9ba6-b9d75195191c container test-container: <nil>
STEP: delete the pod
Aug 21 16:19:17.607: INFO: Waiting for pod pod-42d28adf-88a4-4438-9ba6-b9d75195191c to disappear
Aug 21 16:19:17.617: INFO: Pod pod-42d28adf-88a4-4438-9ba6-b9d75195191c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:19:17.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4169" for this suite.
Aug 21 16:19:25.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:19:27.987: INFO: namespace emptydir-4169 deletion completed in 10.347978807s

• [SLOW TEST:12.790 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:19:27.989: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Aug 21 16:19:29.273: INFO: Waiting up to 5m0s for pod "var-expansion-76786088-ebd8-4b6b-b778-4caecf93a87d" in namespace "var-expansion-2731" to be "success or failure"
Aug 21 16:19:29.284: INFO: Pod "var-expansion-76786088-ebd8-4b6b-b778-4caecf93a87d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.319886ms
Aug 21 16:19:31.296: INFO: Pod "var-expansion-76786088-ebd8-4b6b-b778-4caecf93a87d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022975511s
Aug 21 16:19:33.313: INFO: Pod "var-expansion-76786088-ebd8-4b6b-b778-4caecf93a87d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040250644s
STEP: Saw pod success
Aug 21 16:19:33.313: INFO: Pod "var-expansion-76786088-ebd8-4b6b-b778-4caecf93a87d" satisfied condition "success or failure"
Aug 21 16:19:33.327: INFO: Trying to get logs from node 10.188.240.230 pod var-expansion-76786088-ebd8-4b6b-b778-4caecf93a87d container dapi-container: <nil>
STEP: delete the pod
Aug 21 16:19:33.397: INFO: Waiting for pod var-expansion-76786088-ebd8-4b6b-b778-4caecf93a87d to disappear
Aug 21 16:19:33.409: INFO: Pod var-expansion-76786088-ebd8-4b6b-b778-4caecf93a87d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:19:33.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2731" for this suite.
Aug 21 16:19:43.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:19:45.757: INFO: namespace var-expansion-2731 deletion completed in 12.315441166s

• [SLOW TEST:17.768 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:19:45.758: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 21 16:19:45.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-1990'
Aug 21 16:19:46.168: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 21 16:19:46.168: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Aug 21 16:19:50.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 delete deployment e2e-test-httpd-deployment --namespace=kubectl-1990'
Aug 21 16:19:50.409: INFO: stderr: ""
Aug 21 16:19:50.409: INFO: stdout: "deployment.extensions \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:19:50.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1990" for this suite.
Aug 21 16:19:58.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:20:00.735: INFO: namespace kubectl-1990 deletion completed in 10.293279188s

• [SLOW TEST:14.978 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:20:00.737: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 21 16:20:01.756: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 21 16:20:03.795: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733623601, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733623601, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733623601, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733623601, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 21 16:20:06.842: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 16:20:06.855: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6864-crds.webhook.example.com via the AdmissionRegistration API
Aug 21 16:20:07.518: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:20:08.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2791" for this suite.
Aug 21 16:20:16.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:20:18.745: INFO: namespace webhook-2791 deletion completed in 10.13040077s
STEP: Destroying namespace "webhook-2791-markers" for this suite.
Aug 21 16:20:26.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:20:28.985: INFO: namespace webhook-2791-markers deletion completed in 10.239267218s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:28.325 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:20:29.062: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Aug 21 16:20:29.936: INFO: created pod pod-service-account-defaultsa
Aug 21 16:20:29.936: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 21 16:20:29.993: INFO: created pod pod-service-account-mountsa
Aug 21 16:20:29.993: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 21 16:20:30.056: INFO: created pod pod-service-account-nomountsa
Aug 21 16:20:30.056: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 21 16:20:30.110: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 21 16:20:30.111: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 21 16:20:30.156: INFO: created pod pod-service-account-mountsa-mountspec
Aug 21 16:20:30.162: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 21 16:20:30.215: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 21 16:20:30.216: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 21 16:20:30.258: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 21 16:20:30.258: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 21 16:20:30.302: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 21 16:20:30.302: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 21 16:20:30.346: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 21 16:20:30.346: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:20:30.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-519" for this suite.
Aug 21 16:20:40.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:20:42.532: INFO: namespace svcaccounts-519 deletion completed in 12.1566593s

• [SLOW TEST:13.470 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:20:42.534: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-z2lsz in namespace proxy-9643
I0821 16:20:42.744774      24 runners.go:184] Created replication controller with name: proxy-service-z2lsz, namespace: proxy-9643, replica count: 1
I0821 16:20:43.795424      24 runners.go:184] proxy-service-z2lsz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0821 16:20:44.795714      24 runners.go:184] proxy-service-z2lsz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0821 16:20:45.795957      24 runners.go:184] proxy-service-z2lsz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0821 16:20:46.796262      24 runners.go:184] proxy-service-z2lsz Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 21 16:20:46.809: INFO: setup took 4.114117305s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Aug 21 16:20:46.852: INFO: (0) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 42.212777ms)
Aug 21 16:20:46.852: INFO: (0) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 42.39731ms)
Aug 21 16:20:46.852: INFO: (0) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">test<... (200; 42.882967ms)
Aug 21 16:20:46.852: INFO: (0) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">... (200; 43.15356ms)
Aug 21 16:20:46.852: INFO: (0) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/rewriteme">test</a> (200; 42.933406ms)
Aug 21 16:20:46.852: INFO: (0) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 43.001614ms)
Aug 21 16:20:46.852: INFO: (0) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 42.941892ms)
Aug 21 16:20:46.857: INFO: (0) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname2/proxy/: bar (200; 47.575817ms)
Aug 21 16:20:46.857: INFO: (0) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname2/proxy/: bar (200; 47.721329ms)
Aug 21 16:20:46.858: INFO: (0) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname1/proxy/: foo (200; 48.712527ms)
Aug 21 16:20:46.858: INFO: (0) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname1/proxy/: foo (200; 48.36763ms)
Aug 21 16:20:46.868: INFO: (0) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/tlsrewritem... (200; 58.451597ms)
Aug 21 16:20:46.868: INFO: (0) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:462/proxy/: tls qux (200; 58.843327ms)
Aug 21 16:20:46.868: INFO: (0) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:460/proxy/: tls baz (200; 59.302345ms)
Aug 21 16:20:46.871: INFO: (0) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname2/proxy/: tls qux (200; 62.015145ms)
Aug 21 16:20:46.877: INFO: (0) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname1/proxy/: tls baz (200; 67.668369ms)
Aug 21 16:20:46.896: INFO: (1) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">... (200; 18.572292ms)
Aug 21 16:20:46.896: INFO: (1) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">test<... (200; 18.962284ms)
Aug 21 16:20:46.901: INFO: (1) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 22.651719ms)
Aug 21 16:20:46.901: INFO: (1) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 22.509407ms)
Aug 21 16:20:46.901: INFO: (1) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 22.68763ms)
Aug 21 16:20:46.901: INFO: (1) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:462/proxy/: tls qux (200; 22.691333ms)
Aug 21 16:20:46.902: INFO: (1) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 23.671931ms)
Aug 21 16:20:46.902: INFO: (1) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:460/proxy/: tls baz (200; 23.862968ms)
Aug 21 16:20:46.902: INFO: (1) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/tlsrewritem... (200; 23.936644ms)
Aug 21 16:20:46.906: INFO: (1) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname2/proxy/: bar (200; 28.10797ms)
Aug 21 16:20:46.906: INFO: (1) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/rewriteme">test</a> (200; 27.904079ms)
Aug 21 16:20:46.910: INFO: (1) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname1/proxy/: foo (200; 32.226949ms)
Aug 21 16:20:46.916: INFO: (1) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname1/proxy/: tls baz (200; 38.021702ms)
Aug 21 16:20:46.916: INFO: (1) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname2/proxy/: tls qux (200; 38.057181ms)
Aug 21 16:20:46.916: INFO: (1) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname2/proxy/: bar (200; 38.053672ms)
Aug 21 16:20:46.916: INFO: (1) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname1/proxy/: foo (200; 38.379394ms)
Aug 21 16:20:46.939: INFO: (2) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">test<... (200; 22.826258ms)
Aug 21 16:20:46.939: INFO: (2) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/rewriteme">test</a> (200; 22.841676ms)
Aug 21 16:20:46.942: INFO: (2) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 25.948006ms)
Aug 21 16:20:46.943: INFO: (2) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:462/proxy/: tls qux (200; 25.987856ms)
Aug 21 16:20:46.943: INFO: (2) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 26.191531ms)
Aug 21 16:20:46.943: INFO: (2) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:460/proxy/: tls baz (200; 26.378508ms)
Aug 21 16:20:46.943: INFO: (2) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 26.344707ms)
Aug 21 16:20:46.943: INFO: (2) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">... (200; 26.473109ms)
Aug 21 16:20:46.943: INFO: (2) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 26.496253ms)
Aug 21 16:20:46.943: INFO: (2) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/tlsrewritem... (200; 26.666266ms)
Aug 21 16:20:46.953: INFO: (2) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname2/proxy/: tls qux (200; 36.336598ms)
Aug 21 16:20:46.953: INFO: (2) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname2/proxy/: bar (200; 36.768827ms)
Aug 21 16:20:46.953: INFO: (2) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname1/proxy/: foo (200; 36.645312ms)
Aug 21 16:20:46.953: INFO: (2) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname1/proxy/: tls baz (200; 37.015501ms)
Aug 21 16:20:46.954: INFO: (2) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname2/proxy/: bar (200; 37.090222ms)
Aug 21 16:20:46.954: INFO: (2) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname1/proxy/: foo (200; 37.326659ms)
Aug 21 16:20:46.979: INFO: (3) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 24.336936ms)
Aug 21 16:20:46.986: INFO: (3) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/rewriteme">test</a> (200; 31.260573ms)
Aug 21 16:20:46.986: INFO: (3) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/tlsrewritem... (200; 31.208726ms)
Aug 21 16:20:46.989: INFO: (3) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">... (200; 34.007677ms)
Aug 21 16:20:46.989: INFO: (3) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 34.185472ms)
Aug 21 16:20:46.989: INFO: (3) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 33.986678ms)
Aug 21 16:20:46.989: INFO: (3) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:462/proxy/: tls qux (200; 34.255628ms)
Aug 21 16:20:46.989: INFO: (3) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:460/proxy/: tls baz (200; 34.00554ms)
Aug 21 16:20:46.989: INFO: (3) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 34.159022ms)
Aug 21 16:20:46.989: INFO: (3) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">test<... (200; 34.402408ms)
Aug 21 16:20:46.993: INFO: (3) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname1/proxy/: tls baz (200; 39.417389ms)
Aug 21 16:20:46.993: INFO: (3) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname2/proxy/: bar (200; 38.75457ms)
Aug 21 16:20:47.000: INFO: (3) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname2/proxy/: tls qux (200; 45.223271ms)
Aug 21 16:20:47.006: INFO: (3) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname1/proxy/: foo (200; 51.003857ms)
Aug 21 16:20:47.006: INFO: (3) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname1/proxy/: foo (200; 50.903452ms)
Aug 21 16:20:47.006: INFO: (3) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname2/proxy/: bar (200; 51.144865ms)
Aug 21 16:20:47.027: INFO: (4) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">test<... (200; 21.11113ms)
Aug 21 16:20:47.033: INFO: (4) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:460/proxy/: tls baz (200; 27.215227ms)
Aug 21 16:20:47.035: INFO: (4) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:462/proxy/: tls qux (200; 29.314117ms)
Aug 21 16:20:47.035: INFO: (4) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 29.112455ms)
Aug 21 16:20:47.035: INFO: (4) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 29.00284ms)
Aug 21 16:20:47.035: INFO: (4) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/tlsrewritem... (200; 29.344837ms)
Aug 21 16:20:47.035: INFO: (4) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 29.274093ms)
Aug 21 16:20:47.036: INFO: (4) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/rewriteme">test</a> (200; 29.484053ms)
Aug 21 16:20:47.036: INFO: (4) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 29.496699ms)
Aug 21 16:20:47.036: INFO: (4) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">... (200; 29.753463ms)
Aug 21 16:20:47.041: INFO: (4) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname1/proxy/: tls baz (200; 35.203679ms)
Aug 21 16:20:47.050: INFO: (4) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname2/proxy/: bar (200; 43.574162ms)
Aug 21 16:20:47.058: INFO: (4) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname2/proxy/: tls qux (200; 51.741024ms)
Aug 21 16:20:47.058: INFO: (4) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname1/proxy/: foo (200; 51.810319ms)
Aug 21 16:20:47.058: INFO: (4) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname2/proxy/: bar (200; 51.931767ms)
Aug 21 16:20:47.058: INFO: (4) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname1/proxy/: foo (200; 51.962254ms)
Aug 21 16:20:47.083: INFO: (5) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 23.753235ms)
Aug 21 16:20:47.083: INFO: (5) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:462/proxy/: tls qux (200; 24.344698ms)
Aug 21 16:20:47.083: INFO: (5) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:460/proxy/: tls baz (200; 23.581829ms)
Aug 21 16:20:47.083: INFO: (5) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">... (200; 24.15249ms)
Aug 21 16:20:47.083: INFO: (5) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 23.726619ms)
Aug 21 16:20:47.083: INFO: (5) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/rewriteme">test</a> (200; 24.844835ms)
Aug 21 16:20:47.083: INFO: (5) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">test<... (200; 24.09039ms)
Aug 21 16:20:47.088: INFO: (5) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 28.531627ms)
Aug 21 16:20:47.088: INFO: (5) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/tlsrewritem... (200; 28.579447ms)
Aug 21 16:20:47.088: INFO: (5) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 28.770642ms)
Aug 21 16:20:47.093: INFO: (5) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname2/proxy/: tls qux (200; 33.749769ms)
Aug 21 16:20:47.093: INFO: (5) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname2/proxy/: bar (200; 33.7783ms)
Aug 21 16:20:47.094: INFO: (5) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname1/proxy/: foo (200; 34.375622ms)
Aug 21 16:20:47.094: INFO: (5) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname1/proxy/: tls baz (200; 34.374744ms)
Aug 21 16:20:47.099: INFO: (5) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname2/proxy/: bar (200; 40.127191ms)
Aug 21 16:20:47.100: INFO: (5) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname1/proxy/: foo (200; 40.598984ms)
Aug 21 16:20:47.125: INFO: (6) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:460/proxy/: tls baz (200; 25.213313ms)
Aug 21 16:20:47.132: INFO: (6) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 31.485941ms)
Aug 21 16:20:47.132: INFO: (6) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:462/proxy/: tls qux (200; 31.332277ms)
Aug 21 16:20:47.132: INFO: (6) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">... (200; 31.891318ms)
Aug 21 16:20:47.132: INFO: (6) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">test<... (200; 30.849045ms)
Aug 21 16:20:47.132: INFO: (6) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 31.283863ms)
Aug 21 16:20:47.132: INFO: (6) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/rewriteme">test</a> (200; 30.574434ms)
Aug 21 16:20:47.138: INFO: (6) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 37.483456ms)
Aug 21 16:20:47.139: INFO: (6) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/tlsrewritem... (200; 36.86533ms)
Aug 21 16:20:47.139: INFO: (6) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 37.430868ms)
Aug 21 16:20:47.146: INFO: (6) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname2/proxy/: tls qux (200; 45.149249ms)
Aug 21 16:20:47.146: INFO: (6) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname1/proxy/: tls baz (200; 44.337767ms)
Aug 21 16:20:47.160: INFO: (6) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname2/proxy/: bar (200; 59.684471ms)
Aug 21 16:20:47.160: INFO: (6) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname2/proxy/: bar (200; 58.578137ms)
Aug 21 16:20:47.160: INFO: (6) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname1/proxy/: foo (200; 58.721343ms)
Aug 21 16:20:47.160: INFO: (6) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname1/proxy/: foo (200; 58.373577ms)
Aug 21 16:20:47.187: INFO: (7) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 26.973532ms)
Aug 21 16:20:47.191: INFO: (7) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">test<... (200; 30.429744ms)
Aug 21 16:20:47.192: INFO: (7) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 30.783434ms)
Aug 21 16:20:47.192: INFO: (7) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/rewriteme">test</a> (200; 31.039107ms)
Aug 21 16:20:47.196: INFO: (7) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 33.835847ms)
Aug 21 16:20:47.197: INFO: (7) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:462/proxy/: tls qux (200; 35.064596ms)
Aug 21 16:20:47.197: INFO: (7) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">... (200; 35.522772ms)
Aug 21 16:20:47.197: INFO: (7) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname1/proxy/: tls baz (200; 36.422093ms)
Aug 21 16:20:47.197: INFO: (7) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/tlsrewritem... (200; 36.097489ms)
Aug 21 16:20:47.197: INFO: (7) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 35.608244ms)
Aug 21 16:20:47.197: INFO: (7) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:460/proxy/: tls baz (200; 36.03478ms)
Aug 21 16:20:47.202: INFO: (7) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname2/proxy/: tls qux (200; 40.343796ms)
Aug 21 16:20:47.203: INFO: (7) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname1/proxy/: foo (200; 42.358737ms)
Aug 21 16:20:47.203: INFO: (7) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname2/proxy/: bar (200; 42.186107ms)
Aug 21 16:20:47.215: INFO: (7) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname1/proxy/: foo (200; 54.150943ms)
Aug 21 16:20:47.215: INFO: (7) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname2/proxy/: bar (200; 53.448235ms)
Aug 21 16:20:47.237: INFO: (8) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 21.602286ms)
Aug 21 16:20:47.243: INFO: (8) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 26.346367ms)
Aug 21 16:20:47.243: INFO: (8) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/rewriteme">test</a> (200; 26.571885ms)
Aug 21 16:20:47.244: INFO: (8) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">test<... (200; 27.057325ms)
Aug 21 16:20:47.254: INFO: (8) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/tlsrewritem... (200; 37.238585ms)
Aug 21 16:20:47.255: INFO: (8) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">... (200; 39.136474ms)
Aug 21 16:20:47.255: INFO: (8) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:462/proxy/: tls qux (200; 38.880275ms)
Aug 21 16:20:47.255: INFO: (8) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 38.624686ms)
Aug 21 16:20:47.255: INFO: (8) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:460/proxy/: tls baz (200; 39.896322ms)
Aug 21 16:20:47.255: INFO: (8) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 38.907633ms)
Aug 21 16:20:47.262: INFO: (8) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname2/proxy/: bar (200; 46.71204ms)
Aug 21 16:20:47.271: INFO: (8) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname1/proxy/: foo (200; 54.307538ms)
Aug 21 16:20:47.271: INFO: (8) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname2/proxy/: tls qux (200; 55.163989ms)
Aug 21 16:20:47.271: INFO: (8) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname1/proxy/: foo (200; 54.90401ms)
Aug 21 16:20:47.271: INFO: (8) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname2/proxy/: bar (200; 54.880777ms)
Aug 21 16:20:47.271: INFO: (8) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname1/proxy/: tls baz (200; 54.60945ms)
Aug 21 16:20:47.301: INFO: (9) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 29.411811ms)
Aug 21 16:20:47.301: INFO: (9) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">... (200; 29.185672ms)
Aug 21 16:20:47.301: INFO: (9) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 29.462015ms)
Aug 21 16:20:47.301: INFO: (9) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:460/proxy/: tls baz (200; 29.461742ms)
Aug 21 16:20:47.302: INFO: (9) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 29.489407ms)
Aug 21 16:20:47.302: INFO: (9) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 29.896015ms)
Aug 21 16:20:47.304: INFO: (9) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/tlsrewritem... (200; 32.393304ms)
Aug 21 16:20:47.304: INFO: (9) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/rewriteme">test</a> (200; 32.25969ms)
Aug 21 16:20:47.304: INFO: (9) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:462/proxy/: tls qux (200; 32.361624ms)
Aug 21 16:20:47.304: INFO: (9) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">test<... (200; 32.461847ms)
Aug 21 16:20:47.306: INFO: (9) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname1/proxy/: foo (200; 34.538536ms)
Aug 21 16:20:47.306: INFO: (9) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname1/proxy/: foo (200; 34.47189ms)
Aug 21 16:20:47.308: INFO: (9) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname2/proxy/: tls qux (200; 35.793989ms)
Aug 21 16:20:47.314: INFO: (9) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname2/proxy/: bar (200; 42.020849ms)
Aug 21 16:20:47.314: INFO: (9) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname1/proxy/: tls baz (200; 42.326867ms)
Aug 21 16:20:47.318: INFO: (9) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname2/proxy/: bar (200; 46.130887ms)
Aug 21 16:20:47.337: INFO: (10) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">test<... (200; 19.045467ms)
Aug 21 16:20:47.343: INFO: (10) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 24.546861ms)
Aug 21 16:20:47.343: INFO: (10) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/rewriteme">test</a> (200; 24.6005ms)
Aug 21 16:20:47.343: INFO: (10) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/tlsrewritem... (200; 24.630923ms)
Aug 21 16:20:47.343: INFO: (10) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 24.75702ms)
Aug 21 16:20:47.344: INFO: (10) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:462/proxy/: tls qux (200; 24.694599ms)
Aug 21 16:20:47.347: INFO: (10) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">... (200; 28.388586ms)
Aug 21 16:20:47.347: INFO: (10) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 28.453656ms)
Aug 21 16:20:47.352: INFO: (10) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname1/proxy/: tls baz (200; 33.472419ms)
Aug 21 16:20:47.353: INFO: (10) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:460/proxy/: tls baz (200; 33.876237ms)
Aug 21 16:20:47.353: INFO: (10) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 33.728435ms)
Aug 21 16:20:47.359: INFO: (10) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname2/proxy/: bar (200; 39.513288ms)
Aug 21 16:20:47.361: INFO: (10) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname2/proxy/: tls qux (200; 42.028802ms)
Aug 21 16:20:47.361: INFO: (10) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname1/proxy/: foo (200; 42.810029ms)
Aug 21 16:20:47.361: INFO: (10) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname2/proxy/: bar (200; 42.366045ms)
Aug 21 16:20:47.361: INFO: (10) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname1/proxy/: foo (200; 42.205289ms)
Aug 21 16:20:47.379: INFO: (11) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">... (200; 17.697811ms)
Aug 21 16:20:47.385: INFO: (11) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">test<... (200; 23.109241ms)
Aug 21 16:20:47.385: INFO: (11) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 23.279719ms)
Aug 21 16:20:47.385: INFO: (11) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/rewriteme">test</a> (200; 23.16018ms)
Aug 21 16:20:47.385: INFO: (11) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 23.225427ms)
Aug 21 16:20:47.385: INFO: (11) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:460/proxy/: tls baz (200; 23.076711ms)
Aug 21 16:20:47.385: INFO: (11) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 23.814797ms)
Aug 21 16:20:47.385: INFO: (11) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:462/proxy/: tls qux (200; 23.85956ms)
Aug 21 16:20:47.385: INFO: (11) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/tlsrewritem... (200; 23.329932ms)
Aug 21 16:20:47.385: INFO: (11) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 23.514711ms)
Aug 21 16:20:47.392: INFO: (11) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname2/proxy/: bar (200; 29.990334ms)
Aug 21 16:20:47.399: INFO: (11) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname2/proxy/: bar (200; 36.658988ms)
Aug 21 16:20:47.404: INFO: (11) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname1/proxy/: foo (200; 42.399386ms)
Aug 21 16:20:47.404: INFO: (11) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname1/proxy/: foo (200; 42.07116ms)
Aug 21 16:20:47.404: INFO: (11) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname2/proxy/: tls qux (200; 42.392867ms)
Aug 21 16:20:47.404: INFO: (11) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname1/proxy/: tls baz (200; 42.800163ms)
Aug 21 16:20:47.421: INFO: (12) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:460/proxy/: tls baz (200; 16.384263ms)
Aug 21 16:20:47.424: INFO: (12) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">... (200; 19.024129ms)
Aug 21 16:20:47.424: INFO: (12) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">test<... (200; 18.846394ms)
Aug 21 16:20:47.426: INFO: (12) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 21.433518ms)
Aug 21 16:20:47.427: INFO: (12) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/rewriteme">test</a> (200; 21.527279ms)
Aug 21 16:20:47.427: INFO: (12) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:462/proxy/: tls qux (200; 21.770067ms)
Aug 21 16:20:47.427: INFO: (12) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 21.533855ms)
Aug 21 16:20:47.427: INFO: (12) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 21.975722ms)
Aug 21 16:20:47.427: INFO: (12) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 21.937779ms)
Aug 21 16:20:47.427: INFO: (12) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/tlsrewritem... (200; 21.719401ms)
Aug 21 16:20:47.430: INFO: (12) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname2/proxy/: bar (200; 25.679815ms)
Aug 21 16:20:47.433: INFO: (12) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname1/proxy/: tls baz (200; 27.786354ms)
Aug 21 16:20:47.436: INFO: (12) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname2/proxy/: bar (200; 31.411356ms)
Aug 21 16:20:47.436: INFO: (12) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname1/proxy/: foo (200; 31.618675ms)
Aug 21 16:20:47.436: INFO: (12) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname1/proxy/: foo (200; 31.257779ms)
Aug 21 16:20:47.438: INFO: (12) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname2/proxy/: tls qux (200; 33.075245ms)
Aug 21 16:20:47.453: INFO: (13) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/rewriteme">test</a> (200; 14.63947ms)
Aug 21 16:20:47.460: INFO: (13) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">... (200; 20.893811ms)
Aug 21 16:20:47.461: INFO: (13) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">test<... (200; 22.670727ms)
Aug 21 16:20:47.461: INFO: (13) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 21.668932ms)
Aug 21 16:20:47.462: INFO: (13) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 22.272159ms)
Aug 21 16:20:47.462: INFO: (13) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/tlsrewritem... (200; 22.866821ms)
Aug 21 16:20:47.462: INFO: (13) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:462/proxy/: tls qux (200; 22.282079ms)
Aug 21 16:20:47.462: INFO: (13) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:460/proxy/: tls baz (200; 23.216492ms)
Aug 21 16:20:47.462: INFO: (13) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 22.375277ms)
Aug 21 16:20:47.462: INFO: (13) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 23.515296ms)
Aug 21 16:20:47.463: INFO: (13) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname1/proxy/: tls baz (200; 24.95385ms)
Aug 21 16:20:47.473: INFO: (13) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname2/proxy/: bar (200; 34.248839ms)
Aug 21 16:20:47.478: INFO: (13) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname1/proxy/: foo (200; 40.018524ms)
Aug 21 16:20:47.484: INFO: (13) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname1/proxy/: foo (200; 44.331716ms)
Aug 21 16:20:47.484: INFO: (13) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname2/proxy/: tls qux (200; 44.741748ms)
Aug 21 16:20:47.485: INFO: (13) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname2/proxy/: bar (200; 44.727576ms)
Aug 21 16:20:47.505: INFO: (14) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">... (200; 20.151076ms)
Aug 21 16:20:47.510: INFO: (14) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/tlsrewritem... (200; 23.104752ms)
Aug 21 16:20:47.510: INFO: (14) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:460/proxy/: tls baz (200; 22.756616ms)
Aug 21 16:20:47.510: INFO: (14) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/rewriteme">test</a> (200; 22.838674ms)
Aug 21 16:20:47.510: INFO: (14) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 24.357293ms)
Aug 21 16:20:47.510: INFO: (14) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">test<... (200; 24.000748ms)
Aug 21 16:20:47.511: INFO: (14) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 24.891762ms)
Aug 21 16:20:47.511: INFO: (14) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:462/proxy/: tls qux (200; 25.180379ms)
Aug 21 16:20:47.511: INFO: (14) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 26.317841ms)
Aug 21 16:20:47.511: INFO: (14) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 25.850816ms)
Aug 21 16:20:47.515: INFO: (14) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname2/proxy/: bar (200; 29.615112ms)
Aug 21 16:20:47.520: INFO: (14) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname1/proxy/: tls baz (200; 33.560349ms)
Aug 21 16:20:47.520: INFO: (14) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname2/proxy/: bar (200; 34.155705ms)
Aug 21 16:20:47.520: INFO: (14) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname1/proxy/: foo (200; 33.801288ms)
Aug 21 16:20:47.521: INFO: (14) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname2/proxy/: tls qux (200; 35.263753ms)
Aug 21 16:20:47.528: INFO: (14) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname1/proxy/: foo (200; 42.216047ms)
Aug 21 16:20:47.546: INFO: (15) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">test<... (200; 17.488079ms)
Aug 21 16:20:47.552: INFO: (15) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/rewriteme">test</a> (200; 22.962773ms)
Aug 21 16:20:47.552: INFO: (15) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 23.031437ms)
Aug 21 16:20:47.561: INFO: (15) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 32.331268ms)
Aug 21 16:20:47.561: INFO: (15) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 32.490343ms)
Aug 21 16:20:47.562: INFO: (15) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:460/proxy/: tls baz (200; 32.670922ms)
Aug 21 16:20:47.562: INFO: (15) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:462/proxy/: tls qux (200; 32.647504ms)
Aug 21 16:20:47.562: INFO: (15) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/tlsrewritem... (200; 32.911785ms)
Aug 21 16:20:47.562: INFO: (15) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 32.918487ms)
Aug 21 16:20:47.562: INFO: (15) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">... (200; 33.111266ms)
Aug 21 16:20:47.564: INFO: (15) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname1/proxy/: tls baz (200; 34.842428ms)
Aug 21 16:20:47.567: INFO: (15) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname1/proxy/: foo (200; 38.270746ms)
Aug 21 16:20:47.568: INFO: (15) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname2/proxy/: bar (200; 39.624653ms)
Aug 21 16:20:47.568: INFO: (15) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname1/proxy/: foo (200; 39.364129ms)
Aug 21 16:20:47.568: INFO: (15) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname2/proxy/: bar (200; 39.373954ms)
Aug 21 16:20:47.569: INFO: (15) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname2/proxy/: tls qux (200; 39.505041ms)
Aug 21 16:20:47.584: INFO: (16) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 15.757101ms)
Aug 21 16:20:47.588: INFO: (16) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">... (200; 18.480872ms)
Aug 21 16:20:47.588: INFO: (16) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:460/proxy/: tls baz (200; 18.5716ms)
Aug 21 16:20:47.588: INFO: (16) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">test<... (200; 19.221773ms)
Aug 21 16:20:47.588: INFO: (16) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/rewriteme">test</a> (200; 19.137481ms)
Aug 21 16:20:47.589: INFO: (16) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 19.881194ms)
Aug 21 16:20:47.589: INFO: (16) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/tlsrewritem... (200; 20.047514ms)
Aug 21 16:20:47.590: INFO: (16) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 20.863036ms)
Aug 21 16:20:47.592: INFO: (16) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 22.889584ms)
Aug 21 16:20:47.592: INFO: (16) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:462/proxy/: tls qux (200; 22.979081ms)
Aug 21 16:20:47.592: INFO: (16) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname1/proxy/: foo (200; 23.362567ms)
Aug 21 16:20:47.601: INFO: (16) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname1/proxy/: foo (200; 32.484072ms)
Aug 21 16:20:47.604: INFO: (16) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname2/proxy/: bar (200; 34.856317ms)
Aug 21 16:20:47.604: INFO: (16) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname1/proxy/: tls baz (200; 35.166603ms)
Aug 21 16:20:47.604: INFO: (16) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname2/proxy/: tls qux (200; 34.998401ms)
Aug 21 16:20:47.604: INFO: (16) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname2/proxy/: bar (200; 34.999419ms)
Aug 21 16:20:47.620: INFO: (17) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">test<... (200; 15.52394ms)
Aug 21 16:20:47.625: INFO: (17) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 19.280919ms)
Aug 21 16:20:47.626: INFO: (17) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">... (200; 20.021038ms)
Aug 21 16:20:47.626: INFO: (17) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:460/proxy/: tls baz (200; 20.355259ms)
Aug 21 16:20:47.626: INFO: (17) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:462/proxy/: tls qux (200; 19.838698ms)
Aug 21 16:20:47.626: INFO: (17) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 20.661717ms)
Aug 21 16:20:47.626: INFO: (17) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/tlsrewritem... (200; 20.573584ms)
Aug 21 16:20:47.626: INFO: (17) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/rewriteme">test</a> (200; 20.857787ms)
Aug 21 16:20:47.626: INFO: (17) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 20.120362ms)
Aug 21 16:20:47.630: INFO: (17) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname1/proxy/: tls baz (200; 25.061346ms)
Aug 21 16:20:47.635: INFO: (17) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 29.22294ms)
Aug 21 16:20:47.638: INFO: (17) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname2/proxy/: bar (200; 32.324213ms)
Aug 21 16:20:47.642: INFO: (17) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname1/proxy/: foo (200; 35.471022ms)
Aug 21 16:20:47.642: INFO: (17) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname2/proxy/: tls qux (200; 35.816008ms)
Aug 21 16:20:47.642: INFO: (17) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname1/proxy/: foo (200; 37.345645ms)
Aug 21 16:20:47.642: INFO: (17) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname2/proxy/: bar (200; 35.763925ms)
Aug 21 16:20:47.661: INFO: (18) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:462/proxy/: tls qux (200; 19.250525ms)
Aug 21 16:20:47.664: INFO: (18) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/rewriteme">test</a> (200; 21.872953ms)
Aug 21 16:20:47.665: INFO: (18) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 22.211ms)
Aug 21 16:20:47.665: INFO: (18) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">test<... (200; 22.773589ms)
Aug 21 16:20:47.665: INFO: (18) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 23.029644ms)
Aug 21 16:20:47.666: INFO: (18) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/tlsrewritem... (200; 22.899817ms)
Aug 21 16:20:47.666: INFO: (18) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:460/proxy/: tls baz (200; 23.080453ms)
Aug 21 16:20:47.666: INFO: (18) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">... (200; 23.671057ms)
Aug 21 16:20:47.666: INFO: (18) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 23.729617ms)
Aug 21 16:20:47.666: INFO: (18) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 24.141309ms)
Aug 21 16:20:47.670: INFO: (18) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname1/proxy/: foo (200; 27.67137ms)
Aug 21 16:20:47.675: INFO: (18) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname2/proxy/: bar (200; 32.005839ms)
Aug 21 16:20:47.683: INFO: (18) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname2/proxy/: tls qux (200; 40.469641ms)
Aug 21 16:20:47.683: INFO: (18) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname1/proxy/: foo (200; 40.490922ms)
Aug 21 16:20:47.683: INFO: (18) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname2/proxy/: bar (200; 40.661809ms)
Aug 21 16:20:47.683: INFO: (18) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname1/proxy/: tls baz (200; 40.788883ms)
Aug 21 16:20:47.702: INFO: (19) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:460/proxy/: tls baz (200; 18.312772ms)
Aug 21 16:20:47.710: INFO: (19) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">test<... (200; 25.65102ms)
Aug 21 16:20:47.710: INFO: (19) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 26.323967ms)
Aug 21 16:20:47.710: INFO: (19) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:162/proxy/: bar (200; 26.09923ms)
Aug 21 16:20:47.710: INFO: (19) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 25.593688ms)
Aug 21 16:20:47.710: INFO: (19) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:160/proxy/: foo (200; 26.376639ms)
Aug 21 16:20:47.710: INFO: (19) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:462/proxy/: tls qux (200; 26.618758ms)
Aug 21 16:20:47.711: INFO: (19) /api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/http:proxy-service-z2lsz-8zkfd:1080/proxy/rewriteme">... (200; 26.858334ms)
Aug 21 16:20:47.712: INFO: (19) /api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/proxy-service-z2lsz-8zkfd/proxy/rewriteme">test</a> (200; 27.770752ms)
Aug 21 16:20:47.712: INFO: (19) /api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/: <a href="/api/v1/namespaces/proxy-9643/pods/https:proxy-service-z2lsz-8zkfd:443/proxy/tlsrewritem... (200; 27.694731ms)
Aug 21 16:20:47.717: INFO: (19) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname2/proxy/: tls qux (200; 33.265058ms)
Aug 21 16:20:47.717: INFO: (19) /api/v1/namespaces/proxy-9643/services/https:proxy-service-z2lsz:tlsportname1/proxy/: tls baz (200; 32.810094ms)
Aug 21 16:20:47.717: INFO: (19) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname2/proxy/: bar (200; 33.149994ms)
Aug 21 16:20:47.737: INFO: (19) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname2/proxy/: bar (200; 53.660976ms)
Aug 21 16:20:47.737: INFO: (19) /api/v1/namespaces/proxy-9643/services/http:proxy-service-z2lsz:portname1/proxy/: foo (200; 53.163705ms)
Aug 21 16:20:47.737: INFO: (19) /api/v1/namespaces/proxy-9643/services/proxy-service-z2lsz:portname1/proxy/: foo (200; 53.009252ms)
STEP: deleting ReplicationController proxy-service-z2lsz in namespace proxy-9643, will wait for the garbage collector to delete the pods
Aug 21 16:20:47.838: INFO: Deleting ReplicationController proxy-service-z2lsz took: 39.495622ms
Aug 21 16:20:48.338: INFO: Terminating ReplicationController proxy-service-z2lsz pods took: 500.391102ms
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:21:01.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9643" for this suite.
Aug 21 16:21:11.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:21:12.455: INFO: namespace proxy-9643 deletion completed in 11.190102077s

• [SLOW TEST:29.921 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:21:12.455: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 21 16:21:12.833: INFO: Waiting up to 5m0s for pod "pod-6a4af287-f38a-4376-84cf-5d24993b5801" in namespace "emptydir-8579" to be "success or failure"
Aug 21 16:21:12.845: INFO: Pod "pod-6a4af287-f38a-4376-84cf-5d24993b5801": Phase="Pending", Reason="", readiness=false. Elapsed: 11.549446ms
Aug 21 16:21:14.867: INFO: Pod "pod-6a4af287-f38a-4376-84cf-5d24993b5801": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033384899s
STEP: Saw pod success
Aug 21 16:21:14.867: INFO: Pod "pod-6a4af287-f38a-4376-84cf-5d24993b5801" satisfied condition "success or failure"
Aug 21 16:21:14.890: INFO: Trying to get logs from node 10.188.240.202 pod pod-6a4af287-f38a-4376-84cf-5d24993b5801 container test-container: <nil>
STEP: delete the pod
Aug 21 16:21:15.004: INFO: Waiting for pod pod-6a4af287-f38a-4376-84cf-5d24993b5801 to disappear
Aug 21 16:21:15.020: INFO: Pod pod-6a4af287-f38a-4376-84cf-5d24993b5801 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:21:15.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8579" for this suite.
Aug 21 16:21:25.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:21:27.262: INFO: namespace emptydir-8579 deletion completed in 12.220902094s

• [SLOW TEST:14.806 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:21:27.262: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-883
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-883
STEP: creating replication controller externalsvc in namespace services-883
I0821 16:21:27.537619      24 runners.go:184] Created replication controller with name: externalsvc, namespace: services-883, replica count: 2
I0821 16:21:30.588448      24 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Aug 21 16:21:30.654: INFO: Creating new exec pod
Aug 21 16:21:34.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=services-883 execpod8lfjb -- /bin/sh -x -c nslookup clusterip-service'
Aug 21 16:21:35.392: INFO: stderr: "+ nslookup clusterip-service\n"
Aug 21 16:21:35.392: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-883.svc.cluster.local\tcanonical name = externalsvc.services-883.svc.cluster.local.\nName:\texternalsvc.services-883.svc.cluster.local\nAddress: 172.21.241.142\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-883, will wait for the garbage collector to delete the pods
Aug 21 16:21:35.480: INFO: Deleting ReplicationController externalsvc took: 25.226872ms
Aug 21 16:21:36.081: INFO: Terminating ReplicationController externalsvc pods took: 600.3343ms
Aug 21 16:21:51.349: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:21:51.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-883" for this suite.
Aug 21 16:21:59.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:22:01.563: INFO: namespace services-883 deletion completed in 10.140358792s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:34.301 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:22:01.563: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-89bfdb79-8c32-490a-8611-9fe513c6b0fb
STEP: Creating a pod to test consume configMaps
Aug 21 16:22:01.832: INFO: Waiting up to 5m0s for pod "pod-configmaps-3efa51de-e239-4f21-a4db-ea4ddf14b33b" in namespace "configmap-5685" to be "success or failure"
Aug 21 16:22:01.842: INFO: Pod "pod-configmaps-3efa51de-e239-4f21-a4db-ea4ddf14b33b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.31789ms
Aug 21 16:22:03.855: INFO: Pod "pod-configmaps-3efa51de-e239-4f21-a4db-ea4ddf14b33b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022625382s
STEP: Saw pod success
Aug 21 16:22:03.855: INFO: Pod "pod-configmaps-3efa51de-e239-4f21-a4db-ea4ddf14b33b" satisfied condition "success or failure"
Aug 21 16:22:03.865: INFO: Trying to get logs from node 10.188.240.202 pod pod-configmaps-3efa51de-e239-4f21-a4db-ea4ddf14b33b container configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 16:22:03.924: INFO: Waiting for pod pod-configmaps-3efa51de-e239-4f21-a4db-ea4ddf14b33b to disappear
Aug 21 16:22:03.936: INFO: Pod pod-configmaps-3efa51de-e239-4f21-a4db-ea4ddf14b33b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:22:03.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5685" for this suite.
Aug 21 16:22:14.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:22:16.202: INFO: namespace configmap-5685 deletion completed in 12.23498914s

• [SLOW TEST:14.639 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:22:16.203: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 21 16:22:16.446: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6319d0e5-bd7a-40c0-8214-0ff0cad8b760" in namespace "downward-api-2663" to be "success or failure"
Aug 21 16:22:16.458: INFO: Pod "downwardapi-volume-6319d0e5-bd7a-40c0-8214-0ff0cad8b760": Phase="Pending", Reason="", readiness=false. Elapsed: 12.054564ms
Aug 21 16:22:18.481: INFO: Pod "downwardapi-volume-6319d0e5-bd7a-40c0-8214-0ff0cad8b760": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034315657s
STEP: Saw pod success
Aug 21 16:22:18.481: INFO: Pod "downwardapi-volume-6319d0e5-bd7a-40c0-8214-0ff0cad8b760" satisfied condition "success or failure"
Aug 21 16:22:18.499: INFO: Trying to get logs from node 10.188.240.202 pod downwardapi-volume-6319d0e5-bd7a-40c0-8214-0ff0cad8b760 container client-container: <nil>
STEP: delete the pod
Aug 21 16:22:18.613: INFO: Waiting for pod downwardapi-volume-6319d0e5-bd7a-40c0-8214-0ff0cad8b760 to disappear
Aug 21 16:22:18.630: INFO: Pod downwardapi-volume-6319d0e5-bd7a-40c0-8214-0ff0cad8b760 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:22:18.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2663" for this suite.
Aug 21 16:22:26.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:22:28.920: INFO: namespace downward-api-2663 deletion completed in 10.264026879s

• [SLOW TEST:12.717 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:22:28.920: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-b5d97ed4-82dc-41d6-9777-e14805cdeec6
STEP: Creating a pod to test consume secrets
Aug 21 16:22:29.240: INFO: Waiting up to 5m0s for pod "pod-secrets-e12dc1f7-df39-4a28-a878-7acc5cfad4ae" in namespace "secrets-6752" to be "success or failure"
Aug 21 16:22:29.254: INFO: Pod "pod-secrets-e12dc1f7-df39-4a28-a878-7acc5cfad4ae": Phase="Pending", Reason="", readiness=false. Elapsed: 14.652061ms
Aug 21 16:22:31.268: INFO: Pod "pod-secrets-e12dc1f7-df39-4a28-a878-7acc5cfad4ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028345721s
Aug 21 16:22:33.287: INFO: Pod "pod-secrets-e12dc1f7-df39-4a28-a878-7acc5cfad4ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047371266s
STEP: Saw pod success
Aug 21 16:22:33.287: INFO: Pod "pod-secrets-e12dc1f7-df39-4a28-a878-7acc5cfad4ae" satisfied condition "success or failure"
Aug 21 16:22:33.302: INFO: Trying to get logs from node 10.188.240.202 pod pod-secrets-e12dc1f7-df39-4a28-a878-7acc5cfad4ae container secret-volume-test: <nil>
STEP: delete the pod
Aug 21 16:22:33.381: INFO: Waiting for pod pod-secrets-e12dc1f7-df39-4a28-a878-7acc5cfad4ae to disappear
Aug 21 16:22:33.395: INFO: Pod pod-secrets-e12dc1f7-df39-4a28-a878-7acc5cfad4ae no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:22:33.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6752" for this suite.
Aug 21 16:22:41.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:22:43.712: INFO: namespace secrets-6752 deletion completed in 10.266950873s

• [SLOW TEST:14.792 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:22:43.712: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Aug 21 16:22:44.040: INFO: Waiting up to 5m0s for pod "var-expansion-906d3a6d-0000-445e-8626-b5229b59c2ea" in namespace "var-expansion-3629" to be "success or failure"
Aug 21 16:22:44.052: INFO: Pod "var-expansion-906d3a6d-0000-445e-8626-b5229b59c2ea": Phase="Pending", Reason="", readiness=false. Elapsed: 11.73899ms
Aug 21 16:22:46.065: INFO: Pod "var-expansion-906d3a6d-0000-445e-8626-b5229b59c2ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024902654s
STEP: Saw pod success
Aug 21 16:22:46.065: INFO: Pod "var-expansion-906d3a6d-0000-445e-8626-b5229b59c2ea" satisfied condition "success or failure"
Aug 21 16:22:46.080: INFO: Trying to get logs from node 10.188.240.202 pod var-expansion-906d3a6d-0000-445e-8626-b5229b59c2ea container dapi-container: <nil>
STEP: delete the pod
Aug 21 16:22:46.157: INFO: Waiting for pod var-expansion-906d3a6d-0000-445e-8626-b5229b59c2ea to disappear
Aug 21 16:22:46.169: INFO: Pod var-expansion-906d3a6d-0000-445e-8626-b5229b59c2ea no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:22:46.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3629" for this suite.
Aug 21 16:22:54.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:22:56.432: INFO: namespace var-expansion-3629 deletion completed in 10.208669178s

• [SLOW TEST:12.720 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:22:56.433: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 16:22:56.641: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 21 16:23:05.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 --namespace=crd-publish-openapi-5823 create -f -'
Aug 21 16:23:06.819: INFO: stderr: ""
Aug 21 16:23:06.819: INFO: stdout: "e2e-test-crd-publish-openapi-718-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 21 16:23:06.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 --namespace=crd-publish-openapi-5823 delete e2e-test-crd-publish-openapi-718-crds test-cr'
Aug 21 16:23:07.054: INFO: stderr: ""
Aug 21 16:23:07.054: INFO: stdout: "e2e-test-crd-publish-openapi-718-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Aug 21 16:23:07.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 --namespace=crd-publish-openapi-5823 apply -f -'
Aug 21 16:23:07.614: INFO: stderr: ""
Aug 21 16:23:07.614: INFO: stdout: "e2e-test-crd-publish-openapi-718-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 21 16:23:07.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 --namespace=crd-publish-openapi-5823 delete e2e-test-crd-publish-openapi-718-crds test-cr'
Aug 21 16:23:07.859: INFO: stderr: ""
Aug 21 16:23:07.859: INFO: stdout: "e2e-test-crd-publish-openapi-718-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Aug 21 16:23:07.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 explain e2e-test-crd-publish-openapi-718-crds'
Aug 21 16:23:08.457: INFO: stderr: ""
Aug 21 16:23:08.457: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-718-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:23:17.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5823" for this suite.
Aug 21 16:23:25.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:23:27.220: INFO: namespace crd-publish-openapi-5823 deletion completed in 10.112704685s

• [SLOW TEST:30.787 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:23:27.220: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 21 16:23:28.007: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 21 16:23:30.049: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733623808, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733623808, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733623808, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733623807, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 21 16:23:33.096: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Aug 21 16:23:33.163: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:23:33.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8642" for this suite.
Aug 21 16:23:41.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:23:43.689: INFO: namespace webhook-8642 deletion completed in 10.467262227s
STEP: Destroying namespace "webhook-8642-markers" for this suite.
Aug 21 16:23:51.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:23:53.816: INFO: namespace webhook-8642-markers deletion completed in 10.126477501s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:26.662 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:23:53.882: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 16:23:54.119: INFO: Waiting up to 5m0s for pod "busybox-user-65534-310c66d3-7aef-49de-9e67-c3f6b89f35f3" in namespace "security-context-test-4178" to be "success or failure"
Aug 21 16:23:54.129: INFO: Pod "busybox-user-65534-310c66d3-7aef-49de-9e67-c3f6b89f35f3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.476226ms
Aug 21 16:23:56.142: INFO: Pod "busybox-user-65534-310c66d3-7aef-49de-9e67-c3f6b89f35f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023170176s
Aug 21 16:23:56.142: INFO: Pod "busybox-user-65534-310c66d3-7aef-49de-9e67-c3f6b89f35f3" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:23:56.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4178" for this suite.
Aug 21 16:24:04.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:24:06.471: INFO: namespace security-context-test-4178 deletion completed in 10.312231055s

• [SLOW TEST:12.589 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:24:06.471: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Aug 21 16:24:08.801: INFO: &Pod{ObjectMeta:{send-events-c0ab20f5-dcd4-49ae-809b-f192d0d82335  events-3198 /api/v1/namespaces/events-3198/pods/send-events-c0ab20f5-dcd4-49ae-809b-f192d0d82335 6fdb51d8-88e6-458a-bad4-6a72785e21c2 56202 0 2020-08-21 16:24:06 +0000 UTC <nil> <nil> map[name:foo time:690660408] map[cni.projectcalico.org/podIP:172.30.174.96/32 cni.projectcalico.org/podIPs:172.30.174.96/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.174.96"
    ],
    "dns": {}
}] openshift.io/scc:anyuid] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5kqdd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5kqdd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5kqdd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.230,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c9,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 16:24:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 16:24:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 16:24:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 16:24:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.230,PodIP:172.30.174.96,StartTime:2020-08-21 16:24:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-21 16:24:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:cri-o://99eca9e3f5f4a2ac6a66c177dc60c51d6729930b802a5d41b0f65a2f617061dc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.174.96,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Aug 21 16:24:10.814: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Aug 21 16:24:12.830: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:24:12.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3198" for this suite.
Aug 21 16:24:48.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:24:51.101: INFO: namespace events-3198 deletion completed in 38.21482602s

• [SLOW TEST:44.630 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:24:51.102: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:24:51.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-3576" for this suite.
Aug 21 16:24:59.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:25:01.575: INFO: namespace tables-3576 deletion completed in 10.191748024s

• [SLOW TEST:10.473 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:25:01.575: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-mqhs
STEP: Creating a pod to test atomic-volume-subpath
Aug 21 16:25:01.879: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-mqhs" in namespace "subpath-5467" to be "success or failure"
Aug 21 16:25:01.890: INFO: Pod "pod-subpath-test-projected-mqhs": Phase="Pending", Reason="", readiness=false. Elapsed: 11.026224ms
Aug 21 16:25:03.904: INFO: Pod "pod-subpath-test-projected-mqhs": Phase="Running", Reason="", readiness=true. Elapsed: 2.024799374s
Aug 21 16:25:05.916: INFO: Pod "pod-subpath-test-projected-mqhs": Phase="Running", Reason="", readiness=true. Elapsed: 4.037433228s
Aug 21 16:25:07.930: INFO: Pod "pod-subpath-test-projected-mqhs": Phase="Running", Reason="", readiness=true. Elapsed: 6.051209155s
Aug 21 16:25:09.946: INFO: Pod "pod-subpath-test-projected-mqhs": Phase="Running", Reason="", readiness=true. Elapsed: 8.067054244s
Aug 21 16:25:11.963: INFO: Pod "pod-subpath-test-projected-mqhs": Phase="Running", Reason="", readiness=true. Elapsed: 10.084116114s
Aug 21 16:25:13.975: INFO: Pod "pod-subpath-test-projected-mqhs": Phase="Running", Reason="", readiness=true. Elapsed: 12.095998388s
Aug 21 16:25:15.993: INFO: Pod "pod-subpath-test-projected-mqhs": Phase="Running", Reason="", readiness=true. Elapsed: 14.113865101s
Aug 21 16:25:18.006: INFO: Pod "pod-subpath-test-projected-mqhs": Phase="Running", Reason="", readiness=true. Elapsed: 16.127398184s
Aug 21 16:25:20.018: INFO: Pod "pod-subpath-test-projected-mqhs": Phase="Running", Reason="", readiness=true. Elapsed: 18.139263628s
Aug 21 16:25:22.030: INFO: Pod "pod-subpath-test-projected-mqhs": Phase="Running", Reason="", readiness=true. Elapsed: 20.151100363s
Aug 21 16:25:24.043: INFO: Pod "pod-subpath-test-projected-mqhs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.164350454s
STEP: Saw pod success
Aug 21 16:25:24.043: INFO: Pod "pod-subpath-test-projected-mqhs" satisfied condition "success or failure"
Aug 21 16:25:24.064: INFO: Trying to get logs from node 10.188.240.230 pod pod-subpath-test-projected-mqhs container test-container-subpath-projected-mqhs: <nil>
STEP: delete the pod
Aug 21 16:25:24.181: INFO: Waiting for pod pod-subpath-test-projected-mqhs to disappear
Aug 21 16:25:24.195: INFO: Pod pod-subpath-test-projected-mqhs no longer exists
STEP: Deleting pod pod-subpath-test-projected-mqhs
Aug 21 16:25:24.195: INFO: Deleting pod "pod-subpath-test-projected-mqhs" in namespace "subpath-5467"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:25:24.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5467" for this suite.
Aug 21 16:25:32.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:25:34.529: INFO: namespace subpath-5467 deletion completed in 10.243523847s

• [SLOW TEST:32.954 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:25:34.530: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 21 16:25:35.830: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c146b067-4fbe-4b0f-b86d-cfd1ab182231" in namespace "downward-api-2653" to be "success or failure"
Aug 21 16:25:35.843: INFO: Pod "downwardapi-volume-c146b067-4fbe-4b0f-b86d-cfd1ab182231": Phase="Pending", Reason="", readiness=false. Elapsed: 13.062025ms
Aug 21 16:25:37.857: INFO: Pod "downwardapi-volume-c146b067-4fbe-4b0f-b86d-cfd1ab182231": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027067995s
Aug 21 16:25:39.868: INFO: Pod "downwardapi-volume-c146b067-4fbe-4b0f-b86d-cfd1ab182231": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037623281s
STEP: Saw pod success
Aug 21 16:25:39.868: INFO: Pod "downwardapi-volume-c146b067-4fbe-4b0f-b86d-cfd1ab182231" satisfied condition "success or failure"
Aug 21 16:25:39.881: INFO: Trying to get logs from node 10.188.240.230 pod downwardapi-volume-c146b067-4fbe-4b0f-b86d-cfd1ab182231 container client-container: <nil>
STEP: delete the pod
Aug 21 16:25:39.948: INFO: Waiting for pod downwardapi-volume-c146b067-4fbe-4b0f-b86d-cfd1ab182231 to disappear
Aug 21 16:25:39.958: INFO: Pod downwardapi-volume-c146b067-4fbe-4b0f-b86d-cfd1ab182231 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:25:39.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2653" for this suite.
Aug 21 16:25:48.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:25:50.221: INFO: namespace downward-api-2653 deletion completed in 10.23759363s

• [SLOW TEST:15.691 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:25:50.222: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-042574de-08ec-4f12-bf5a-f4b2ee2a4843
STEP: Creating a pod to test consume secrets
Aug 21 16:25:50.558: INFO: Waiting up to 5m0s for pod "pod-secrets-9628aa49-fe9b-4544-bc4e-7bdeee571bcc" in namespace "secrets-5278" to be "success or failure"
Aug 21 16:25:50.570: INFO: Pod "pod-secrets-9628aa49-fe9b-4544-bc4e-7bdeee571bcc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.326442ms
Aug 21 16:25:52.582: INFO: Pod "pod-secrets-9628aa49-fe9b-4544-bc4e-7bdeee571bcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024524971s
STEP: Saw pod success
Aug 21 16:25:52.582: INFO: Pod "pod-secrets-9628aa49-fe9b-4544-bc4e-7bdeee571bcc" satisfied condition "success or failure"
Aug 21 16:25:52.596: INFO: Trying to get logs from node 10.188.240.202 pod pod-secrets-9628aa49-fe9b-4544-bc4e-7bdeee571bcc container secret-volume-test: <nil>
STEP: delete the pod
Aug 21 16:25:52.683: INFO: Waiting for pod pod-secrets-9628aa49-fe9b-4544-bc4e-7bdeee571bcc to disappear
Aug 21 16:25:52.695: INFO: Pod pod-secrets-9628aa49-fe9b-4544-bc4e-7bdeee571bcc no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:25:52.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5278" for this suite.
Aug 21 16:26:02.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:26:05.099: INFO: namespace secrets-5278 deletion completed in 12.373270377s

• [SLOW TEST:14.877 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:26:05.100: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 21 16:26:06.031: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 21 16:26:08.082: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733623966, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733623966, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733623966, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733623966, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 21 16:26:11.126: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 16:26:11.148: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:26:12.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7843" for this suite.
Aug 21 16:26:22.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:26:24.833: INFO: namespace webhook-7843 deletion completed in 12.212083494s
STEP: Destroying namespace "webhook-7843-markers" for this suite.
Aug 21 16:26:32.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:26:35.065: INFO: namespace webhook-7843-markers deletion completed in 10.232239707s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:30.053 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:26:35.152: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Aug 21 16:26:35.413: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-660 /api/v1/namespaces/watch-660/configmaps/e2e-watch-test-configmap-a fe339c86-7629-4ac7-a6a9-23a1c708bbb1 57227 0 2020-08-21 16:26:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 21 16:26:35.413: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-660 /api/v1/namespaces/watch-660/configmaps/e2e-watch-test-configmap-a fe339c86-7629-4ac7-a6a9-23a1c708bbb1 57227 0 2020-08-21 16:26:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Aug 21 16:26:45.445: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-660 /api/v1/namespaces/watch-660/configmaps/e2e-watch-test-configmap-a fe339c86-7629-4ac7-a6a9-23a1c708bbb1 57273 0 2020-08-21 16:26:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug 21 16:26:45.445: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-660 /api/v1/namespaces/watch-660/configmaps/e2e-watch-test-configmap-a fe339c86-7629-4ac7-a6a9-23a1c708bbb1 57273 0 2020-08-21 16:26:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Aug 21 16:26:55.473: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-660 /api/v1/namespaces/watch-660/configmaps/e2e-watch-test-configmap-a fe339c86-7629-4ac7-a6a9-23a1c708bbb1 57319 0 2020-08-21 16:26:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 21 16:26:55.473: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-660 /api/v1/namespaces/watch-660/configmaps/e2e-watch-test-configmap-a fe339c86-7629-4ac7-a6a9-23a1c708bbb1 57319 0 2020-08-21 16:26:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Aug 21 16:27:05.505: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-660 /api/v1/namespaces/watch-660/configmaps/e2e-watch-test-configmap-a fe339c86-7629-4ac7-a6a9-23a1c708bbb1 57366 0 2020-08-21 16:26:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 21 16:27:05.505: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-660 /api/v1/namespaces/watch-660/configmaps/e2e-watch-test-configmap-a fe339c86-7629-4ac7-a6a9-23a1c708bbb1 57366 0 2020-08-21 16:26:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Aug 21 16:27:15.528: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-660 /api/v1/namespaces/watch-660/configmaps/e2e-watch-test-configmap-b 74ff83a4-afd1-456b-9dcd-cecbc5c06da1 57413 0 2020-08-21 16:27:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 21 16:27:15.528: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-660 /api/v1/namespaces/watch-660/configmaps/e2e-watch-test-configmap-b 74ff83a4-afd1-456b-9dcd-cecbc5c06da1 57413 0 2020-08-21 16:27:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Aug 21 16:27:25.560: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-660 /api/v1/namespaces/watch-660/configmaps/e2e-watch-test-configmap-b 74ff83a4-afd1-456b-9dcd-cecbc5c06da1 57457 0 2020-08-21 16:27:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 21 16:27:25.560: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-660 /api/v1/namespaces/watch-660/configmaps/e2e-watch-test-configmap-b 74ff83a4-afd1-456b-9dcd-cecbc5c06da1 57457 0 2020-08-21 16:27:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:27:35.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-660" for this suite.
Aug 21 16:27:43.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:27:45.852: INFO: namespace watch-660 deletion completed in 10.259064911s

• [SLOW TEST:70.700 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:27:45.852: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Aug 21 16:27:50.605: INFO: Successfully updated pod "adopt-release-hk4lv"
STEP: Checking that the Job readopts the Pod
Aug 21 16:27:50.606: INFO: Waiting up to 15m0s for pod "adopt-release-hk4lv" in namespace "job-9016" to be "adopted"
Aug 21 16:27:50.618: INFO: Pod "adopt-release-hk4lv": Phase="Running", Reason="", readiness=true. Elapsed: 11.887176ms
Aug 21 16:27:52.630: INFO: Pod "adopt-release-hk4lv": Phase="Running", Reason="", readiness=true. Elapsed: 2.024271605s
Aug 21 16:27:52.630: INFO: Pod "adopt-release-hk4lv" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Aug 21 16:27:53.192: INFO: Successfully updated pod "adopt-release-hk4lv"
STEP: Checking that the Job releases the Pod
Aug 21 16:27:53.193: INFO: Waiting up to 15m0s for pod "adopt-release-hk4lv" in namespace "job-9016" to be "released"
Aug 21 16:27:53.204: INFO: Pod "adopt-release-hk4lv": Phase="Running", Reason="", readiness=true. Elapsed: 11.273844ms
Aug 21 16:27:55.216: INFO: Pod "adopt-release-hk4lv": Phase="Running", Reason="", readiness=true. Elapsed: 2.023136819s
Aug 21 16:27:55.216: INFO: Pod "adopt-release-hk4lv" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:27:55.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9016" for this suite.
Aug 21 16:28:45.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:28:47.624: INFO: namespace job-9016 deletion completed in 52.379044771s

• [SLOW TEST:61.771 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:28:47.624: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 16:28:50.053: INFO: Waiting up to 5m0s for pod "client-envvars-69c32219-8e9e-4a7d-af9f-e23a68a98173" in namespace "pods-7913" to be "success or failure"
Aug 21 16:28:50.065: INFO: Pod "client-envvars-69c32219-8e9e-4a7d-af9f-e23a68a98173": Phase="Pending", Reason="", readiness=false. Elapsed: 11.41309ms
Aug 21 16:28:52.077: INFO: Pod "client-envvars-69c32219-8e9e-4a7d-af9f-e23a68a98173": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023721588s
STEP: Saw pod success
Aug 21 16:28:52.077: INFO: Pod "client-envvars-69c32219-8e9e-4a7d-af9f-e23a68a98173" satisfied condition "success or failure"
Aug 21 16:28:52.089: INFO: Trying to get logs from node 10.188.240.202 pod client-envvars-69c32219-8e9e-4a7d-af9f-e23a68a98173 container env3cont: <nil>
STEP: delete the pod
Aug 21 16:28:52.204: INFO: Waiting for pod client-envvars-69c32219-8e9e-4a7d-af9f-e23a68a98173 to disappear
Aug 21 16:28:52.220: INFO: Pod client-envvars-69c32219-8e9e-4a7d-af9f-e23a68a98173 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:28:52.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7913" for this suite.
Aug 21 16:29:08.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:29:10.458: INFO: namespace pods-7913 deletion completed in 18.217765045s

• [SLOW TEST:22.835 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:29:10.459: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Aug 21 16:29:10.705: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-6835" to be "success or failure"
Aug 21 16:29:10.718: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 12.617333ms
Aug 21 16:29:12.731: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025151485s
STEP: Saw pod success
Aug 21 16:29:12.731: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Aug 21 16:29:12.742: INFO: Trying to get logs from node 10.188.240.202 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Aug 21 16:29:12.809: INFO: Waiting for pod pod-host-path-test to disappear
Aug 21 16:29:12.821: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:29:12.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-6835" for this suite.
Aug 21 16:29:20.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:29:23.011: INFO: namespace hostpath-6835 deletion completed in 10.165569977s

• [SLOW TEST:12.552 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:29:23.011: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-3df0f5d5-f9ea-4362-bd50-0013f95e007f
STEP: Creating a pod to test consume secrets
Aug 21 16:29:23.279: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-45607f09-1b8f-490d-9303-7d84fcd3d185" in namespace "projected-1836" to be "success or failure"
Aug 21 16:29:23.292: INFO: Pod "pod-projected-secrets-45607f09-1b8f-490d-9303-7d84fcd3d185": Phase="Pending", Reason="", readiness=false. Elapsed: 12.325956ms
Aug 21 16:29:25.307: INFO: Pod "pod-projected-secrets-45607f09-1b8f-490d-9303-7d84fcd3d185": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028045316s
Aug 21 16:29:27.320: INFO: Pod "pod-projected-secrets-45607f09-1b8f-490d-9303-7d84fcd3d185": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04051138s
STEP: Saw pod success
Aug 21 16:29:27.320: INFO: Pod "pod-projected-secrets-45607f09-1b8f-490d-9303-7d84fcd3d185" satisfied condition "success or failure"
Aug 21 16:29:27.332: INFO: Trying to get logs from node 10.188.240.230 pod pod-projected-secrets-45607f09-1b8f-490d-9303-7d84fcd3d185 container secret-volume-test: <nil>
STEP: delete the pod
Aug 21 16:29:27.460: INFO: Waiting for pod pod-projected-secrets-45607f09-1b8f-490d-9303-7d84fcd3d185 to disappear
Aug 21 16:29:27.475: INFO: Pod pod-projected-secrets-45607f09-1b8f-490d-9303-7d84fcd3d185 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:29:27.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1836" for this suite.
Aug 21 16:29:37.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:29:39.890: INFO: namespace projected-1836 deletion completed in 12.383645365s

• [SLOW TEST:16.878 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:29:39.890: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Aug 21 16:29:42.995: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1841 pod-service-account-59e1f2fc-bfd3-45ff-a308-58f3b7c7638e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Aug 21 16:29:43.536: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1841 pod-service-account-59e1f2fc-bfd3-45ff-a308-58f3b7c7638e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Aug 21 16:29:44.268: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1841 pod-service-account-59e1f2fc-bfd3-45ff-a308-58f3b7c7638e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:29:44.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1841" for this suite.
Aug 21 16:29:52.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:29:54.918: INFO: namespace svcaccounts-1841 deletion completed in 10.166182544s

• [SLOW TEST:15.028 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:29:54.920: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-18ddd9df-de99-4341-9822-2a59082e86b9
STEP: Creating a pod to test consume configMaps
Aug 21 16:29:55.191: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ad09a006-8211-4c33-bf91-a252290d317f" in namespace "projected-3647" to be "success or failure"
Aug 21 16:29:55.207: INFO: Pod "pod-projected-configmaps-ad09a006-8211-4c33-bf91-a252290d317f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.893454ms
Aug 21 16:29:57.222: INFO: Pod "pod-projected-configmaps-ad09a006-8211-4c33-bf91-a252290d317f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031381459s
STEP: Saw pod success
Aug 21 16:29:57.223: INFO: Pod "pod-projected-configmaps-ad09a006-8211-4c33-bf91-a252290d317f" satisfied condition "success or failure"
Aug 21 16:29:57.237: INFO: Trying to get logs from node 10.188.240.202 pod pod-projected-configmaps-ad09a006-8211-4c33-bf91-a252290d317f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 16:29:57.306: INFO: Waiting for pod pod-projected-configmaps-ad09a006-8211-4c33-bf91-a252290d317f to disappear
Aug 21 16:29:57.323: INFO: Pod pod-projected-configmaps-ad09a006-8211-4c33-bf91-a252290d317f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:29:57.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3647" for this suite.
Aug 21 16:30:05.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:30:07.754: INFO: namespace projected-3647 deletion completed in 10.405765364s

• [SLOW TEST:12.834 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:30:07.754: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 16:30:07.971: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:30:08.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3913" for this suite.
Aug 21 16:30:16.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:30:19.212: INFO: namespace custom-resource-definition-3913 deletion completed in 10.487789282s

• [SLOW TEST:11.458 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:30:19.213: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:30:19.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5163" for this suite.
Aug 21 16:30:28.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:30:30.284: INFO: namespace resourcequota-5163 deletion completed in 10.329189278s

• [SLOW TEST:11.071 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:30:30.284: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Aug 21 16:30:35.214: INFO: Successfully updated pod "annotationupdate98e54be6-d309-465b-ab14-78e453fbebb0"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:30:37.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7163" for this suite.
Aug 21 16:30:53.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:30:55.573: INFO: namespace downward-api-7163 deletion completed in 18.244388523s

• [SLOW TEST:25.289 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:30:55.575: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 21 16:30:55.837: INFO: Waiting up to 5m0s for pod "downwardapi-volume-36c58d79-8d75-4f80-94c6-b5fd2675bc87" in namespace "downward-api-9015" to be "success or failure"
Aug 21 16:30:55.849: INFO: Pod "downwardapi-volume-36c58d79-8d75-4f80-94c6-b5fd2675bc87": Phase="Pending", Reason="", readiness=false. Elapsed: 12.316115ms
Aug 21 16:30:57.863: INFO: Pod "downwardapi-volume-36c58d79-8d75-4f80-94c6-b5fd2675bc87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025643415s
STEP: Saw pod success
Aug 21 16:30:57.863: INFO: Pod "downwardapi-volume-36c58d79-8d75-4f80-94c6-b5fd2675bc87" satisfied condition "success or failure"
Aug 21 16:30:57.874: INFO: Trying to get logs from node 10.188.240.222 pod downwardapi-volume-36c58d79-8d75-4f80-94c6-b5fd2675bc87 container client-container: <nil>
STEP: delete the pod
Aug 21 16:30:58.020: INFO: Waiting for pod downwardapi-volume-36c58d79-8d75-4f80-94c6-b5fd2675bc87 to disappear
Aug 21 16:30:58.031: INFO: Pod downwardapi-volume-36c58d79-8d75-4f80-94c6-b5fd2675bc87 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:30:58.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9015" for this suite.
Aug 21 16:31:06.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:31:08.424: INFO: namespace downward-api-9015 deletion completed in 10.362195144s

• [SLOW TEST:12.850 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:31:08.425: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-1876
STEP: creating replication controller nodeport-test in namespace services-1876
I0821 16:31:08.764384      24 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-1876, replica count: 2
Aug 21 16:31:11.819: INFO: Creating new exec pod
I0821 16:31:11.819408      24 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 21 16:31:16.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=services-1876 execpod258jg -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Aug 21 16:31:17.358: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Aug 21 16:31:17.358: INFO: stdout: ""
Aug 21 16:31:17.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=services-1876 execpod258jg -- /bin/sh -x -c nc -zv -t -w 2 172.21.72.216 80'
Aug 21 16:31:17.780: INFO: stderr: "+ nc -zv -t -w 2 172.21.72.216 80\nConnection to 172.21.72.216 80 port [tcp/http] succeeded!\n"
Aug 21 16:31:17.780: INFO: stdout: ""
Aug 21 16:31:17.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=services-1876 execpod258jg -- /bin/sh -x -c nc -zv -t -w 2 10.188.240.202 30199'
Aug 21 16:31:18.216: INFO: stderr: "+ nc -zv -t -w 2 10.188.240.202 30199\nConnection to 10.188.240.202 30199 port [tcp/30199] succeeded!\n"
Aug 21 16:31:18.216: INFO: stdout: ""
Aug 21 16:31:18.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=services-1876 execpod258jg -- /bin/sh -x -c nc -zv -t -w 2 10.188.240.222 30199'
Aug 21 16:31:18.613: INFO: stderr: "+ nc -zv -t -w 2 10.188.240.222 30199\nConnection to 10.188.240.222 30199 port [tcp/30199] succeeded!\n"
Aug 21 16:31:18.613: INFO: stdout: ""
Aug 21 16:31:18.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=services-1876 execpod258jg -- /bin/sh -x -c nc -zv -t -w 2 169.63.148.208 30199'
Aug 21 16:31:19.052: INFO: stderr: "+ nc -zv -t -w 2 169.63.148.208 30199\nConnection to 169.63.148.208 30199 port [tcp/30199] succeeded!\n"
Aug 21 16:31:19.052: INFO: stdout: ""
Aug 21 16:31:19.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=services-1876 execpod258jg -- /bin/sh -x -c nc -zv -t -w 2 169.63.130.222 30199'
Aug 21 16:31:19.469: INFO: stderr: "+ nc -zv -t -w 2 169.63.130.222 30199\nConnection to 169.63.130.222 30199 port [tcp/30199] succeeded!\n"
Aug 21 16:31:19.469: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:31:19.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1876" for this suite.
Aug 21 16:31:29.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:31:31.656: INFO: namespace services-1876 deletion completed in 12.160396454s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:23.231 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:31:31.656: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Aug 21 16:31:32.766: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Aug 21 16:31:34.818: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733624292, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733624292, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733624292, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733624292, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 21 16:31:37.866: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 16:31:37.882: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:31:39.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7861" for this suite.
Aug 21 16:31:47.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:31:49.551: INFO: namespace crd-webhook-7861 deletion completed in 10.234912209s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:17.969 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:31:49.626: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 21 16:31:49.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-6830'
Aug 21 16:31:50.043: INFO: stderr: ""
Aug 21 16:31:50.043: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Aug 21 16:31:55.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pod e2e-test-httpd-pod --namespace=kubectl-6830 -o json'
Aug 21 16:31:55.241: INFO: stderr: ""
Aug 21 16:31:55.241: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.30.245.113/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.245.113/32\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.245.113\\\"\\n    ],\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2020-08-21T16:31:50Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6830\",\n        \"resourceVersion\": \"59654\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6830/pods/e2e-test-httpd-pod\",\n        \"uid\": \"7a5e4715-ae05-40fa-9a01-d7eee0190fc6\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-fxwvl\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-46v47\"\n            }\n        ],\n        \"nodeName\": \"10.188.240.202\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c51,c10\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-fxwvl\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-fxwvl\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-21T16:31:50Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-21T16:31:52Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-21T16:31:52Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-21T16:31:50Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://53ee4dced6b748f1050937d5349766cb54cea9764d94ccf8476fa22202434641\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-08-21T16:31:51Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.188.240.202\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.245.113\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.245.113\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-08-21T16:31:50Z\"\n    }\n}\n"
STEP: replace the image in the pod
Aug 21 16:31:55.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 replace -f - --namespace=kubectl-6830'
Aug 21 16:31:55.893: INFO: stderr: ""
Aug 21 16:31:55.893: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Aug 21 16:31:55.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 delete pods e2e-test-httpd-pod --namespace=kubectl-6830'
Aug 21 16:32:01.195: INFO: stderr: ""
Aug 21 16:32:01.195: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:32:01.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6830" for this suite.
Aug 21 16:32:09.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:32:11.850: INFO: namespace kubectl-6830 deletion completed in 10.613943562s

• [SLOW TEST:22.224 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:32:11.850: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1828
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1828
STEP: creating replication controller externalsvc in namespace services-1828
I0821 16:32:12.178742      24 runners.go:184] Created replication controller with name: externalsvc, namespace: services-1828, replica count: 2
I0821 16:32:15.229833      24 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Aug 21 16:32:15.306: INFO: Creating new exec pod
Aug 21 16:32:19.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=services-1828 execpod2sbrq -- /bin/sh -x -c nslookup nodeport-service'
Aug 21 16:32:19.792: INFO: stderr: "+ nslookup nodeport-service\n"
Aug 21 16:32:19.792: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-1828.svc.cluster.local\tcanonical name = externalsvc.services-1828.svc.cluster.local.\nName:\texternalsvc.services-1828.svc.cluster.local\nAddress: 172.21.246.152\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1828, will wait for the garbage collector to delete the pods
Aug 21 16:32:19.888: INFO: Deleting ReplicationController externalsvc took: 35.244889ms
Aug 21 16:32:21.089: INFO: Terminating ReplicationController externalsvc pods took: 1.200284598s
Aug 21 16:32:27.578: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:32:27.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1828" for this suite.
Aug 21 16:32:37.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:32:39.924: INFO: namespace services-1828 deletion completed in 12.243820232s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:28.074 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:32:39.925: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:32:51.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3651" for this suite.
Aug 21 16:32:59.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:33:01.661: INFO: namespace resourcequota-3651 deletion completed in 10.200900091s

• [SLOW TEST:21.736 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:33:01.661: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9774.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9774.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9774.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9774.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 21 16:33:06.106: INFO: DNS probes using dns-test-20e0a404-6e20-422c-9f5b-d01faa9aefd5 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9774.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9774.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9774.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9774.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 21 16:33:10.309: INFO: File wheezy_udp@dns-test-service-3.dns-9774.svc.cluster.local from pod  dns-9774/dns-test-6ebcd8e4-3f9e-4641-aee5-e62cebeea3d6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 21 16:33:10.331: INFO: Lookups using dns-9774/dns-test-6ebcd8e4-3f9e-4641-aee5-e62cebeea3d6 failed for: [wheezy_udp@dns-test-service-3.dns-9774.svc.cluster.local]

Aug 21 16:33:15.393: INFO: DNS probes using dns-test-6ebcd8e4-3f9e-4641-aee5-e62cebeea3d6 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9774.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9774.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9774.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9774.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 21 16:33:19.659: INFO: DNS probes using dns-test-9f8d4ddf-fcc3-48a8-abca-0b0760d06b58 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:33:19.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9774" for this suite.
Aug 21 16:33:29.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:33:32.049: INFO: namespace dns-9774 deletion completed in 12.222964467s

• [SLOW TEST:30.388 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:33:32.049: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:33:48.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9643" for this suite.
Aug 21 16:33:59.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:34:01.129: INFO: namespace resourcequota-9643 deletion completed in 12.137620959s

• [SLOW TEST:29.080 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:34:01.129: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 16:34:01.402: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Creating first CR 
Aug 21 16:34:02.100: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-21T16:34:02Z generation:1 name:name1 resourceVersion:60779 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:5c57631e-3afa-4758-bb07-7d10087fded0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Aug 21 16:34:12.116: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-21T16:34:12Z generation:1 name:name2 resourceVersion:60816 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:efb33ec4-50cf-4d7b-99dd-26edaec300f6] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Aug 21 16:34:22.132: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-21T16:34:02Z generation:2 name:name1 resourceVersion:60852 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:5c57631e-3afa-4758-bb07-7d10087fded0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Aug 21 16:34:32.157: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-21T16:34:12Z generation:2 name:name2 resourceVersion:60887 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:efb33ec4-50cf-4d7b-99dd-26edaec300f6] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Aug 21 16:34:42.184: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-21T16:34:02Z generation:2 name:name1 resourceVersion:60926 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:5c57631e-3afa-4758-bb07-7d10087fded0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Aug 21 16:34:52.241: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-21T16:34:12Z generation:2 name:name2 resourceVersion:60962 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:efb33ec4-50cf-4d7b-99dd-26edaec300f6] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:35:02.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-2957" for this suite.
Aug 21 16:35:12.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:35:15.042: INFO: namespace crd-watch-2957 deletion completed in 12.245848344s

• [SLOW TEST:73.913 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:35:15.044: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-614fd416-f96b-4006-8725-35dc1f172ab3 in namespace container-probe-8573
Aug 21 16:35:19.335: INFO: Started pod busybox-614fd416-f96b-4006-8725-35dc1f172ab3 in namespace container-probe-8573
STEP: checking the pod's current state and verifying that restartCount is present
Aug 21 16:35:19.346: INFO: Initial restart count of pod busybox-614fd416-f96b-4006-8725-35dc1f172ab3 is 0
Aug 21 16:36:11.726: INFO: Restart count of pod container-probe-8573/busybox-614fd416-f96b-4006-8725-35dc1f172ab3 is now 1 (52.380225803s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:36:11.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8573" for this suite.
Aug 21 16:36:19.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:36:22.014: INFO: namespace container-probe-8573 deletion completed in 10.160810449s

• [SLOW TEST:66.970 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:36:22.015: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 16:36:22.405: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Aug 21 16:36:22.485: INFO: Number of nodes with available pods: 0
Aug 21 16:36:22.485: INFO: Node 10.188.240.202 is running more than one daemon pod
Aug 21 16:36:23.519: INFO: Number of nodes with available pods: 0
Aug 21 16:36:23.519: INFO: Node 10.188.240.202 is running more than one daemon pod
Aug 21 16:36:24.563: INFO: Number of nodes with available pods: 2
Aug 21 16:36:24.563: INFO: Node 10.188.240.202 is running more than one daemon pod
Aug 21 16:36:25.524: INFO: Number of nodes with available pods: 3
Aug 21 16:36:25.525: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Aug 21 16:36:25.638: INFO: Wrong image for pod: daemon-set-b5lt6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:25.638: INFO: Wrong image for pod: daemon-set-cv8r9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:25.638: INFO: Wrong image for pod: daemon-set-ngx5r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:26.676: INFO: Wrong image for pod: daemon-set-b5lt6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:26.676: INFO: Wrong image for pod: daemon-set-cv8r9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:26.676: INFO: Wrong image for pod: daemon-set-ngx5r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:27.675: INFO: Wrong image for pod: daemon-set-b5lt6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:27.675: INFO: Wrong image for pod: daemon-set-cv8r9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:27.675: INFO: Wrong image for pod: daemon-set-ngx5r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:28.679: INFO: Wrong image for pod: daemon-set-b5lt6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:28.679: INFO: Pod daemon-set-b5lt6 is not available
Aug 21 16:36:28.679: INFO: Wrong image for pod: daemon-set-cv8r9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:28.679: INFO: Wrong image for pod: daemon-set-ngx5r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:29.675: INFO: Wrong image for pod: daemon-set-b5lt6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:29.675: INFO: Pod daemon-set-b5lt6 is not available
Aug 21 16:36:29.675: INFO: Wrong image for pod: daemon-set-cv8r9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:29.675: INFO: Wrong image for pod: daemon-set-ngx5r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:30.678: INFO: Wrong image for pod: daemon-set-b5lt6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:30.678: INFO: Pod daemon-set-b5lt6 is not available
Aug 21 16:36:30.678: INFO: Wrong image for pod: daemon-set-cv8r9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:30.678: INFO: Wrong image for pod: daemon-set-ngx5r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:31.676: INFO: Wrong image for pod: daemon-set-b5lt6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:31.676: INFO: Pod daemon-set-b5lt6 is not available
Aug 21 16:36:31.676: INFO: Wrong image for pod: daemon-set-cv8r9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:31.676: INFO: Wrong image for pod: daemon-set-ngx5r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:32.677: INFO: Wrong image for pod: daemon-set-b5lt6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:32.677: INFO: Pod daemon-set-b5lt6 is not available
Aug 21 16:36:32.677: INFO: Wrong image for pod: daemon-set-cv8r9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:32.677: INFO: Wrong image for pod: daemon-set-ngx5r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:33.677: INFO: Wrong image for pod: daemon-set-b5lt6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:33.677: INFO: Pod daemon-set-b5lt6 is not available
Aug 21 16:36:33.677: INFO: Wrong image for pod: daemon-set-cv8r9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:33.677: INFO: Wrong image for pod: daemon-set-ngx5r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:34.678: INFO: Wrong image for pod: daemon-set-b5lt6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:34.678: INFO: Pod daemon-set-b5lt6 is not available
Aug 21 16:36:34.678: INFO: Wrong image for pod: daemon-set-cv8r9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:34.678: INFO: Wrong image for pod: daemon-set-ngx5r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:35.677: INFO: Wrong image for pod: daemon-set-b5lt6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:35.677: INFO: Pod daemon-set-b5lt6 is not available
Aug 21 16:36:35.677: INFO: Wrong image for pod: daemon-set-cv8r9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:35.677: INFO: Wrong image for pod: daemon-set-ngx5r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:36.679: INFO: Wrong image for pod: daemon-set-b5lt6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:36.679: INFO: Pod daemon-set-b5lt6 is not available
Aug 21 16:36:36.679: INFO: Wrong image for pod: daemon-set-cv8r9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:36.679: INFO: Wrong image for pod: daemon-set-ngx5r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:37.683: INFO: Wrong image for pod: daemon-set-cv8r9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:37.683: INFO: Wrong image for pod: daemon-set-ngx5r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:37.683: INFO: Pod daemon-set-vmc97 is not available
Aug 21 16:36:38.678: INFO: Wrong image for pod: daemon-set-cv8r9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:38.678: INFO: Wrong image for pod: daemon-set-ngx5r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:38.678: INFO: Pod daemon-set-vmc97 is not available
Aug 21 16:36:39.675: INFO: Wrong image for pod: daemon-set-cv8r9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:39.675: INFO: Wrong image for pod: daemon-set-ngx5r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:40.681: INFO: Wrong image for pod: daemon-set-cv8r9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:40.681: INFO: Wrong image for pod: daemon-set-ngx5r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:41.677: INFO: Wrong image for pod: daemon-set-cv8r9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:41.677: INFO: Wrong image for pod: daemon-set-ngx5r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:41.677: INFO: Pod daemon-set-ngx5r is not available
Aug 21 16:36:42.679: INFO: Wrong image for pod: daemon-set-cv8r9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:42.679: INFO: Pod daemon-set-kbtw2 is not available
Aug 21 16:36:43.678: INFO: Wrong image for pod: daemon-set-cv8r9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:43.678: INFO: Pod daemon-set-kbtw2 is not available
Aug 21 16:36:44.676: INFO: Wrong image for pod: daemon-set-cv8r9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:45.675: INFO: Wrong image for pod: daemon-set-cv8r9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 21 16:36:45.676: INFO: Pod daemon-set-cv8r9 is not available
Aug 21 16:36:46.676: INFO: Pod daemon-set-kc9sr is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Aug 21 16:36:46.745: INFO: Number of nodes with available pods: 2
Aug 21 16:36:46.745: INFO: Node 10.188.240.222 is running more than one daemon pod
Aug 21 16:36:47.784: INFO: Number of nodes with available pods: 2
Aug 21 16:36:47.784: INFO: Node 10.188.240.222 is running more than one daemon pod
Aug 21 16:36:48.791: INFO: Number of nodes with available pods: 2
Aug 21 16:36:48.791: INFO: Node 10.188.240.222 is running more than one daemon pod
Aug 21 16:36:49.786: INFO: Number of nodes with available pods: 2
Aug 21 16:36:49.787: INFO: Node 10.188.240.222 is running more than one daemon pod
Aug 21 16:36:50.791: INFO: Number of nodes with available pods: 2
Aug 21 16:36:50.791: INFO: Node 10.188.240.222 is running more than one daemon pod
Aug 21 16:36:51.812: INFO: Number of nodes with available pods: 3
Aug 21 16:36:51.812: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3983, will wait for the garbage collector to delete the pods
Aug 21 16:36:52.010: INFO: Deleting DaemonSet.extensions daemon-set took: 46.986282ms
Aug 21 16:36:52.611: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.329379ms
Aug 21 16:37:02.523: INFO: Number of nodes with available pods: 0
Aug 21 16:37:02.523: INFO: Number of running nodes: 0, number of available pods: 0
Aug 21 16:37:02.552: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3983/daemonsets","resourceVersion":"61757"},"items":null}

Aug 21 16:37:02.586: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3983/pods","resourceVersion":"61757"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:37:02.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3983" for this suite.
Aug 21 16:37:12.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:37:14.926: INFO: namespace daemonsets-3983 deletion completed in 12.249888089s

• [SLOW TEST:52.911 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:37:14.926: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Aug 21 16:37:15.145: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Aug 21 16:37:50.459: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 16:38:00.156: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:38:35.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6793" for this suite.
Aug 21 16:38:43.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:38:45.956: INFO: namespace crd-publish-openapi-6793 deletion completed in 10.421320393s

• [SLOW TEST:91.030 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:38:45.956: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 16:38:46.207: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-6ed8bde3-c972-4758-b80f-1593ac7abe5b
STEP: Creating configMap with name cm-test-opt-upd-6108f449-7d5e-479d-9737-c0cd4b196529
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-6ed8bde3-c972-4758-b80f-1593ac7abe5b
STEP: Updating configmap cm-test-opt-upd-6108f449-7d5e-479d-9737-c0cd4b196529
STEP: Creating configMap with name cm-test-opt-create-7aac82f9-de6a-4ee4-ac44-fa036974edb0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:38:50.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2090" for this suite.
Aug 21 16:39:12.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:39:15.128: INFO: namespace configmap-2090 deletion completed in 24.280684804s

• [SLOW TEST:29.172 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:39:15.129: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 21 16:39:15.389: INFO: Waiting up to 5m0s for pod "downwardapi-volume-83759611-e4eb-401d-8d8e-c2fbbc21a9b5" in namespace "projected-2899" to be "success or failure"
Aug 21 16:39:15.400: INFO: Pod "downwardapi-volume-83759611-e4eb-401d-8d8e-c2fbbc21a9b5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.119609ms
Aug 21 16:39:17.413: INFO: Pod "downwardapi-volume-83759611-e4eb-401d-8d8e-c2fbbc21a9b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024143048s
STEP: Saw pod success
Aug 21 16:39:17.413: INFO: Pod "downwardapi-volume-83759611-e4eb-401d-8d8e-c2fbbc21a9b5" satisfied condition "success or failure"
Aug 21 16:39:17.428: INFO: Trying to get logs from node 10.188.240.202 pod downwardapi-volume-83759611-e4eb-401d-8d8e-c2fbbc21a9b5 container client-container: <nil>
STEP: delete the pod
Aug 21 16:39:17.496: INFO: Waiting for pod downwardapi-volume-83759611-e4eb-401d-8d8e-c2fbbc21a9b5 to disappear
Aug 21 16:39:17.509: INFO: Pod downwardapi-volume-83759611-e4eb-401d-8d8e-c2fbbc21a9b5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:39:17.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2899" for this suite.
Aug 21 16:39:25.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:39:27.755: INFO: namespace projected-2899 deletion completed in 10.194186171s

• [SLOW TEST:12.626 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:39:27.756: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:39:41.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5749" for this suite.
Aug 21 16:39:49.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:39:51.800: INFO: namespace resourcequota-5749 deletion completed in 10.450065398s

• [SLOW TEST:24.044 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:39:51.801: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 21 16:39:52.134: INFO: Waiting up to 5m0s for pod "pod-1a84187a-473d-4f1c-851c-4f0b4cee7abc" in namespace "emptydir-3138" to be "success or failure"
Aug 21 16:39:52.151: INFO: Pod "pod-1a84187a-473d-4f1c-851c-4f0b4cee7abc": Phase="Pending", Reason="", readiness=false. Elapsed: 17.195994ms
Aug 21 16:39:54.168: INFO: Pod "pod-1a84187a-473d-4f1c-851c-4f0b4cee7abc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034330226s
Aug 21 16:39:56.190: INFO: Pod "pod-1a84187a-473d-4f1c-851c-4f0b4cee7abc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055873123s
STEP: Saw pod success
Aug 21 16:39:56.190: INFO: Pod "pod-1a84187a-473d-4f1c-851c-4f0b4cee7abc" satisfied condition "success or failure"
Aug 21 16:39:56.209: INFO: Trying to get logs from node 10.188.240.202 pod pod-1a84187a-473d-4f1c-851c-4f0b4cee7abc container test-container: <nil>
STEP: delete the pod
Aug 21 16:39:56.286: INFO: Waiting for pod pod-1a84187a-473d-4f1c-851c-4f0b4cee7abc to disappear
Aug 21 16:39:56.298: INFO: Pod pod-1a84187a-473d-4f1c-851c-4f0b4cee7abc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:39:56.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3138" for this suite.
Aug 21 16:40:04.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:40:06.905: INFO: namespace emptydir-3138 deletion completed in 10.578199775s

• [SLOW TEST:15.104 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:40:06.905: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Aug 21 16:40:07.112: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 21 16:40:07.229: INFO: Waiting for terminating namespaces to be deleted...
Aug 21 16:40:07.254: INFO: 
Logging pods the kubelet thinks is on node 10.188.240.202 before test
Aug 21 16:40:07.339: INFO: multus-59vjb from openshift-multus started at 2020-08-21 14:24:42 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.339: INFO: 	Container kube-multus ready: true, restart count 0
Aug 21 16:40:07.339: INFO: tuned-hfsdd from openshift-cluster-node-tuning-operator started at 2020-08-21 14:26:30 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.339: INFO: 	Container tuned ready: true, restart count 0
Aug 21 16:40:07.339: INFO: community-operators-68975cd7c8-sf6zn from openshift-marketplace started at 2020-08-21 14:28:43 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.339: INFO: 	Container community-operators ready: true, restart count 0
Aug 21 16:40:07.339: INFO: image-registry-86f8b76dcb-jdpb9 from openshift-image-registry started at 2020-08-21 14:30:33 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.340: INFO: 	Container registry ready: true, restart count 0
Aug 21 16:40:07.340: INFO: prometheus-operator-56d9d699cb-drn75 from openshift-monitoring started at 2020-08-21 14:33:56 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.340: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 21 16:40:07.340: INFO: grafana-c9c7455d7-gcf82 from openshift-monitoring started at 2020-08-21 14:34:15 +0000 UTC (2 container statuses recorded)
Aug 21 16:40:07.340: INFO: 	Container grafana ready: true, restart count 0
Aug 21 16:40:07.340: INFO: 	Container grafana-proxy ready: true, restart count 0
Aug 21 16:40:07.340: INFO: ibmcloud-block-storage-driver-wmdvn from kube-system started at 2020-08-21 14:24:16 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.340: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 21 16:40:07.340: INFO: console-5bf7799b6-qfl8j from openshift-console started at 2020-08-21 14:29:21 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.340: INFO: 	Container console ready: true, restart count 0
Aug 21 16:40:07.340: INFO: ibm-cloud-provider-ip-169-60-87-181-7df9c474cf-m5bdn from ibm-system started at 2020-08-21 14:30:31 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.340: INFO: 	Container ibm-cloud-provider-ip-169-60-87-181 ready: true, restart count 0
Aug 21 16:40:07.340: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-08-21 14:35:11 +0000 UTC (7 container statuses recorded)
Aug 21 16:40:07.341: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 16:40:07.341: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 21 16:40:07.341: INFO: 	Container prometheus ready: true, restart count 1
Aug 21 16:40:07.341: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 21 16:40:07.341: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 21 16:40:07.341: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 21 16:40:07.341: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 21 16:40:07.341: INFO: apiservice-cabundle-injector-594fd4555f-pd5j2 from openshift-service-ca started at 2020-08-21 14:26:58 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.341: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Aug 21 16:40:07.341: INFO: node-exporter-vw6s8 from openshift-monitoring started at 2020-08-21 14:26:49 +0000 UTC (2 container statuses recorded)
Aug 21 16:40:07.341: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 16:40:07.341: INFO: 	Container node-exporter ready: true, restart count 0
Aug 21 16:40:07.341: INFO: thanos-querier-5bcc6bd6c4-w2z4d from openshift-monitoring started at 2020-08-21 14:35:05 +0000 UTC (4 container statuses recorded)
Aug 21 16:40:07.341: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 16:40:07.341: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 21 16:40:07.342: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 21 16:40:07.342: INFO: 	Container thanos-querier ready: true, restart count 0
Aug 21 16:40:07.342: INFO: packageserver-75777686bd-sxs4v from openshift-operator-lifecycle-manager started at 2020-08-21 14:31:38 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.342: INFO: 	Container packageserver ready: true, restart count 0
Aug 21 16:40:07.342: INFO: tigera-operator-679798d94d-5dj78 from tigera-operator started at 2020-08-21 14:24:16 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.342: INFO: 	Container tigera-operator ready: true, restart count 1
Aug 21 16:40:07.342: INFO: calico-node-lfw9m from calico-system started at 2020-08-21 14:25:08 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.342: INFO: 	Container calico-node ready: true, restart count 0
Aug 21 16:40:07.342: INFO: calico-typha-6c986fbc8c-mtt9h from calico-system started at 2020-08-21 14:27:04 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.342: INFO: 	Container calico-typha ready: true, restart count 0
Aug 21 16:40:07.342: INFO: node-ca-2zlsz from openshift-image-registry started at 2020-08-21 14:27:50 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.342: INFO: 	Container node-ca ready: true, restart count 0
Aug 21 16:40:07.342: INFO: ibm-master-proxy-static-10.188.240.202 from kube-system started at 2020-08-21 14:24:11 +0000 UTC (2 container statuses recorded)
Aug 21 16:40:07.342: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 21 16:40:07.343: INFO: 	Container pause ready: true, restart count 0
Aug 21 16:40:07.343: INFO: dns-default-zd27g from openshift-dns started at 2020-08-21 14:28:16 +0000 UTC (2 container statuses recorded)
Aug 21 16:40:07.343: INFO: 	Container dns ready: true, restart count 0
Aug 21 16:40:07.343: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 21 16:40:07.343: INFO: router-default-79bfbd48f7-bdqc5 from openshift-ingress started at 2020-08-21 14:28:24 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.343: INFO: 	Container router ready: true, restart count 0
Aug 21 16:40:07.343: INFO: redhat-operators-6d986fdd47-bvpx2 from openshift-marketplace started at 2020-08-21 14:28:43 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.343: INFO: 	Container redhat-operators ready: true, restart count 0
Aug 21 16:40:07.343: INFO: prometheus-adapter-5697b6dddd-ldgqf from openshift-monitoring started at 2020-08-21 14:34:09 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.343: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 21 16:40:07.343: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-08-21 14:34:31 +0000 UTC (3 container statuses recorded)
Aug 21 16:40:07.343: INFO: 	Container alertmanager ready: true, restart count 0
Aug 21 16:40:07.343: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 21 16:40:07.343: INFO: 	Container config-reloader ready: true, restart count 0
Aug 21 16:40:07.344: INFO: sonobuoy-systemd-logs-daemon-set-5cea973cc7904f8c-nfmgc from sonobuoy started at 2020-08-21 15:39:10 +0000 UTC (2 container statuses recorded)
Aug 21 16:40:07.344: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 21 16:40:07.344: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 21 16:40:07.344: INFO: network-operator-7986644c85-778wc from openshift-network-operator started at 2020-08-21 14:24:16 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.344: INFO: 	Container network-operator ready: true, restart count 0
Aug 21 16:40:07.344: INFO: openshift-kube-proxy-dqhxm from openshift-kube-proxy started at 2020-08-21 14:24:48 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.344: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 21 16:40:07.344: INFO: multus-admission-controller-7ccn4 from openshift-multus started at 2020-08-21 14:25:32 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.344: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 21 16:40:07.344: INFO: openshift-state-metrics-5849d797d8-h6klv from openshift-monitoring started at 2020-08-21 14:26:49 +0000 UTC (3 container statuses recorded)
Aug 21 16:40:07.344: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 21 16:40:07.345: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 21 16:40:07.345: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 21 16:40:07.345: INFO: service-serving-cert-signer-7879bf8d9f-db6v2 from openshift-service-ca started at 2020-08-21 14:26:58 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.345: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Aug 21 16:40:07.345: INFO: ibm-keepalived-watcher-hgfkl from kube-system started at 2020-08-21 14:24:13 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.345: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 21 16:40:07.345: INFO: 
Logging pods the kubelet thinks is on node 10.188.240.222 before test
Aug 21 16:40:07.458: INFO: registry-pvc-permissions-px8dj from openshift-image-registry started at 2020-08-21 14:30:33 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.458: INFO: 	Container pvc-permissions ready: false, restart count 0
Aug 21 16:40:07.458: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-08-21 14:34:42 +0000 UTC (3 container statuses recorded)
Aug 21 16:40:07.458: INFO: 	Container alertmanager ready: true, restart count 0
Aug 21 16:40:07.458: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 21 16:40:07.458: INFO: 	Container config-reloader ready: true, restart count 0
Aug 21 16:40:07.458: INFO: multus-4xs2x from openshift-multus started at 2020-08-21 14:24:42 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.458: INFO: 	Container kube-multus ready: true, restart count 0
Aug 21 16:40:07.458: INFO: thanos-querier-5bcc6bd6c4-6wnxk from openshift-monitoring started at 2020-08-21 14:34:55 +0000 UTC (4 container statuses recorded)
Aug 21 16:40:07.458: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 16:40:07.458: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 21 16:40:07.458: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 21 16:40:07.458: INFO: 	Container thanos-querier ready: true, restart count 0
Aug 21 16:40:07.458: INFO: cluster-samples-operator-55944b8f44-hpv4t from openshift-cluster-samples-operator started at 2020-08-21 14:27:28 +0000 UTC (2 container statuses recorded)
Aug 21 16:40:07.458: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Aug 21 16:40:07.458: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Aug 21 16:40:07.458: INFO: router-default-79bfbd48f7-wq29r from openshift-ingress started at 2020-08-21 14:28:25 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.458: INFO: 	Container router ready: true, restart count 0
Aug 21 16:40:07.458: INFO: ibmcloud-block-storage-driver-88f6v from kube-system started at 2020-08-21 14:24:22 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.458: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 21 16:40:07.458: INFO: kube-state-metrics-c5f65645-vgmjg from openshift-monitoring started at 2020-08-21 14:26:47 +0000 UTC (3 container statuses recorded)
Aug 21 16:40:07.458: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 21 16:40:07.458: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 21 16:40:07.458: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 21 16:40:07.458: INFO: node-ca-wn9jz from openshift-image-registry started at 2020-08-21 14:27:50 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.458: INFO: 	Container node-ca ready: true, restart count 0
Aug 21 16:40:07.458: INFO: ibm-master-proxy-static-10.188.240.222 from kube-system started at 2020-08-21 14:24:12 +0000 UTC (2 container statuses recorded)
Aug 21 16:40:07.458: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 21 16:40:07.458: INFO: 	Container pause ready: true, restart count 0
Aug 21 16:40:07.458: INFO: tuned-xffjb from openshift-cluster-node-tuning-operator started at 2020-08-21 14:26:30 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.458: INFO: 	Container tuned ready: true, restart count 0
Aug 21 16:40:07.458: INFO: console-5bf7799b6-qbrjl from openshift-console started at 2020-08-21 14:29:07 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.458: INFO: 	Container console ready: true, restart count 0
Aug 21 16:40:07.458: INFO: openshift-kube-proxy-w7zk7 from openshift-kube-proxy started at 2020-08-21 14:24:48 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.458: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 21 16:40:07.458: INFO: configmap-cabundle-injector-8446d4b88f-lc4xk from openshift-service-ca started at 2020-08-21 14:26:59 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.458: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Aug 21 16:40:07.458: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-08-21 14:35:32 +0000 UTC (7 container statuses recorded)
Aug 21 16:40:07.458: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 16:40:07.458: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 21 16:40:07.458: INFO: 	Container prometheus ready: true, restart count 1
Aug 21 16:40:07.458: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 21 16:40:07.458: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 21 16:40:07.458: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 21 16:40:07.458: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 21 16:40:07.458: INFO: vpn-69dd866c84-4s9m7 from kube-system started at 2020-08-21 14:31:55 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.458: INFO: 	Container vpn ready: true, restart count 0
Aug 21 16:40:07.458: INFO: calico-node-s8hvd from calico-system started at 2020-08-21 14:25:08 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.458: INFO: 	Container calico-node ready: true, restart count 0
Aug 21 16:40:07.458: INFO: multus-admission-controller-tmrh2 from openshift-multus started at 2020-08-21 14:25:45 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.458: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 21 16:40:07.458: INFO: node-exporter-67tp6 from openshift-monitoring started at 2020-08-21 14:26:50 +0000 UTC (2 container statuses recorded)
Aug 21 16:40:07.458: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 16:40:07.458: INFO: 	Container node-exporter ready: true, restart count 0
Aug 21 16:40:07.458: INFO: dns-default-xxclv from openshift-dns started at 2020-08-21 14:28:16 +0000 UTC (2 container statuses recorded)
Aug 21 16:40:07.458: INFO: 	Container dns ready: true, restart count 0
Aug 21 16:40:07.458: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 21 16:40:07.458: INFO: telemeter-client-6fdb57d68d-f82cq from openshift-monitoring started at 2020-08-21 14:34:07 +0000 UTC (3 container statuses recorded)
Aug 21 16:40:07.458: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 16:40:07.458: INFO: 	Container reload ready: true, restart count 0
Aug 21 16:40:07.458: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 21 16:40:07.458: INFO: sonobuoy-systemd-logs-daemon-set-5cea973cc7904f8c-m7w4l from sonobuoy started at 2020-08-21 15:39:10 +0000 UTC (2 container statuses recorded)
Aug 21 16:40:07.458: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 21 16:40:07.458: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 21 16:40:07.458: INFO: ibm-keepalived-watcher-pqhzw from kube-system started at 2020-08-21 14:24:15 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.458: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 21 16:40:07.458: INFO: calico-typha-6c986fbc8c-d5w6g from calico-system started at 2020-08-21 14:25:07 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.458: INFO: 	Container calico-typha ready: true, restart count 0
Aug 21 16:40:07.458: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-08-21 14:30:18 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.458: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Aug 21 16:40:07.458: INFO: 
Logging pods the kubelet thinks is on node 10.188.240.230 before test
Aug 21 16:40:07.582: INFO: ibm-file-plugin-6c96899f79-5r8lq from kube-system started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.582: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 21 16:40:07.582: INFO: dns-operator-6f9cf66db7-2qjt6 from openshift-dns-operator started at 2020-08-21 14:25:29 +0000 UTC (2 container statuses recorded)
Aug 21 16:40:07.582: INFO: 	Container dns-operator ready: true, restart count 0
Aug 21 16:40:07.583: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 16:40:07.583: INFO: ibmcloud-block-storage-plugin-68d5c65db9-clx4d from kube-system started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.583: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Aug 21 16:40:07.583: INFO: multus-admission-controller-9x9x2 from openshift-multus started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.583: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 21 16:40:07.583: INFO: ibm-storage-watcher-68df9b45c4-9rvr8 from kube-system started at 2020-08-21 14:25:31 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.583: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 21 16:40:07.583: INFO: node-exporter-jkv5q from openshift-monitoring started at 2020-08-21 14:26:50 +0000 UTC (2 container statuses recorded)
Aug 21 16:40:07.583: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 16:40:07.583: INFO: 	Container node-exporter ready: true, restart count 0
Aug 21 16:40:07.583: INFO: calico-node-rw476 from calico-system started at 2020-08-21 14:25:08 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.583: INFO: 	Container calico-node ready: true, restart count 0
Aug 21 16:40:07.583: INFO: cluster-storage-operator-557b75f8d5-bpz57 from openshift-cluster-storage-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.583: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Aug 21 16:40:07.583: INFO: cluster-image-registry-operator-6cfd58b66c-rsxhh from openshift-image-registry started at 2020-08-21 14:25:29 +0000 UTC (2 container statuses recorded)
Aug 21 16:40:07.583: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Aug 21 16:40:07.583: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Aug 21 16:40:07.583: INFO: cluster-node-tuning-operator-b5f884945-f92th from openshift-cluster-node-tuning-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.583: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Aug 21 16:40:07.583: INFO: tuned-bh9bt from openshift-cluster-node-tuning-operator started at 2020-08-21 14:26:30 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.583: INFO: 	Container tuned ready: true, restart count 0
Aug 21 16:40:07.583: INFO: packageserver-75777686bd-6jlwz from openshift-operator-lifecycle-manager started at 2020-08-21 14:31:50 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.584: INFO: 	Container packageserver ready: true, restart count 0
Aug 21 16:40:07.584: INFO: ibm-keepalived-watcher-9tt65 from kube-system started at 2020-08-21 14:24:19 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.584: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 21 16:40:07.584: INFO: downloads-678f5d6564-sxpw2 from openshift-console started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.584: INFO: 	Container download-server ready: true, restart count 0
Aug 21 16:40:07.584: INFO: calico-typha-6c986fbc8c-rw4j9 from calico-system started at 2020-08-21 14:27:04 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.584: INFO: 	Container calico-typha ready: true, restart count 0
Aug 21 16:40:07.584: INFO: sonobuoy from sonobuoy started at 2020-08-21 15:39:03 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.584: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 21 16:40:07.584: INFO: console-operator-9878d4766-tfdhf from openshift-console-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.584: INFO: 	Container console-operator ready: true, restart count 1
Aug 21 16:40:07.584: INFO: openshift-service-catalog-controller-manager-operator-5496stt64 from openshift-service-catalog-controller-manager-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.584: INFO: 	Container operator ready: true, restart count 1
Aug 21 16:40:07.584: INFO: calico-kube-controllers-79d75767dd-bznnm from calico-system started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.584: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 21 16:40:07.584: INFO: node-ca-7vtw7 from openshift-image-registry started at 2020-08-21 14:27:50 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.584: INFO: 	Container node-ca ready: true, restart count 0
Aug 21 16:40:07.584: INFO: prometheus-adapter-5697b6dddd-hpnk5 from openshift-monitoring started at 2020-08-21 14:34:09 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.584: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 21 16:40:07.584: INFO: multus-5fbcs from openshift-multus started at 2020-08-21 14:24:42 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.584: INFO: 	Container kube-multus ready: true, restart count 0
Aug 21 16:40:07.585: INFO: cluster-monitoring-operator-5b5659466f-lbtcv from openshift-monitoring started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.585: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Aug 21 16:40:07.585: INFO: service-ca-operator-694cfbf5d5-vxbp2 from openshift-service-ca-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.585: INFO: 	Container operator ready: true, restart count 0
Aug 21 16:40:07.585: INFO: ingress-operator-695bc545b9-ps8wd from openshift-ingress-operator started at 2020-08-21 14:25:29 +0000 UTC (2 container statuses recorded)
Aug 21 16:40:07.585: INFO: 	Container ingress-operator ready: true, restart count 0
Aug 21 16:40:07.585: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 16:40:07.585: INFO: openshift-kube-proxy-j677g from openshift-kube-proxy started at 2020-08-21 14:24:48 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.585: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 21 16:40:07.585: INFO: olm-operator-b5f57cdbb-nw9x4 from openshift-operator-lifecycle-manager started at 2020-08-21 14:25:30 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.585: INFO: 	Container olm-operator ready: true, restart count 0
Aug 21 16:40:07.585: INFO: dns-default-qhhzk from openshift-dns started at 2020-08-21 14:28:16 +0000 UTC (2 container statuses recorded)
Aug 21 16:40:07.585: INFO: 	Container dns ready: true, restart count 0
Aug 21 16:40:07.585: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 21 16:40:07.585: INFO: ibm-cloud-provider-ip-169-60-87-181-7df9c474cf-v8lnf from ibm-system started at 2020-08-21 14:30:19 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.585: INFO: 	Container ibm-cloud-provider-ip-169-60-87-181 ready: true, restart count 0
Aug 21 16:40:07.585: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-08-21 14:34:17 +0000 UTC (3 container statuses recorded)
Aug 21 16:40:07.585: INFO: 	Container alertmanager ready: true, restart count 0
Aug 21 16:40:07.585: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 21 16:40:07.585: INFO: 	Container config-reloader ready: true, restart count 0
Aug 21 16:40:07.585: INFO: openshift-service-catalog-apiserver-operator-78b9dddb6f-jxgbd from openshift-service-catalog-apiserver-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.586: INFO: 	Container operator ready: true, restart count 1
Aug 21 16:40:07.586: INFO: catalog-operator-85f6c659cc-dqh5m from openshift-operator-lifecycle-manager started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.586: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 21 16:40:07.586: INFO: certified-operators-6b6b9f965f-fthsb from openshift-marketplace started at 2020-08-21 16:28:56 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.586: INFO: 	Container certified-operators ready: true, restart count 0
Aug 21 16:40:07.586: INFO: ibm-master-proxy-static-10.188.240.230 from kube-system started at 2020-08-21 14:24:17 +0000 UTC (2 container statuses recorded)
Aug 21 16:40:07.586: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 21 16:40:07.586: INFO: 	Container pause ready: true, restart count 0
Aug 21 16:40:07.586: INFO: ibmcloud-block-storage-driver-jzb5f from kube-system started at 2020-08-21 14:24:22 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.586: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 21 16:40:07.586: INFO: marketplace-operator-6957767d58-5m7kk from openshift-marketplace started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.586: INFO: 	Container marketplace-operator ready: true, restart count 0
Aug 21 16:40:07.586: INFO: downloads-678f5d6564-78bw4 from openshift-console started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:40:07.586: INFO: 	Container download-server ready: true, restart count 0
Aug 21 16:40:07.586: INFO: sonobuoy-e2e-job-3399d07d932e4f9d from sonobuoy started at 2020-08-21 15:39:10 +0000 UTC (2 container statuses recorded)
Aug 21 16:40:07.586: INFO: 	Container e2e ready: true, restart count 0
Aug 21 16:40:07.586: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 21 16:40:07.586: INFO: sonobuoy-systemd-logs-daemon-set-5cea973cc7904f8c-rjxm6 from sonobuoy started at 2020-08-21 15:39:10 +0000 UTC (2 container statuses recorded)
Aug 21 16:40:07.587: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 21 16:40:07.587: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-743ddece-caf8-4a3c-9318-2e5856fef2f6 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-743ddece-caf8-4a3c-9318-2e5856fef2f6 off the node 10.188.240.230
STEP: verifying the node doesn't have the label kubernetes.io/e2e-743ddece-caf8-4a3c-9318-2e5856fef2f6
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:45:15.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1730" for this suite.
Aug 21 16:45:30.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:45:32.208: INFO: namespace sched-pred-1730 deletion completed in 16.197048909s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:325.303 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:45:32.208: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:45:37.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5910" for this suite.
Aug 21 16:45:45.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:45:47.769: INFO: namespace watch-5910 deletion completed in 10.280195542s

• [SLOW TEST:15.561 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:45:47.769: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 16:45:48.003: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-45587845-81c9-4ff9-a88b-a3e4921c3adf
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:45:52.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7749" for this suite.
Aug 21 16:46:08.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:46:10.591: INFO: namespace configmap-7749 deletion completed in 18.364467096s

• [SLOW TEST:22.822 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:46:10.591: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Aug 21 16:46:10.870: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 21 16:46:10.979: INFO: Waiting for terminating namespaces to be deleted...
Aug 21 16:46:10.998: INFO: 
Logging pods the kubelet thinks is on node 10.188.240.202 before test
Aug 21 16:46:11.128: INFO: ibm-master-proxy-static-10.188.240.202 from kube-system started at 2020-08-21 14:24:11 +0000 UTC (2 container statuses recorded)
Aug 21 16:46:11.128: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 21 16:46:11.128: INFO: 	Container pause ready: true, restart count 0
Aug 21 16:46:11.128: INFO: calico-node-lfw9m from calico-system started at 2020-08-21 14:25:08 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.128: INFO: 	Container calico-node ready: true, restart count 0
Aug 21 16:46:11.128: INFO: calico-typha-6c986fbc8c-mtt9h from calico-system started at 2020-08-21 14:27:04 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.128: INFO: 	Container calico-typha ready: true, restart count 0
Aug 21 16:46:11.128: INFO: node-ca-2zlsz from openshift-image-registry started at 2020-08-21 14:27:50 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.128: INFO: 	Container node-ca ready: true, restart count 0
Aug 21 16:46:11.128: INFO: dns-default-zd27g from openshift-dns started at 2020-08-21 14:28:16 +0000 UTC (2 container statuses recorded)
Aug 21 16:46:11.128: INFO: 	Container dns ready: true, restart count 0
Aug 21 16:46:11.128: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 21 16:46:11.128: INFO: network-operator-7986644c85-778wc from openshift-network-operator started at 2020-08-21 14:24:16 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.128: INFO: 	Container network-operator ready: true, restart count 0
Aug 21 16:46:11.128: INFO: router-default-79bfbd48f7-bdqc5 from openshift-ingress started at 2020-08-21 14:28:24 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.128: INFO: 	Container router ready: true, restart count 0
Aug 21 16:46:11.128: INFO: redhat-operators-6d986fdd47-bvpx2 from openshift-marketplace started at 2020-08-21 14:28:43 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.128: INFO: 	Container redhat-operators ready: true, restart count 0
Aug 21 16:46:11.128: INFO: prometheus-adapter-5697b6dddd-ldgqf from openshift-monitoring started at 2020-08-21 14:34:09 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.128: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 21 16:46:11.128: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-08-21 14:34:31 +0000 UTC (3 container statuses recorded)
Aug 21 16:46:11.128: INFO: 	Container alertmanager ready: true, restart count 0
Aug 21 16:46:11.128: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 21 16:46:11.128: INFO: 	Container config-reloader ready: true, restart count 0
Aug 21 16:46:11.128: INFO: sonobuoy-systemd-logs-daemon-set-5cea973cc7904f8c-nfmgc from sonobuoy started at 2020-08-21 15:39:10 +0000 UTC (2 container statuses recorded)
Aug 21 16:46:11.129: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 21 16:46:11.129: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 21 16:46:11.129: INFO: ibm-keepalived-watcher-hgfkl from kube-system started at 2020-08-21 14:24:13 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.129: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 21 16:46:11.129: INFO: openshift-kube-proxy-dqhxm from openshift-kube-proxy started at 2020-08-21 14:24:48 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.129: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 21 16:46:11.129: INFO: multus-admission-controller-7ccn4 from openshift-multus started at 2020-08-21 14:25:32 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.129: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 21 16:46:11.129: INFO: openshift-state-metrics-5849d797d8-h6klv from openshift-monitoring started at 2020-08-21 14:26:49 +0000 UTC (3 container statuses recorded)
Aug 21 16:46:11.129: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 21 16:46:11.129: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 21 16:46:11.129: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 21 16:46:11.129: INFO: service-serving-cert-signer-7879bf8d9f-db6v2 from openshift-service-ca started at 2020-08-21 14:26:58 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.129: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Aug 21 16:46:11.129: INFO: grafana-c9c7455d7-gcf82 from openshift-monitoring started at 2020-08-21 14:34:15 +0000 UTC (2 container statuses recorded)
Aug 21 16:46:11.129: INFO: 	Container grafana ready: true, restart count 0
Aug 21 16:46:11.129: INFO: 	Container grafana-proxy ready: true, restart count 0
Aug 21 16:46:11.129: INFO: ibmcloud-block-storage-driver-wmdvn from kube-system started at 2020-08-21 14:24:16 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.129: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 21 16:46:11.129: INFO: multus-59vjb from openshift-multus started at 2020-08-21 14:24:42 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.129: INFO: 	Container kube-multus ready: true, restart count 0
Aug 21 16:46:11.129: INFO: tuned-hfsdd from openshift-cluster-node-tuning-operator started at 2020-08-21 14:26:30 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.129: INFO: 	Container tuned ready: true, restart count 0
Aug 21 16:46:11.129: INFO: community-operators-68975cd7c8-sf6zn from openshift-marketplace started at 2020-08-21 14:28:43 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.129: INFO: 	Container community-operators ready: true, restart count 0
Aug 21 16:46:11.129: INFO: image-registry-86f8b76dcb-jdpb9 from openshift-image-registry started at 2020-08-21 14:30:33 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.129: INFO: 	Container registry ready: true, restart count 0
Aug 21 16:46:11.129: INFO: prometheus-operator-56d9d699cb-drn75 from openshift-monitoring started at 2020-08-21 14:33:56 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.129: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 21 16:46:11.129: INFO: ibm-cloud-provider-ip-169-60-87-181-7df9c474cf-m5bdn from ibm-system started at 2020-08-21 14:30:31 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.129: INFO: 	Container ibm-cloud-provider-ip-169-60-87-181 ready: true, restart count 0
Aug 21 16:46:11.129: INFO: console-5bf7799b6-qfl8j from openshift-console started at 2020-08-21 14:29:21 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.129: INFO: 	Container console ready: true, restart count 0
Aug 21 16:46:11.129: INFO: apiservice-cabundle-injector-594fd4555f-pd5j2 from openshift-service-ca started at 2020-08-21 14:26:58 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.129: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Aug 21 16:46:11.129: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-08-21 14:35:11 +0000 UTC (7 container statuses recorded)
Aug 21 16:46:11.129: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 16:46:11.129: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 21 16:46:11.129: INFO: 	Container prometheus ready: true, restart count 1
Aug 21 16:46:11.129: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 21 16:46:11.129: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 21 16:46:11.129: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 21 16:46:11.129: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 21 16:46:11.129: INFO: tigera-operator-679798d94d-5dj78 from tigera-operator started at 2020-08-21 14:24:16 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.129: INFO: 	Container tigera-operator ready: true, restart count 1
Aug 21 16:46:11.129: INFO: node-exporter-vw6s8 from openshift-monitoring started at 2020-08-21 14:26:49 +0000 UTC (2 container statuses recorded)
Aug 21 16:46:11.129: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 16:46:11.129: INFO: 	Container node-exporter ready: true, restart count 0
Aug 21 16:46:11.129: INFO: thanos-querier-5bcc6bd6c4-w2z4d from openshift-monitoring started at 2020-08-21 14:35:05 +0000 UTC (4 container statuses recorded)
Aug 21 16:46:11.129: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 16:46:11.129: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 21 16:46:11.129: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 21 16:46:11.129: INFO: 	Container thanos-querier ready: true, restart count 0
Aug 21 16:46:11.129: INFO: packageserver-75777686bd-sxs4v from openshift-operator-lifecycle-manager started at 2020-08-21 14:31:38 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.129: INFO: 	Container packageserver ready: true, restart count 0
Aug 21 16:46:11.129: INFO: 
Logging pods the kubelet thinks is on node 10.188.240.222 before test
Aug 21 16:46:11.246: INFO: multus-4xs2x from openshift-multus started at 2020-08-21 14:24:42 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.246: INFO: 	Container kube-multus ready: true, restart count 0
Aug 21 16:46:11.246: INFO: thanos-querier-5bcc6bd6c4-6wnxk from openshift-monitoring started at 2020-08-21 14:34:55 +0000 UTC (4 container statuses recorded)
Aug 21 16:46:11.246: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 16:46:11.246: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 21 16:46:11.246: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 21 16:46:11.246: INFO: 	Container thanos-querier ready: true, restart count 0
Aug 21 16:46:11.246: INFO: ibmcloud-block-storage-driver-88f6v from kube-system started at 2020-08-21 14:24:22 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.246: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 21 16:46:11.246: INFO: kube-state-metrics-c5f65645-vgmjg from openshift-monitoring started at 2020-08-21 14:26:47 +0000 UTC (3 container statuses recorded)
Aug 21 16:46:11.246: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 21 16:46:11.246: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 21 16:46:11.247: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 21 16:46:11.247: INFO: cluster-samples-operator-55944b8f44-hpv4t from openshift-cluster-samples-operator started at 2020-08-21 14:27:28 +0000 UTC (2 container statuses recorded)
Aug 21 16:46:11.247: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Aug 21 16:46:11.247: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Aug 21 16:46:11.247: INFO: router-default-79bfbd48f7-wq29r from openshift-ingress started at 2020-08-21 14:28:25 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.247: INFO: 	Container router ready: true, restart count 0
Aug 21 16:46:11.247: INFO: ibm-master-proxy-static-10.188.240.222 from kube-system started at 2020-08-21 14:24:12 +0000 UTC (2 container statuses recorded)
Aug 21 16:46:11.247: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 21 16:46:11.247: INFO: 	Container pause ready: true, restart count 0
Aug 21 16:46:11.247: INFO: tuned-xffjb from openshift-cluster-node-tuning-operator started at 2020-08-21 14:26:30 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.247: INFO: 	Container tuned ready: true, restart count 0
Aug 21 16:46:11.247: INFO: node-ca-wn9jz from openshift-image-registry started at 2020-08-21 14:27:50 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.247: INFO: 	Container node-ca ready: true, restart count 0
Aug 21 16:46:11.247: INFO: openshift-kube-proxy-w7zk7 from openshift-kube-proxy started at 2020-08-21 14:24:48 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.247: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 21 16:46:11.247: INFO: configmap-cabundle-injector-8446d4b88f-lc4xk from openshift-service-ca started at 2020-08-21 14:26:59 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.247: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Aug 21 16:46:11.248: INFO: console-5bf7799b6-qbrjl from openshift-console started at 2020-08-21 14:29:07 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.248: INFO: 	Container console ready: true, restart count 0
Aug 21 16:46:11.248: INFO: calico-node-s8hvd from calico-system started at 2020-08-21 14:25:08 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.248: INFO: 	Container calico-node ready: true, restart count 0
Aug 21 16:46:11.248: INFO: multus-admission-controller-tmrh2 from openshift-multus started at 2020-08-21 14:25:45 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.248: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 21 16:46:11.248: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-08-21 14:35:32 +0000 UTC (7 container statuses recorded)
Aug 21 16:46:11.248: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 16:46:11.248: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 21 16:46:11.248: INFO: 	Container prometheus ready: true, restart count 1
Aug 21 16:46:11.248: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 21 16:46:11.248: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 21 16:46:11.248: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 21 16:46:11.248: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 21 16:46:11.248: INFO: vpn-69dd866c84-4s9m7 from kube-system started at 2020-08-21 14:31:55 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.248: INFO: 	Container vpn ready: true, restart count 0
Aug 21 16:46:11.249: INFO: sonobuoy-systemd-logs-daemon-set-5cea973cc7904f8c-m7w4l from sonobuoy started at 2020-08-21 15:39:10 +0000 UTC (2 container statuses recorded)
Aug 21 16:46:11.249: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 21 16:46:11.249: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 21 16:46:11.249: INFO: ibm-keepalived-watcher-pqhzw from kube-system started at 2020-08-21 14:24:15 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.249: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 21 16:46:11.249: INFO: calico-typha-6c986fbc8c-d5w6g from calico-system started at 2020-08-21 14:25:07 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.249: INFO: 	Container calico-typha ready: true, restart count 0
Aug 21 16:46:11.249: INFO: node-exporter-67tp6 from openshift-monitoring started at 2020-08-21 14:26:50 +0000 UTC (2 container statuses recorded)
Aug 21 16:46:11.249: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 16:46:11.249: INFO: 	Container node-exporter ready: true, restart count 0
Aug 21 16:46:11.249: INFO: dns-default-xxclv from openshift-dns started at 2020-08-21 14:28:16 +0000 UTC (2 container statuses recorded)
Aug 21 16:46:11.249: INFO: 	Container dns ready: true, restart count 0
Aug 21 16:46:11.249: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 21 16:46:11.249: INFO: telemeter-client-6fdb57d68d-f82cq from openshift-monitoring started at 2020-08-21 14:34:07 +0000 UTC (3 container statuses recorded)
Aug 21 16:46:11.249: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 16:46:11.249: INFO: 	Container reload ready: true, restart count 0
Aug 21 16:46:11.250: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 21 16:46:11.250: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-08-21 14:30:18 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.250: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Aug 21 16:46:11.250: INFO: registry-pvc-permissions-px8dj from openshift-image-registry started at 2020-08-21 14:30:33 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.250: INFO: 	Container pvc-permissions ready: false, restart count 0
Aug 21 16:46:11.250: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-08-21 14:34:42 +0000 UTC (3 container statuses recorded)
Aug 21 16:46:11.250: INFO: 	Container alertmanager ready: true, restart count 0
Aug 21 16:46:11.250: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 21 16:46:11.250: INFO: 	Container config-reloader ready: true, restart count 0
Aug 21 16:46:11.250: INFO: 
Logging pods the kubelet thinks is on node 10.188.240.230 before test
Aug 21 16:46:11.383: INFO: service-ca-operator-694cfbf5d5-vxbp2 from openshift-service-ca-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.383: INFO: 	Container operator ready: true, restart count 0
Aug 21 16:46:11.384: INFO: ingress-operator-695bc545b9-ps8wd from openshift-ingress-operator started at 2020-08-21 14:25:29 +0000 UTC (2 container statuses recorded)
Aug 21 16:46:11.384: INFO: 	Container ingress-operator ready: true, restart count 0
Aug 21 16:46:11.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 16:46:11.384: INFO: multus-5fbcs from openshift-multus started at 2020-08-21 14:24:42 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.384: INFO: 	Container kube-multus ready: true, restart count 0
Aug 21 16:46:11.384: INFO: cluster-monitoring-operator-5b5659466f-lbtcv from openshift-monitoring started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.384: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Aug 21 16:46:11.384: INFO: dns-default-qhhzk from openshift-dns started at 2020-08-21 14:28:16 +0000 UTC (2 container statuses recorded)
Aug 21 16:46:11.384: INFO: 	Container dns ready: true, restart count 0
Aug 21 16:46:11.384: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 21 16:46:11.384: INFO: ibm-cloud-provider-ip-169-60-87-181-7df9c474cf-v8lnf from ibm-system started at 2020-08-21 14:30:19 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.384: INFO: 	Container ibm-cloud-provider-ip-169-60-87-181 ready: true, restart count 0
Aug 21 16:46:11.384: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-08-21 14:34:17 +0000 UTC (3 container statuses recorded)
Aug 21 16:46:11.384: INFO: 	Container alertmanager ready: true, restart count 0
Aug 21 16:46:11.384: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 21 16:46:11.384: INFO: 	Container config-reloader ready: true, restart count 0
Aug 21 16:46:11.384: INFO: openshift-kube-proxy-j677g from openshift-kube-proxy started at 2020-08-21 14:24:48 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.385: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 21 16:46:11.385: INFO: olm-operator-b5f57cdbb-nw9x4 from openshift-operator-lifecycle-manager started at 2020-08-21 14:25:30 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.385: INFO: 	Container olm-operator ready: true, restart count 0
Aug 21 16:46:11.385: INFO: certified-operators-6b6b9f965f-fthsb from openshift-marketplace started at 2020-08-21 16:28:56 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.385: INFO: 	Container certified-operators ready: true, restart count 0
Aug 21 16:46:11.385: INFO: openshift-service-catalog-apiserver-operator-78b9dddb6f-jxgbd from openshift-service-catalog-apiserver-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.385: INFO: 	Container operator ready: true, restart count 1
Aug 21 16:46:11.385: INFO: catalog-operator-85f6c659cc-dqh5m from openshift-operator-lifecycle-manager started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.385: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 21 16:46:11.385: INFO: marketplace-operator-6957767d58-5m7kk from openshift-marketplace started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.385: INFO: 	Container marketplace-operator ready: true, restart count 0
Aug 21 16:46:11.385: INFO: downloads-678f5d6564-78bw4 from openshift-console started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.385: INFO: 	Container download-server ready: true, restart count 0
Aug 21 16:46:11.385: INFO: sonobuoy-e2e-job-3399d07d932e4f9d from sonobuoy started at 2020-08-21 15:39:10 +0000 UTC (2 container statuses recorded)
Aug 21 16:46:11.385: INFO: 	Container e2e ready: true, restart count 0
Aug 21 16:46:11.386: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 21 16:46:11.386: INFO: sonobuoy-systemd-logs-daemon-set-5cea973cc7904f8c-rjxm6 from sonobuoy started at 2020-08-21 15:39:10 +0000 UTC (2 container statuses recorded)
Aug 21 16:46:11.386: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 21 16:46:11.386: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 21 16:46:11.386: INFO: ibm-master-proxy-static-10.188.240.230 from kube-system started at 2020-08-21 14:24:17 +0000 UTC (2 container statuses recorded)
Aug 21 16:46:11.386: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 21 16:46:11.386: INFO: 	Container pause ready: true, restart count 0
Aug 21 16:46:11.386: INFO: ibmcloud-block-storage-driver-jzb5f from kube-system started at 2020-08-21 14:24:22 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.386: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 21 16:46:11.386: INFO: ibmcloud-block-storage-plugin-68d5c65db9-clx4d from kube-system started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.386: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Aug 21 16:46:11.386: INFO: multus-admission-controller-9x9x2 from openshift-multus started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.386: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 21 16:46:11.386: INFO: ibm-storage-watcher-68df9b45c4-9rvr8 from kube-system started at 2020-08-21 14:25:31 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.387: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 21 16:46:11.387: INFO: node-exporter-jkv5q from openshift-monitoring started at 2020-08-21 14:26:50 +0000 UTC (2 container statuses recorded)
Aug 21 16:46:11.387: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 16:46:11.387: INFO: 	Container node-exporter ready: true, restart count 0
Aug 21 16:46:11.387: INFO: ibm-file-plugin-6c96899f79-5r8lq from kube-system started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.387: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 21 16:46:11.387: INFO: dns-operator-6f9cf66db7-2qjt6 from openshift-dns-operator started at 2020-08-21 14:25:29 +0000 UTC (2 container statuses recorded)
Aug 21 16:46:11.387: INFO: 	Container dns-operator ready: true, restart count 0
Aug 21 16:46:11.387: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 16:46:11.387: INFO: cluster-image-registry-operator-6cfd58b66c-rsxhh from openshift-image-registry started at 2020-08-21 14:25:29 +0000 UTC (2 container statuses recorded)
Aug 21 16:46:11.387: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Aug 21 16:46:11.387: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Aug 21 16:46:11.387: INFO: cluster-node-tuning-operator-b5f884945-f92th from openshift-cluster-node-tuning-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.387: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Aug 21 16:46:11.387: INFO: tuned-bh9bt from openshift-cluster-node-tuning-operator started at 2020-08-21 14:26:30 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.387: INFO: 	Container tuned ready: true, restart count 0
Aug 21 16:46:11.387: INFO: packageserver-75777686bd-6jlwz from openshift-operator-lifecycle-manager started at 2020-08-21 14:31:50 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.388: INFO: 	Container packageserver ready: true, restart count 0
Aug 21 16:46:11.388: INFO: calico-node-rw476 from calico-system started at 2020-08-21 14:25:08 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.388: INFO: 	Container calico-node ready: true, restart count 0
Aug 21 16:46:11.388: INFO: cluster-storage-operator-557b75f8d5-bpz57 from openshift-cluster-storage-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.388: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Aug 21 16:46:11.388: INFO: calico-typha-6c986fbc8c-rw4j9 from calico-system started at 2020-08-21 14:27:04 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.388: INFO: 	Container calico-typha ready: true, restart count 0
Aug 21 16:46:11.388: INFO: sonobuoy from sonobuoy started at 2020-08-21 15:39:03 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.388: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 21 16:46:11.388: INFO: ibm-keepalived-watcher-9tt65 from kube-system started at 2020-08-21 14:24:19 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.388: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 21 16:46:11.388: INFO: downloads-678f5d6564-sxpw2 from openshift-console started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.388: INFO: 	Container download-server ready: true, restart count 0
Aug 21 16:46:11.388: INFO: calico-kube-controllers-79d75767dd-bznnm from calico-system started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.388: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 21 16:46:11.388: INFO: node-ca-7vtw7 from openshift-image-registry started at 2020-08-21 14:27:50 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.388: INFO: 	Container node-ca ready: true, restart count 0
Aug 21 16:46:11.388: INFO: prometheus-adapter-5697b6dddd-hpnk5 from openshift-monitoring started at 2020-08-21 14:34:09 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.388: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 21 16:46:11.389: INFO: console-operator-9878d4766-tfdhf from openshift-console-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.389: INFO: 	Container console-operator ready: true, restart count 1
Aug 21 16:46:11.389: INFO: openshift-service-catalog-controller-manager-operator-5496stt64 from openshift-service-catalog-controller-manager-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 16:46:11.389: INFO: 	Container operator ready: true, restart count 1
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.162d5656529b33e0], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:46:12.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2644" for this suite.
Aug 21 16:46:20.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:46:23.063: INFO: namespace sched-pred-2644 deletion completed in 10.490889719s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:12.472 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:46:23.068: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 21 16:46:23.804: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 21 16:46:25.849: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733625183, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733625183, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733625184, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733625183, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 21 16:46:28.899: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:46:29.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5110" for this suite.
Aug 21 16:46:37.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:46:39.296: INFO: namespace webhook-5110 deletion completed in 10.232842015s
STEP: Destroying namespace "webhook-5110-markers" for this suite.
Aug 21 16:46:47.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:46:49.643: INFO: namespace webhook-5110-markers deletion completed in 10.346773293s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:26.664 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:46:49.732: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Aug 21 16:46:49.985: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 16:47:00.265: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:47:36.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9823" for this suite.
Aug 21 16:47:44.262: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:47:46.533: INFO: namespace crd-publish-openapi-9823 deletion completed in 10.345122512s

• [SLOW TEST:56.802 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:47:46.535: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 21 16:47:50.961: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 21 16:47:50.973: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 21 16:47:52.974: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 21 16:47:52.986: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 21 16:47:54.974: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 21 16:47:54.986: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 21 16:47:56.974: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 21 16:47:56.986: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 21 16:47:58.974: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 21 16:47:58.985: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 21 16:48:00.974: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 21 16:48:00.988: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 21 16:48:02.974: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 21 16:48:03.000: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:48:03.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9174" for this suite.
Aug 21 16:48:25.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:48:27.296: INFO: namespace container-lifecycle-hook-9174 deletion completed in 24.272687471s

• [SLOW TEST:40.761 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:48:27.296: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:48:31.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6865" for this suite.
Aug 21 16:49:23.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:49:26.096: INFO: namespace kubelet-test-6865 deletion completed in 54.39676191s

• [SLOW TEST:58.799 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:49:26.096: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:49:29.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9304" for this suite.
Aug 21 16:49:45.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:49:47.912: INFO: namespace replication-controller-9304 deletion completed in 18.339252473s

• [SLOW TEST:21.816 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:49:47.912: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Aug 21 16:49:48.118: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:49:52.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9612" for this suite.
Aug 21 16:50:00.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:50:02.446: INFO: namespace init-container-9612 deletion completed in 10.256303511s

• [SLOW TEST:14.534 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:50:02.448: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 21 16:50:03.449: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 21 16:50:06.538: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:50:06.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7070" for this suite.
Aug 21 16:50:14.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:50:17.434: INFO: namespace webhook-7070 deletion completed in 10.522461106s
STEP: Destroying namespace "webhook-7070-markers" for this suite.
Aug 21 16:50:27.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:50:29.897: INFO: namespace webhook-7070-markers deletion completed in 12.463100593s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:27.530 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:50:29.981: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Aug 21 16:50:30.168: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:51:21.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-575" for this suite.
Aug 21 16:51:29.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:51:32.062: INFO: namespace crd-publish-openapi-575 deletion completed in 10.253692693s

• [SLOW TEST:62.081 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:51:32.063: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:51:36.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6713" for this suite.
Aug 21 16:51:44.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:51:46.476: INFO: namespace kubelet-test-6713 deletion completed in 10.135223311s

• [SLOW TEST:14.414 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:51:46.477: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Aug 21 16:51:50.462: INFO: Successfully updated pod "annotationupdatee7790b99-e392-40cc-9ad7-bf7a9d7c1d0b"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:51:52.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-633" for this suite.
Aug 21 16:52:08.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:52:10.798: INFO: namespace projected-633 deletion completed in 18.250185984s

• [SLOW TEST:24.321 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:52:10.798: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:52:19.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8301" for this suite.
Aug 21 16:52:29.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:52:31.603: INFO: namespace job-8301 deletion completed in 12.554742602s

• [SLOW TEST:20.804 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:52:31.605: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 16:52:31.776: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:52:38.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5927" for this suite.
Aug 21 16:52:46.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:52:48.557: INFO: namespace custom-resource-definition-5927 deletion completed in 10.220090505s

• [SLOW TEST:16.952 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:52:48.558: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 21 16:52:48.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-9087'
Aug 21 16:52:49.259: INFO: stderr: ""
Aug 21 16:52:49.259: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Aug 21 16:52:49.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 delete pods e2e-test-httpd-pod --namespace=kubectl-9087'
Aug 21 16:52:57.486: INFO: stderr: ""
Aug 21 16:52:57.486: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:52:57.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9087" for this suite.
Aug 21 16:53:05.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:53:07.726: INFO: namespace kubectl-9087 deletion completed in 10.21098831s

• [SLOW TEST:19.168 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:53:07.726: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-dc62242c-af52-4743-8c45-fa9dedebf706
STEP: Creating a pod to test consume configMaps
Aug 21 16:53:08.008: INFO: Waiting up to 5m0s for pod "pod-configmaps-cfefc4b4-2624-40f8-ab11-b201eaa8f024" in namespace "configmap-8728" to be "success or failure"
Aug 21 16:53:08.037: INFO: Pod "pod-configmaps-cfefc4b4-2624-40f8-ab11-b201eaa8f024": Phase="Pending", Reason="", readiness=false. Elapsed: 28.543602ms
Aug 21 16:53:10.049: INFO: Pod "pod-configmaps-cfefc4b4-2624-40f8-ab11-b201eaa8f024": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040446038s
STEP: Saw pod success
Aug 21 16:53:10.049: INFO: Pod "pod-configmaps-cfefc4b4-2624-40f8-ab11-b201eaa8f024" satisfied condition "success or failure"
Aug 21 16:53:10.065: INFO: Trying to get logs from node 10.188.240.202 pod pod-configmaps-cfefc4b4-2624-40f8-ab11-b201eaa8f024 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 16:53:10.225: INFO: Waiting for pod pod-configmaps-cfefc4b4-2624-40f8-ab11-b201eaa8f024 to disappear
Aug 21 16:53:10.240: INFO: Pod pod-configmaps-cfefc4b4-2624-40f8-ab11-b201eaa8f024 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:53:10.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8728" for this suite.
Aug 21 16:53:18.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:53:20.749: INFO: namespace configmap-8728 deletion completed in 10.478348362s

• [SLOW TEST:13.023 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:53:20.749: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 21 16:53:22.010: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 21 16:53:24.088: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733625601, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733625601, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733625601, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733625601, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 21 16:53:27.131: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:53:27.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9238" for this suite.
Aug 21 16:53:43.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:53:45.595: INFO: namespace webhook-9238 deletion completed in 18.193396834s
STEP: Destroying namespace "webhook-9238-markers" for this suite.
Aug 21 16:53:53.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:53:56.027: INFO: namespace webhook-9238-markers deletion completed in 10.431494634s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:35.361 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:53:56.113: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Aug 21 16:53:56.347: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Aug 21 16:53:56.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 create -f - --namespace=kubectl-345'
Aug 21 16:53:56.998: INFO: stderr: ""
Aug 21 16:53:56.998: INFO: stdout: "service/redis-slave created\n"
Aug 21 16:53:56.998: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Aug 21 16:53:56.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 create -f - --namespace=kubectl-345'
Aug 21 16:53:57.684: INFO: stderr: ""
Aug 21 16:53:57.684: INFO: stdout: "service/redis-master created\n"
Aug 21 16:53:57.684: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 21 16:53:57.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 create -f - --namespace=kubectl-345'
Aug 21 16:53:58.342: INFO: stderr: ""
Aug 21 16:53:58.342: INFO: stdout: "service/frontend created\n"
Aug 21 16:53:58.343: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Aug 21 16:53:58.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 create -f - --namespace=kubectl-345'
Aug 21 16:53:58.887: INFO: stderr: ""
Aug 21 16:53:58.887: INFO: stdout: "deployment.apps/frontend created\n"
Aug 21 16:53:58.887: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 21 16:53:58.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 create -f - --namespace=kubectl-345'
Aug 21 16:53:59.294: INFO: stderr: ""
Aug 21 16:53:59.294: INFO: stdout: "deployment.apps/redis-master created\n"
Aug 21 16:53:59.295: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Aug 21 16:53:59.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 create -f - --namespace=kubectl-345'
Aug 21 16:53:59.828: INFO: stderr: ""
Aug 21 16:53:59.828: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Aug 21 16:53:59.828: INFO: Waiting for all frontend pods to be Running.
Aug 21 16:54:19.879: INFO: Waiting for frontend to serve content.
Aug 21 16:54:19.948: INFO: Trying to add a new entry to the guestbook.
Aug 21 16:54:20.008: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Aug 21 16:54:20.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 delete --grace-period=0 --force -f - --namespace=kubectl-345'
Aug 21 16:54:20.267: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 21 16:54:20.267: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Aug 21 16:54:20.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 delete --grace-period=0 --force -f - --namespace=kubectl-345'
Aug 21 16:54:20.515: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 21 16:54:20.515: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 21 16:54:20.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 delete --grace-period=0 --force -f - --namespace=kubectl-345'
Aug 21 16:54:20.719: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 21 16:54:20.719: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 21 16:54:20.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 delete --grace-period=0 --force -f - --namespace=kubectl-345'
Aug 21 16:54:20.914: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 21 16:54:20.914: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 21 16:54:20.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 delete --grace-period=0 --force -f - --namespace=kubectl-345'
Aug 21 16:54:21.079: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 21 16:54:21.079: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 21 16:54:21.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 delete --grace-period=0 --force -f - --namespace=kubectl-345'
Aug 21 16:54:21.281: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 21 16:54:21.281: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:54:21.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-345" for this suite.
Aug 21 16:54:45.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:54:47.636: INFO: namespace kubectl-345 deletion completed in 26.325219081s

• [SLOW TEST:51.523 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:54:47.636: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 16:54:47.931: INFO: (0) /api/v1/nodes/10.188.240.202/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 64.071423ms)
Aug 21 16:54:47.955: INFO: (1) /api/v1/nodes/10.188.240.202/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 23.918493ms)
Aug 21 16:54:47.987: INFO: (2) /api/v1/nodes/10.188.240.202/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 31.419105ms)
Aug 21 16:54:48.020: INFO: (3) /api/v1/nodes/10.188.240.202/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 32.796897ms)
Aug 21 16:54:48.053: INFO: (4) /api/v1/nodes/10.188.240.202/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 33.262144ms)
Aug 21 16:54:48.082: INFO: (5) /api/v1/nodes/10.188.240.202/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 28.885709ms)
Aug 21 16:54:48.112: INFO: (6) /api/v1/nodes/10.188.240.202/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 29.639854ms)
Aug 21 16:54:48.132: INFO: (7) /api/v1/nodes/10.188.240.202/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.690333ms)
Aug 21 16:54:48.155: INFO: (8) /api/v1/nodes/10.188.240.202/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 22.333451ms)
Aug 21 16:54:48.182: INFO: (9) /api/v1/nodes/10.188.240.202/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 27.38571ms)
Aug 21 16:54:48.209: INFO: (10) /api/v1/nodes/10.188.240.202/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 26.646021ms)
Aug 21 16:54:48.238: INFO: (11) /api/v1/nodes/10.188.240.202/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 28.46582ms)
Aug 21 16:54:48.263: INFO: (12) /api/v1/nodes/10.188.240.202/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 25.20588ms)
Aug 21 16:54:48.295: INFO: (13) /api/v1/nodes/10.188.240.202/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 31.849461ms)
Aug 21 16:54:48.324: INFO: (14) /api/v1/nodes/10.188.240.202/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 28.82869ms)
Aug 21 16:54:48.357: INFO: (15) /api/v1/nodes/10.188.240.202/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 33.060176ms)
Aug 21 16:54:48.389: INFO: (16) /api/v1/nodes/10.188.240.202/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 31.831753ms)
Aug 21 16:54:48.414: INFO: (17) /api/v1/nodes/10.188.240.202/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 25.651802ms)
Aug 21 16:54:48.438: INFO: (18) /api/v1/nodes/10.188.240.202/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 23.15206ms)
Aug 21 16:54:48.461: INFO: (19) /api/v1/nodes/10.188.240.202/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 23.324106ms)
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:54:48.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3327" for this suite.
Aug 21 16:54:56.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:54:57.990: INFO: namespace proxy-3327 deletion completed in 9.505632856s

• [SLOW TEST:10.354 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:54:57.992: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 21 16:54:58.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-6229'
Aug 21 16:54:58.435: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 21 16:54:58.435: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Aug 21 16:55:00.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 delete deployment e2e-test-httpd-deployment --namespace=kubectl-6229'
Aug 21 16:55:00.867: INFO: stderr: ""
Aug 21 16:55:00.867: INFO: stdout: "deployment.extensions \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:55:00.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6229" for this suite.
Aug 21 16:55:16.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:55:19.168: INFO: namespace kubectl-6229 deletion completed in 18.274944993s

• [SLOW TEST:21.175 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:55:19.168: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 21 16:55:19.477: INFO: Waiting up to 5m0s for pod "pod-23826225-b55e-4232-9ae7-4ba3677b6a77" in namespace "emptydir-1984" to be "success or failure"
Aug 21 16:55:19.492: INFO: Pod "pod-23826225-b55e-4232-9ae7-4ba3677b6a77": Phase="Pending", Reason="", readiness=false. Elapsed: 15.438138ms
Aug 21 16:55:21.505: INFO: Pod "pod-23826225-b55e-4232-9ae7-4ba3677b6a77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028292214s
Aug 21 16:55:23.520: INFO: Pod "pod-23826225-b55e-4232-9ae7-4ba3677b6a77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043225997s
STEP: Saw pod success
Aug 21 16:55:23.520: INFO: Pod "pod-23826225-b55e-4232-9ae7-4ba3677b6a77" satisfied condition "success or failure"
Aug 21 16:55:23.539: INFO: Trying to get logs from node 10.188.240.230 pod pod-23826225-b55e-4232-9ae7-4ba3677b6a77 container test-container: <nil>
STEP: delete the pod
Aug 21 16:55:24.673: INFO: Waiting for pod pod-23826225-b55e-4232-9ae7-4ba3677b6a77 to disappear
Aug 21 16:55:24.683: INFO: Pod pod-23826225-b55e-4232-9ae7-4ba3677b6a77 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:55:24.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1984" for this suite.
Aug 21 16:55:32.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:55:35.086: INFO: namespace emptydir-1984 deletion completed in 10.3688656s

• [SLOW TEST:15.918 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:55:35.086: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:55:51.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7998" for this suite.
Aug 21 16:56:01.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:56:04.125: INFO: namespace resourcequota-7998 deletion completed in 12.346997818s

• [SLOW TEST:29.039 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:56:04.126: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-a0095eed-5780-485f-9f62-1f0ae25c86ba
STEP: Creating a pod to test consume configMaps
Aug 21 16:56:05.470: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-228987cd-b4eb-40be-a266-f3255faf9931" in namespace "projected-6792" to be "success or failure"
Aug 21 16:56:05.486: INFO: Pod "pod-projected-configmaps-228987cd-b4eb-40be-a266-f3255faf9931": Phase="Pending", Reason="", readiness=false. Elapsed: 16.121385ms
Aug 21 16:56:07.501: INFO: Pod "pod-projected-configmaps-228987cd-b4eb-40be-a266-f3255faf9931": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031200102s
STEP: Saw pod success
Aug 21 16:56:07.501: INFO: Pod "pod-projected-configmaps-228987cd-b4eb-40be-a266-f3255faf9931" satisfied condition "success or failure"
Aug 21 16:56:07.519: INFO: Trying to get logs from node 10.188.240.202 pod pod-projected-configmaps-228987cd-b4eb-40be-a266-f3255faf9931 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 16:56:07.631: INFO: Waiting for pod pod-projected-configmaps-228987cd-b4eb-40be-a266-f3255faf9931 to disappear
Aug 21 16:56:07.654: INFO: Pod pod-projected-configmaps-228987cd-b4eb-40be-a266-f3255faf9931 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:56:07.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6792" for this suite.
Aug 21 16:56:15.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:56:17.936: INFO: namespace projected-6792 deletion completed in 10.243616055s

• [SLOW TEST:13.810 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:56:17.936: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 21 16:56:19.122: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 21 16:56:21.164: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733625779, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733625779, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733625779, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733625779, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 21 16:56:24.242: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:56:24.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3193" for this suite.
Aug 21 16:56:34.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:56:36.723: INFO: namespace webhook-3193 deletion completed in 12.161160716s
STEP: Destroying namespace "webhook-3193-markers" for this suite.
Aug 21 16:56:44.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:56:46.879: INFO: namespace webhook-3193-markers deletion completed in 10.155658109s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:29.023 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:56:46.960: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 21 16:56:47.276: INFO: Waiting up to 5m0s for pod "downwardapi-volume-af8993b4-9d55-440c-8ca1-4ab2d5fd33d4" in namespace "projected-8716" to be "success or failure"
Aug 21 16:56:47.294: INFO: Pod "downwardapi-volume-af8993b4-9d55-440c-8ca1-4ab2d5fd33d4": Phase="Pending", Reason="", readiness=false. Elapsed: 17.269748ms
Aug 21 16:56:49.314: INFO: Pod "downwardapi-volume-af8993b4-9d55-440c-8ca1-4ab2d5fd33d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.038006486s
STEP: Saw pod success
Aug 21 16:56:49.315: INFO: Pod "downwardapi-volume-af8993b4-9d55-440c-8ca1-4ab2d5fd33d4" satisfied condition "success or failure"
Aug 21 16:56:49.328: INFO: Trying to get logs from node 10.188.240.230 pod downwardapi-volume-af8993b4-9d55-440c-8ca1-4ab2d5fd33d4 container client-container: <nil>
STEP: delete the pod
Aug 21 16:56:49.389: INFO: Waiting for pod downwardapi-volume-af8993b4-9d55-440c-8ca1-4ab2d5fd33d4 to disappear
Aug 21 16:56:49.404: INFO: Pod downwardapi-volume-af8993b4-9d55-440c-8ca1-4ab2d5fd33d4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:56:49.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8716" for this suite.
Aug 21 16:56:57.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:56:59.763: INFO: namespace projected-8716 deletion completed in 10.327527861s

• [SLOW TEST:12.804 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:56:59.764: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Aug 21 16:57:00.160: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7948 /api/v1/namespaces/watch-7948/configmaps/e2e-watch-test-label-changed c144e864-5f45-4bb1-988d-2b9c95d46cf5 69658 0 2020-08-21 16:57:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 21 16:57:00.160: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7948 /api/v1/namespaces/watch-7948/configmaps/e2e-watch-test-label-changed c144e864-5f45-4bb1-988d-2b9c95d46cf5 69665 0 2020-08-21 16:57:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug 21 16:57:00.161: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7948 /api/v1/namespaces/watch-7948/configmaps/e2e-watch-test-label-changed c144e864-5f45-4bb1-988d-2b9c95d46cf5 69668 0 2020-08-21 16:57:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Aug 21 16:57:10.309: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7948 /api/v1/namespaces/watch-7948/configmaps/e2e-watch-test-label-changed c144e864-5f45-4bb1-988d-2b9c95d46cf5 69717 0 2020-08-21 16:57:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 21 16:57:10.309: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7948 /api/v1/namespaces/watch-7948/configmaps/e2e-watch-test-label-changed c144e864-5f45-4bb1-988d-2b9c95d46cf5 69718 0 2020-08-21 16:57:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Aug 21 16:57:10.309: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7948 /api/v1/namespaces/watch-7948/configmaps/e2e-watch-test-label-changed c144e864-5f45-4bb1-988d-2b9c95d46cf5 69719 0 2020-08-21 16:57:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:57:10.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7948" for this suite.
Aug 21 16:57:18.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:57:20.619: INFO: namespace watch-7948 deletion completed in 10.274847181s

• [SLOW TEST:20.856 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:57:20.619: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Aug 21 16:57:20.807: INFO: PodSpec: initContainers in spec.initContainers
Aug 21 16:58:07.274: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-4aadb222-6440-4d57-9d6d-0163ce216f1f", GenerateName:"", Namespace:"init-container-8225", SelfLink:"/api/v1/namespaces/init-container-8225/pods/pod-init-4aadb222-6440-4d57-9d6d-0163ce216f1f", UID:"a74496df-66e0-4130-8a98-4f65218de495", ResourceVersion:"70022", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63733625840, loc:(*time.Location)(0x84c02a0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"807531865"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.30.174.127/32", "cni.projectcalico.org/podIPs":"172.30.174.127/32", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.174.127\"\n    ],\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-6l4h9", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc003d5e740), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-6l4h9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00310fef0), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-6l4h9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc003f243c0), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-6l4h9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00310fc70), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0025310a0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.188.240.230", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00579e6c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002531320)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002531340)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0025313fc), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002531400), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733625840, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733625840, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733625840, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733625840, loc:(*time.Location)(0x84c02a0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.188.240.230", PodIP:"172.30.174.127", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.174.127"}}, StartTime:(*v1.Time)(0xc0098483a0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003157490)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003157500)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"cri-o://a060973936fb348f45d7e8b20b35ee354384761986e167fa6fa6970ef9b29f39", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0098483e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0098483c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc002531554)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:58:07.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8225" for this suite.
Aug 21 16:58:23.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:58:25.541: INFO: namespace init-container-8225 deletion completed in 18.236401387s

• [SLOW TEST:64.922 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:58:25.542: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-zrz6
STEP: Creating a pod to test atomic-volume-subpath
Aug 21 16:58:25.874: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-zrz6" in namespace "subpath-7697" to be "success or failure"
Aug 21 16:58:25.885: INFO: Pod "pod-subpath-test-downwardapi-zrz6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.395019ms
Aug 21 16:58:27.898: INFO: Pod "pod-subpath-test-downwardapi-zrz6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023573218s
Aug 21 16:58:29.914: INFO: Pod "pod-subpath-test-downwardapi-zrz6": Phase="Running", Reason="", readiness=true. Elapsed: 4.039706232s
Aug 21 16:58:31.927: INFO: Pod "pod-subpath-test-downwardapi-zrz6": Phase="Running", Reason="", readiness=true. Elapsed: 6.052431038s
Aug 21 16:58:33.939: INFO: Pod "pod-subpath-test-downwardapi-zrz6": Phase="Running", Reason="", readiness=true. Elapsed: 8.064635267s
Aug 21 16:58:35.950: INFO: Pod "pod-subpath-test-downwardapi-zrz6": Phase="Running", Reason="", readiness=true. Elapsed: 10.076060624s
Aug 21 16:58:37.968: INFO: Pod "pod-subpath-test-downwardapi-zrz6": Phase="Running", Reason="", readiness=true. Elapsed: 12.09406959s
Aug 21 16:58:39.982: INFO: Pod "pod-subpath-test-downwardapi-zrz6": Phase="Running", Reason="", readiness=true. Elapsed: 14.1074802s
Aug 21 16:58:41.998: INFO: Pod "pod-subpath-test-downwardapi-zrz6": Phase="Running", Reason="", readiness=true. Elapsed: 16.124070491s
Aug 21 16:58:44.010: INFO: Pod "pod-subpath-test-downwardapi-zrz6": Phase="Running", Reason="", readiness=true. Elapsed: 18.135668576s
Aug 21 16:58:46.027: INFO: Pod "pod-subpath-test-downwardapi-zrz6": Phase="Running", Reason="", readiness=true. Elapsed: 20.152520275s
Aug 21 16:58:48.046: INFO: Pod "pod-subpath-test-downwardapi-zrz6": Phase="Running", Reason="", readiness=true. Elapsed: 22.171872898s
Aug 21 16:58:50.059: INFO: Pod "pod-subpath-test-downwardapi-zrz6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.184384443s
STEP: Saw pod success
Aug 21 16:58:50.059: INFO: Pod "pod-subpath-test-downwardapi-zrz6" satisfied condition "success or failure"
Aug 21 16:58:50.070: INFO: Trying to get logs from node 10.188.240.230 pod pod-subpath-test-downwardapi-zrz6 container test-container-subpath-downwardapi-zrz6: <nil>
STEP: delete the pod
Aug 21 16:58:50.187: INFO: Waiting for pod pod-subpath-test-downwardapi-zrz6 to disappear
Aug 21 16:58:50.198: INFO: Pod pod-subpath-test-downwardapi-zrz6 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-zrz6
Aug 21 16:58:50.198: INFO: Deleting pod "pod-subpath-test-downwardapi-zrz6" in namespace "subpath-7697"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:58:50.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7697" for this suite.
Aug 21 16:58:58.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:59:00.441: INFO: namespace subpath-7697 deletion completed in 10.208335091s

• [SLOW TEST:34.899 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:59:00.442: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3141.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3141.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3141.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3141.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3141.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3141.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 21 16:59:04.893: INFO: DNS probes using dns-3141/dns-test-1ba74f68-9718-4a38-93d8-2a4c8f90a8ea succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:59:05.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3141" for this suite.
Aug 21 16:59:15.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:59:17.340: INFO: namespace dns-3141 deletion completed in 12.305475372s

• [SLOW TEST:16.898 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:59:17.341: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-276a4aea-a36f-4e76-9ebe-eee357d96821
STEP: Creating a pod to test consume secrets
Aug 21 16:59:17.644: INFO: Waiting up to 5m0s for pod "pod-secrets-694e0b51-4d5b-4e45-b236-08dc4c868713" in namespace "secrets-8089" to be "success or failure"
Aug 21 16:59:17.662: INFO: Pod "pod-secrets-694e0b51-4d5b-4e45-b236-08dc4c868713": Phase="Pending", Reason="", readiness=false. Elapsed: 17.367411ms
Aug 21 16:59:19.674: INFO: Pod "pod-secrets-694e0b51-4d5b-4e45-b236-08dc4c868713": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029496256s
STEP: Saw pod success
Aug 21 16:59:19.674: INFO: Pod "pod-secrets-694e0b51-4d5b-4e45-b236-08dc4c868713" satisfied condition "success or failure"
Aug 21 16:59:19.687: INFO: Trying to get logs from node 10.188.240.230 pod pod-secrets-694e0b51-4d5b-4e45-b236-08dc4c868713 container secret-volume-test: <nil>
STEP: delete the pod
Aug 21 16:59:19.756: INFO: Waiting for pod pod-secrets-694e0b51-4d5b-4e45-b236-08dc4c868713 to disappear
Aug 21 16:59:19.767: INFO: Pod pod-secrets-694e0b51-4d5b-4e45-b236-08dc4c868713 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:59:19.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8089" for this suite.
Aug 21 16:59:27.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:59:29.970: INFO: namespace secrets-8089 deletion completed in 10.171904233s

• [SLOW TEST:12.629 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:59:29.972: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:59:34.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5358" for this suite.
Aug 21 16:59:44.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 16:59:46.828: INFO: namespace emptydir-wrapper-5358 deletion completed in 12.239044481s

• [SLOW TEST:16.857 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 16:59:46.829: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-756249cb-768f-4c14-9c13-7b9ca9d15dc7
STEP: Creating a pod to test consume secrets
Aug 21 16:59:48.209: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8db5a46c-5c92-4869-b2b1-a2996278867a" in namespace "projected-269" to be "success or failure"
Aug 21 16:59:48.221: INFO: Pod "pod-projected-secrets-8db5a46c-5c92-4869-b2b1-a2996278867a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.305188ms
Aug 21 16:59:50.231: INFO: Pod "pod-projected-secrets-8db5a46c-5c92-4869-b2b1-a2996278867a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02145457s
Aug 21 16:59:52.242: INFO: Pod "pod-projected-secrets-8db5a46c-5c92-4869-b2b1-a2996278867a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033041882s
STEP: Saw pod success
Aug 21 16:59:52.242: INFO: Pod "pod-projected-secrets-8db5a46c-5c92-4869-b2b1-a2996278867a" satisfied condition "success or failure"
Aug 21 16:59:52.253: INFO: Trying to get logs from node 10.188.240.230 pod pod-projected-secrets-8db5a46c-5c92-4869-b2b1-a2996278867a container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 21 16:59:52.327: INFO: Waiting for pod pod-projected-secrets-8db5a46c-5c92-4869-b2b1-a2996278867a to disappear
Aug 21 16:59:52.341: INFO: Pod pod-projected-secrets-8db5a46c-5c92-4869-b2b1-a2996278867a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 16:59:52.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-269" for this suite.
Aug 21 17:00:00.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:00:02.596: INFO: namespace projected-269 deletion completed in 10.224329842s

• [SLOW TEST:15.768 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:00:02.598: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Aug 21 17:00:02.857: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8698 /api/v1/namespaces/watch-8698/configmaps/e2e-watch-test-watch-closed f110a6dc-754d-4faf-ae2d-eb78991074f3 70898 0 2020-08-21 17:00:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 21 17:00:02.857: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8698 /api/v1/namespaces/watch-8698/configmaps/e2e-watch-test-watch-closed f110a6dc-754d-4faf-ae2d-eb78991074f3 70901 0 2020-08-21 17:00:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Aug 21 17:00:02.914: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8698 /api/v1/namespaces/watch-8698/configmaps/e2e-watch-test-watch-closed f110a6dc-754d-4faf-ae2d-eb78991074f3 70903 0 2020-08-21 17:00:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 21 17:00:02.915: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8698 /api/v1/namespaces/watch-8698/configmaps/e2e-watch-test-watch-closed f110a6dc-754d-4faf-ae2d-eb78991074f3 70904 0 2020-08-21 17:00:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:00:02.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8698" for this suite.
Aug 21 17:00:13.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:00:15.374: INFO: namespace watch-8698 deletion completed in 12.416394041s

• [SLOW TEST:12.777 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:00:15.376: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 21 17:00:16.237: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 21 17:00:18.315: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733626016, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733626016, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733626016, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733626016, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 21 17:00:21.365: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 17:00:21.380: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2435-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:00:22.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7772" for this suite.
Aug 21 17:00:32.854: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:00:35.026: INFO: namespace webhook-7772 deletion completed in 12.234827408s
STEP: Destroying namespace "webhook-7772-markers" for this suite.
Aug 21 17:00:43.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:00:45.347: INFO: namespace webhook-7772-markers deletion completed in 10.321423273s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:30.050 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:00:45.426: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:01:45.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7221" for this suite.
Aug 21 17:02:01.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:02:04.168: INFO: namespace container-probe-7221 deletion completed in 18.462707792s

• [SLOW TEST:78.743 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:02:04.170: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Aug 21 17:02:05.465: INFO: Waiting up to 5m0s for pod "client-containers-a5936e0e-7b6b-4ad0-80bf-8d5ab5adc03d" in namespace "containers-5270" to be "success or failure"
Aug 21 17:02:05.479: INFO: Pod "client-containers-a5936e0e-7b6b-4ad0-80bf-8d5ab5adc03d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.731521ms
Aug 21 17:02:07.492: INFO: Pod "client-containers-a5936e0e-7b6b-4ad0-80bf-8d5ab5adc03d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02681498s
Aug 21 17:02:09.505: INFO: Pod "client-containers-a5936e0e-7b6b-4ad0-80bf-8d5ab5adc03d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04012685s
STEP: Saw pod success
Aug 21 17:02:09.505: INFO: Pod "client-containers-a5936e0e-7b6b-4ad0-80bf-8d5ab5adc03d" satisfied condition "success or failure"
Aug 21 17:02:09.520: INFO: Trying to get logs from node 10.188.240.202 pod client-containers-a5936e0e-7b6b-4ad0-80bf-8d5ab5adc03d container test-container: <nil>
STEP: delete the pod
Aug 21 17:02:09.658: INFO: Waiting for pod client-containers-a5936e0e-7b6b-4ad0-80bf-8d5ab5adc03d to disappear
Aug 21 17:02:09.677: INFO: Pod client-containers-a5936e0e-7b6b-4ad0-80bf-8d5ab5adc03d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:02:09.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5270" for this suite.
Aug 21 17:02:17.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:02:19.966: INFO: namespace containers-5270 deletion completed in 10.242267891s

• [SLOW TEST:15.797 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:02:19.968: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 21 17:02:20.191: INFO: Waiting up to 5m0s for pod "pod-d594b04e-8afc-4025-ad70-cded16c562b9" in namespace "emptydir-2894" to be "success or failure"
Aug 21 17:02:20.202: INFO: Pod "pod-d594b04e-8afc-4025-ad70-cded16c562b9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.666202ms
Aug 21 17:02:22.229: INFO: Pod "pod-d594b04e-8afc-4025-ad70-cded16c562b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037384221s
Aug 21 17:02:24.241: INFO: Pod "pod-d594b04e-8afc-4025-ad70-cded16c562b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049070442s
STEP: Saw pod success
Aug 21 17:02:24.241: INFO: Pod "pod-d594b04e-8afc-4025-ad70-cded16c562b9" satisfied condition "success or failure"
Aug 21 17:02:24.253: INFO: Trying to get logs from node 10.188.240.230 pod pod-d594b04e-8afc-4025-ad70-cded16c562b9 container test-container: <nil>
STEP: delete the pod
Aug 21 17:02:24.336: INFO: Waiting for pod pod-d594b04e-8afc-4025-ad70-cded16c562b9 to disappear
Aug 21 17:02:24.349: INFO: Pod pod-d594b04e-8afc-4025-ad70-cded16c562b9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:02:24.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2894" for this suite.
Aug 21 17:02:32.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:02:34.768: INFO: namespace emptydir-2894 deletion completed in 10.37256598s

• [SLOW TEST:14.801 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:02:34.769: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:03:01.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7379" for this suite.
Aug 21 17:03:09.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:03:11.342: INFO: namespace container-runtime-7379 deletion completed in 10.302742289s

• [SLOW TEST:36.573 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:03:11.343: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 17:03:11.577: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-f4992cc8-4e61-455b-88f5-e1d58f6a962d
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-f4992cc8-4e61-455b-88f5-e1d58f6a962d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:03:15.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-267" for this suite.
Aug 21 17:03:31.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:03:34.040: INFO: namespace configmap-267 deletion completed in 18.160881435s

• [SLOW TEST:22.697 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:03:34.041: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 21 17:03:34.325: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c1ee0277-4261-4daa-a2f7-f81a2dc78d0b" in namespace "projected-9923" to be "success or failure"
Aug 21 17:03:34.339: INFO: Pod "downwardapi-volume-c1ee0277-4261-4daa-a2f7-f81a2dc78d0b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.706037ms
Aug 21 17:03:36.353: INFO: Pod "downwardapi-volume-c1ee0277-4261-4daa-a2f7-f81a2dc78d0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027571505s
STEP: Saw pod success
Aug 21 17:03:36.353: INFO: Pod "downwardapi-volume-c1ee0277-4261-4daa-a2f7-f81a2dc78d0b" satisfied condition "success or failure"
Aug 21 17:03:36.365: INFO: Trying to get logs from node 10.188.240.202 pod downwardapi-volume-c1ee0277-4261-4daa-a2f7-f81a2dc78d0b container client-container: <nil>
STEP: delete the pod
Aug 21 17:03:36.443: INFO: Waiting for pod downwardapi-volume-c1ee0277-4261-4daa-a2f7-f81a2dc78d0b to disappear
Aug 21 17:03:36.455: INFO: Pod downwardapi-volume-c1ee0277-4261-4daa-a2f7-f81a2dc78d0b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:03:36.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9923" for this suite.
Aug 21 17:03:44.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:03:46.904: INFO: namespace projected-9923 deletion completed in 10.420389211s

• [SLOW TEST:12.863 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:03:46.904: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 21 17:03:48.211: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e593a708-69b9-4cd2-b5d6-d09667225da2" in namespace "downward-api-7459" to be "success or failure"
Aug 21 17:03:48.222: INFO: Pod "downwardapi-volume-e593a708-69b9-4cd2-b5d6-d09667225da2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.740983ms
Aug 21 17:03:50.236: INFO: Pod "downwardapi-volume-e593a708-69b9-4cd2-b5d6-d09667225da2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024509901s
STEP: Saw pod success
Aug 21 17:03:50.236: INFO: Pod "downwardapi-volume-e593a708-69b9-4cd2-b5d6-d09667225da2" satisfied condition "success or failure"
Aug 21 17:03:50.252: INFO: Trying to get logs from node 10.188.240.202 pod downwardapi-volume-e593a708-69b9-4cd2-b5d6-d09667225da2 container client-container: <nil>
STEP: delete the pod
Aug 21 17:03:50.330: INFO: Waiting for pod downwardapi-volume-e593a708-69b9-4cd2-b5d6-d09667225da2 to disappear
Aug 21 17:03:50.342: INFO: Pod downwardapi-volume-e593a708-69b9-4cd2-b5d6-d09667225da2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:03:50.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7459" for this suite.
Aug 21 17:03:58.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:04:00.665: INFO: namespace downward-api-7459 deletion completed in 10.284374559s

• [SLOW TEST:13.761 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:04:00.666: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Aug 21 17:04:00.867: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 17:04:11.611: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:04:47.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3661" for this suite.
Aug 21 17:04:55.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:04:57.674: INFO: namespace crd-publish-openapi-3661 deletion completed in 10.171288214s

• [SLOW TEST:57.009 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:04:57.675: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-2abd9e0a-d560-4748-95cb-4194c67621c7 in namespace container-probe-5203
Aug 21 17:04:59.967: INFO: Started pod test-webserver-2abd9e0a-d560-4748-95cb-4194c67621c7 in namespace container-probe-5203
STEP: checking the pod's current state and verifying that restartCount is present
Aug 21 17:04:59.977: INFO: Initial restart count of pod test-webserver-2abd9e0a-d560-4748-95cb-4194c67621c7 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:09:02.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5203" for this suite.
Aug 21 17:09:10.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:09:12.274: INFO: namespace container-probe-5203 deletion completed in 10.24540152s

• [SLOW TEST:254.599 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:09:12.274: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 17:09:12.615: INFO: Create a RollingUpdate DaemonSet
Aug 21 17:09:12.635: INFO: Check that daemon pods launch on every node of the cluster
Aug 21 17:09:12.687: INFO: Number of nodes with available pods: 0
Aug 21 17:09:12.687: INFO: Node 10.188.240.202 is running more than one daemon pod
Aug 21 17:09:13.733: INFO: Number of nodes with available pods: 0
Aug 21 17:09:13.734: INFO: Node 10.188.240.202 is running more than one daemon pod
Aug 21 17:09:14.720: INFO: Number of nodes with available pods: 2
Aug 21 17:09:14.720: INFO: Node 10.188.240.230 is running more than one daemon pod
Aug 21 17:09:15.715: INFO: Number of nodes with available pods: 3
Aug 21 17:09:15.715: INFO: Number of running nodes: 3, number of available pods: 3
Aug 21 17:09:15.715: INFO: Update the DaemonSet to trigger a rollout
Aug 21 17:09:15.821: INFO: Updating DaemonSet daemon-set
Aug 21 17:09:21.885: INFO: Roll back the DaemonSet before rollout is complete
Aug 21 17:09:21.917: INFO: Updating DaemonSet daemon-set
Aug 21 17:09:21.917: INFO: Make sure DaemonSet rollback is complete
Aug 21 17:09:21.928: INFO: Wrong image for pod: daemon-set-kqx9h. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 21 17:09:21.928: INFO: Pod daemon-set-kqx9h is not available
Aug 21 17:09:22.960: INFO: Wrong image for pod: daemon-set-kqx9h. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 21 17:09:22.960: INFO: Pod daemon-set-kqx9h is not available
Aug 21 17:09:23.958: INFO: Pod daemon-set-v74s9 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1821, will wait for the garbage collector to delete the pods
Aug 21 17:09:24.106: INFO: Deleting DaemonSet.extensions daemon-set took: 35.527164ms
Aug 21 17:09:24.706: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.285004ms
Aug 21 17:09:37.522: INFO: Number of nodes with available pods: 0
Aug 21 17:09:37.522: INFO: Number of running nodes: 0, number of available pods: 0
Aug 21 17:09:37.536: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1821/daemonsets","resourceVersion":"74167"},"items":null}

Aug 21 17:09:37.551: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1821/pods","resourceVersion":"74167"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:09:37.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1821" for this suite.
Aug 21 17:09:47.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:09:49.873: INFO: namespace daemonsets-1821 deletion completed in 12.229313826s

• [SLOW TEST:37.598 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:09:49.873: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 21 17:09:51.153: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7948362c-9552-4030-aff2-83b93fb35e7f" in namespace "downward-api-6948" to be "success or failure"
Aug 21 17:09:51.166: INFO: Pod "downwardapi-volume-7948362c-9552-4030-aff2-83b93fb35e7f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.379639ms
Aug 21 17:09:53.180: INFO: Pod "downwardapi-volume-7948362c-9552-4030-aff2-83b93fb35e7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027370161s
STEP: Saw pod success
Aug 21 17:09:53.180: INFO: Pod "downwardapi-volume-7948362c-9552-4030-aff2-83b93fb35e7f" satisfied condition "success or failure"
Aug 21 17:09:53.192: INFO: Trying to get logs from node 10.188.240.202 pod downwardapi-volume-7948362c-9552-4030-aff2-83b93fb35e7f container client-container: <nil>
STEP: delete the pod
Aug 21 17:09:53.310: INFO: Waiting for pod downwardapi-volume-7948362c-9552-4030-aff2-83b93fb35e7f to disappear
Aug 21 17:09:53.327: INFO: Pod downwardapi-volume-7948362c-9552-4030-aff2-83b93fb35e7f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:09:53.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6948" for this suite.
Aug 21 17:10:01.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:10:03.528: INFO: namespace downward-api-6948 deletion completed in 10.17677315s

• [SLOW TEST:13.655 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:10:03.529: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 21 17:10:11.888: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 21 17:10:11.902: INFO: Pod pod-with-prestop-http-hook still exists
Aug 21 17:10:13.902: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 21 17:10:13.925: INFO: Pod pod-with-prestop-http-hook still exists
Aug 21 17:10:15.902: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 21 17:10:15.916: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:10:15.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-719" for this suite.
Aug 21 17:10:32.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:10:34.262: INFO: namespace container-lifecycle-hook-719 deletion completed in 18.250893406s

• [SLOW TEST:30.734 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:10:34.263: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Aug 21 17:10:39.620: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-192619208 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Aug 21 17:10:54.830: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:10:54.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3287" for this suite.
Aug 21 17:11:02.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:11:05.082: INFO: namespace pods-3287 deletion completed in 10.219718796s

• [SLOW TEST:30.820 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:11:05.084: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-801288b1-dbb8-44bc-a2a1-4e2f0440ae5a
STEP: Creating a pod to test consume secrets
Aug 21 17:11:05.374: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3a464665-0500-413a-b44b-68d55bf69d9f" in namespace "projected-6518" to be "success or failure"
Aug 21 17:11:05.419: INFO: Pod "pod-projected-secrets-3a464665-0500-413a-b44b-68d55bf69d9f": Phase="Pending", Reason="", readiness=false. Elapsed: 44.76509ms
Aug 21 17:11:07.431: INFO: Pod "pod-projected-secrets-3a464665-0500-413a-b44b-68d55bf69d9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.056427052s
STEP: Saw pod success
Aug 21 17:11:07.431: INFO: Pod "pod-projected-secrets-3a464665-0500-413a-b44b-68d55bf69d9f" satisfied condition "success or failure"
Aug 21 17:11:07.442: INFO: Trying to get logs from node 10.188.240.202 pod pod-projected-secrets-3a464665-0500-413a-b44b-68d55bf69d9f container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 21 17:11:07.514: INFO: Waiting for pod pod-projected-secrets-3a464665-0500-413a-b44b-68d55bf69d9f to disappear
Aug 21 17:11:07.525: INFO: Pod pod-projected-secrets-3a464665-0500-413a-b44b-68d55bf69d9f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:11:07.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6518" for this suite.
Aug 21 17:11:15.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:11:17.766: INFO: namespace projected-6518 deletion completed in 10.221598285s

• [SLOW TEST:12.683 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:11:17.767: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 17:11:18.025: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 21 17:11:23.041: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 21 17:11:23.041: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Aug 21 17:11:27.178: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1438 /apis/apps/v1/namespaces/deployment-1438/deployments/test-cleanup-deployment 027dde49-5a63-4617-9129-a86b1155952f 75025 1 2020-08-21 17:11:23 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003e0fa68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-08-21 17:11:23 +0000 UTC,LastTransitionTime:2020-08-21 17:11:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-65db99849b" has successfully progressed.,LastUpdateTime:2020-08-21 17:11:25 +0000 UTC,LastTransitionTime:2020-08-21 17:11:23 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 21 17:11:27.191: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-1438 /apis/apps/v1/namespaces/deployment-1438/replicasets/test-cleanup-deployment-65db99849b 3006bf81-2ba8-4cb5-9226-0b75514da3b3 75014 1 2020-08-21 17:11:23 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 027dde49-5a63-4617-9129-a86b1155952f 0xc003e0fe87 0xc003e0fe88}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003e0fee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 21 17:11:27.204: INFO: Pod "test-cleanup-deployment-65db99849b-r9h2n" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-65db99849b-r9h2n test-cleanup-deployment-65db99849b- deployment-1438 /api/v1/namespaces/deployment-1438/pods/test-cleanup-deployment-65db99849b-r9h2n 12e2bf30-e515-487a-9927-f44125db7cf6 75013 0 2020-08-21 17:11:23 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[cni.projectcalico.org/podIP:172.30.174.101/32 cni.projectcalico.org/podIPs:172.30.174.101/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.174.101"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-cleanup-deployment-65db99849b 3006bf81-2ba8-4cb5-9226-0b75514da3b3 0xc00275c407 0xc00275c408}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-55jmq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-55jmq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-55jmq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.230,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rtshj,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 17:11:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 17:11:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 17:11:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 17:11:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.230,PodIP:172.30.174.101,StartTime:2020-08-21 17:11:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-21 17:11:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://e48375c1c22c40ec951f582f7caa58d0efc924694f64b49e235eb367f05e7c7b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.174.101,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:11:27.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1438" for this suite.
Aug 21 17:11:35.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:11:37.347: INFO: namespace deployment-1438 deletion completed in 10.118997812s

• [SLOW TEST:19.581 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:11:37.348: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Aug 21 17:11:37.534: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:12:28.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5538" for this suite.
Aug 21 17:12:36.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:12:38.756: INFO: namespace crd-publish-openapi-5538 deletion completed in 10.123926123s

• [SLOW TEST:61.408 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:12:38.758: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-1229
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1229 to expose endpoints map[]
Aug 21 17:12:39.068: INFO: successfully validated that service multi-endpoint-test in namespace services-1229 exposes endpoints map[] (17.766491ms elapsed)
STEP: Creating pod pod1 in namespace services-1229
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1229 to expose endpoints map[pod1:[100]]
Aug 21 17:12:42.246: INFO: successfully validated that service multi-endpoint-test in namespace services-1229 exposes endpoints map[pod1:[100]] (3.111263241s elapsed)
STEP: Creating pod pod2 in namespace services-1229
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1229 to expose endpoints map[pod1:[100] pod2:[101]]
Aug 21 17:12:44.400: INFO: successfully validated that service multi-endpoint-test in namespace services-1229 exposes endpoints map[pod1:[100] pod2:[101]] (2.115782214s elapsed)
STEP: Deleting pod pod1 in namespace services-1229
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1229 to expose endpoints map[pod2:[101]]
Aug 21 17:12:45.483: INFO: successfully validated that service multi-endpoint-test in namespace services-1229 exposes endpoints map[pod2:[101]] (1.058420055s elapsed)
STEP: Deleting pod pod2 in namespace services-1229
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1229 to expose endpoints map[]
Aug 21 17:12:46.540: INFO: successfully validated that service multi-endpoint-test in namespace services-1229 exposes endpoints map[] (1.036049935s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:12:46.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1229" for this suite.
Aug 21 17:13:02.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:13:04.883: INFO: namespace services-1229 deletion completed in 18.226166062s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:26.125 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:13:04.883: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Aug 21 17:13:06.237: INFO: Waiting up to 5m0s for pod "downward-api-437191fc-4e4a-45bf-89e2-8a4e5982923c" in namespace "downward-api-9558" to be "success or failure"
Aug 21 17:13:06.249: INFO: Pod "downward-api-437191fc-4e4a-45bf-89e2-8a4e5982923c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.608736ms
Aug 21 17:13:08.269: INFO: Pod "downward-api-437191fc-4e4a-45bf-89e2-8a4e5982923c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03119775s
Aug 21 17:13:10.284: INFO: Pod "downward-api-437191fc-4e4a-45bf-89e2-8a4e5982923c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046203894s
Aug 21 17:13:12.296: INFO: Pod "downward-api-437191fc-4e4a-45bf-89e2-8a4e5982923c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.058618471s
STEP: Saw pod success
Aug 21 17:13:12.296: INFO: Pod "downward-api-437191fc-4e4a-45bf-89e2-8a4e5982923c" satisfied condition "success or failure"
Aug 21 17:13:12.309: INFO: Trying to get logs from node 10.188.240.222 pod downward-api-437191fc-4e4a-45bf-89e2-8a4e5982923c container dapi-container: <nil>
STEP: delete the pod
Aug 21 17:13:12.417: INFO: Waiting for pod downward-api-437191fc-4e4a-45bf-89e2-8a4e5982923c to disappear
Aug 21 17:13:12.427: INFO: Pod downward-api-437191fc-4e4a-45bf-89e2-8a4e5982923c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:13:12.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9558" for this suite.
Aug 21 17:13:20.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:13:23.135: INFO: namespace downward-api-9558 deletion completed in 10.679082062s

• [SLOW TEST:18.251 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:13:23.135: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-5064/configmap-test-f55a3e5a-f573-4fa1-bb49-394a23e31047
STEP: Creating a pod to test consume configMaps
Aug 21 17:13:24.404: INFO: Waiting up to 5m0s for pod "pod-configmaps-1855cb7c-69b5-4fe7-94b8-886842450ff1" in namespace "configmap-5064" to be "success or failure"
Aug 21 17:13:24.416: INFO: Pod "pod-configmaps-1855cb7c-69b5-4fe7-94b8-886842450ff1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.623115ms
Aug 21 17:13:26.430: INFO: Pod "pod-configmaps-1855cb7c-69b5-4fe7-94b8-886842450ff1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025575619s
Aug 21 17:13:28.441: INFO: Pod "pod-configmaps-1855cb7c-69b5-4fe7-94b8-886842450ff1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036409144s
STEP: Saw pod success
Aug 21 17:13:28.441: INFO: Pod "pod-configmaps-1855cb7c-69b5-4fe7-94b8-886842450ff1" satisfied condition "success or failure"
Aug 21 17:13:28.453: INFO: Trying to get logs from node 10.188.240.230 pod pod-configmaps-1855cb7c-69b5-4fe7-94b8-886842450ff1 container env-test: <nil>
STEP: delete the pod
Aug 21 17:13:28.579: INFO: Waiting for pod pod-configmaps-1855cb7c-69b5-4fe7-94b8-886842450ff1 to disappear
Aug 21 17:13:28.590: INFO: Pod pod-configmaps-1855cb7c-69b5-4fe7-94b8-886842450ff1 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:13:28.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5064" for this suite.
Aug 21 17:13:36.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:13:38.882: INFO: namespace configmap-5064 deletion completed in 10.258881651s

• [SLOW TEST:15.748 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:13:38.884: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Aug 21 17:13:39.253: INFO: Waiting up to 5m0s for pod "pod-05031d44-c556-477e-9d64-79bd6f843de8" in namespace "emptydir-1305" to be "success or failure"
Aug 21 17:13:39.265: INFO: Pod "pod-05031d44-c556-477e-9d64-79bd6f843de8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.221895ms
Aug 21 17:13:41.276: INFO: Pod "pod-05031d44-c556-477e-9d64-79bd6f843de8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023116645s
Aug 21 17:13:43.307: INFO: Pod "pod-05031d44-c556-477e-9d64-79bd6f843de8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053732225s
STEP: Saw pod success
Aug 21 17:13:43.307: INFO: Pod "pod-05031d44-c556-477e-9d64-79bd6f843de8" satisfied condition "success or failure"
Aug 21 17:13:43.319: INFO: Trying to get logs from node 10.188.240.202 pod pod-05031d44-c556-477e-9d64-79bd6f843de8 container test-container: <nil>
STEP: delete the pod
Aug 21 17:13:43.436: INFO: Waiting for pod pod-05031d44-c556-477e-9d64-79bd6f843de8 to disappear
Aug 21 17:13:43.449: INFO: Pod pod-05031d44-c556-477e-9d64-79bd6f843de8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:13:43.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1305" for this suite.
Aug 21 17:13:51.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:13:53.644: INFO: namespace emptydir-1305 deletion completed in 10.166799481s

• [SLOW TEST:14.760 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:13:53.644: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-de382d7f-b416-4000-be6e-e9cfe05f026a
STEP: Creating a pod to test consume secrets
Aug 21 17:13:53.946: INFO: Waiting up to 5m0s for pod "pod-secrets-7771af89-4f88-48fe-ba46-6860402ccd08" in namespace "secrets-9096" to be "success or failure"
Aug 21 17:13:53.959: INFO: Pod "pod-secrets-7771af89-4f88-48fe-ba46-6860402ccd08": Phase="Pending", Reason="", readiness=false. Elapsed: 13.133503ms
Aug 21 17:13:55.987: INFO: Pod "pod-secrets-7771af89-4f88-48fe-ba46-6860402ccd08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.04154475s
STEP: Saw pod success
Aug 21 17:13:55.988: INFO: Pod "pod-secrets-7771af89-4f88-48fe-ba46-6860402ccd08" satisfied condition "success or failure"
Aug 21 17:13:56.001: INFO: Trying to get logs from node 10.188.240.230 pod pod-secrets-7771af89-4f88-48fe-ba46-6860402ccd08 container secret-volume-test: <nil>
STEP: delete the pod
Aug 21 17:13:56.109: INFO: Waiting for pod pod-secrets-7771af89-4f88-48fe-ba46-6860402ccd08 to disappear
Aug 21 17:13:56.120: INFO: Pod pod-secrets-7771af89-4f88-48fe-ba46-6860402ccd08 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:13:56.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9096" for this suite.
Aug 21 17:14:04.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:14:06.358: INFO: namespace secrets-9096 deletion completed in 10.204524232s

• [SLOW TEST:12.714 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:14:06.359: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-tvgz
STEP: Creating a pod to test atomic-volume-subpath
Aug 21 17:14:06.769: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tvgz" in namespace "subpath-1201" to be "success or failure"
Aug 21 17:14:06.782: INFO: Pod "pod-subpath-test-configmap-tvgz": Phase="Pending", Reason="", readiness=false. Elapsed: 12.658359ms
Aug 21 17:14:08.795: INFO: Pod "pod-subpath-test-configmap-tvgz": Phase="Running", Reason="", readiness=true. Elapsed: 2.02579764s
Aug 21 17:14:10.809: INFO: Pod "pod-subpath-test-configmap-tvgz": Phase="Running", Reason="", readiness=true. Elapsed: 4.039084626s
Aug 21 17:14:12.820: INFO: Pod "pod-subpath-test-configmap-tvgz": Phase="Running", Reason="", readiness=true. Elapsed: 6.050375982s
Aug 21 17:14:14.833: INFO: Pod "pod-subpath-test-configmap-tvgz": Phase="Running", Reason="", readiness=true. Elapsed: 8.063128944s
Aug 21 17:14:16.851: INFO: Pod "pod-subpath-test-configmap-tvgz": Phase="Running", Reason="", readiness=true. Elapsed: 10.081294158s
Aug 21 17:14:18.868: INFO: Pod "pod-subpath-test-configmap-tvgz": Phase="Running", Reason="", readiness=true. Elapsed: 12.098461122s
Aug 21 17:14:20.880: INFO: Pod "pod-subpath-test-configmap-tvgz": Phase="Running", Reason="", readiness=true. Elapsed: 14.110175043s
Aug 21 17:14:22.891: INFO: Pod "pod-subpath-test-configmap-tvgz": Phase="Running", Reason="", readiness=true. Elapsed: 16.121430129s
Aug 21 17:14:24.905: INFO: Pod "pod-subpath-test-configmap-tvgz": Phase="Running", Reason="", readiness=true. Elapsed: 18.135109818s
Aug 21 17:14:26.920: INFO: Pod "pod-subpath-test-configmap-tvgz": Phase="Running", Reason="", readiness=true. Elapsed: 20.150256724s
Aug 21 17:14:28.935: INFO: Pod "pod-subpath-test-configmap-tvgz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.16576106s
STEP: Saw pod success
Aug 21 17:14:28.935: INFO: Pod "pod-subpath-test-configmap-tvgz" satisfied condition "success or failure"
Aug 21 17:14:28.947: INFO: Trying to get logs from node 10.188.240.202 pod pod-subpath-test-configmap-tvgz container test-container-subpath-configmap-tvgz: <nil>
STEP: delete the pod
Aug 21 17:14:29.026: INFO: Waiting for pod pod-subpath-test-configmap-tvgz to disappear
Aug 21 17:14:29.037: INFO: Pod pod-subpath-test-configmap-tvgz no longer exists
STEP: Deleting pod pod-subpath-test-configmap-tvgz
Aug 21 17:14:29.037: INFO: Deleting pod "pod-subpath-test-configmap-tvgz" in namespace "subpath-1201"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:14:29.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1201" for this suite.
Aug 21 17:14:39.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:14:41.374: INFO: namespace subpath-1201 deletion completed in 12.288032423s

• [SLOW TEST:35.015 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:14:41.374: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 21 17:14:42.257: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 21 17:14:44.312: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733626882, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733626882, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733626882, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733626882, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 21 17:14:47.365: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:14:57.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6551" for this suite.
Aug 21 17:15:05.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:15:08.164: INFO: namespace webhook-6551 deletion completed in 10.234926235s
STEP: Destroying namespace "webhook-6551-markers" for this suite.
Aug 21 17:15:18.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:15:20.449: INFO: namespace webhook-6551-markers deletion completed in 12.285196868s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:39.174 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:15:20.549: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 21 17:15:21.872: INFO: Waiting up to 5m0s for pod "pod-1dd2bab3-f08d-4266-be30-3fda1674d7ab" in namespace "emptydir-353" to be "success or failure"
Aug 21 17:15:21.893: INFO: Pod "pod-1dd2bab3-f08d-4266-be30-3fda1674d7ab": Phase="Pending", Reason="", readiness=false. Elapsed: 21.571555ms
Aug 21 17:15:23.909: INFO: Pod "pod-1dd2bab3-f08d-4266-be30-3fda1674d7ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.037489225s
STEP: Saw pod success
Aug 21 17:15:23.909: INFO: Pod "pod-1dd2bab3-f08d-4266-be30-3fda1674d7ab" satisfied condition "success or failure"
Aug 21 17:15:23.922: INFO: Trying to get logs from node 10.188.240.202 pod pod-1dd2bab3-f08d-4266-be30-3fda1674d7ab container test-container: <nil>
STEP: delete the pod
Aug 21 17:15:24.017: INFO: Waiting for pod pod-1dd2bab3-f08d-4266-be30-3fda1674d7ab to disappear
Aug 21 17:15:24.029: INFO: Pod pod-1dd2bab3-f08d-4266-be30-3fda1674d7ab no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:15:24.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-353" for this suite.
Aug 21 17:15:34.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:15:36.222: INFO: namespace emptydir-353 deletion completed in 12.154530826s

• [SLOW TEST:15.674 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:15:36.223: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 17:15:36.441: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:15:39.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-364" for this suite.
Aug 21 17:16:21.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:16:23.844: INFO: namespace pods-364 deletion completed in 44.14734199s

• [SLOW TEST:47.621 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:16:23.844: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:16:59.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8548" for this suite.
Aug 21 17:17:07.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:17:09.862: INFO: namespace namespaces-8548 deletion completed in 10.227504867s
STEP: Destroying namespace "nsdeletetest-735" for this suite.
Aug 21 17:17:09.877: INFO: Namespace nsdeletetest-735 was already deleted
STEP: Destroying namespace "nsdeletetest-645" for this suite.
Aug 21 17:17:17.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:17:20.104: INFO: namespace nsdeletetest-645 deletion completed in 10.22650557s

• [SLOW TEST:56.260 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:17:20.105: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 17:17:20.284: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:17:24.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6625" for this suite.
Aug 21 17:18:10.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:18:12.746: INFO: namespace pods-6625 deletion completed in 48.103713994s

• [SLOW TEST:52.641 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:18:12.751: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9278
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-9278
I0821 17:18:13.045501      24 runners.go:184] Created replication controller with name: externalname-service, namespace: services-9278, replica count: 2
I0821 17:18:16.095997      24 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 21 17:18:16.096: INFO: Creating new exec pod
Aug 21 17:18:21.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=services-9278 execpodhd8rk -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Aug 21 17:18:21.904: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 21 17:18:21.904: INFO: stdout: ""
Aug 21 17:18:21.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=services-9278 execpodhd8rk -- /bin/sh -x -c nc -zv -t -w 2 172.21.209.165 80'
Aug 21 17:18:22.307: INFO: stderr: "+ nc -zv -t -w 2 172.21.209.165 80\nConnection to 172.21.209.165 80 port [tcp/http] succeeded!\n"
Aug 21 17:18:22.307: INFO: stdout: ""
Aug 21 17:18:22.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=services-9278 execpodhd8rk -- /bin/sh -x -c nc -zv -t -w 2 10.188.240.202 31639'
Aug 21 17:18:22.748: INFO: stderr: "+ nc -zv -t -w 2 10.188.240.202 31639\nConnection to 10.188.240.202 31639 port [tcp/31639] succeeded!\n"
Aug 21 17:18:22.748: INFO: stdout: ""
Aug 21 17:18:22.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=services-9278 execpodhd8rk -- /bin/sh -x -c nc -zv -t -w 2 10.188.240.222 31639'
Aug 21 17:18:23.159: INFO: stderr: "+ nc -zv -t -w 2 10.188.240.222 31639\nConnection to 10.188.240.222 31639 port [tcp/31639] succeeded!\n"
Aug 21 17:18:23.159: INFO: stdout: ""
Aug 21 17:18:23.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=services-9278 execpodhd8rk -- /bin/sh -x -c nc -zv -t -w 2 169.63.148.208 31639'
Aug 21 17:18:23.564: INFO: stderr: "+ nc -zv -t -w 2 169.63.148.208 31639\nConnection to 169.63.148.208 31639 port [tcp/31639] succeeded!\n"
Aug 21 17:18:23.564: INFO: stdout: ""
Aug 21 17:18:23.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=services-9278 execpodhd8rk -- /bin/sh -x -c nc -zv -t -w 2 169.63.130.222 31639'
Aug 21 17:18:23.948: INFO: stderr: "+ nc -zv -t -w 2 169.63.130.222 31639\nConnection to 169.63.130.222 31639 port [tcp/31639] succeeded!\n"
Aug 21 17:18:23.948: INFO: stdout: ""
Aug 21 17:18:23.948: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:18:24.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9278" for this suite.
Aug 21 17:18:34.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:18:36.268: INFO: namespace services-9278 deletion completed in 12.205164952s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:23.517 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:18:36.268: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 21 17:18:40.733: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 21 17:18:40.747: INFO: Pod pod-with-poststart-http-hook still exists
Aug 21 17:18:42.749: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 21 17:18:42.769: INFO: Pod pod-with-poststart-http-hook still exists
Aug 21 17:18:44.748: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 21 17:18:44.763: INFO: Pod pod-with-poststart-http-hook still exists
Aug 21 17:18:46.748: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 21 17:18:46.760: INFO: Pod pod-with-poststart-http-hook still exists
Aug 21 17:18:48.748: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 21 17:18:48.777: INFO: Pod pod-with-poststart-http-hook still exists
Aug 21 17:18:50.748: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 21 17:18:50.758: INFO: Pod pod-with-poststart-http-hook still exists
Aug 21 17:18:52.748: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 21 17:18:52.762: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:18:52.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8248" for this suite.
Aug 21 17:19:26.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:19:29.114: INFO: namespace container-lifecycle-hook-8248 deletion completed in 36.321060529s

• [SLOW TEST:52.846 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:19:29.114: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Aug 21 17:19:29.321: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 21 17:19:29.433: INFO: Waiting for terminating namespaces to be deleted...
Aug 21 17:19:29.462: INFO: 
Logging pods the kubelet thinks is on node 10.188.240.202 before test
Aug 21 17:19:29.555: INFO: tuned-hfsdd from openshift-cluster-node-tuning-operator started at 2020-08-21 14:26:30 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container tuned ready: true, restart count 0
Aug 21 17:19:29.555: INFO: community-operators-68975cd7c8-sf6zn from openshift-marketplace started at 2020-08-21 14:28:43 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container community-operators ready: true, restart count 0
Aug 21 17:19:29.555: INFO: image-registry-86f8b76dcb-jdpb9 from openshift-image-registry started at 2020-08-21 14:30:33 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container registry ready: true, restart count 0
Aug 21 17:19:29.555: INFO: prometheus-operator-56d9d699cb-drn75 from openshift-monitoring started at 2020-08-21 14:33:56 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 21 17:19:29.555: INFO: grafana-c9c7455d7-gcf82 from openshift-monitoring started at 2020-08-21 14:34:15 +0000 UTC (2 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container grafana ready: true, restart count 0
Aug 21 17:19:29.555: INFO: 	Container grafana-proxy ready: true, restart count 0
Aug 21 17:19:29.555: INFO: ibmcloud-block-storage-driver-wmdvn from kube-system started at 2020-08-21 14:24:16 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 21 17:19:29.555: INFO: multus-59vjb from openshift-multus started at 2020-08-21 14:24:42 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container kube-multus ready: true, restart count 0
Aug 21 17:19:29.555: INFO: ibm-cloud-provider-ip-169-60-87-181-7df9c474cf-m5bdn from ibm-system started at 2020-08-21 14:30:31 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container ibm-cloud-provider-ip-169-60-87-181 ready: true, restart count 0
Aug 21 17:19:29.555: INFO: console-5bf7799b6-qfl8j from openshift-console started at 2020-08-21 14:29:21 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container console ready: true, restart count 0
Aug 21 17:19:29.555: INFO: apiservice-cabundle-injector-594fd4555f-pd5j2 from openshift-service-ca started at 2020-08-21 14:26:58 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Aug 21 17:19:29.555: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-08-21 14:35:11 +0000 UTC (7 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 17:19:29.555: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 21 17:19:29.555: INFO: 	Container prometheus ready: true, restart count 1
Aug 21 17:19:29.555: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 21 17:19:29.555: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 21 17:19:29.555: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 21 17:19:29.555: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 21 17:19:29.555: INFO: thanos-querier-5bcc6bd6c4-w2z4d from openshift-monitoring started at 2020-08-21 14:35:05 +0000 UTC (4 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 17:19:29.555: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 21 17:19:29.555: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 21 17:19:29.555: INFO: 	Container thanos-querier ready: true, restart count 0
Aug 21 17:19:29.555: INFO: packageserver-75777686bd-sxs4v from openshift-operator-lifecycle-manager started at 2020-08-21 14:31:38 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container packageserver ready: true, restart count 0
Aug 21 17:19:29.555: INFO: tigera-operator-679798d94d-5dj78 from tigera-operator started at 2020-08-21 14:24:16 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container tigera-operator ready: true, restart count 1
Aug 21 17:19:29.555: INFO: node-exporter-vw6s8 from openshift-monitoring started at 2020-08-21 14:26:49 +0000 UTC (2 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 17:19:29.555: INFO: 	Container node-exporter ready: true, restart count 0
Aug 21 17:19:29.555: INFO: calico-typha-6c986fbc8c-mtt9h from calico-system started at 2020-08-21 14:27:04 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container calico-typha ready: true, restart count 0
Aug 21 17:19:29.555: INFO: node-ca-2zlsz from openshift-image-registry started at 2020-08-21 14:27:50 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container node-ca ready: true, restart count 0
Aug 21 17:19:29.555: INFO: ibm-master-proxy-static-10.188.240.202 from kube-system started at 2020-08-21 14:24:11 +0000 UTC (2 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 21 17:19:29.555: INFO: 	Container pause ready: true, restart count 0
Aug 21 17:19:29.555: INFO: calico-node-lfw9m from calico-system started at 2020-08-21 14:25:08 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container calico-node ready: true, restart count 0
Aug 21 17:19:29.555: INFO: dns-default-zd27g from openshift-dns started at 2020-08-21 14:28:16 +0000 UTC (2 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container dns ready: true, restart count 0
Aug 21 17:19:29.555: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 21 17:19:29.555: INFO: redhat-operators-6d986fdd47-bvpx2 from openshift-marketplace started at 2020-08-21 14:28:43 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container redhat-operators ready: true, restart count 0
Aug 21 17:19:29.555: INFO: prometheus-adapter-5697b6dddd-ldgqf from openshift-monitoring started at 2020-08-21 14:34:09 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 21 17:19:29.555: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-08-21 14:34:31 +0000 UTC (3 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container alertmanager ready: true, restart count 0
Aug 21 17:19:29.555: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 21 17:19:29.555: INFO: 	Container config-reloader ready: true, restart count 0
Aug 21 17:19:29.555: INFO: sonobuoy-systemd-logs-daemon-set-5cea973cc7904f8c-nfmgc from sonobuoy started at 2020-08-21 15:39:10 +0000 UTC (2 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 21 17:19:29.555: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 21 17:19:29.555: INFO: network-operator-7986644c85-778wc from openshift-network-operator started at 2020-08-21 14:24:16 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container network-operator ready: true, restart count 0
Aug 21 17:19:29.555: INFO: router-default-79bfbd48f7-bdqc5 from openshift-ingress started at 2020-08-21 14:28:24 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container router ready: true, restart count 0
Aug 21 17:19:29.555: INFO: multus-admission-controller-7ccn4 from openshift-multus started at 2020-08-21 14:25:32 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 21 17:19:29.555: INFO: openshift-state-metrics-5849d797d8-h6klv from openshift-monitoring started at 2020-08-21 14:26:49 +0000 UTC (3 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 21 17:19:29.555: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 21 17:19:29.555: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 21 17:19:29.555: INFO: service-serving-cert-signer-7879bf8d9f-db6v2 from openshift-service-ca started at 2020-08-21 14:26:58 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Aug 21 17:19:29.555: INFO: ibm-keepalived-watcher-hgfkl from kube-system started at 2020-08-21 14:24:13 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 21 17:19:29.555: INFO: openshift-kube-proxy-dqhxm from openshift-kube-proxy started at 2020-08-21 14:24:48 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.555: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 21 17:19:29.555: INFO: 
Logging pods the kubelet thinks is on node 10.188.240.222 before test
Aug 21 17:19:29.750: INFO: ibm-master-proxy-static-10.188.240.222 from kube-system started at 2020-08-21 14:24:12 +0000 UTC (2 container statuses recorded)
Aug 21 17:19:29.750: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 21 17:19:29.750: INFO: 	Container pause ready: true, restart count 0
Aug 21 17:19:29.750: INFO: tuned-xffjb from openshift-cluster-node-tuning-operator started at 2020-08-21 14:26:30 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.750: INFO: 	Container tuned ready: true, restart count 0
Aug 21 17:19:29.750: INFO: node-ca-wn9jz from openshift-image-registry started at 2020-08-21 14:27:50 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.750: INFO: 	Container node-ca ready: true, restart count 0
Aug 21 17:19:29.750: INFO: openshift-kube-proxy-w7zk7 from openshift-kube-proxy started at 2020-08-21 14:24:48 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.750: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 21 17:19:29.750: INFO: configmap-cabundle-injector-8446d4b88f-lc4xk from openshift-service-ca started at 2020-08-21 14:26:59 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.750: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Aug 21 17:19:29.750: INFO: console-5bf7799b6-qbrjl from openshift-console started at 2020-08-21 14:29:07 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.750: INFO: 	Container console ready: true, restart count 0
Aug 21 17:19:29.750: INFO: calico-node-s8hvd from calico-system started at 2020-08-21 14:25:08 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.750: INFO: 	Container calico-node ready: true, restart count 0
Aug 21 17:19:29.750: INFO: multus-admission-controller-tmrh2 from openshift-multus started at 2020-08-21 14:25:45 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.750: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 21 17:19:29.750: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-08-21 14:35:32 +0000 UTC (7 container statuses recorded)
Aug 21 17:19:29.750: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 17:19:29.750: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 21 17:19:29.750: INFO: 	Container prometheus ready: true, restart count 1
Aug 21 17:19:29.750: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 21 17:19:29.750: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 21 17:19:29.750: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 21 17:19:29.750: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 21 17:19:29.750: INFO: vpn-69dd866c84-4s9m7 from kube-system started at 2020-08-21 14:31:55 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.750: INFO: 	Container vpn ready: true, restart count 0
Aug 21 17:19:29.750: INFO: ibm-keepalived-watcher-pqhzw from kube-system started at 2020-08-21 14:24:15 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.750: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 21 17:19:29.750: INFO: calico-typha-6c986fbc8c-d5w6g from calico-system started at 2020-08-21 14:25:07 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.750: INFO: 	Container calico-typha ready: true, restart count 0
Aug 21 17:19:29.750: INFO: node-exporter-67tp6 from openshift-monitoring started at 2020-08-21 14:26:50 +0000 UTC (2 container statuses recorded)
Aug 21 17:19:29.750: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 17:19:29.750: INFO: 	Container node-exporter ready: true, restart count 0
Aug 21 17:19:29.750: INFO: dns-default-xxclv from openshift-dns started at 2020-08-21 14:28:16 +0000 UTC (2 container statuses recorded)
Aug 21 17:19:29.750: INFO: 	Container dns ready: true, restart count 0
Aug 21 17:19:29.750: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 21 17:19:29.750: INFO: telemeter-client-6fdb57d68d-f82cq from openshift-monitoring started at 2020-08-21 14:34:07 +0000 UTC (3 container statuses recorded)
Aug 21 17:19:29.750: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 17:19:29.750: INFO: 	Container reload ready: true, restart count 0
Aug 21 17:19:29.750: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 21 17:19:29.750: INFO: sonobuoy-systemd-logs-daemon-set-5cea973cc7904f8c-m7w4l from sonobuoy started at 2020-08-21 15:39:10 +0000 UTC (2 container statuses recorded)
Aug 21 17:19:29.750: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 21 17:19:29.750: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 21 17:19:29.750: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-08-21 14:30:18 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.750: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Aug 21 17:19:29.750: INFO: registry-pvc-permissions-px8dj from openshift-image-registry started at 2020-08-21 14:30:33 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.750: INFO: 	Container pvc-permissions ready: false, restart count 0
Aug 21 17:19:29.750: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-08-21 14:34:42 +0000 UTC (3 container statuses recorded)
Aug 21 17:19:29.751: INFO: 	Container alertmanager ready: true, restart count 0
Aug 21 17:19:29.751: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 21 17:19:29.751: INFO: 	Container config-reloader ready: true, restart count 0
Aug 21 17:19:29.751: INFO: multus-4xs2x from openshift-multus started at 2020-08-21 14:24:42 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.751: INFO: 	Container kube-multus ready: true, restart count 0
Aug 21 17:19:29.751: INFO: thanos-querier-5bcc6bd6c4-6wnxk from openshift-monitoring started at 2020-08-21 14:34:55 +0000 UTC (4 container statuses recorded)
Aug 21 17:19:29.751: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 17:19:29.751: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 21 17:19:29.751: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 21 17:19:29.751: INFO: 	Container thanos-querier ready: true, restart count 0
Aug 21 17:19:29.751: INFO: ibmcloud-block-storage-driver-88f6v from kube-system started at 2020-08-21 14:24:22 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.751: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 21 17:19:29.751: INFO: kube-state-metrics-c5f65645-vgmjg from openshift-monitoring started at 2020-08-21 14:26:47 +0000 UTC (3 container statuses recorded)
Aug 21 17:19:29.751: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 21 17:19:29.751: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 21 17:19:29.751: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 21 17:19:29.751: INFO: cluster-samples-operator-55944b8f44-hpv4t from openshift-cluster-samples-operator started at 2020-08-21 14:27:28 +0000 UTC (2 container statuses recorded)
Aug 21 17:19:29.751: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Aug 21 17:19:29.751: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Aug 21 17:19:29.751: INFO: router-default-79bfbd48f7-wq29r from openshift-ingress started at 2020-08-21 14:28:25 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.751: INFO: 	Container router ready: true, restart count 0
Aug 21 17:19:29.751: INFO: 
Logging pods the kubelet thinks is on node 10.188.240.230 before test
Aug 21 17:19:29.850: INFO: certified-operators-6b6b9f965f-fthsb from openshift-marketplace started at 2020-08-21 16:28:56 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.850: INFO: 	Container certified-operators ready: true, restart count 0
Aug 21 17:19:29.850: INFO: openshift-service-catalog-apiserver-operator-78b9dddb6f-jxgbd from openshift-service-catalog-apiserver-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container operator ready: true, restart count 1
Aug 21 17:19:29.851: INFO: catalog-operator-85f6c659cc-dqh5m from openshift-operator-lifecycle-manager started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 21 17:19:29.851: INFO: marketplace-operator-6957767d58-5m7kk from openshift-marketplace started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container marketplace-operator ready: true, restart count 0
Aug 21 17:19:29.851: INFO: downloads-678f5d6564-78bw4 from openshift-console started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container download-server ready: true, restart count 0
Aug 21 17:19:29.851: INFO: sonobuoy-e2e-job-3399d07d932e4f9d from sonobuoy started at 2020-08-21 15:39:10 +0000 UTC (2 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container e2e ready: true, restart count 0
Aug 21 17:19:29.851: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 21 17:19:29.851: INFO: sonobuoy-systemd-logs-daemon-set-5cea973cc7904f8c-rjxm6 from sonobuoy started at 2020-08-21 15:39:10 +0000 UTC (2 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 21 17:19:29.851: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 21 17:19:29.851: INFO: ibm-master-proxy-static-10.188.240.230 from kube-system started at 2020-08-21 14:24:17 +0000 UTC (2 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 21 17:19:29.851: INFO: 	Container pause ready: true, restart count 0
Aug 21 17:19:29.851: INFO: ibmcloud-block-storage-driver-jzb5f from kube-system started at 2020-08-21 14:24:22 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 21 17:19:29.851: INFO: ibmcloud-block-storage-plugin-68d5c65db9-clx4d from kube-system started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Aug 21 17:19:29.851: INFO: multus-admission-controller-9x9x2 from openshift-multus started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 21 17:19:29.851: INFO: ibm-storage-watcher-68df9b45c4-9rvr8 from kube-system started at 2020-08-21 14:25:31 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 21 17:19:29.851: INFO: node-exporter-jkv5q from openshift-monitoring started at 2020-08-21 14:26:50 +0000 UTC (2 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 17:19:29.851: INFO: 	Container node-exporter ready: true, restart count 0
Aug 21 17:19:29.851: INFO: ibm-file-plugin-6c96899f79-5r8lq from kube-system started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 21 17:19:29.851: INFO: dns-operator-6f9cf66db7-2qjt6 from openshift-dns-operator started at 2020-08-21 14:25:29 +0000 UTC (2 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container dns-operator ready: true, restart count 0
Aug 21 17:19:29.851: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 17:19:29.851: INFO: cluster-image-registry-operator-6cfd58b66c-rsxhh from openshift-image-registry started at 2020-08-21 14:25:29 +0000 UTC (2 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Aug 21 17:19:29.851: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Aug 21 17:19:29.851: INFO: cluster-node-tuning-operator-b5f884945-f92th from openshift-cluster-node-tuning-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Aug 21 17:19:29.851: INFO: tuned-bh9bt from openshift-cluster-node-tuning-operator started at 2020-08-21 14:26:30 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container tuned ready: true, restart count 0
Aug 21 17:19:29.851: INFO: packageserver-75777686bd-6jlwz from openshift-operator-lifecycle-manager started at 2020-08-21 14:31:50 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container packageserver ready: true, restart count 0
Aug 21 17:19:29.851: INFO: calico-node-rw476 from calico-system started at 2020-08-21 14:25:08 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container calico-node ready: true, restart count 0
Aug 21 17:19:29.851: INFO: cluster-storage-operator-557b75f8d5-bpz57 from openshift-cluster-storage-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Aug 21 17:19:29.851: INFO: calico-typha-6c986fbc8c-rw4j9 from calico-system started at 2020-08-21 14:27:04 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container calico-typha ready: true, restart count 0
Aug 21 17:19:29.851: INFO: sonobuoy from sonobuoy started at 2020-08-21 15:39:03 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 21 17:19:29.851: INFO: ibm-keepalived-watcher-9tt65 from kube-system started at 2020-08-21 14:24:19 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 21 17:19:29.851: INFO: downloads-678f5d6564-sxpw2 from openshift-console started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container download-server ready: true, restart count 0
Aug 21 17:19:29.851: INFO: calico-kube-controllers-79d75767dd-bznnm from calico-system started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 21 17:19:29.851: INFO: node-ca-7vtw7 from openshift-image-registry started at 2020-08-21 14:27:50 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container node-ca ready: true, restart count 0
Aug 21 17:19:29.851: INFO: prometheus-adapter-5697b6dddd-hpnk5 from openshift-monitoring started at 2020-08-21 14:34:09 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 21 17:19:29.851: INFO: console-operator-9878d4766-tfdhf from openshift-console-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container console-operator ready: true, restart count 1
Aug 21 17:19:29.851: INFO: openshift-service-catalog-controller-manager-operator-5496stt64 from openshift-service-catalog-controller-manager-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container operator ready: true, restart count 1
Aug 21 17:19:29.851: INFO: service-ca-operator-694cfbf5d5-vxbp2 from openshift-service-ca-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container operator ready: true, restart count 0
Aug 21 17:19:29.851: INFO: ingress-operator-695bc545b9-ps8wd from openshift-ingress-operator started at 2020-08-21 14:25:29 +0000 UTC (2 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container ingress-operator ready: true, restart count 0
Aug 21 17:19:29.851: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 17:19:29.851: INFO: multus-5fbcs from openshift-multus started at 2020-08-21 14:24:42 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container kube-multus ready: true, restart count 0
Aug 21 17:19:29.851: INFO: cluster-monitoring-operator-5b5659466f-lbtcv from openshift-monitoring started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Aug 21 17:19:29.851: INFO: dns-default-qhhzk from openshift-dns started at 2020-08-21 14:28:16 +0000 UTC (2 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container dns ready: true, restart count 0
Aug 21 17:19:29.851: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 21 17:19:29.851: INFO: ibm-cloud-provider-ip-169-60-87-181-7df9c474cf-v8lnf from ibm-system started at 2020-08-21 14:30:19 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container ibm-cloud-provider-ip-169-60-87-181 ready: true, restart count 0
Aug 21 17:19:29.851: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-08-21 14:34:17 +0000 UTC (3 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container alertmanager ready: true, restart count 0
Aug 21 17:19:29.851: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 21 17:19:29.851: INFO: 	Container config-reloader ready: true, restart count 0
Aug 21 17:19:29.851: INFO: openshift-kube-proxy-j677g from openshift-kube-proxy started at 2020-08-21 14:24:48 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 21 17:19:29.851: INFO: olm-operator-b5f57cdbb-nw9x4 from openshift-operator-lifecycle-manager started at 2020-08-21 14:25:30 +0000 UTC (1 container statuses recorded)
Aug 21 17:19:29.851: INFO: 	Container olm-operator ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-1762839f-fba3-4629-8160-450559c57912 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-1762839f-fba3-4629-8160-450559c57912 off the node 10.188.240.230
STEP: verifying the node doesn't have the label kubernetes.io/e2e-1762839f-fba3-4629-8160-450559c57912
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:19:42.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3189" for this suite.
Aug 21 17:20:02.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:20:04.795: INFO: namespace sched-pred-3189 deletion completed in 22.319412173s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:35.681 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:20:04.796: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 17:20:05.066: INFO: (0) /api/v1/nodes/10.188.240.202:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 53.319265ms)
Aug 21 17:20:05.094: INFO: (1) /api/v1/nodes/10.188.240.202:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 27.93389ms)
Aug 21 17:20:05.125: INFO: (2) /api/v1/nodes/10.188.240.202:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 30.306847ms)
Aug 21 17:20:05.153: INFO: (3) /api/v1/nodes/10.188.240.202:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 28.009665ms)
Aug 21 17:20:05.185: INFO: (4) /api/v1/nodes/10.188.240.202:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 31.748244ms)
Aug 21 17:20:05.219: INFO: (5) /api/v1/nodes/10.188.240.202:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 33.956449ms)
Aug 21 17:20:05.266: INFO: (6) /api/v1/nodes/10.188.240.202:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 47.70126ms)
Aug 21 17:20:05.297: INFO: (7) /api/v1/nodes/10.188.240.202:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 30.468081ms)
Aug 21 17:20:05.321: INFO: (8) /api/v1/nodes/10.188.240.202:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 24.162585ms)
Aug 21 17:20:05.353: INFO: (9) /api/v1/nodes/10.188.240.202:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 31.924779ms)
Aug 21 17:20:05.383: INFO: (10) /api/v1/nodes/10.188.240.202:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 30.111671ms)
Aug 21 17:20:05.408: INFO: (11) /api/v1/nodes/10.188.240.202:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 24.967672ms)
Aug 21 17:20:05.431: INFO: (12) /api/v1/nodes/10.188.240.202:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 23.136791ms)
Aug 21 17:20:05.461: INFO: (13) /api/v1/nodes/10.188.240.202:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 29.020511ms)
Aug 21 17:20:05.483: INFO: (14) /api/v1/nodes/10.188.240.202:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 22.169945ms)
Aug 21 17:20:05.505: INFO: (15) /api/v1/nodes/10.188.240.202:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 22.183472ms)
Aug 21 17:20:05.526: INFO: (16) /api/v1/nodes/10.188.240.202:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.553685ms)
Aug 21 17:20:05.548: INFO: (17) /api/v1/nodes/10.188.240.202:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 22.477532ms)
Aug 21 17:20:05.577: INFO: (18) /api/v1/nodes/10.188.240.202:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 28.585012ms)
Aug 21 17:20:05.608: INFO: (19) /api/v1/nodes/10.188.240.202:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 31.266341ms)
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:20:05.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5700" for this suite.
Aug 21 17:20:15.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:20:16.793: INFO: namespace proxy-5700 deletion completed in 11.164125231s

• [SLOW TEST:11.997 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:20:16.793: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0821 17:20:27.091039      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 21 17:20:27.091: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:20:27.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7325" for this suite.
Aug 21 17:20:37.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:20:39.412: INFO: namespace gc-7325 deletion completed in 12.297598387s

• [SLOW TEST:22.619 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:20:39.413: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 21 17:20:40.206: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 21 17:20:42.253: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733627240, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733627240, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733627240, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733627240, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 21 17:20:45.313: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:20:45.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-627" for this suite.
Aug 21 17:20:53.631: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:20:55.925: INFO: namespace webhook-627 deletion completed in 10.3555737s
STEP: Destroying namespace "webhook-627-markers" for this suite.
Aug 21 17:21:03.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:21:06.273: INFO: namespace webhook-627-markers deletion completed in 10.347334954s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:26.971 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:21:06.385: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 21 17:21:06.630: INFO: Waiting up to 5m0s for pod "pod-4c666563-1e29-4cf3-b174-cb0336826962" in namespace "emptydir-9669" to be "success or failure"
Aug 21 17:21:06.641: INFO: Pod "pod-4c666563-1e29-4cf3-b174-cb0336826962": Phase="Pending", Reason="", readiness=false. Elapsed: 10.359828ms
Aug 21 17:21:08.655: INFO: Pod "pod-4c666563-1e29-4cf3-b174-cb0336826962": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024316118s
STEP: Saw pod success
Aug 21 17:21:08.655: INFO: Pod "pod-4c666563-1e29-4cf3-b174-cb0336826962" satisfied condition "success or failure"
Aug 21 17:21:08.668: INFO: Trying to get logs from node 10.188.240.202 pod pod-4c666563-1e29-4cf3-b174-cb0336826962 container test-container: <nil>
STEP: delete the pod
Aug 21 17:21:08.754: INFO: Waiting for pod pod-4c666563-1e29-4cf3-b174-cb0336826962 to disappear
Aug 21 17:21:08.767: INFO: Pod pod-4c666563-1e29-4cf3-b174-cb0336826962 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:21:08.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9669" for this suite.
Aug 21 17:21:16.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:21:19.071: INFO: namespace emptydir-9669 deletion completed in 10.267071785s

• [SLOW TEST:12.687 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:21:19.072: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Aug 21 17:21:19.342: INFO: Waiting up to 5m0s for pod "downward-api-9d400a69-8b6e-4574-a3ec-5aa80164160b" in namespace "downward-api-9474" to be "success or failure"
Aug 21 17:21:19.356: INFO: Pod "downward-api-9d400a69-8b6e-4574-a3ec-5aa80164160b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.778905ms
Aug 21 17:21:21.369: INFO: Pod "downward-api-9d400a69-8b6e-4574-a3ec-5aa80164160b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027154894s
STEP: Saw pod success
Aug 21 17:21:21.369: INFO: Pod "downward-api-9d400a69-8b6e-4574-a3ec-5aa80164160b" satisfied condition "success or failure"
Aug 21 17:21:21.384: INFO: Trying to get logs from node 10.188.240.222 pod downward-api-9d400a69-8b6e-4574-a3ec-5aa80164160b container dapi-container: <nil>
STEP: delete the pod
Aug 21 17:21:21.489: INFO: Waiting for pod downward-api-9d400a69-8b6e-4574-a3ec-5aa80164160b to disappear
Aug 21 17:21:21.501: INFO: Pod downward-api-9d400a69-8b6e-4574-a3ec-5aa80164160b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:21:21.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9474" for this suite.
Aug 21 17:21:30.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:21:32.144: INFO: namespace downward-api-9474 deletion completed in 10.611055022s

• [SLOW TEST:13.073 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:21:32.148: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 21 17:21:32.402: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc101d53-fc64-4c18-a951-b63410f91e63" in namespace "downward-api-2294" to be "success or failure"
Aug 21 17:21:32.413: INFO: Pod "downwardapi-volume-fc101d53-fc64-4c18-a951-b63410f91e63": Phase="Pending", Reason="", readiness=false. Elapsed: 10.823877ms
Aug 21 17:21:34.426: INFO: Pod "downwardapi-volume-fc101d53-fc64-4c18-a951-b63410f91e63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024103309s
STEP: Saw pod success
Aug 21 17:21:34.426: INFO: Pod "downwardapi-volume-fc101d53-fc64-4c18-a951-b63410f91e63" satisfied condition "success or failure"
Aug 21 17:21:34.443: INFO: Trying to get logs from node 10.188.240.230 pod downwardapi-volume-fc101d53-fc64-4c18-a951-b63410f91e63 container client-container: <nil>
STEP: delete the pod
Aug 21 17:21:34.557: INFO: Waiting for pod downwardapi-volume-fc101d53-fc64-4c18-a951-b63410f91e63 to disappear
Aug 21 17:21:34.570: INFO: Pod downwardapi-volume-fc101d53-fc64-4c18-a951-b63410f91e63 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:21:34.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2294" for this suite.
Aug 21 17:21:42.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:21:44.827: INFO: namespace downward-api-2294 deletion completed in 10.215698943s

• [SLOW TEST:12.680 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:21:44.828: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 21 17:21:45.156: INFO: Waiting up to 5m0s for pod "pod-ab049ac2-0a7d-4ac9-8e9c-920b029ff1d3" in namespace "emptydir-6249" to be "success or failure"
Aug 21 17:21:45.179: INFO: Pod "pod-ab049ac2-0a7d-4ac9-8e9c-920b029ff1d3": Phase="Pending", Reason="", readiness=false. Elapsed: 23.577408ms
Aug 21 17:21:47.192: INFO: Pod "pod-ab049ac2-0a7d-4ac9-8e9c-920b029ff1d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03618786s
STEP: Saw pod success
Aug 21 17:21:47.192: INFO: Pod "pod-ab049ac2-0a7d-4ac9-8e9c-920b029ff1d3" satisfied condition "success or failure"
Aug 21 17:21:47.205: INFO: Trying to get logs from node 10.188.240.202 pod pod-ab049ac2-0a7d-4ac9-8e9c-920b029ff1d3 container test-container: <nil>
STEP: delete the pod
Aug 21 17:21:47.274: INFO: Waiting for pod pod-ab049ac2-0a7d-4ac9-8e9c-920b029ff1d3 to disappear
Aug 21 17:21:47.289: INFO: Pod pod-ab049ac2-0a7d-4ac9-8e9c-920b029ff1d3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:21:47.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6249" for this suite.
Aug 21 17:21:55.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:21:57.527: INFO: namespace emptydir-6249 deletion completed in 10.205585643s

• [SLOW TEST:12.699 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:21:57.527: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Aug 21 17:21:57.729: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-192619208 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:21:57.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2248" for this suite.
Aug 21 17:22:05.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:22:08.172: INFO: namespace kubectl-2248 deletion completed in 10.276005085s

• [SLOW TEST:10.645 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:22:08.172: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 21 17:22:09.167: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 21 17:22:12.242: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:22:12.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8655" for this suite.
Aug 21 17:22:22.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:22:25.089: INFO: namespace webhook-8655 deletion completed in 12.256958659s
STEP: Destroying namespace "webhook-8655-markers" for this suite.
Aug 21 17:22:33.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:22:35.370: INFO: namespace webhook-8655-markers deletion completed in 10.28106994s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:27.270 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:22:35.442: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Aug 21 17:22:45.933: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0821 17:22:45.933527      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 21 17:22:45.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9575" for this suite.
Aug 21 17:22:56.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:22:58.267: INFO: namespace gc-9575 deletion completed in 12.310595748s

• [SLOW TEST:22.825 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:22:58.268: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-625
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Aug 21 17:22:58.524: INFO: Found 0 stateful pods, waiting for 3
Aug 21 17:23:08.538: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 17:23:08.538: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 17:23:08.538: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Aug 21 17:23:08.611: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Aug 21 17:23:18.695: INFO: Updating stateful set ss2
Aug 21 17:23:18.718: INFO: Waiting for Pod statefulset-625/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 21 17:23:28.746: INFO: Waiting for Pod statefulset-625/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Aug 21 17:23:38.844: INFO: Found 2 stateful pods, waiting for 3
Aug 21 17:23:48.872: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 17:23:48.872: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 17:23:48.872: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Aug 21 17:23:48.968: INFO: Updating stateful set ss2
Aug 21 17:23:48.989: INFO: Waiting for Pod statefulset-625/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 21 17:23:59.012: INFO: Waiting for Pod statefulset-625/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 21 17:24:09.082: INFO: Updating stateful set ss2
Aug 21 17:24:09.110: INFO: Waiting for StatefulSet statefulset-625/ss2 to complete update
Aug 21 17:24:09.110: INFO: Waiting for Pod statefulset-625/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Aug 21 17:24:19.137: INFO: Deleting all statefulset in ns statefulset-625
Aug 21 17:24:19.148: INFO: Scaling statefulset ss2 to 0
Aug 21 17:24:49.217: INFO: Waiting for statefulset status.replicas updated to 0
Aug 21 17:24:49.227: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:24:49.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-625" for this suite.
Aug 21 17:24:59.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:25:01.477: INFO: namespace statefulset-625 deletion completed in 12.167610258s

• [SLOW TEST:123.209 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:25:01.477: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8105
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-8105
I0821 17:25:01.801868      24 runners.go:184] Created replication controller with name: externalname-service, namespace: services-8105, replica count: 2
I0821 17:25:04.853385      24 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 21 17:25:04.853: INFO: Creating new exec pod
Aug 21 17:25:09.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=services-8105 execpodnkvm4 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Aug 21 17:25:10.332: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 21 17:25:10.333: INFO: stdout: ""
Aug 21 17:25:10.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=services-8105 execpodnkvm4 -- /bin/sh -x -c nc -zv -t -w 2 172.21.32.152 80'
Aug 21 17:25:10.713: INFO: stderr: "+ nc -zv -t -w 2 172.21.32.152 80\nConnection to 172.21.32.152 80 port [tcp/http] succeeded!\n"
Aug 21 17:25:10.713: INFO: stdout: ""
Aug 21 17:25:10.713: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:25:10.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8105" for this suite.
Aug 21 17:25:20.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:25:23.006: INFO: namespace services-8105 deletion completed in 12.16878421s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:21.528 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:25:23.006: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
Aug 21 17:25:23.242: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 21 17:26:23.403: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 17:26:23.426: INFO: Starting informer...
STEP: Starting pod...
Aug 21 17:26:23.698: INFO: Pod is running on 10.188.240.202. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Aug 21 17:26:23.767: INFO: Pod wasn't evicted. Proceeding
Aug 21 17:26:23.767: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Aug 21 17:27:38.834: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:27:38.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-6323" for this suite.
Aug 21 17:27:54.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:27:57.047: INFO: namespace taint-single-pod-6323 deletion completed in 18.189984216s

• [SLOW TEST:154.041 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:27:57.047: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Aug 21 17:27:57.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-5653 -- logs-generator --log-lines-total 100 --run-duration 20s'
Aug 21 17:27:57.485: INFO: stderr: ""
Aug 21 17:27:57.485: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Aug 21 17:27:57.485: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Aug 21 17:27:57.485: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5653" to be "running and ready, or succeeded"
Aug 21 17:27:57.498: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 13.033751ms
Aug 21 17:27:59.510: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.025572194s
Aug 21 17:27:59.510: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Aug 21 17:27:59.510: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Aug 21 17:27:59.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 logs logs-generator logs-generator --namespace=kubectl-5653'
Aug 21 17:27:59.832: INFO: stderr: ""
Aug 21 17:27:59.832: INFO: stdout: "I0821 17:27:58.951626       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/swwg 382\nI0821 17:27:59.151785       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/nxt 303\nI0821 17:27:59.351925       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/x6c 304\nI0821 17:27:59.551848       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/qcz9 377\nI0821 17:27:59.751777       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/msw 202\n"
STEP: limiting log lines
Aug 21 17:27:59.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 logs logs-generator logs-generator --namespace=kubectl-5653 --tail=1'
Aug 21 17:28:00.059: INFO: stderr: ""
Aug 21 17:28:00.059: INFO: stdout: "I0821 17:27:59.951799       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/g7m6 596\n"
STEP: limiting log bytes
Aug 21 17:28:00.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 logs logs-generator logs-generator --namespace=kubectl-5653 --limit-bytes=1'
Aug 21 17:28:00.279: INFO: stderr: ""
Aug 21 17:28:00.279: INFO: stdout: "I"
STEP: exposing timestamps
Aug 21 17:28:00.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 logs logs-generator logs-generator --namespace=kubectl-5653 --tail=1 --timestamps'
Aug 21 17:28:00.729: INFO: stderr: ""
Aug 21 17:28:00.729: INFO: stdout: "2020-08-21T12:28:00.551931508-05:00 I0821 17:28:00.551780       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/4rqr 341\n"
STEP: restricting to a time range
Aug 21 17:28:03.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 logs logs-generator logs-generator --namespace=kubectl-5653 --since=1s'
Aug 21 17:28:03.424: INFO: stderr: ""
Aug 21 17:28:03.424: INFO: stdout: "I0821 17:28:02.551765       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/zf6f 219\nI0821 17:28:02.751766       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/wk7f 254\nI0821 17:28:02.951791       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/tsj2 461\nI0821 17:28:03.151788       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/rw9 375\nI0821 17:28:03.351810       1 logs_generator.go:76] 22 POST /api/v1/namespaces/default/pods/8q9 379\n"
Aug 21 17:28:03.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 logs logs-generator logs-generator --namespace=kubectl-5653 --since=24h'
Aug 21 17:28:03.624: INFO: stderr: ""
Aug 21 17:28:03.624: INFO: stdout: "I0821 17:27:58.951626       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/swwg 382\nI0821 17:27:59.151785       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/nxt 303\nI0821 17:27:59.351925       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/x6c 304\nI0821 17:27:59.551848       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/qcz9 377\nI0821 17:27:59.751777       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/msw 202\nI0821 17:27:59.951799       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/g7m6 596\nI0821 17:28:00.151884       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/659 288\nI0821 17:28:00.351784       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/pjdq 464\nI0821 17:28:00.551780       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/4rqr 341\nI0821 17:28:00.751779       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/q7rc 313\nI0821 17:28:00.951782       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/58v 490\nI0821 17:28:01.151788       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/6mx 320\nI0821 17:28:01.351784       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/fzk 512\nI0821 17:28:01.551773       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/7fg 395\nI0821 17:28:01.751974       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/4fs 350\nI0821 17:28:01.951790       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/rvj 514\nI0821 17:28:02.151773       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/4vb2 340\nI0821 17:28:02.351778       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/6v2r 256\nI0821 17:28:02.551765       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/zf6f 219\nI0821 17:28:02.751766       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/wk7f 254\nI0821 17:28:02.951791       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/tsj2 461\nI0821 17:28:03.151788       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/rw9 375\nI0821 17:28:03.351810       1 logs_generator.go:76] 22 POST /api/v1/namespaces/default/pods/8q9 379\nI0821 17:28:03.551786       1 logs_generator.go:76] 23 POST /api/v1/namespaces/ns/pods/6p5 317\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Aug 21 17:28:03.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 delete pod logs-generator --namespace=kubectl-5653'
Aug 21 17:28:11.198: INFO: stderr: ""
Aug 21 17:28:11.198: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:28:11.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5653" for this suite.
Aug 21 17:28:19.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:28:21.359: INFO: namespace kubectl-5653 deletion completed in 10.138876432s

• [SLOW TEST:24.312 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:28:21.360: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:28:37.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5217" for this suite.
Aug 21 17:28:45.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:28:48.006: INFO: namespace resourcequota-5217 deletion completed in 10.225195357s

• [SLOW TEST:26.647 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:28:48.009: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-c4e1f27d-0758-4572-8410-b4596ef23431
STEP: Creating a pod to test consume configMaps
Aug 21 17:28:48.362: INFO: Waiting up to 5m0s for pod "pod-configmaps-37ba47ee-95e7-4f8f-9b96-b424c2fb0840" in namespace "configmap-9009" to be "success or failure"
Aug 21 17:28:48.374: INFO: Pod "pod-configmaps-37ba47ee-95e7-4f8f-9b96-b424c2fb0840": Phase="Pending", Reason="", readiness=false. Elapsed: 11.937483ms
Aug 21 17:28:50.386: INFO: Pod "pod-configmaps-37ba47ee-95e7-4f8f-9b96-b424c2fb0840": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02409928s
Aug 21 17:28:52.399: INFO: Pod "pod-configmaps-37ba47ee-95e7-4f8f-9b96-b424c2fb0840": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037437076s
STEP: Saw pod success
Aug 21 17:28:52.399: INFO: Pod "pod-configmaps-37ba47ee-95e7-4f8f-9b96-b424c2fb0840" satisfied condition "success or failure"
Aug 21 17:28:52.410: INFO: Trying to get logs from node 10.188.240.202 pod pod-configmaps-37ba47ee-95e7-4f8f-9b96-b424c2fb0840 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 17:28:52.476: INFO: Waiting for pod pod-configmaps-37ba47ee-95e7-4f8f-9b96-b424c2fb0840 to disappear
Aug 21 17:28:52.488: INFO: Pod pod-configmaps-37ba47ee-95e7-4f8f-9b96-b424c2fb0840 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:28:52.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9009" for this suite.
Aug 21 17:29:00.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:29:02.785: INFO: namespace configmap-9009 deletion completed in 10.26502405s

• [SLOW TEST:14.777 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:29:02.796: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-a8815262-fb33-47a9-8be2-7f9b28c38441
STEP: Creating a pod to test consume configMaps
Aug 21 17:29:04.090: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-19948e1e-7afa-4932-8a30-d7ecd69b76a5" in namespace "projected-3391" to be "success or failure"
Aug 21 17:29:04.102: INFO: Pod "pod-projected-configmaps-19948e1e-7afa-4932-8a30-d7ecd69b76a5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.735758ms
Aug 21 17:29:06.118: INFO: Pod "pod-projected-configmaps-19948e1e-7afa-4932-8a30-d7ecd69b76a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02759263s
Aug 21 17:29:08.132: INFO: Pod "pod-projected-configmaps-19948e1e-7afa-4932-8a30-d7ecd69b76a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042059672s
STEP: Saw pod success
Aug 21 17:29:08.132: INFO: Pod "pod-projected-configmaps-19948e1e-7afa-4932-8a30-d7ecd69b76a5" satisfied condition "success or failure"
Aug 21 17:29:08.145: INFO: Trying to get logs from node 10.188.240.202 pod pod-projected-configmaps-19948e1e-7afa-4932-8a30-d7ecd69b76a5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 17:29:08.220: INFO: Waiting for pod pod-projected-configmaps-19948e1e-7afa-4932-8a30-d7ecd69b76a5 to disappear
Aug 21 17:29:08.234: INFO: Pod pod-projected-configmaps-19948e1e-7afa-4932-8a30-d7ecd69b76a5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:29:08.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3391" for this suite.
Aug 21 17:29:16.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:29:18.459: INFO: namespace projected-3391 deletion completed in 10.184294599s

• [SLOW TEST:15.662 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:29:18.459: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 21 17:29:18.847: INFO: Number of nodes with available pods: 0
Aug 21 17:29:18.847: INFO: Node 10.188.240.202 is running more than one daemon pod
Aug 21 17:29:19.885: INFO: Number of nodes with available pods: 0
Aug 21 17:29:19.885: INFO: Node 10.188.240.202 is running more than one daemon pod
Aug 21 17:29:20.891: INFO: Number of nodes with available pods: 1
Aug 21 17:29:20.891: INFO: Node 10.188.240.222 is running more than one daemon pod
Aug 21 17:29:21.892: INFO: Number of nodes with available pods: 3
Aug 21 17:29:21.892: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Aug 21 17:29:21.971: INFO: Number of nodes with available pods: 2
Aug 21 17:29:21.971: INFO: Node 10.188.240.202 is running more than one daemon pod
Aug 21 17:29:23.006: INFO: Number of nodes with available pods: 2
Aug 21 17:29:23.006: INFO: Node 10.188.240.202 is running more than one daemon pod
Aug 21 17:29:24.006: INFO: Number of nodes with available pods: 3
Aug 21 17:29:24.006: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9265, will wait for the garbage collector to delete the pods
Aug 21 17:29:24.120: INFO: Deleting DaemonSet.extensions daemon-set took: 29.030658ms
Aug 21 17:29:24.220: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.215193ms
Aug 21 17:29:37.531: INFO: Number of nodes with available pods: 0
Aug 21 17:29:37.531: INFO: Number of running nodes: 0, number of available pods: 0
Aug 21 17:29:37.543: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9265/daemonsets","resourceVersion":"84446"},"items":null}

Aug 21 17:29:37.555: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9265/pods","resourceVersion":"84446"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:29:37.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9265" for this suite.
Aug 21 17:29:47.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:29:49.884: INFO: namespace daemonsets-9265 deletion completed in 12.253280511s

• [SLOW TEST:31.425 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:29:49.885: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-808b7124-02b9-4ad2-a9a5-38a79809cd91
STEP: Creating secret with name secret-projected-all-test-volume-87692989-400a-4e92-a55b-54d140eac9a9
STEP: Creating a pod to test Check all projections for projected volume plugin
Aug 21 17:29:50.243: INFO: Waiting up to 5m0s for pod "projected-volume-56efdc6e-3461-4154-b5d2-bd8fd69b4dc0" in namespace "projected-5805" to be "success or failure"
Aug 21 17:29:50.255: INFO: Pod "projected-volume-56efdc6e-3461-4154-b5d2-bd8fd69b4dc0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.235417ms
Aug 21 17:29:52.271: INFO: Pod "projected-volume-56efdc6e-3461-4154-b5d2-bd8fd69b4dc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027778836s
STEP: Saw pod success
Aug 21 17:29:52.271: INFO: Pod "projected-volume-56efdc6e-3461-4154-b5d2-bd8fd69b4dc0" satisfied condition "success or failure"
Aug 21 17:29:52.282: INFO: Trying to get logs from node 10.188.240.202 pod projected-volume-56efdc6e-3461-4154-b5d2-bd8fd69b4dc0 container projected-all-volume-test: <nil>
STEP: delete the pod
Aug 21 17:29:52.345: INFO: Waiting for pod projected-volume-56efdc6e-3461-4154-b5d2-bd8fd69b4dc0 to disappear
Aug 21 17:29:52.355: INFO: Pod projected-volume-56efdc6e-3461-4154-b5d2-bd8fd69b4dc0 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:29:52.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5805" for this suite.
Aug 21 17:30:00.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:30:02.605: INFO: namespace projected-5805 deletion completed in 10.230796232s

• [SLOW TEST:12.721 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:30:02.606: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-ce30fde2-4195-497f-b623-313f8b3c7c95 in namespace container-probe-6253
Aug 21 17:30:05.872: INFO: Started pod liveness-ce30fde2-4195-497f-b623-313f8b3c7c95 in namespace container-probe-6253
STEP: checking the pod's current state and verifying that restartCount is present
Aug 21 17:30:05.886: INFO: Initial restart count of pod liveness-ce30fde2-4195-497f-b623-313f8b3c7c95 is 0
Aug 21 17:30:28.068: INFO: Restart count of pod container-probe-6253/liveness-ce30fde2-4195-497f-b623-313f8b3c7c95 is now 1 (22.181580032s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:30:28.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6253" for this suite.
Aug 21 17:30:36.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:30:38.295: INFO: namespace container-probe-6253 deletion completed in 10.173598466s

• [SLOW TEST:35.689 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:30:38.295: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 21 17:30:39.545: INFO: Waiting up to 5m0s for pod "downwardapi-volume-754199ef-bad6-471e-833f-edad881fa8f9" in namespace "projected-803" to be "success or failure"
Aug 21 17:30:39.556: INFO: Pod "downwardapi-volume-754199ef-bad6-471e-833f-edad881fa8f9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.087304ms
Aug 21 17:30:41.567: INFO: Pod "downwardapi-volume-754199ef-bad6-471e-833f-edad881fa8f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021560202s
Aug 21 17:30:43.577: INFO: Pod "downwardapi-volume-754199ef-bad6-471e-833f-edad881fa8f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032157598s
STEP: Saw pod success
Aug 21 17:30:43.577: INFO: Pod "downwardapi-volume-754199ef-bad6-471e-833f-edad881fa8f9" satisfied condition "success or failure"
Aug 21 17:30:43.587: INFO: Trying to get logs from node 10.188.240.202 pod downwardapi-volume-754199ef-bad6-471e-833f-edad881fa8f9 container client-container: <nil>
STEP: delete the pod
Aug 21 17:30:43.648: INFO: Waiting for pod downwardapi-volume-754199ef-bad6-471e-833f-edad881fa8f9 to disappear
Aug 21 17:30:43.660: INFO: Pod downwardapi-volume-754199ef-bad6-471e-833f-edad881fa8f9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:30:43.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-803" for this suite.
Aug 21 17:30:51.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:30:53.849: INFO: namespace projected-803 deletion completed in 10.167919801s

• [SLOW TEST:15.553 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:30:53.849: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
Aug 21 17:30:54.051: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 21 17:31:54.195: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 17:31:54.213: INFO: Starting informer...
STEP: Starting pods...
Aug 21 17:31:54.493: INFO: Pod1 is running on 10.188.240.202. Tainting Node
Aug 21 17:31:56.780: INFO: Pod2 is running on 10.188.240.202. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Aug 21 17:32:11.510: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Aug 21 17:32:31.277: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:32:31.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-2677" for this suite.
Aug 21 17:32:39.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:32:41.854: INFO: namespace taint-multiple-pods-2677 deletion completed in 10.501641859s

• [SLOW TEST:108.005 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:32:41.857: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 21 17:32:42.134: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ced5be5d-b742-4c89-af22-f5a6f1c4449b" in namespace "projected-3950" to be "success or failure"
Aug 21 17:32:42.152: INFO: Pod "downwardapi-volume-ced5be5d-b742-4c89-af22-f5a6f1c4449b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.170737ms
Aug 21 17:32:44.171: INFO: Pod "downwardapi-volume-ced5be5d-b742-4c89-af22-f5a6f1c4449b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037024444s
Aug 21 17:32:46.187: INFO: Pod "downwardapi-volume-ced5be5d-b742-4c89-af22-f5a6f1c4449b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052999598s
STEP: Saw pod success
Aug 21 17:32:46.187: INFO: Pod "downwardapi-volume-ced5be5d-b742-4c89-af22-f5a6f1c4449b" satisfied condition "success or failure"
Aug 21 17:32:46.198: INFO: Trying to get logs from node 10.188.240.202 pod downwardapi-volume-ced5be5d-b742-4c89-af22-f5a6f1c4449b container client-container: <nil>
STEP: delete the pod
Aug 21 17:32:46.305: INFO: Waiting for pod downwardapi-volume-ced5be5d-b742-4c89-af22-f5a6f1c4449b to disappear
Aug 21 17:32:46.315: INFO: Pod downwardapi-volume-ced5be5d-b742-4c89-af22-f5a6f1c4449b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:32:46.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3950" for this suite.
Aug 21 17:32:54.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:32:56.611: INFO: namespace projected-3950 deletion completed in 10.273697631s

• [SLOW TEST:14.753 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:32:56.611: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-c5de1c4f-551b-4821-961e-953d6d82770a
STEP: Creating a pod to test consume configMaps
Aug 21 17:32:56.925: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-934c36ef-8c14-4a1a-9c41-bf468c977df0" in namespace "projected-914" to be "success or failure"
Aug 21 17:32:56.942: INFO: Pod "pod-projected-configmaps-934c36ef-8c14-4a1a-9c41-bf468c977df0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.030923ms
Aug 21 17:32:58.969: INFO: Pod "pod-projected-configmaps-934c36ef-8c14-4a1a-9c41-bf468c977df0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.042604128s
STEP: Saw pod success
Aug 21 17:32:58.969: INFO: Pod "pod-projected-configmaps-934c36ef-8c14-4a1a-9c41-bf468c977df0" satisfied condition "success or failure"
Aug 21 17:32:58.979: INFO: Trying to get logs from node 10.188.240.202 pod pod-projected-configmaps-934c36ef-8c14-4a1a-9c41-bf468c977df0 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 17:32:59.042: INFO: Waiting for pod pod-projected-configmaps-934c36ef-8c14-4a1a-9c41-bf468c977df0 to disappear
Aug 21 17:32:59.051: INFO: Pod pod-projected-configmaps-934c36ef-8c14-4a1a-9c41-bf468c977df0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:32:59.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-914" for this suite.
Aug 21 17:33:07.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:33:09.356: INFO: namespace projected-914 deletion completed in 10.271901491s

• [SLOW TEST:12.745 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:33:09.357: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Aug 21 17:33:09.619: INFO: Waiting up to 5m0s for pod "pod-36657ca6-9c54-46c3-8d6f-52aaa3517de2" in namespace "emptydir-3447" to be "success or failure"
Aug 21 17:33:09.634: INFO: Pod "pod-36657ca6-9c54-46c3-8d6f-52aaa3517de2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.975301ms
Aug 21 17:33:11.664: INFO: Pod "pod-36657ca6-9c54-46c3-8d6f-52aaa3517de2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.045243265s
STEP: Saw pod success
Aug 21 17:33:11.664: INFO: Pod "pod-36657ca6-9c54-46c3-8d6f-52aaa3517de2" satisfied condition "success or failure"
Aug 21 17:33:11.680: INFO: Trying to get logs from node 10.188.240.202 pod pod-36657ca6-9c54-46c3-8d6f-52aaa3517de2 container test-container: <nil>
STEP: delete the pod
Aug 21 17:33:11.770: INFO: Waiting for pod pod-36657ca6-9c54-46c3-8d6f-52aaa3517de2 to disappear
Aug 21 17:33:11.780: INFO: Pod pod-36657ca6-9c54-46c3-8d6f-52aaa3517de2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:33:11.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3447" for this suite.
Aug 21 17:33:19.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:33:21.877: INFO: namespace emptydir-3447 deletion completed in 10.076767017s

• [SLOW TEST:12.521 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:33:21.880: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Aug 21 17:33:22.073: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:33:26.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4112" for this suite.
Aug 21 17:33:36.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:33:38.729: INFO: namespace init-container-4112 deletion completed in 12.261847863s

• [SLOW TEST:16.849 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:33:38.732: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:33:38.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4" for this suite.
Aug 21 17:33:47.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:33:49.405: INFO: namespace custom-resource-definition-4 deletion completed in 10.398664091s

• [SLOW TEST:10.673 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:33:49.408: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Aug 21 17:34:19.766: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0821 17:34:19.766792      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 21 17:34:19.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2487" for this suite.
Aug 21 17:34:27.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:34:30.022: INFO: namespace gc-2487 deletion completed in 10.209505859s

• [SLOW TEST:40.614 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:34:30.022: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Aug 21 17:34:30.244: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-192619208 proxy --unix-socket=/tmp/kubectl-proxy-unix142915555/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:34:30.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3396" for this suite.
Aug 21 17:34:38.448: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:34:40.920: INFO: namespace kubectl-3396 deletion completed in 10.547332936s

• [SLOW TEST:10.898 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:34:40.921: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-fjqc
STEP: Creating a pod to test atomic-volume-subpath
Aug 21 17:34:41.243: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-fjqc" in namespace "subpath-5252" to be "success or failure"
Aug 21 17:34:41.255: INFO: Pod "pod-subpath-test-secret-fjqc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.96828ms
Aug 21 17:34:43.267: INFO: Pod "pod-subpath-test-secret-fjqc": Phase="Running", Reason="", readiness=true. Elapsed: 2.024515691s
Aug 21 17:34:45.279: INFO: Pod "pod-subpath-test-secret-fjqc": Phase="Running", Reason="", readiness=true. Elapsed: 4.03604213s
Aug 21 17:34:47.290: INFO: Pod "pod-subpath-test-secret-fjqc": Phase="Running", Reason="", readiness=true. Elapsed: 6.047470691s
Aug 21 17:34:49.301: INFO: Pod "pod-subpath-test-secret-fjqc": Phase="Running", Reason="", readiness=true. Elapsed: 8.058653604s
Aug 21 17:34:51.314: INFO: Pod "pod-subpath-test-secret-fjqc": Phase="Running", Reason="", readiness=true. Elapsed: 10.071555019s
Aug 21 17:34:53.326: INFO: Pod "pod-subpath-test-secret-fjqc": Phase="Running", Reason="", readiness=true. Elapsed: 12.083411819s
Aug 21 17:34:55.338: INFO: Pod "pod-subpath-test-secret-fjqc": Phase="Running", Reason="", readiness=true. Elapsed: 14.095117914s
Aug 21 17:34:57.371: INFO: Pod "pod-subpath-test-secret-fjqc": Phase="Running", Reason="", readiness=true. Elapsed: 16.128678787s
Aug 21 17:34:59.383: INFO: Pod "pod-subpath-test-secret-fjqc": Phase="Running", Reason="", readiness=true. Elapsed: 18.140035977s
Aug 21 17:35:01.396: INFO: Pod "pod-subpath-test-secret-fjqc": Phase="Running", Reason="", readiness=true. Elapsed: 20.152951393s
Aug 21 17:35:03.408: INFO: Pod "pod-subpath-test-secret-fjqc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.165625842s
STEP: Saw pod success
Aug 21 17:35:03.408: INFO: Pod "pod-subpath-test-secret-fjqc" satisfied condition "success or failure"
Aug 21 17:35:03.425: INFO: Trying to get logs from node 10.188.240.202 pod pod-subpath-test-secret-fjqc container test-container-subpath-secret-fjqc: <nil>
STEP: delete the pod
Aug 21 17:35:03.538: INFO: Waiting for pod pod-subpath-test-secret-fjqc to disappear
Aug 21 17:35:03.552: INFO: Pod pod-subpath-test-secret-fjqc no longer exists
STEP: Deleting pod pod-subpath-test-secret-fjqc
Aug 21 17:35:03.552: INFO: Deleting pod "pod-subpath-test-secret-fjqc" in namespace "subpath-5252"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:35:03.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5252" for this suite.
Aug 21 17:35:11.631: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:35:13.786: INFO: namespace subpath-5252 deletion completed in 10.205219552s

• [SLOW TEST:32.866 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:35:13.789: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 17:35:14.121: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-f1be51a3-3f27-496a-bec4-0f34e6d605cd
STEP: Creating secret with name s-test-opt-upd-8159366c-7a82-4d53-b498-79a2852104aa
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-f1be51a3-3f27-496a-bec4-0f34e6d605cd
STEP: Updating secret s-test-opt-upd-8159366c-7a82-4d53-b498-79a2852104aa
STEP: Creating secret with name s-test-opt-create-9b3165e8-56d1-45df-b08d-42f4873144cb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:36:42.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8249" for this suite.
Aug 21 17:37:16.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:37:18.363: INFO: namespace projected-8249 deletion completed in 36.16894252s

• [SLOW TEST:124.575 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:37:18.364: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Aug 21 17:37:18.642: INFO: Waiting up to 5m0s for pod "downward-api-2d7361cd-4717-41e1-abf6-ff825bb0ed5d" in namespace "downward-api-9856" to be "success or failure"
Aug 21 17:37:18.654: INFO: Pod "downward-api-2d7361cd-4717-41e1-abf6-ff825bb0ed5d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.093419ms
Aug 21 17:37:20.669: INFO: Pod "downward-api-2d7361cd-4717-41e1-abf6-ff825bb0ed5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026788533s
STEP: Saw pod success
Aug 21 17:37:20.669: INFO: Pod "downward-api-2d7361cd-4717-41e1-abf6-ff825bb0ed5d" satisfied condition "success or failure"
Aug 21 17:37:20.680: INFO: Trying to get logs from node 10.188.240.202 pod downward-api-2d7361cd-4717-41e1-abf6-ff825bb0ed5d container dapi-container: <nil>
STEP: delete the pod
Aug 21 17:37:20.744: INFO: Waiting for pod downward-api-2d7361cd-4717-41e1-abf6-ff825bb0ed5d to disappear
Aug 21 17:37:20.754: INFO: Pod downward-api-2d7361cd-4717-41e1-abf6-ff825bb0ed5d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:37:20.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9856" for this suite.
Aug 21 17:37:28.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:37:30.952: INFO: namespace downward-api-9856 deletion completed in 10.169758431s

• [SLOW TEST:12.589 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:37:30.952: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Aug 21 17:37:31.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 api-versions'
Aug 21 17:37:31.325: INFO: stderr: ""
Aug 21 17:37:31.325: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1beta1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.openshift.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:37:31.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9034" for this suite.
Aug 21 17:37:39.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:37:41.657: INFO: namespace kubectl-9034 deletion completed in 10.280899154s

• [SLOW TEST:10.705 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:37:41.658: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Aug 21 17:37:41.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 create -f - --namespace=kubectl-9168'
Aug 21 17:37:42.745: INFO: stderr: ""
Aug 21 17:37:42.745: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 21 17:37:43.760: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 17:37:43.760: INFO: Found 0 / 1
Aug 21 17:37:44.759: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 17:37:44.759: INFO: Found 0 / 1
Aug 21 17:37:45.759: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 17:37:45.759: INFO: Found 1 / 1
Aug 21 17:37:45.759: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Aug 21 17:37:45.772: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 17:37:45.772: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 21 17:37:45.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 patch pod redis-master-g7rvl --namespace=kubectl-9168 -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 21 17:37:45.978: INFO: stderr: ""
Aug 21 17:37:45.978: INFO: stdout: "pod/redis-master-g7rvl patched\n"
STEP: checking annotations
Aug 21 17:37:45.990: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 17:37:45.990: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:37:45.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9168" for this suite.
Aug 21 17:38:20.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:38:22.375: INFO: namespace kubectl-9168 deletion completed in 36.324520822s

• [SLOW TEST:40.717 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:38:22.375: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-83a0ec0c-ddbd-4446-b8ca-0857a2a898dd
STEP: Creating a pod to test consume configMaps
Aug 21 17:38:22.642: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2c7de8f3-b0a0-4b3e-9bdb-4471c2fd6361" in namespace "projected-9708" to be "success or failure"
Aug 21 17:38:22.656: INFO: Pod "pod-projected-configmaps-2c7de8f3-b0a0-4b3e-9bdb-4471c2fd6361": Phase="Pending", Reason="", readiness=false. Elapsed: 13.807397ms
Aug 21 17:38:24.668: INFO: Pod "pod-projected-configmaps-2c7de8f3-b0a0-4b3e-9bdb-4471c2fd6361": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026084795s
STEP: Saw pod success
Aug 21 17:38:24.668: INFO: Pod "pod-projected-configmaps-2c7de8f3-b0a0-4b3e-9bdb-4471c2fd6361" satisfied condition "success or failure"
Aug 21 17:38:24.683: INFO: Trying to get logs from node 10.188.240.202 pod pod-projected-configmaps-2c7de8f3-b0a0-4b3e-9bdb-4471c2fd6361 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 17:38:24.779: INFO: Waiting for pod pod-projected-configmaps-2c7de8f3-b0a0-4b3e-9bdb-4471c2fd6361 to disappear
Aug 21 17:38:24.792: INFO: Pod pod-projected-configmaps-2c7de8f3-b0a0-4b3e-9bdb-4471c2fd6361 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:38:24.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9708" for this suite.
Aug 21 17:38:32.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:38:35.205: INFO: namespace projected-9708 deletion completed in 10.365799801s

• [SLOW TEST:12.830 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:38:35.205: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-ktpb
STEP: Creating a pod to test atomic-volume-subpath
Aug 21 17:38:35.515: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ktpb" in namespace "subpath-8385" to be "success or failure"
Aug 21 17:38:35.526: INFO: Pod "pod-subpath-test-configmap-ktpb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.33146ms
Aug 21 17:38:37.539: INFO: Pod "pod-subpath-test-configmap-ktpb": Phase="Running", Reason="", readiness=true. Elapsed: 2.024576581s
Aug 21 17:38:39.551: INFO: Pod "pod-subpath-test-configmap-ktpb": Phase="Running", Reason="", readiness=true. Elapsed: 4.035922233s
Aug 21 17:38:41.566: INFO: Pod "pod-subpath-test-configmap-ktpb": Phase="Running", Reason="", readiness=true. Elapsed: 6.051157096s
Aug 21 17:38:43.590: INFO: Pod "pod-subpath-test-configmap-ktpb": Phase="Running", Reason="", readiness=true. Elapsed: 8.075166348s
Aug 21 17:38:45.603: INFO: Pod "pod-subpath-test-configmap-ktpb": Phase="Running", Reason="", readiness=true. Elapsed: 10.088217767s
Aug 21 17:38:47.614: INFO: Pod "pod-subpath-test-configmap-ktpb": Phase="Running", Reason="", readiness=true. Elapsed: 12.099648755s
Aug 21 17:38:49.626: INFO: Pod "pod-subpath-test-configmap-ktpb": Phase="Running", Reason="", readiness=true. Elapsed: 14.111528675s
Aug 21 17:38:51.792: INFO: Pod "pod-subpath-test-configmap-ktpb": Phase="Running", Reason="", readiness=true. Elapsed: 16.277090544s
Aug 21 17:38:53.806: INFO: Pod "pod-subpath-test-configmap-ktpb": Phase="Running", Reason="", readiness=true. Elapsed: 18.290914262s
Aug 21 17:38:55.819: INFO: Pod "pod-subpath-test-configmap-ktpb": Phase="Running", Reason="", readiness=true. Elapsed: 20.304056124s
Aug 21 17:38:57.830: INFO: Pod "pod-subpath-test-configmap-ktpb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.315596332s
STEP: Saw pod success
Aug 21 17:38:57.830: INFO: Pod "pod-subpath-test-configmap-ktpb" satisfied condition "success or failure"
Aug 21 17:38:57.841: INFO: Trying to get logs from node 10.188.240.202 pod pod-subpath-test-configmap-ktpb container test-container-subpath-configmap-ktpb: <nil>
STEP: delete the pod
Aug 21 17:38:57.920: INFO: Waiting for pod pod-subpath-test-configmap-ktpb to disappear
Aug 21 17:38:57.930: INFO: Pod pod-subpath-test-configmap-ktpb no longer exists
STEP: Deleting pod pod-subpath-test-configmap-ktpb
Aug 21 17:38:57.931: INFO: Deleting pod "pod-subpath-test-configmap-ktpb" in namespace "subpath-8385"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:38:57.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8385" for this suite.
Aug 21 17:39:06.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:39:08.168: INFO: namespace subpath-8385 deletion completed in 10.188960459s

• [SLOW TEST:32.962 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:39:08.168: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Aug 21 17:39:10.440: INFO: Pod pod-hostip-f4e94dca-afd8-4497-a46c-c10cda29d04e has hostIP: 10.188.240.202
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:39:10.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-564" for this suite.
Aug 21 17:39:24.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:39:26.638: INFO: namespace pods-564 deletion completed in 16.166049119s

• [SLOW TEST:18.470 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:39:26.638: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 17:39:26.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 create -f - --namespace=kubectl-4686'
Aug 21 17:39:27.755: INFO: stderr: ""
Aug 21 17:39:27.755: INFO: stdout: "replicationcontroller/redis-master created\n"
Aug 21 17:39:27.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 create -f - --namespace=kubectl-4686'
Aug 21 17:39:28.244: INFO: stderr: ""
Aug 21 17:39:28.244: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 21 17:39:29.261: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 17:39:29.261: INFO: Found 1 / 1
Aug 21 17:39:29.261: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 21 17:39:29.275: INFO: Selector matched 1 pods for map[app:redis]
Aug 21 17:39:29.275: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 21 17:39:29.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 describe pod redis-master-ltd6c --namespace=kubectl-4686'
Aug 21 17:39:29.465: INFO: stderr: ""
Aug 21 17:39:29.465: INFO: stdout: "Name:         redis-master-ltd6c\nNamespace:    kubectl-4686\nPriority:     0\nNode:         10.188.240.202/10.188.240.202\nStart Time:   Fri, 21 Aug 2020 17:39:27 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 172.30.245.120/32\n              cni.projectcalico.org/podIPs: 172.30.245.120/32\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"k8s-pod-network\",\n                    \"ips\": [\n                        \"172.30.245.120\"\n                    ],\n                    \"dns\": {}\n                }]\n              openshift.io/scc: privileged\nStatus:       Running\nIP:           172.30.245.120\nIPs:\n  IP:           172.30.245.120\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   cri-o://3ae74385e4bcf6b5fc7788e7ff0b8069fb8dc2afca99baadf10a50a6d3e74770\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 21 Aug 2020 17:39:28 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-kg5d9 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-kg5d9:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-kg5d9\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                     Message\n  ----    ------     ----       ----                     -------\n  Normal  Scheduled  <unknown>  default-scheduler        Successfully assigned kubectl-4686/redis-master-ltd6c to 10.188.240.202\n  Normal  Pulled     1s         kubelet, 10.188.240.202  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    1s         kubelet, 10.188.240.202  Created container redis-master\n  Normal  Started    0s         kubelet, 10.188.240.202  Started container redis-master\n"
Aug 21 17:39:29.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 describe rc redis-master --namespace=kubectl-4686'
Aug 21 17:39:29.724: INFO: stderr: ""
Aug 21 17:39:29.724: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-4686\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-ltd6c\n"
Aug 21 17:39:29.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 describe service redis-master --namespace=kubectl-4686'
Aug 21 17:39:29.900: INFO: stderr: ""
Aug 21 17:39:29.900: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-4686\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.21.96.182\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.30.245.120:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 21 17:39:29.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 describe node 10.188.240.202'
Aug 21 17:39:30.269: INFO: stderr: ""
Aug 21 17:39:30.269: INFO: stdout: "Name:               10.188.240.202\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east\n                    failure-domain.beta.kubernetes.io/zone=wdc06\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=169.63.148.208\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.188.240.202\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_7_64\n                    ibm-cloud.kubernetes.io/region=us-east\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-bsvtbd8w0gsskpblb590-kubee2epvgn-default-00000319\n                    ibm-cloud.kubernetes.io/worker-pool-id=bsvtbd8w0gsskpblb590-fec9df0\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.3.31_1534_openshift\n                    ibm-cloud.kubernetes.io/zone=wdc06\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.188.240.202\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    node.openshift.io/os_id=rhel\n                    privateVLAN=2722950\n                    publicVLAN=2722948\n                    topology.kubernetes.io/region=us-east\n                    topology.kubernetes.io/zone=wdc06\nAnnotations:        projectcalico.org/IPv4Address: 10.188.240.202/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.245.64\nCreationTimestamp:  Fri, 21 Aug 2020 14:24:12 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 21 Aug 2020 14:25:30 +0000   Fri, 21 Aug 2020 14:25:30 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 21 Aug 2020 17:38:54 +0000   Fri, 21 Aug 2020 14:24:12 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 21 Aug 2020 17:38:54 +0000   Fri, 21 Aug 2020 14:24:12 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 21 Aug 2020 17:38:54 +0000   Fri, 21 Aug 2020 14:24:12 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 21 Aug 2020 17:38:54 +0000   Fri, 21 Aug 2020 14:25:32 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.188.240.202\n  ExternalIP:  169.63.148.208\n  Hostname:    10.188.240.202\nCapacity:\n cpu:                4\n ephemeral-storage:  103078840Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16260876Ki\n pods:               110\nAllocatable:\n cpu:                3910m\n ephemeral-storage:  100275095474\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             13484812Ki\n pods:               110\nSystem Info:\n Machine ID:                              e863ea6680d7478f986c0763fd79535a\n System UUID:                             BED090C0-2AD8-73EA-148C-9574466EC2F7\n Boot ID:                                 926ccafa-3072-4cb7-b23b-9cdfadb8c96d\n Kernel Version:                          3.10.0-1127.18.2.el7.x86_64\n OS Image:                                Red Hat\n Operating System:                        linux\n Architecture:                            amd64\n Container Runtime Version:               cri-o://1.16.6-18.rhaos4.3.git538d861.el7\n Kubelet Version:                         v1.16.2+554af56\n Kube-Proxy Version:                      v1.16.2+554af56\nProviderID:                               ibm://fee034388aa6435883a1f720010ab3a2///bsvtbd8w0gsskpblb590/kube-bsvtbd8w0gsskpblb590-kubee2epvgn-default-00000319\nNon-terminated Pods:                      (14 in total)\n  Namespace                               Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                               ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                           calico-node-lfw9m                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h14m\n  calico-system                           calico-typha-6c986fbc8c-mtt9h                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h12m\n  kube-system                             ibm-keepalived-watcher-hgfkl                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         3h15m\n  kube-system                             ibm-master-proxy-static-10.188.240.202                     25m (0%)      300m (7%)   32M (0%)         512M (3%)      3h15m\n  kube-system                             ibmcloud-block-storage-driver-wmdvn                        50m (1%)      300m (7%)   100Mi (0%)       300Mi (2%)     3h15m\n  kubectl-4686                            redis-master-ltd6c                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\n  openshift-cluster-node-tuning-operator  tuned-hfsdd                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h13m\n  openshift-dns                           dns-default-xn4tw                                          110m (2%)     0 (0%)      70Mi (0%)        512Mi (3%)     6m49s\n  openshift-image-registry                node-ca-2zlsz                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         3h11m\n  openshift-kube-proxy                    openshift-kube-proxy-dqhxm                                 100m (2%)     0 (0%)      200Mi (1%)       0 (0%)         3h14m\n  openshift-monitoring                    node-exporter-vw6s8                                        9m (0%)       0 (0%)      210Mi (1%)       0 (0%)         3h12m\n  openshift-multus                        multus-59vjb                                               10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         3h14m\n  openshift-multus                        multus-admission-controller-t7nxb                          10m (0%)      0 (0%)      0 (0%)           0 (0%)         6m59s\n  sonobuoy                                sonobuoy-systemd-logs-daemon-set-5cea973cc7904f8c-nfmgc    0 (0%)        0 (0%)      0 (0%)           0 (0%)         120m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests       Limits\n  --------           --------       ------\n  cpu                339m (8%)      600m (15%)\n  memory             850450Ki (6%)  1363443712 (9%)\n  ephemeral-storage  0 (0%)         0 (0%)\nEvents:              <none>\n"
Aug 21 17:39:30.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 describe namespace kubectl-4686'
Aug 21 17:39:30.510: INFO: stderr: ""
Aug 21 17:39:30.510: INFO: stdout: "Name:         kubectl-4686\nLabels:       e2e-framework=kubectl\n              e2e-run=89df089f-3ac2-455a-b770-fd122f035cbd\nAnnotations:  openshift.io/sa.scc.mcs: s0:c66,c40\n              openshift.io/sa.scc.supplemental-groups: 1004370000/10000\n              openshift.io/sa.scc.uid-range: 1004370000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:39:30.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4686" for this suite.
Aug 21 17:39:46.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:39:48.735: INFO: namespace kubectl-4686 deletion completed in 18.194059989s

• [SLOW TEST:22.097 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:39:48.735: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 pods, got 1 pods
STEP: Gathering metrics
W0821 17:39:50.633767      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 21 17:39:50.634: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:39:50.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8643" for this suite.
Aug 21 17:39:58.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:40:01.064: INFO: namespace gc-8643 deletion completed in 10.400299094s

• [SLOW TEST:12.330 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:40:01.065: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Aug 21 17:40:01.349: INFO: Waiting up to 5m0s for pod "downward-api-5d962e2c-93e6-4dec-adba-f138e41bae21" in namespace "downward-api-4829" to be "success or failure"
Aug 21 17:40:01.366: INFO: Pod "downward-api-5d962e2c-93e6-4dec-adba-f138e41bae21": Phase="Pending", Reason="", readiness=false. Elapsed: 12.546298ms
Aug 21 17:40:03.379: INFO: Pod "downward-api-5d962e2c-93e6-4dec-adba-f138e41bae21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02549729s
STEP: Saw pod success
Aug 21 17:40:03.379: INFO: Pod "downward-api-5d962e2c-93e6-4dec-adba-f138e41bae21" satisfied condition "success or failure"
Aug 21 17:40:03.390: INFO: Trying to get logs from node 10.188.240.202 pod downward-api-5d962e2c-93e6-4dec-adba-f138e41bae21 container dapi-container: <nil>
STEP: delete the pod
Aug 21 17:40:03.460: INFO: Waiting for pod downward-api-5d962e2c-93e6-4dec-adba-f138e41bae21 to disappear
Aug 21 17:40:03.472: INFO: Pod downward-api-5d962e2c-93e6-4dec-adba-f138e41bae21 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:40:03.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4829" for this suite.
Aug 21 17:40:11.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:40:13.646: INFO: namespace downward-api-4829 deletion completed in 10.142572469s

• [SLOW TEST:12.580 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:40:13.648: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 21 17:40:13.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-8071'
Aug 21 17:40:14.095: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 21 17:40:14.095: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Aug 21 17:40:14.110: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Aug 21 17:40:14.150: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Aug 21 17:40:14.171: INFO: scanned /root for discovery docs: <nil>
Aug 21 17:40:14.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-8071'
Aug 21 17:40:30.386: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug 21 17:40:30.386: INFO: stdout: "Created e2e-test-httpd-rc-8e28a4632be0783ba2aaf11c050cdcc3\nScaling up e2e-test-httpd-rc-8e28a4632be0783ba2aaf11c050cdcc3 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-8e28a4632be0783ba2aaf11c050cdcc3 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-8e28a4632be0783ba2aaf11c050cdcc3 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Aug 21 17:40:30.386: INFO: stdout: "Created e2e-test-httpd-rc-8e28a4632be0783ba2aaf11c050cdcc3\nScaling up e2e-test-httpd-rc-8e28a4632be0783ba2aaf11c050cdcc3 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-8e28a4632be0783ba2aaf11c050cdcc3 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-8e28a4632be0783ba2aaf11c050cdcc3 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Aug 21 17:40:30.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-8071'
Aug 21 17:40:30.620: INFO: stderr: ""
Aug 21 17:40:30.620: INFO: stdout: "e2e-test-httpd-rc-8e28a4632be0783ba2aaf11c050cdcc3-5vj56 "
Aug 21 17:40:30.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods e2e-test-httpd-rc-8e28a4632be0783ba2aaf11c050cdcc3-5vj56 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8071'
Aug 21 17:40:30.783: INFO: stderr: ""
Aug 21 17:40:30.783: INFO: stdout: "true"
Aug 21 17:40:30.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 get pods e2e-test-httpd-rc-8e28a4632be0783ba2aaf11c050cdcc3-5vj56 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8071'
Aug 21 17:40:30.978: INFO: stderr: ""
Aug 21 17:40:30.979: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Aug 21 17:40:30.979: INFO: e2e-test-httpd-rc-8e28a4632be0783ba2aaf11c050cdcc3-5vj56 is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Aug 21 17:40:30.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 delete rc e2e-test-httpd-rc --namespace=kubectl-8071'
Aug 21 17:40:31.196: INFO: stderr: ""
Aug 21 17:40:31.196: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:40:31.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8071" for this suite.
Aug 21 17:40:47.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:40:49.440: INFO: namespace kubectl-8071 deletion completed in 18.207114721s

• [SLOW TEST:35.792 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:40:49.440: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 21 17:40:50.712: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6c5f5b95-2fae-4a85-9513-86f60e0d33e4" in namespace "projected-5639" to be "success or failure"
Aug 21 17:40:50.733: INFO: Pod "downwardapi-volume-6c5f5b95-2fae-4a85-9513-86f60e0d33e4": Phase="Pending", Reason="", readiness=false. Elapsed: 21.160239ms
Aug 21 17:40:52.748: INFO: Pod "downwardapi-volume-6c5f5b95-2fae-4a85-9513-86f60e0d33e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0359621s
Aug 21 17:40:54.761: INFO: Pod "downwardapi-volume-6c5f5b95-2fae-4a85-9513-86f60e0d33e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049210958s
STEP: Saw pod success
Aug 21 17:40:54.761: INFO: Pod "downwardapi-volume-6c5f5b95-2fae-4a85-9513-86f60e0d33e4" satisfied condition "success or failure"
Aug 21 17:40:54.773: INFO: Trying to get logs from node 10.188.240.202 pod downwardapi-volume-6c5f5b95-2fae-4a85-9513-86f60e0d33e4 container client-container: <nil>
STEP: delete the pod
Aug 21 17:40:54.859: INFO: Waiting for pod downwardapi-volume-6c5f5b95-2fae-4a85-9513-86f60e0d33e4 to disappear
Aug 21 17:40:54.871: INFO: Pod downwardapi-volume-6c5f5b95-2fae-4a85-9513-86f60e0d33e4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:40:54.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5639" for this suite.
Aug 21 17:41:02.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:41:05.116: INFO: namespace projected-5639 deletion completed in 10.201717463s

• [SLOW TEST:15.676 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:41:05.119: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-4083
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-4083
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4083
Aug 21 17:41:05.382: INFO: Found 0 stateful pods, waiting for 1
Aug 21 17:41:15.396: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Aug 21 17:41:15.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 21 17:41:15.799: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 21 17:41:15.799: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 21 17:41:15.799: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 21 17:41:15.814: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 21 17:41:25.828: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 21 17:41:25.828: INFO: Waiting for statefulset status.replicas updated to 0
Aug 21 17:41:25.877: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 21 17:41:25.877: INFO: ss-0  10.188.240.202  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:05 +0000 UTC  }]
Aug 21 17:41:25.877: INFO: 
Aug 21 17:41:25.877: INFO: StatefulSet ss has not reached scale 3, at 1
Aug 21 17:41:26.899: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.985700387s
Aug 21 17:41:27.915: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.964053036s
Aug 21 17:41:28.953: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.947592427s
Aug 21 17:41:29.980: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.910185522s
Aug 21 17:41:30.995: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.882898879s
Aug 21 17:41:32.011: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.867912946s
Aug 21 17:41:33.075: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.850161234s
Aug 21 17:41:34.089: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.788241263s
Aug 21 17:41:35.107: INFO: Verifying statefulset ss doesn't scale past 3 for another 774.411595ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4083
Aug 21 17:41:36.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:41:36.535: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 21 17:41:36.535: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 21 17:41:36.535: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 21 17:41:36.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:41:37.044: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 21 17:41:37.045: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 21 17:41:37.045: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 21 17:41:37.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:41:37.554: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 21 17:41:37.554: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 21 17:41:37.554: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 21 17:41:37.567: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 17:41:37.567: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 17:41:37.567: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Aug 21 17:41:37.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 21 17:41:38.016: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 21 17:41:38.016: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 21 17:41:38.016: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 21 17:41:38.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 21 17:41:38.591: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 21 17:41:38.591: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 21 17:41:38.591: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 21 17:41:38.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 21 17:41:39.041: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 21 17:41:39.041: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 21 17:41:39.041: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 21 17:41:39.041: INFO: Waiting for statefulset status.replicas updated to 0
Aug 21 17:41:39.054: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Aug 21 17:41:49.116: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 21 17:41:49.116: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 21 17:41:49.116: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 21 17:41:49.166: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 21 17:41:49.166: INFO: ss-0  10.188.240.202  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:05 +0000 UTC  }]
Aug 21 17:41:49.166: INFO: ss-1  10.188.240.222  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:25 +0000 UTC  }]
Aug 21 17:41:49.166: INFO: ss-2  10.188.240.202  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:25 +0000 UTC  }]
Aug 21 17:41:49.166: INFO: 
Aug 21 17:41:49.166: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 21 17:41:50.178: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 21 17:41:50.178: INFO: ss-0  10.188.240.202  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:05 +0000 UTC  }]
Aug 21 17:41:50.179: INFO: ss-1  10.188.240.222  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:25 +0000 UTC  }]
Aug 21 17:41:50.179: INFO: ss-2  10.188.240.202  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:25 +0000 UTC  }]
Aug 21 17:41:50.179: INFO: 
Aug 21 17:41:50.179: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 21 17:41:51.202: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 21 17:41:51.202: INFO: ss-0  10.188.240.202  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:05 +0000 UTC  }]
Aug 21 17:41:51.202: INFO: ss-1  10.188.240.222  Running  0s     [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:25 +0000 UTC  }]
Aug 21 17:41:51.202: INFO: ss-2  10.188.240.202  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:25 +0000 UTC  }]
Aug 21 17:41:51.202: INFO: 
Aug 21 17:41:51.202: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 21 17:41:52.213: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 21 17:41:52.213: INFO: ss-2  10.188.240.202  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:25 +0000 UTC  }]
Aug 21 17:41:52.213: INFO: 
Aug 21 17:41:52.213: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 21 17:41:53.225: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 21 17:41:53.225: INFO: ss-2  10.188.240.202  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:25 +0000 UTC  }]
Aug 21 17:41:53.225: INFO: 
Aug 21 17:41:53.225: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 21 17:41:54.239: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 21 17:41:54.239: INFO: ss-2  10.188.240.202  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:25 +0000 UTC  }]
Aug 21 17:41:54.239: INFO: 
Aug 21 17:41:54.239: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 21 17:41:55.254: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 21 17:41:55.254: INFO: ss-2  10.188.240.202  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:25 +0000 UTC  }]
Aug 21 17:41:55.254: INFO: 
Aug 21 17:41:55.254: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 21 17:41:56.267: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 21 17:41:56.267: INFO: ss-2  10.188.240.202  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:25 +0000 UTC  }]
Aug 21 17:41:56.267: INFO: 
Aug 21 17:41:56.267: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 21 17:41:57.280: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 21 17:41:57.280: INFO: ss-2  10.188.240.202  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:25 +0000 UTC  }]
Aug 21 17:41:57.280: INFO: 
Aug 21 17:41:57.280: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 21 17:41:58.292: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 21 17:41:58.292: INFO: ss-2  10.188.240.202  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-21 17:41:25 +0000 UTC  }]
Aug 21 17:41:58.294: INFO: 
Aug 21 17:41:58.294: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4083
Aug 21 17:41:59.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:41:59.640: INFO: rc: 1
Aug 21 17:41:59.641: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  error: unable to upgrade connection: container not found ("webserver")
 [] <nil> 0xc00399f650 exit status 1 <nil> <nil> true [0xc001d8c9b8 0xc001d8cb68 0xc001d8cc08] [0xc001d8c9b8 0xc001d8cb68 0xc001d8cc08] [0xc001d8cb20 0xc001d8cbc0] [0x10efce0 0x10efce0] 0xc003d44d20 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Aug 21 17:42:09.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:42:09.811: INFO: rc: 1
Aug 21 17:42:09.811: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc005f38750 exit status 1 <nil> <nil> true [0xc0024480a0 0xc0024480b8 0xc0024480d0] [0xc0024480a0 0xc0024480b8 0xc0024480d0] [0xc0024480b0 0xc0024480c8] [0x10efce0 0x10efce0] 0xc001e68360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:42:19.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:42:19.988: INFO: rc: 1
Aug 21 17:42:19.988: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc005f38a50 exit status 1 <nil> <nil> true [0xc0024480d8 0xc0024480f8 0xc002448110] [0xc0024480d8 0xc0024480f8 0xc002448110] [0xc0024480f0 0xc002448108] [0x10efce0 0x10efce0] 0xc001e68780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:42:29.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:42:30.225: INFO: rc: 1
Aug 21 17:42:30.225: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc005f38d50 exit status 1 <nil> <nil> true [0xc002448118 0xc002448130 0xc002448148] [0xc002448118 0xc002448130 0xc002448148] [0xc002448128 0xc002448140] [0x10efce0 0x10efce0] 0xc001e68ae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:42:40.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:42:40.378: INFO: rc: 1
Aug 21 17:42:40.378: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002e76e70 exit status 1 <nil> <nil> true [0xc000010d28 0xc000587f38 0xc0032c8020] [0xc000010d28 0xc000587f38 0xc0032c8020] [0xc000587e40 0xc0032c8018] [0x10efce0 0x10efce0] 0xc005e28360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:42:50.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:42:50.568: INFO: rc: 1
Aug 21 17:42:50.568: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc005f39050 exit status 1 <nil> <nil> true [0xc002448150 0xc002448168 0xc002448180] [0xc002448150 0xc002448168 0xc002448180] [0xc002448160 0xc002448178] [0x10efce0 0x10efce0] 0xc001e68e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:43:00.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:43:00.763: INFO: rc: 1
Aug 21 17:43:00.763: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc005f39380 exit status 1 <nil> <nil> true [0xc002448188 0xc0024481a0 0xc0024481b8] [0xc002448188 0xc0024481a0 0xc0024481b8] [0xc002448198 0xc0024481b0] [0x10efce0 0x10efce0] 0xc001e691a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:43:10.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:43:10.931: INFO: rc: 1
Aug 21 17:43:10.931: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00399f980 exit status 1 <nil> <nil> true [0xc001d8cc30 0xc001d8cd58 0xc001d8ce20] [0xc001d8cc30 0xc001d8cd58 0xc001d8ce20] [0xc001d8ccf8 0xc001d8ce08] [0x10efce0 0x10efce0] 0xc003d45080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:43:20.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:43:21.125: INFO: rc: 1
Aug 21 17:43:21.125: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc005f396b0 exit status 1 <nil> <nil> true [0xc0024481c0 0xc0024481d8 0xc0024481f0] [0xc0024481c0 0xc0024481d8 0xc0024481f0] [0xc0024481d0 0xc0024481e8] [0x10efce0 0x10efce0] 0xc001e69500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:43:31.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:43:31.325: INFO: rc: 1
Aug 21 17:43:31.326: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002e771a0 exit status 1 <nil> <nil> true [0xc0032c8028 0xc0032c8060 0xc0032c8078] [0xc0032c8028 0xc0032c8060 0xc0032c8078] [0xc0032c8058 0xc0032c8070] [0x10efce0 0x10efce0] 0xc005e286c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:43:41.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:43:41.498: INFO: rc: 1
Aug 21 17:43:41.499: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002e774d0 exit status 1 <nil> <nil> true [0xc0032c8080 0xc0032c8098 0xc0032c8108] [0xc0032c8080 0xc0032c8098 0xc0032c8108] [0xc0032c8090 0xc0032c80e0] [0x10efce0 0x10efce0] 0xc005e28a20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:43:51.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:43:51.681: INFO: rc: 1
Aug 21 17:43:51.681: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003120360 exit status 1 <nil> <nil> true [0xc000587e40 0xc000010d28 0xc0004e5618] [0xc000587e40 0xc000010d28 0xc0004e5618] [0xc000010628 0xc0004e5050] [0x10efce0 0x10efce0] 0xc001f40a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:44:01.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:44:01.861: INFO: rc: 1
Aug 21 17:44:01.861: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002ada630 exit status 1 <nil> <nil> true [0xc000d7ea40 0xc000d7f1b0 0xc000d7f4e0] [0xc000d7ea40 0xc000d7f1b0 0xc000d7f4e0] [0xc000d7f0e8 0xc000d7f4b0] [0x10efce0 0x10efce0] 0xc003d44360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:44:11.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:44:12.032: INFO: rc: 1
Aug 21 17:44:12.032: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00399e2d0 exit status 1 <nil> <nil> true [0xc001d8c010 0xc001d8c108 0xc001d8c1e8] [0xc001d8c010 0xc001d8c108 0xc001d8c1e8] [0xc001d8c078 0xc001d8c160] [0x10efce0 0x10efce0] 0xc005e282a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:44:22.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:44:22.208: INFO: rc: 1
Aug 21 17:44:22.208: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00399e5d0 exit status 1 <nil> <nil> true [0xc001d8c218 0xc001d8c368 0xc001d8c440] [0xc001d8c218 0xc001d8c368 0xc001d8c440] [0xc001d8c2f0 0xc001d8c3d8] [0x10efce0 0x10efce0] 0xc005e28600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:44:32.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:44:32.410: INFO: rc: 1
Aug 21 17:44:32.410: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002adaae0 exit status 1 <nil> <nil> true [0xc000d7f5a0 0xc000d7f888 0xc000d7fad0] [0xc000d7f5a0 0xc000d7f888 0xc000d7fad0] [0xc000d7f800 0xc000d7fa18] [0x10efce0 0x10efce0] 0xc003d446c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:44:42.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:44:42.562: INFO: rc: 1
Aug 21 17:44:42.562: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00399e960 exit status 1 <nil> <nil> true [0xc001d8c490 0xc001d8c578 0xc001d8c608] [0xc001d8c490 0xc001d8c578 0xc001d8c608] [0xc001d8c548 0xc001d8c5f0] [0x10efce0 0x10efce0] 0xc005e28c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:44:52.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:44:52.739: INFO: rc: 1
Aug 21 17:44:52.739: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0031206c0 exit status 1 <nil> <nil> true [0xc0004e5bd0 0xc0032c8020 0xc0032c8058] [0xc0004e5bd0 0xc0032c8020 0xc0032c8058] [0xc0032c8018 0xc0032c8040] [0x10efce0 0x10efce0] 0xc001f41920 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:45:02.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:45:02.910: INFO: rc: 1
Aug 21 17:45:02.910: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0031209f0 exit status 1 <nil> <nil> true [0xc0032c8060 0xc0032c8078 0xc0032c8128] [0xc0032c8060 0xc0032c8078 0xc0032c8128] [0xc0032c8070 0xc0032c80d0] [0x10efce0 0x10efce0] 0xc001e68120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:45:12.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:45:13.098: INFO: rc: 1
Aug 21 17:45:13.098: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002adae10 exit status 1 <nil> <nil> true [0xc000d7fb68 0xc000d7fd10 0xc000d7fe58] [0xc000d7fb68 0xc000d7fd10 0xc000d7fe58] [0xc000d7fc00 0xc000d7fdf8] [0x10efce0 0x10efce0] 0xc003d44a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:45:23.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:45:23.269: INFO: rc: 1
Aug 21 17:45:23.269: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00399ecc0 exit status 1 <nil> <nil> true [0xc001d8c668 0xc001d8c788 0xc001d8c850] [0xc001d8c668 0xc001d8c788 0xc001d8c850] [0xc001d8c750 0xc001d8c800] [0x10efce0 0x10efce0] 0xc005e29080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:45:33.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:45:33.444: INFO: rc: 1
Aug 21 17:45:33.444: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003120d20 exit status 1 <nil> <nil> true [0xc0032c8138 0xc0032c8190 0xc0032c81f8] [0xc0032c8138 0xc0032c8190 0xc0032c81f8] [0xc0032c8180 0xc0032c81d8] [0x10efce0 0x10efce0] 0xc001e68480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:45:43.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:45:43.624: INFO: rc: 1
Aug 21 17:45:43.624: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003120330 exit status 1 <nil> <nil> true [0xc0004e5050 0xc000010628 0xc000587e40] [0xc0004e5050 0xc000010628 0xc000587e40] [0xc0004e5bd0 0xc000587da8] [0x10efce0 0x10efce0] 0xc001f40a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:45:53.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:45:53.804: INFO: rc: 1
Aug 21 17:45:53.804: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003120660 exit status 1 <nil> <nil> true [0xc000587f38 0xc0032c8020 0xc0032c8058] [0xc000587f38 0xc0032c8020 0xc0032c8058] [0xc0032c8018 0xc0032c8040] [0x10efce0 0x10efce0] 0xc001f41920 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:46:03.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:46:03.988: INFO: rc: 1
Aug 21 17:46:03.988: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00399e330 exit status 1 <nil> <nil> true [0xc001d8c010 0xc001d8c108 0xc001d8c1e8] [0xc001d8c010 0xc001d8c108 0xc001d8c1e8] [0xc001d8c078 0xc001d8c160] [0x10efce0 0x10efce0] 0xc001e682a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:46:13.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:46:14.167: INFO: rc: 1
Aug 21 17:46:14.167: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002ada600 exit status 1 <nil> <nil> true [0xc000d7ea40 0xc000d7f1b0 0xc000d7f4e0] [0xc000d7ea40 0xc000d7f1b0 0xc000d7f4e0] [0xc000d7f0e8 0xc000d7f4b0] [0x10efce0 0x10efce0] 0xc005e282a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:46:24.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:46:24.334: INFO: rc: 1
Aug 21 17:46:24.334: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00399e6f0 exit status 1 <nil> <nil> true [0xc001d8c218 0xc001d8c368 0xc001d8c440] [0xc001d8c218 0xc001d8c368 0xc001d8c440] [0xc001d8c2f0 0xc001d8c3d8] [0x10efce0 0x10efce0] 0xc001e68660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:46:34.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:46:34.511: INFO: rc: 1
Aug 21 17:46:34.511: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002adaab0 exit status 1 <nil> <nil> true [0xc000d7f5a0 0xc000d7f888 0xc000d7fad0] [0xc000d7f5a0 0xc000d7f888 0xc000d7fad0] [0xc000d7f800 0xc000d7fa18] [0x10efce0 0x10efce0] 0xc005e28600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:46:44.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:46:44.681: INFO: rc: 1
Aug 21 17:46:44.681: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002adaf60 exit status 1 <nil> <nil> true [0xc000d7fe58 0xc000d7ff58 0xc002448008] [0xc000d7fe58 0xc000d7ff58 0xc002448008] [0xc000d7ff08 0xc000d7ffa8] [0x10efce0 0x10efce0] 0xc005e28b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:46:54.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:46:54.927: INFO: rc: 1
Aug 21 17:46:54.927: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002adb290 exit status 1 <nil> <nil> true [0xc002448018 0xc002448048 0xc002448080] [0xc002448018 0xc002448048 0xc002448080] [0xc002448038 0xc002448078] [0x10efce0 0x10efce0] 0xc005e28fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 21 17:47:04.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4083 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:47:05.119: INFO: rc: 1
Aug 21 17:47:05.120: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Aug 21 17:47:05.120: INFO: Scaling statefulset ss to 0
Aug 21 17:47:05.165: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Aug 21 17:47:05.177: INFO: Deleting all statefulset in ns statefulset-4083
Aug 21 17:47:05.190: INFO: Scaling statefulset ss to 0
Aug 21 17:47:05.266: INFO: Waiting for statefulset status.replicas updated to 0
Aug 21 17:47:05.282: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:47:05.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4083" for this suite.
Aug 21 17:47:15.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:47:17.665: INFO: namespace statefulset-4083 deletion completed in 12.268923101s

• [SLOW TEST:372.546 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:47:17.667: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Aug 21 17:47:17.958: INFO: Waiting up to 5m0s for pod "client-containers-07112c6a-831e-44d9-a273-bf873d5523fc" in namespace "containers-5860" to be "success or failure"
Aug 21 17:47:17.971: INFO: Pod "client-containers-07112c6a-831e-44d9-a273-bf873d5523fc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.46688ms
Aug 21 17:47:19.982: INFO: Pod "client-containers-07112c6a-831e-44d9-a273-bf873d5523fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023880349s
Aug 21 17:47:21.993: INFO: Pod "client-containers-07112c6a-831e-44d9-a273-bf873d5523fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035060993s
STEP: Saw pod success
Aug 21 17:47:21.994: INFO: Pod "client-containers-07112c6a-831e-44d9-a273-bf873d5523fc" satisfied condition "success or failure"
Aug 21 17:47:22.007: INFO: Trying to get logs from node 10.188.240.202 pod client-containers-07112c6a-831e-44d9-a273-bf873d5523fc container test-container: <nil>
STEP: delete the pod
Aug 21 17:47:22.115: INFO: Waiting for pod client-containers-07112c6a-831e-44d9-a273-bf873d5523fc to disappear
Aug 21 17:47:22.126: INFO: Pod client-containers-07112c6a-831e-44d9-a273-bf873d5523fc no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:47:22.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5860" for this suite.
Aug 21 17:47:32.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:47:34.395: INFO: namespace containers-5860 deletion completed in 12.240767996s

• [SLOW TEST:16.728 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:47:34.396: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-4385
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-4385
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4385
Aug 21 17:47:34.672: INFO: Found 0 stateful pods, waiting for 1
Aug 21 17:47:44.695: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Aug 21 17:47:44.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4385 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 21 17:47:45.378: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 21 17:47:45.378: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 21 17:47:45.378: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 21 17:47:45.395: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 21 17:47:55.432: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 21 17:47:55.432: INFO: Waiting for statefulset status.replicas updated to 0
Aug 21 17:47:55.484: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999997265s
Aug 21 17:47:56.498: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.98822398s
Aug 21 17:47:57.509: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.974981334s
Aug 21 17:47:58.521: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.963292845s
Aug 21 17:47:59.533: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.951396294s
Aug 21 17:48:00.547: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.939238342s
Aug 21 17:48:01.560: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.925484765s
Aug 21 17:48:02.587: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.912704991s
Aug 21 17:48:03.600: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.885140406s
Aug 21 17:48:04.613: INFO: Verifying statefulset ss doesn't scale past 1 for another 872.523244ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4385
Aug 21 17:48:05.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4385 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:48:06.034: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 21 17:48:06.034: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 21 17:48:06.034: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 21 17:48:06.054: INFO: Found 1 stateful pods, waiting for 3
Aug 21 17:48:16.068: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 17:48:16.068: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 21 17:48:16.068: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Aug 21 17:48:16.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4385 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 21 17:48:16.443: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 21 17:48:16.443: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 21 17:48:16.443: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 21 17:48:16.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4385 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 21 17:48:16.816: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 21 17:48:16.816: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 21 17:48:16.816: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 21 17:48:16.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4385 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 21 17:48:17.235: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 21 17:48:17.235: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 21 17:48:17.235: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 21 17:48:17.235: INFO: Waiting for statefulset status.replicas updated to 0
Aug 21 17:48:17.250: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 21 17:48:27.286: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 21 17:48:27.286: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 21 17:48:27.286: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 21 17:48:27.347: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999996881s
Aug 21 17:48:28.369: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.985641789s
Aug 21 17:48:29.383: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.963227867s
Aug 21 17:48:30.411: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.948640293s
Aug 21 17:48:31.427: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.9208298s
Aug 21 17:48:32.447: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.905336455s
Aug 21 17:48:33.461: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.885050803s
Aug 21 17:48:34.477: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.870780755s
Aug 21 17:48:35.490: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.855026881s
Aug 21 17:48:36.518: INFO: Verifying statefulset ss doesn't scale past 3 for another 842.199653ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4385
Aug 21 17:48:37.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4385 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:48:37.956: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 21 17:48:37.956: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 21 17:48:37.956: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 21 17:48:37.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4385 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:48:38.356: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 21 17:48:38.356: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 21 17:48:38.356: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 21 17:48:38.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 exec --namespace=statefulset-4385 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 21 17:48:38.768: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 21 17:48:38.768: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 21 17:48:38.768: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 21 17:48:38.768: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Aug 21 17:49:08.824: INFO: Deleting all statefulset in ns statefulset-4385
Aug 21 17:49:08.839: INFO: Scaling statefulset ss to 0
Aug 21 17:49:08.886: INFO: Waiting for statefulset status.replicas updated to 0
Aug 21 17:49:08.898: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:49:08.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4385" for this suite.
Aug 21 17:49:17.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:49:19.146: INFO: namespace statefulset-4385 deletion completed in 10.153222673s

• [SLOW TEST:104.750 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:49:19.146: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 21 17:49:20.185: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 21 17:49:22.229: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733628960, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733628960, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733628960, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733628960, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 21 17:49:25.284: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 17:49:25.298: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-989-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:49:26.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6999" for this suite.
Aug 21 17:49:34.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:49:36.828: INFO: namespace webhook-6999 deletion completed in 10.175302083s
STEP: Destroying namespace "webhook-6999-markers" for this suite.
Aug 21 17:49:44.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:49:47.115: INFO: namespace webhook-6999-markers deletion completed in 10.286828579s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:28.050 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:49:47.197: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Aug 21 17:49:51.647: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:49:52.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5229" for this suite.
Aug 21 17:50:26.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:50:28.949: INFO: namespace replicaset-5229 deletion completed in 36.178214448s

• [SLOW TEST:41.753 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:50:28.950: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 21 17:50:30.218: INFO: Waiting up to 5m0s for pod "pod-86bd0100-787c-42a8-94c9-a4f50dd442e8" in namespace "emptydir-2594" to be "success or failure"
Aug 21 17:50:30.235: INFO: Pod "pod-86bd0100-787c-42a8-94c9-a4f50dd442e8": Phase="Pending", Reason="", readiness=false. Elapsed: 17.234955ms
Aug 21 17:50:32.248: INFO: Pod "pod-86bd0100-787c-42a8-94c9-a4f50dd442e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030296652s
STEP: Saw pod success
Aug 21 17:50:32.249: INFO: Pod "pod-86bd0100-787c-42a8-94c9-a4f50dd442e8" satisfied condition "success or failure"
Aug 21 17:50:32.258: INFO: Trying to get logs from node 10.188.240.202 pod pod-86bd0100-787c-42a8-94c9-a4f50dd442e8 container test-container: <nil>
STEP: delete the pod
Aug 21 17:50:32.350: INFO: Waiting for pod pod-86bd0100-787c-42a8-94c9-a4f50dd442e8 to disappear
Aug 21 17:50:32.361: INFO: Pod pod-86bd0100-787c-42a8-94c9-a4f50dd442e8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:50:32.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2594" for this suite.
Aug 21 17:50:40.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:50:42.568: INFO: namespace emptydir-2594 deletion completed in 10.170667905s

• [SLOW TEST:13.618 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:50:42.568: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Aug 21 17:50:43.680: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Aug 21 17:50:45.728: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733629043, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733629043, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733629043, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733629043, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 21 17:50:48.775: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 17:50:48.789: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:50:50.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3058" for this suite.
Aug 21 17:50:58.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:51:00.652: INFO: namespace crd-webhook-3058 deletion completed in 10.298614156s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:18.163 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:51:00.731: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 21 17:51:01.048: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7729e091-ead2-4b17-a895-036b6bd24d31" in namespace "downward-api-6803" to be "success or failure"
Aug 21 17:51:01.061: INFO: Pod "downwardapi-volume-7729e091-ead2-4b17-a895-036b6bd24d31": Phase="Pending", Reason="", readiness=false. Elapsed: 13.032218ms
Aug 21 17:51:03.074: INFO: Pod "downwardapi-volume-7729e091-ead2-4b17-a895-036b6bd24d31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0253003s
STEP: Saw pod success
Aug 21 17:51:03.074: INFO: Pod "downwardapi-volume-7729e091-ead2-4b17-a895-036b6bd24d31" satisfied condition "success or failure"
Aug 21 17:51:03.085: INFO: Trying to get logs from node 10.188.240.202 pod downwardapi-volume-7729e091-ead2-4b17-a895-036b6bd24d31 container client-container: <nil>
STEP: delete the pod
Aug 21 17:51:03.163: INFO: Waiting for pod downwardapi-volume-7729e091-ead2-4b17-a895-036b6bd24d31 to disappear
Aug 21 17:51:03.174: INFO: Pod downwardapi-volume-7729e091-ead2-4b17-a895-036b6bd24d31 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:51:03.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6803" for this suite.
Aug 21 17:51:11.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:51:13.486: INFO: namespace downward-api-6803 deletion completed in 10.283382257s

• [SLOW TEST:12.755 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:51:13.486: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 17:51:13.776: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-097e6411-9a79-4610-a3f7-9ff4626c5af2" in namespace "security-context-test-5681" to be "success or failure"
Aug 21 17:51:13.787: INFO: Pod "busybox-readonly-false-097e6411-9a79-4610-a3f7-9ff4626c5af2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.431998ms
Aug 21 17:51:15.800: INFO: Pod "busybox-readonly-false-097e6411-9a79-4610-a3f7-9ff4626c5af2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023976453s
Aug 21 17:51:15.800: INFO: Pod "busybox-readonly-false-097e6411-9a79-4610-a3f7-9ff4626c5af2" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:51:15.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5681" for this suite.
Aug 21 17:51:23.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:51:26.105: INFO: namespace security-context-test-5681 deletion completed in 10.2552473s

• [SLOW TEST:12.619 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:51:26.105: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Aug 21 17:51:31.023: INFO: Successfully updated pod "labelsupdate889d4035-3ad9-4714-9aed-d26ce17b9cbc"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:51:33.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3835" for this suite.
Aug 21 17:51:55.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:51:57.293: INFO: namespace projected-3835 deletion completed in 24.169786297s

• [SLOW TEST:31.188 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:51:57.295: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 21 17:51:57.537: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f6facbc7-e1f2-480f-8b25-1c7671154840" in namespace "downward-api-2575" to be "success or failure"
Aug 21 17:51:57.549: INFO: Pod "downwardapi-volume-f6facbc7-e1f2-480f-8b25-1c7671154840": Phase="Pending", Reason="", readiness=false. Elapsed: 11.891111ms
Aug 21 17:51:59.567: INFO: Pod "downwardapi-volume-f6facbc7-e1f2-480f-8b25-1c7671154840": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029570699s
Aug 21 17:52:01.579: INFO: Pod "downwardapi-volume-f6facbc7-e1f2-480f-8b25-1c7671154840": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041823963s
STEP: Saw pod success
Aug 21 17:52:01.579: INFO: Pod "downwardapi-volume-f6facbc7-e1f2-480f-8b25-1c7671154840" satisfied condition "success or failure"
Aug 21 17:52:01.591: INFO: Trying to get logs from node 10.188.240.202 pod downwardapi-volume-f6facbc7-e1f2-480f-8b25-1c7671154840 container client-container: <nil>
STEP: delete the pod
Aug 21 17:52:01.664: INFO: Waiting for pod downwardapi-volume-f6facbc7-e1f2-480f-8b25-1c7671154840 to disappear
Aug 21 17:52:01.674: INFO: Pod downwardapi-volume-f6facbc7-e1f2-480f-8b25-1c7671154840 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:52:01.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2575" for this suite.
Aug 21 17:52:09.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:52:12.077: INFO: namespace downward-api-2575 deletion completed in 10.368607366s

• [SLOW TEST:14.782 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:52:12.078: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-6173
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 21 17:52:12.258: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 21 17:52:36.764: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.245.87:8080/dial?request=hostName&protocol=udp&host=172.30.172.35&port=8081&tries=1'] Namespace:pod-network-test-6173 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 17:52:36.764: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 17:52:36.978: INFO: Waiting for endpoints: map[]
Aug 21 17:52:36.989: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.245.87:8080/dial?request=hostName&protocol=udp&host=172.30.245.80&port=8081&tries=1'] Namespace:pod-network-test-6173 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 17:52:36.989: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 17:52:37.201: INFO: Waiting for endpoints: map[]
Aug 21 17:52:37.212: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.245.87:8080/dial?request=hostName&protocol=udp&host=172.30.174.103&port=8081&tries=1'] Namespace:pod-network-test-6173 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 17:52:37.212: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 17:52:37.415: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:52:37.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6173" for this suite.
Aug 21 17:52:45.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:52:47.601: INFO: namespace pod-network-test-6173 deletion completed in 10.154708059s

• [SLOW TEST:35.523 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:52:47.605: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 21 17:52:49.874: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:52:49.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9458" for this suite.
Aug 21 17:52:58.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:53:00.127: INFO: namespace container-runtime-9458 deletion completed in 10.171651251s

• [SLOW TEST:12.523 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:53:00.128: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 21 17:53:00.389: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1fcc628f-2550-4378-b522-a2954acca7fb" in namespace "projected-2021" to be "success or failure"
Aug 21 17:53:00.401: INFO: Pod "downwardapi-volume-1fcc628f-2550-4378-b522-a2954acca7fb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.01226ms
Aug 21 17:53:02.416: INFO: Pod "downwardapi-volume-1fcc628f-2550-4378-b522-a2954acca7fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026573533s
STEP: Saw pod success
Aug 21 17:53:02.416: INFO: Pod "downwardapi-volume-1fcc628f-2550-4378-b522-a2954acca7fb" satisfied condition "success or failure"
Aug 21 17:53:02.431: INFO: Trying to get logs from node 10.188.240.202 pod downwardapi-volume-1fcc628f-2550-4378-b522-a2954acca7fb container client-container: <nil>
STEP: delete the pod
Aug 21 17:53:02.524: INFO: Waiting for pod downwardapi-volume-1fcc628f-2550-4378-b522-a2954acca7fb to disappear
Aug 21 17:53:02.535: INFO: Pod downwardapi-volume-1fcc628f-2550-4378-b522-a2954acca7fb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:53:02.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2021" for this suite.
Aug 21 17:53:10.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:53:12.828: INFO: namespace projected-2021 deletion completed in 10.254714056s

• [SLOW TEST:12.700 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:53:12.829: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 17:53:13.013: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 21 17:53:26.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 --namespace=crd-publish-openapi-2299 create -f -'
Aug 21 17:53:27.212: INFO: stderr: ""
Aug 21 17:53:27.212: INFO: stdout: "e2e-test-crd-publish-openapi-571-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 21 17:53:27.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 --namespace=crd-publish-openapi-2299 delete e2e-test-crd-publish-openapi-571-crds test-cr'
Aug 21 17:53:27.484: INFO: stderr: ""
Aug 21 17:53:27.484: INFO: stdout: "e2e-test-crd-publish-openapi-571-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Aug 21 17:53:27.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 --namespace=crd-publish-openapi-2299 apply -f -'
Aug 21 17:53:28.139: INFO: stderr: ""
Aug 21 17:53:28.139: INFO: stdout: "e2e-test-crd-publish-openapi-571-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 21 17:53:28.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 --namespace=crd-publish-openapi-2299 delete e2e-test-crd-publish-openapi-571-crds test-cr'
Aug 21 17:53:28.348: INFO: stderr: ""
Aug 21 17:53:28.348: INFO: stdout: "e2e-test-crd-publish-openapi-571-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Aug 21 17:53:28.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 explain e2e-test-crd-publish-openapi-571-crds'
Aug 21 17:53:29.033: INFO: stderr: ""
Aug 21 17:53:29.033: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-571-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:53:38.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2299" for this suite.
Aug 21 17:53:46.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:53:48.471: INFO: namespace crd-publish-openapi-2299 deletion completed in 10.196032919s

• [SLOW TEST:35.642 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:53:48.471: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 21 17:53:50.756: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:53:50.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9624" for this suite.
Aug 21 17:53:58.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:54:01.030: INFO: namespace container-runtime-9624 deletion completed in 10.206974847s

• [SLOW TEST:12.560 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:54:01.031: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 17:54:23.330: INFO: Container started at 2020-08-21 17:54:02 +0000 UTC, pod became ready at 2020-08-21 17:54:21 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:54:23.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6439" for this suite.
Aug 21 17:54:57.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:54:59.548: INFO: namespace container-probe-6439 deletion completed in 36.190831632s

• [SLOW TEST:58.517 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:54:59.551: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:55:08.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3332" for this suite.
Aug 21 17:55:16.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:55:18.565: INFO: namespace namespaces-3332 deletion completed in 10.272292908s
STEP: Destroying namespace "nsdeletetest-8476" for this suite.
Aug 21 17:55:18.579: INFO: Namespace nsdeletetest-8476 was already deleted
STEP: Destroying namespace "nsdeletetest-6634" for this suite.
Aug 21 17:55:26.643: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:55:28.806: INFO: namespace nsdeletetest-6634 deletion completed in 10.227029903s

• [SLOW TEST:29.256 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:55:28.808: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Aug 21 17:55:36.394: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-900 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 17:55:36.394: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 17:55:36.653: INFO: Exec stderr: ""
Aug 21 17:55:36.653: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-900 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 17:55:36.653: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 17:55:36.895: INFO: Exec stderr: ""
Aug 21 17:55:36.895: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-900 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 17:55:36.895: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 17:55:37.100: INFO: Exec stderr: ""
Aug 21 17:55:37.100: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-900 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 17:55:37.100: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 17:55:37.309: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Aug 21 17:55:37.309: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-900 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 17:55:37.309: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 17:55:37.535: INFO: Exec stderr: ""
Aug 21 17:55:37.535: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-900 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 17:55:37.535: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 17:55:37.746: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Aug 21 17:55:37.746: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-900 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 17:55:37.746: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 17:55:37.949: INFO: Exec stderr: ""
Aug 21 17:55:37.949: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-900 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 17:55:37.949: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 17:55:38.177: INFO: Exec stderr: ""
Aug 21 17:55:38.177: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-900 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 17:55:38.177: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 17:55:38.401: INFO: Exec stderr: ""
Aug 21 17:55:38.401: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-900 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 21 17:55:38.401: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
Aug 21 17:55:38.590: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:55:38.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-900" for this suite.
Aug 21 17:56:38.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:56:40.905: INFO: namespace e2e-kubelet-etc-hosts-900 deletion completed in 1m2.290430224s

• [SLOW TEST:72.097 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:56:40.906: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-1f93c445-6558-4917-84ee-6b25dfc15d95
STEP: Creating a pod to test consume secrets
Aug 21 17:56:41.181: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0f87051a-684e-4360-a9c6-d612076f093c" in namespace "projected-7678" to be "success or failure"
Aug 21 17:56:41.194: INFO: Pod "pod-projected-secrets-0f87051a-684e-4360-a9c6-d612076f093c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.903096ms
Aug 21 17:56:43.210: INFO: Pod "pod-projected-secrets-0f87051a-684e-4360-a9c6-d612076f093c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029101047s
Aug 21 17:56:45.222: INFO: Pod "pod-projected-secrets-0f87051a-684e-4360-a9c6-d612076f093c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041004105s
STEP: Saw pod success
Aug 21 17:56:45.222: INFO: Pod "pod-projected-secrets-0f87051a-684e-4360-a9c6-d612076f093c" satisfied condition "success or failure"
Aug 21 17:56:45.237: INFO: Trying to get logs from node 10.188.240.202 pod pod-projected-secrets-0f87051a-684e-4360-a9c6-d612076f093c container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 21 17:56:45.338: INFO: Waiting for pod pod-projected-secrets-0f87051a-684e-4360-a9c6-d612076f093c to disappear
Aug 21 17:56:45.351: INFO: Pod pod-projected-secrets-0f87051a-684e-4360-a9c6-d612076f093c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:56:45.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7678" for this suite.
Aug 21 17:56:53.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:56:55.543: INFO: namespace projected-7678 deletion completed in 10.160849029s

• [SLOW TEST:14.638 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:56:55.543: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 21 17:56:56.650: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 21 17:56:58.699: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733629416, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733629416, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733629416, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733629416, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 21 17:57:01.743: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:57:14.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-394" for this suite.
Aug 21 17:57:24.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:57:26.585: INFO: namespace webhook-394 deletion completed in 12.261182019s
STEP: Destroying namespace "webhook-394-markers" for this suite.
Aug 21 17:57:34.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:57:36.880: INFO: namespace webhook-394-markers deletion completed in 10.295523588s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:41.415 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:57:36.962: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Aug 21 17:57:37.182: INFO: Waiting up to 5m0s for pod "client-containers-0714bd1a-0b3a-47e9-9ad4-37b2c21fcf4e" in namespace "containers-7193" to be "success or failure"
Aug 21 17:57:37.211: INFO: Pod "client-containers-0714bd1a-0b3a-47e9-9ad4-37b2c21fcf4e": Phase="Pending", Reason="", readiness=false. Elapsed: 25.999819ms
Aug 21 17:57:39.222: INFO: Pod "client-containers-0714bd1a-0b3a-47e9-9ad4-37b2c21fcf4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037760932s
Aug 21 17:57:41.234: INFO: Pod "client-containers-0714bd1a-0b3a-47e9-9ad4-37b2c21fcf4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049824006s
STEP: Saw pod success
Aug 21 17:57:41.234: INFO: Pod "client-containers-0714bd1a-0b3a-47e9-9ad4-37b2c21fcf4e" satisfied condition "success or failure"
Aug 21 17:57:41.245: INFO: Trying to get logs from node 10.188.240.202 pod client-containers-0714bd1a-0b3a-47e9-9ad4-37b2c21fcf4e container test-container: <nil>
STEP: delete the pod
Aug 21 17:57:41.316: INFO: Waiting for pod client-containers-0714bd1a-0b3a-47e9-9ad4-37b2c21fcf4e to disappear
Aug 21 17:57:41.330: INFO: Pod client-containers-0714bd1a-0b3a-47e9-9ad4-37b2c21fcf4e no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:57:41.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7193" for this suite.
Aug 21 17:57:49.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:57:51.985: INFO: namespace containers-7193 deletion completed in 10.622015338s

• [SLOW TEST:15.024 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:57:51.986: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 21 17:57:52.663: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 21 17:57:54.710: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733629472, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733629472, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733629472, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733629472, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 21 17:57:57.758: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:57:58.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7738" for this suite.
Aug 21 17:58:06.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:58:08.203: INFO: namespace webhook-7738 deletion completed in 10.131322135s
STEP: Destroying namespace "webhook-7738-markers" for this suite.
Aug 21 17:58:16.262: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:58:18.393: INFO: namespace webhook-7738-markers deletion completed in 10.189236568s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:26.486 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:58:18.472: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:58:25.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4551" for this suite.
Aug 21 17:58:33.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:58:36.115: INFO: namespace resourcequota-4551 deletion completed in 10.327505942s

• [SLOW TEST:17.643 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:58:36.116: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 21 17:58:36.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-1984'
Aug 21 17:58:36.469: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 21 17:58:36.469: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Aug 21 17:58:38.505: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-h4m46]
Aug 21 17:58:38.505: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-h4m46" in namespace "kubectl-1984" to be "running and ready"
Aug 21 17:58:38.518: INFO: Pod "e2e-test-httpd-rc-h4m46": Phase="Pending", Reason="", readiness=false. Elapsed: 12.363878ms
Aug 21 17:58:40.534: INFO: Pod "e2e-test-httpd-rc-h4m46": Phase="Running", Reason="", readiness=true. Elapsed: 2.02874184s
Aug 21 17:58:40.534: INFO: Pod "e2e-test-httpd-rc-h4m46" satisfied condition "running and ready"
Aug 21 17:58:40.534: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-h4m46]
Aug 21 17:58:40.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 logs rc/e2e-test-httpd-rc --namespace=kubectl-1984'
Aug 21 17:58:40.815: INFO: stderr: ""
Aug 21 17:58:40.815: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.30.245.103. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.30.245.103. Set the 'ServerName' directive globally to suppress this message\n[Fri Aug 21 17:58:37.812446 2020] [mpm_event:notice] [pid 1:tid 140269175352168] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Fri Aug 21 17:58:37.812626 2020] [core:notice] [pid 1:tid 140269175352168] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Aug 21 17:58:40.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 delete rc e2e-test-httpd-rc --namespace=kubectl-1984'
Aug 21 17:58:41.027: INFO: stderr: ""
Aug 21 17:58:41.027: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:58:41.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1984" for this suite.
Aug 21 17:58:57.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:58:59.247: INFO: namespace kubectl-1984 deletion completed in 18.179835551s

• [SLOW TEST:23.131 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:58:59.247: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 17:58:59.602: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Aug 21 17:58:59.630: INFO: Number of nodes with available pods: 0
Aug 21 17:58:59.630: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Aug 21 17:58:59.715: INFO: Number of nodes with available pods: 0
Aug 21 17:58:59.715: INFO: Node 10.188.240.202 is running more than one daemon pod
Aug 21 17:59:00.728: INFO: Number of nodes with available pods: 0
Aug 21 17:59:00.728: INFO: Node 10.188.240.202 is running more than one daemon pod
Aug 21 17:59:01.728: INFO: Number of nodes with available pods: 1
Aug 21 17:59:01.728: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Aug 21 17:59:01.810: INFO: Number of nodes with available pods: 1
Aug 21 17:59:01.810: INFO: Number of running nodes: 0, number of available pods: 1
Aug 21 17:59:02.822: INFO: Number of nodes with available pods: 0
Aug 21 17:59:02.822: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Aug 21 17:59:02.859: INFO: Number of nodes with available pods: 0
Aug 21 17:59:02.859: INFO: Node 10.188.240.202 is running more than one daemon pod
Aug 21 17:59:03.873: INFO: Number of nodes with available pods: 0
Aug 21 17:59:03.873: INFO: Node 10.188.240.202 is running more than one daemon pod
Aug 21 17:59:04.871: INFO: Number of nodes with available pods: 0
Aug 21 17:59:04.871: INFO: Node 10.188.240.202 is running more than one daemon pod
Aug 21 17:59:05.871: INFO: Number of nodes with available pods: 0
Aug 21 17:59:05.871: INFO: Node 10.188.240.202 is running more than one daemon pod
Aug 21 17:59:06.870: INFO: Number of nodes with available pods: 0
Aug 21 17:59:06.870: INFO: Node 10.188.240.202 is running more than one daemon pod
Aug 21 17:59:07.872: INFO: Number of nodes with available pods: 1
Aug 21 17:59:07.872: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4086, will wait for the garbage collector to delete the pods
Aug 21 17:59:07.989: INFO: Deleting DaemonSet.extensions daemon-set took: 33.221204ms
Aug 21 17:59:08.089: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.247896ms
Aug 21 17:59:21.204: INFO: Number of nodes with available pods: 0
Aug 21 17:59:21.204: INFO: Number of running nodes: 0, number of available pods: 0
Aug 21 17:59:21.213: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4086/daemonsets","resourceVersion":"96935"},"items":null}

Aug 21 17:59:21.223: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4086/pods","resourceVersion":"96935"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:59:21.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4086" for this suite.
Aug 21 17:59:29.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 17:59:31.496: INFO: namespace daemonsets-4086 deletion completed in 10.164168136s

• [SLOW TEST:32.249 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 17:59:31.499: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-1461
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1461
STEP: Deleting pre-stop pod
Aug 21 17:59:40.911: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 17:59:40.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1461" for this suite.
Aug 21 18:00:23.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:00:25.612: INFO: namespace prestop-1461 deletion completed in 44.638990273s

• [SLOW TEST:54.113 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 18:00:25.612: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-3745
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-3745
Aug 21 18:00:25.871: INFO: Found 0 stateful pods, waiting for 1
Aug 21 18:00:35.884: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Aug 21 18:00:35.959: INFO: Deleting all statefulset in ns statefulset-3745
Aug 21 18:00:35.969: INFO: Scaling statefulset ss to 0
Aug 21 18:01:06.021: INFO: Waiting for statefulset status.replicas updated to 0
Aug 21 18:01:06.031: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 18:01:06.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3745" for this suite.
Aug 21 18:01:14.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:01:16.243: INFO: namespace statefulset-3745 deletion completed in 10.131439131s

• [SLOW TEST:50.631 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 18:01:16.243: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 18:01:16.450: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 21 18:01:16.524: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 21 18:01:21.537: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 21 18:01:21.537: INFO: Creating deployment "test-rolling-update-deployment"
Aug 21 18:01:21.554: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 21 18:01:22.018: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Aug 21 18:01:24.044: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 21 18:01:24.056: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Aug 21 18:01:24.095: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8271 /apis/apps/v1/namespaces/deployment-8271/deployments/test-rolling-update-deployment 167c4d2a-19ad-4b3b-8f0f-d4754edbaf04 97758 1 2020-08-21 18:01:21 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001cda3f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-08-21 18:01:22 +0000 UTC,LastTransitionTime:2020-08-21 18:01:22 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2020-08-21 18:01:24 +0000 UTC,LastTransitionTime:2020-08-21 18:01:22 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 21 18:01:24.106: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-8271 /apis/apps/v1/namespaces/deployment-8271/replicasets/test-rolling-update-deployment-55d946486 7bf79dcc-451e-43c8-84aa-0ab03fdcdc20 97748 1 2020-08-21 18:01:21 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 167c4d2a-19ad-4b3b-8f0f-d4754edbaf04 0xc001cdac30 0xc001cdac31}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001cdacd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 21 18:01:24.106: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 21 18:01:24.106: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8271 /apis/apps/v1/namespaces/deployment-8271/replicasets/test-rolling-update-controller 64f2d9ba-a8f6-47e3-b305-0de49c6bb308 97757 2 2020-08-21 18:01:16 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 167c4d2a-19ad-4b3b-8f0f-d4754edbaf04 0xc001cdaae7 0xc001cdaae8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001cdab78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 21 18:01:24.118: INFO: Pod "test-rolling-update-deployment-55d946486-fvw7l" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-fvw7l test-rolling-update-deployment-55d946486- deployment-8271 /api/v1/namespaces/deployment-8271/pods/test-rolling-update-deployment-55d946486-fvw7l 109a0f46-b1fa-4af5-8311-0409fa5be0a6 97747 0 2020-08-21 18:01:22 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[cni.projectcalico.org/podIP:172.30.245.88/32 cni.projectcalico.org/podIPs:172.30.245.88/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.245.88"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 7bf79dcc-451e-43c8-84aa-0ab03fdcdc20 0xc001cdb740 0xc001cdb741}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tns6t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tns6t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tns6t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-255nl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:01:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:01:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:01:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:01:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.202,PodIP:172.30.245.88,StartTime:2020-08-21 18:01:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-21 18:01:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://d34f3b0620fc04a58144db1642430ddd5ffc520a4bd3c7e40807f6ae89f0fcaa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.245.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 18:01:24.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8271" for this suite.
Aug 21 18:01:32.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:01:34.333: INFO: namespace deployment-8271 deletion completed in 10.183942357s

• [SLOW TEST:18.089 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 18:01:34.333: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Aug 21 18:01:34.574: INFO: Waiting up to 5m0s for pod "var-expansion-2a4bab29-fbd6-488e-9e05-c83e40132986" in namespace "var-expansion-5507" to be "success or failure"
Aug 21 18:01:34.585: INFO: Pod "var-expansion-2a4bab29-fbd6-488e-9e05-c83e40132986": Phase="Pending", Reason="", readiness=false. Elapsed: 10.957109ms
Aug 21 18:01:36.597: INFO: Pod "var-expansion-2a4bab29-fbd6-488e-9e05-c83e40132986": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023088907s
STEP: Saw pod success
Aug 21 18:01:36.597: INFO: Pod "var-expansion-2a4bab29-fbd6-488e-9e05-c83e40132986" satisfied condition "success or failure"
Aug 21 18:01:36.609: INFO: Trying to get logs from node 10.188.240.202 pod var-expansion-2a4bab29-fbd6-488e-9e05-c83e40132986 container dapi-container: <nil>
STEP: delete the pod
Aug 21 18:01:36.721: INFO: Waiting for pod var-expansion-2a4bab29-fbd6-488e-9e05-c83e40132986 to disappear
Aug 21 18:01:36.735: INFO: Pod var-expansion-2a4bab29-fbd6-488e-9e05-c83e40132986 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 18:01:36.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5507" for this suite.
Aug 21 18:01:44.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:01:46.912: INFO: namespace var-expansion-5507 deletion completed in 10.143765461s

• [SLOW TEST:12.579 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 18:01:46.912: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-6200/configmap-test-d6c12912-bea3-4ef7-be3b-5755e7ff37b1
STEP: Creating a pod to test consume configMaps
Aug 21 18:01:48.160: INFO: Waiting up to 5m0s for pod "pod-configmaps-97325af3-8071-4543-a0fe-a5240497eabb" in namespace "configmap-6200" to be "success or failure"
Aug 21 18:01:48.171: INFO: Pod "pod-configmaps-97325af3-8071-4543-a0fe-a5240497eabb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.06148ms
Aug 21 18:01:50.185: INFO: Pod "pod-configmaps-97325af3-8071-4543-a0fe-a5240497eabb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025061716s
STEP: Saw pod success
Aug 21 18:01:50.185: INFO: Pod "pod-configmaps-97325af3-8071-4543-a0fe-a5240497eabb" satisfied condition "success or failure"
Aug 21 18:01:50.196: INFO: Trying to get logs from node 10.188.240.202 pod pod-configmaps-97325af3-8071-4543-a0fe-a5240497eabb container env-test: <nil>
STEP: delete the pod
Aug 21 18:01:50.259: INFO: Waiting for pod pod-configmaps-97325af3-8071-4543-a0fe-a5240497eabb to disappear
Aug 21 18:01:50.271: INFO: Pod pod-configmaps-97325af3-8071-4543-a0fe-a5240497eabb no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 18:01:50.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6200" for this suite.
Aug 21 18:01:58.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:02:00.442: INFO: namespace configmap-6200 deletion completed in 10.131760352s

• [SLOW TEST:13.529 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 18:02:00.442: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Aug 21 18:02:05.360: INFO: Successfully updated pod "labelsupdate0c6670fc-af83-47a4-a2d2-408f7a81805e"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 18:02:07.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3775" for this suite.
Aug 21 18:02:41.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:02:43.716: INFO: namespace downward-api-3775 deletion completed in 36.251898045s

• [SLOW TEST:43.274 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 18:02:43.718: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 21 18:02:50.163: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 21 18:02:50.177: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 21 18:02:52.177: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 21 18:02:52.191: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 21 18:02:54.177: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 21 18:02:54.191: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 21 18:02:56.177: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 21 18:02:56.192: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 21 18:02:58.177: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 21 18:02:58.192: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 21 18:03:00.177: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 21 18:03:00.191: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 21 18:03:02.177: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 21 18:03:02.190: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 18:03:02.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8654" for this suite.
Aug 21 18:03:24.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:03:26.455: INFO: namespace container-lifecycle-hook-8654 deletion completed in 24.194681725s

• [SLOW TEST:42.738 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 18:03:26.456: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1707.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1707.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1707.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1707.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1707.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1707.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1707.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1707.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1707.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1707.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1707.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1707.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1707.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 167.215.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.215.167_udp@PTR;check="$$(dig +tcp +noall +answer +search 167.215.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.215.167_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1707.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1707.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1707.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1707.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1707.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1707.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1707.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1707.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1707.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1707.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1707.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1707.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1707.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 167.215.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.215.167_udp@PTR;check="$$(dig +tcp +noall +answer +search 167.215.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.215.167_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 21 18:03:30.906: INFO: Unable to read wheezy_udp@dns-test-service.dns-1707.svc.cluster.local from pod dns-1707/dns-test-a5289dd4-f07b-4a21-a06f-2f8ef365d926: the server could not find the requested resource (get pods dns-test-a5289dd4-f07b-4a21-a06f-2f8ef365d926)
Aug 21 18:03:30.928: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1707.svc.cluster.local from pod dns-1707/dns-test-a5289dd4-f07b-4a21-a06f-2f8ef365d926: the server could not find the requested resource (get pods dns-test-a5289dd4-f07b-4a21-a06f-2f8ef365d926)
Aug 21 18:03:30.946: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1707.svc.cluster.local from pod dns-1707/dns-test-a5289dd4-f07b-4a21-a06f-2f8ef365d926: the server could not find the requested resource (get pods dns-test-a5289dd4-f07b-4a21-a06f-2f8ef365d926)
Aug 21 18:03:30.968: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1707.svc.cluster.local from pod dns-1707/dns-test-a5289dd4-f07b-4a21-a06f-2f8ef365d926: the server could not find the requested resource (get pods dns-test-a5289dd4-f07b-4a21-a06f-2f8ef365d926)
Aug 21 18:03:31.136: INFO: Unable to read jessie_udp@dns-test-service.dns-1707.svc.cluster.local from pod dns-1707/dns-test-a5289dd4-f07b-4a21-a06f-2f8ef365d926: the server could not find the requested resource (get pods dns-test-a5289dd4-f07b-4a21-a06f-2f8ef365d926)
Aug 21 18:03:31.160: INFO: Unable to read jessie_tcp@dns-test-service.dns-1707.svc.cluster.local from pod dns-1707/dns-test-a5289dd4-f07b-4a21-a06f-2f8ef365d926: the server could not find the requested resource (get pods dns-test-a5289dd4-f07b-4a21-a06f-2f8ef365d926)
Aug 21 18:03:31.190: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1707.svc.cluster.local from pod dns-1707/dns-test-a5289dd4-f07b-4a21-a06f-2f8ef365d926: the server could not find the requested resource (get pods dns-test-a5289dd4-f07b-4a21-a06f-2f8ef365d926)
Aug 21 18:03:31.214: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1707.svc.cluster.local from pod dns-1707/dns-test-a5289dd4-f07b-4a21-a06f-2f8ef365d926: the server could not find the requested resource (get pods dns-test-a5289dd4-f07b-4a21-a06f-2f8ef365d926)
Aug 21 18:03:31.361: INFO: Lookups using dns-1707/dns-test-a5289dd4-f07b-4a21-a06f-2f8ef365d926 failed for: [wheezy_udp@dns-test-service.dns-1707.svc.cluster.local wheezy_tcp@dns-test-service.dns-1707.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1707.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1707.svc.cluster.local jessie_udp@dns-test-service.dns-1707.svc.cluster.local jessie_tcp@dns-test-service.dns-1707.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1707.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1707.svc.cluster.local]

Aug 21 18:03:36.758: INFO: DNS probes using dns-1707/dns-test-a5289dd4-f07b-4a21-a06f-2f8ef365d926 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 18:03:36.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1707" for this suite.
Aug 21 18:03:45.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:03:47.109: INFO: namespace dns-1707 deletion completed in 10.122782245s

• [SLOW TEST:20.653 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 18:03:47.109: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-f4f77e58-22e7-4df0-85fa-052d2b7128e7
STEP: Creating a pod to test consume secrets
Aug 21 18:03:47.386: INFO: Waiting up to 5m0s for pod "pod-secrets-36ba4a87-0a98-4f5c-868e-8bb73b0a6a49" in namespace "secrets-432" to be "success or failure"
Aug 21 18:03:47.395: INFO: Pod "pod-secrets-36ba4a87-0a98-4f5c-868e-8bb73b0a6a49": Phase="Pending", Reason="", readiness=false. Elapsed: 9.484875ms
Aug 21 18:03:49.406: INFO: Pod "pod-secrets-36ba4a87-0a98-4f5c-868e-8bb73b0a6a49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01978047s
STEP: Saw pod success
Aug 21 18:03:49.406: INFO: Pod "pod-secrets-36ba4a87-0a98-4f5c-868e-8bb73b0a6a49" satisfied condition "success or failure"
Aug 21 18:03:49.417: INFO: Trying to get logs from node 10.188.240.202 pod pod-secrets-36ba4a87-0a98-4f5c-868e-8bb73b0a6a49 container secret-volume-test: <nil>
STEP: delete the pod
Aug 21 18:03:49.506: INFO: Waiting for pod pod-secrets-36ba4a87-0a98-4f5c-868e-8bb73b0a6a49 to disappear
Aug 21 18:03:49.516: INFO: Pod pod-secrets-36ba4a87-0a98-4f5c-868e-8bb73b0a6a49 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 18:03:49.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-432" for this suite.
Aug 21 18:03:57.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:03:59.754: INFO: namespace secrets-432 deletion completed in 10.1957619s

• [SLOW TEST:12.645 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 18:03:59.755: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 18:04:17.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4475" for this suite.
Aug 21 18:04:25.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:04:27.343: INFO: namespace resourcequota-4475 deletion completed in 10.230101245s

• [SLOW TEST:27.588 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 18:04:27.344: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-e81baf5f-bdc6-4c3f-9163-c67283db315c
STEP: Creating a pod to test consume configMaps
Aug 21 18:04:27.638: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3e3ed187-2ec0-4364-8645-1447bd5f3740" in namespace "projected-8251" to be "success or failure"
Aug 21 18:04:27.650: INFO: Pod "pod-projected-configmaps-3e3ed187-2ec0-4364-8645-1447bd5f3740": Phase="Pending", Reason="", readiness=false. Elapsed: 12.149131ms
Aug 21 18:04:29.666: INFO: Pod "pod-projected-configmaps-3e3ed187-2ec0-4364-8645-1447bd5f3740": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027852297s
STEP: Saw pod success
Aug 21 18:04:29.666: INFO: Pod "pod-projected-configmaps-3e3ed187-2ec0-4364-8645-1447bd5f3740" satisfied condition "success or failure"
Aug 21 18:04:29.677: INFO: Trying to get logs from node 10.188.240.202 pod pod-projected-configmaps-3e3ed187-2ec0-4364-8645-1447bd5f3740 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 18:04:29.761: INFO: Waiting for pod pod-projected-configmaps-3e3ed187-2ec0-4364-8645-1447bd5f3740 to disappear
Aug 21 18:04:29.771: INFO: Pod pod-projected-configmaps-3e3ed187-2ec0-4364-8645-1447bd5f3740 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 18:04:29.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8251" for this suite.
Aug 21 18:04:37.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:04:40.298: INFO: namespace projected-8251 deletion completed in 10.491556043s

• [SLOW TEST:12.954 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 18:04:40.298: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Aug 21 18:04:46.644: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 18:04:46.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0821 18:04:46.644061      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-9466" for this suite.
Aug 21 18:04:56.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:04:58.800: INFO: namespace gc-9466 deletion completed in 12.128906297s

• [SLOW TEST:18.502 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 18:04:58.802: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Aug 21 18:04:58.980: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 21 18:04:59.068: INFO: Waiting for terminating namespaces to be deleted...
Aug 21 18:04:59.091: INFO: 
Logging pods the kubelet thinks is on node 10.188.240.202 before test
Aug 21 18:04:59.155: INFO: ibm-keepalived-watcher-hgfkl from kube-system started at 2020-08-21 14:24:13 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.156: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 21 18:04:59.156: INFO: openshift-kube-proxy-dqhxm from openshift-kube-proxy started at 2020-08-21 14:24:48 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.156: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 21 18:04:59.156: INFO: ibmcloud-block-storage-driver-wmdvn from kube-system started at 2020-08-21 14:24:16 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.156: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 21 18:04:59.156: INFO: multus-59vjb from openshift-multus started at 2020-08-21 14:24:42 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.156: INFO: 	Container kube-multus ready: true, restart count 0
Aug 21 18:04:59.156: INFO: tuned-hfsdd from openshift-cluster-node-tuning-operator started at 2020-08-21 14:26:30 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.156: INFO: 	Container tuned ready: true, restart count 0
Aug 21 18:04:59.156: INFO: multus-admission-controller-t7nxb from openshift-multus started at 2020-08-21 17:32:31 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.156: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 21 18:04:59.156: INFO: node-exporter-vw6s8 from openshift-monitoring started at 2020-08-21 14:26:49 +0000 UTC (2 container statuses recorded)
Aug 21 18:04:59.157: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 18:04:59.157: INFO: 	Container node-exporter ready: true, restart count 0
Aug 21 18:04:59.157: INFO: dns-default-xn4tw from openshift-dns started at 2020-08-21 17:32:41 +0000 UTC (2 container statuses recorded)
Aug 21 18:04:59.157: INFO: 	Container dns ready: true, restart count 0
Aug 21 18:04:59.157: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 21 18:04:59.157: INFO: ibm-master-proxy-static-10.188.240.202 from kube-system started at 2020-08-21 14:24:11 +0000 UTC (2 container statuses recorded)
Aug 21 18:04:59.157: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 21 18:04:59.157: INFO: 	Container pause ready: true, restart count 0
Aug 21 18:04:59.157: INFO: calico-node-lfw9m from calico-system started at 2020-08-21 14:25:08 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.157: INFO: 	Container calico-node ready: true, restart count 0
Aug 21 18:04:59.157: INFO: calico-typha-6c986fbc8c-mtt9h from calico-system started at 2020-08-21 14:27:04 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.157: INFO: 	Container calico-typha ready: true, restart count 0
Aug 21 18:04:59.158: INFO: node-ca-2zlsz from openshift-image-registry started at 2020-08-21 14:27:50 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.158: INFO: 	Container node-ca ready: true, restart count 0
Aug 21 18:04:59.158: INFO: sonobuoy-systemd-logs-daemon-set-5cea973cc7904f8c-nfmgc from sonobuoy started at 2020-08-21 15:39:10 +0000 UTC (2 container statuses recorded)
Aug 21 18:04:59.158: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Aug 21 18:04:59.158: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 21 18:04:59.158: INFO: 
Logging pods the kubelet thinks is on node 10.188.240.222 before test
Aug 21 18:04:59.292: INFO: multus-4xs2x from openshift-multus started at 2020-08-21 14:24:42 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.292: INFO: 	Container kube-multus ready: true, restart count 0
Aug 21 18:04:59.292: INFO: thanos-querier-5bcc6bd6c4-6wnxk from openshift-monitoring started at 2020-08-21 14:34:55 +0000 UTC (4 container statuses recorded)
Aug 21 18:04:59.292: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 18:04:59.292: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 21 18:04:59.292: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 21 18:04:59.292: INFO: 	Container thanos-querier ready: true, restart count 0
Aug 21 18:04:59.292: INFO: tigera-operator-679798d94d-twz9q from tigera-operator started at 2020-08-21 17:31:56 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.293: INFO: 	Container tigera-operator ready: true, restart count 0
Aug 21 18:04:59.293: INFO: network-operator-7986644c85-l5gh7 from openshift-network-operator started at 2020-08-21 17:31:57 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.293: INFO: 	Container network-operator ready: true, restart count 0
Aug 21 18:04:59.293: INFO: ibmcloud-block-storage-driver-88f6v from kube-system started at 2020-08-21 14:24:22 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.293: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 21 18:04:59.293: INFO: kube-state-metrics-c5f65645-vgmjg from openshift-monitoring started at 2020-08-21 14:26:47 +0000 UTC (3 container statuses recorded)
Aug 21 18:04:59.293: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 21 18:04:59.293: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 21 18:04:59.293: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 21 18:04:59.293: INFO: cluster-samples-operator-55944b8f44-hpv4t from openshift-cluster-samples-operator started at 2020-08-21 14:27:28 +0000 UTC (2 container statuses recorded)
Aug 21 18:04:59.293: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Aug 21 18:04:59.293: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Aug 21 18:04:59.293: INFO: router-default-79bfbd48f7-wq29r from openshift-ingress started at 2020-08-21 14:28:25 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.293: INFO: 	Container router ready: true, restart count 0
Aug 21 18:04:59.293: INFO: ibm-master-proxy-static-10.188.240.222 from kube-system started at 2020-08-21 14:24:12 +0000 UTC (2 container statuses recorded)
Aug 21 18:04:59.293: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 21 18:04:59.293: INFO: 	Container pause ready: true, restart count 0
Aug 21 18:04:59.293: INFO: tuned-xffjb from openshift-cluster-node-tuning-operator started at 2020-08-21 14:26:30 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.293: INFO: 	Container tuned ready: true, restart count 0
Aug 21 18:04:59.293: INFO: node-ca-wn9jz from openshift-image-registry started at 2020-08-21 14:27:50 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.293: INFO: 	Container node-ca ready: true, restart count 0
Aug 21 18:04:59.293: INFO: ibm-cloud-provider-ip-169-60-87-181-7df9c474cf-gjd6l from ibm-system started at 2020-08-21 17:26:24 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.293: INFO: 	Container ibm-cloud-provider-ip-169-60-87-181 ready: true, restart count 0
Aug 21 18:04:59.293: INFO: thanos-querier-5bcc6bd6c4-7f4tt from openshift-monitoring started at 2020-08-21 17:31:57 +0000 UTC (4 container statuses recorded)
Aug 21 18:04:59.293: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 18:04:59.293: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 21 18:04:59.293: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 21 18:04:59.293: INFO: 	Container thanos-querier ready: true, restart count 0
Aug 21 18:04:59.293: INFO: openshift-kube-proxy-w7zk7 from openshift-kube-proxy started at 2020-08-21 14:24:48 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.293: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 21 18:04:59.293: INFO: configmap-cabundle-injector-8446d4b88f-lc4xk from openshift-service-ca started at 2020-08-21 14:26:59 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.294: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Aug 21 18:04:59.294: INFO: console-5bf7799b6-qbrjl from openshift-console started at 2020-08-21 14:29:07 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.294: INFO: 	Container console ready: true, restart count 0
Aug 21 18:04:59.294: INFO: packageserver-7f69bd69d9-d8d9l from openshift-operator-lifecycle-manager started at 2020-08-21 17:26:30 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.294: INFO: 	Container packageserver ready: true, restart count 0
Aug 21 18:04:59.294: INFO: apiservice-cabundle-injector-594fd4555f-5fdl8 from openshift-service-ca started at 2020-08-21 17:31:57 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.294: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Aug 21 18:04:59.294: INFO: calico-node-s8hvd from calico-system started at 2020-08-21 14:25:08 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.294: INFO: 	Container calico-node ready: true, restart count 0
Aug 21 18:04:59.294: INFO: multus-admission-controller-tmrh2 from openshift-multus started at 2020-08-21 14:25:45 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.294: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 21 18:04:59.294: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-08-21 14:35:32 +0000 UTC (7 container statuses recorded)
Aug 21 18:04:59.294: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 18:04:59.294: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 21 18:04:59.294: INFO: 	Container prometheus ready: true, restart count 1
Aug 21 18:04:59.294: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 21 18:04:59.294: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 21 18:04:59.294: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 21 18:04:59.294: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 21 18:04:59.294: INFO: vpn-69dd866c84-4s9m7 from kube-system started at 2020-08-21 14:31:55 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.294: INFO: 	Container vpn ready: true, restart count 0
Aug 21 18:04:59.294: INFO: prometheus-adapter-5697b6dddd-jmhzs from openshift-monitoring started at 2020-08-21 17:31:57 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.294: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 21 18:04:59.294: INFO: sonobuoy-systemd-logs-daemon-set-5cea973cc7904f8c-m7w4l from sonobuoy started at 2020-08-21 15:39:10 +0000 UTC (2 container statuses recorded)
Aug 21 18:04:59.294: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Aug 21 18:04:59.294: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 21 18:04:59.294: INFO: prometheus-operator-56d9d699cb-bvsq5 from openshift-monitoring started at 2020-08-21 17:31:57 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.294: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 21 18:04:59.294: INFO: ibm-keepalived-watcher-pqhzw from kube-system started at 2020-08-21 14:24:15 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.294: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 21 18:04:59.294: INFO: calico-typha-6c986fbc8c-d5w6g from calico-system started at 2020-08-21 14:25:07 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.294: INFO: 	Container calico-typha ready: true, restart count 0
Aug 21 18:04:59.294: INFO: node-exporter-67tp6 from openshift-monitoring started at 2020-08-21 14:26:50 +0000 UTC (2 container statuses recorded)
Aug 21 18:04:59.294: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 18:04:59.294: INFO: 	Container node-exporter ready: true, restart count 0
Aug 21 18:04:59.295: INFO: dns-default-xxclv from openshift-dns started at 2020-08-21 14:28:16 +0000 UTC (2 container statuses recorded)
Aug 21 18:04:59.295: INFO: 	Container dns ready: true, restart count 0
Aug 21 18:04:59.295: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 21 18:04:59.295: INFO: telemeter-client-6fdb57d68d-f82cq from openshift-monitoring started at 2020-08-21 14:34:07 +0000 UTC (3 container statuses recorded)
Aug 21 18:04:59.295: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 18:04:59.295: INFO: 	Container reload ready: true, restart count 0
Aug 21 18:04:59.295: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 21 18:04:59.295: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-08-21 14:30:18 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.295: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Aug 21 18:04:59.295: INFO: registry-pvc-permissions-px8dj from openshift-image-registry started at 2020-08-21 14:30:33 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.295: INFO: 	Container pvc-permissions ready: false, restart count 0
Aug 21 18:04:59.295: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-08-21 14:34:42 +0000 UTC (3 container statuses recorded)
Aug 21 18:04:59.295: INFO: 	Container alertmanager ready: true, restart count 0
Aug 21 18:04:59.296: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 21 18:04:59.296: INFO: 	Container config-reloader ready: true, restart count 0
Aug 21 18:04:59.296: INFO: 
Logging pods the kubelet thinks is on node 10.188.240.230 before test
Aug 21 18:04:59.455: INFO: ibm-file-plugin-6c96899f79-5r8lq from kube-system started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.455: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 21 18:04:59.455: INFO: dns-operator-6f9cf66db7-2qjt6 from openshift-dns-operator started at 2020-08-21 14:25:29 +0000 UTC (2 container statuses recorded)
Aug 21 18:04:59.455: INFO: 	Container dns-operator ready: true, restart count 0
Aug 21 18:04:59.455: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 18:04:59.455: INFO: ibmcloud-block-storage-plugin-68d5c65db9-clx4d from kube-system started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.455: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Aug 21 18:04:59.455: INFO: multus-admission-controller-9x9x2 from openshift-multus started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.455: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 21 18:04:59.455: INFO: ibm-storage-watcher-68df9b45c4-9rvr8 from kube-system started at 2020-08-21 14:25:31 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.455: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 21 18:04:59.455: INFO: node-exporter-jkv5q from openshift-monitoring started at 2020-08-21 14:26:50 +0000 UTC (2 container statuses recorded)
Aug 21 18:04:59.455: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 18:04:59.456: INFO: 	Container node-exporter ready: true, restart count 0
Aug 21 18:04:59.456: INFO: calico-node-rw476 from calico-system started at 2020-08-21 14:25:08 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container calico-node ready: true, restart count 0
Aug 21 18:04:59.456: INFO: cluster-storage-operator-557b75f8d5-bpz57 from openshift-cluster-storage-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Aug 21 18:04:59.456: INFO: cluster-image-registry-operator-6cfd58b66c-rsxhh from openshift-image-registry started at 2020-08-21 14:25:29 +0000 UTC (2 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Aug 21 18:04:59.456: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Aug 21 18:04:59.456: INFO: cluster-node-tuning-operator-b5f884945-f92th from openshift-cluster-node-tuning-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Aug 21 18:04:59.456: INFO: tuned-bh9bt from openshift-cluster-node-tuning-operator started at 2020-08-21 14:26:30 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container tuned ready: true, restart count 0
Aug 21 18:04:59.456: INFO: ibm-keepalived-watcher-9tt65 from kube-system started at 2020-08-21 14:24:19 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 21 18:04:59.456: INFO: downloads-678f5d6564-sxpw2 from openshift-console started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container download-server ready: true, restart count 0
Aug 21 18:04:59.456: INFO: calico-typha-6c986fbc8c-rw4j9 from calico-system started at 2020-08-21 14:27:04 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container calico-typha ready: true, restart count 0
Aug 21 18:04:59.456: INFO: sonobuoy from sonobuoy started at 2020-08-21 15:39:03 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 21 18:04:59.456: INFO: image-registry-86f8b76dcb-c9ht9 from openshift-image-registry started at 2020-08-21 17:26:24 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container registry ready: true, restart count 0
Aug 21 18:04:59.456: INFO: console-operator-9878d4766-tfdhf from openshift-console-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container console-operator ready: true, restart count 1
Aug 21 18:04:59.456: INFO: openshift-service-catalog-controller-manager-operator-5496stt64 from openshift-service-catalog-controller-manager-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container operator ready: true, restart count 1
Aug 21 18:04:59.456: INFO: calico-kube-controllers-79d75767dd-bznnm from calico-system started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 21 18:04:59.456: INFO: node-ca-7vtw7 from openshift-image-registry started at 2020-08-21 14:27:50 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container node-ca ready: true, restart count 0
Aug 21 18:04:59.456: INFO: prometheus-adapter-5697b6dddd-hpnk5 from openshift-monitoring started at 2020-08-21 14:34:09 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 21 18:04:59.456: INFO: router-default-79bfbd48f7-8pr9z from openshift-ingress started at 2020-08-21 17:26:24 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container router ready: true, restart count 0
Aug 21 18:04:59.456: INFO: service-serving-cert-signer-7879bf8d9f-nsbnq from openshift-service-ca started at 2020-08-21 17:26:24 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Aug 21 18:04:59.456: INFO: openshift-state-metrics-5849d797d8-lj8k9 from openshift-monitoring started at 2020-08-21 17:26:24 +0000 UTC (3 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 21 18:04:59.456: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 21 18:04:59.456: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 21 18:04:59.456: INFO: console-5bf7799b6-64fjd from openshift-console started at 2020-08-21 17:31:57 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container console ready: true, restart count 0
Aug 21 18:04:59.456: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-08-21 17:32:12 +0000 UTC (7 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 18:04:59.456: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 21 18:04:59.456: INFO: 	Container prometheus ready: true, restart count 1
Aug 21 18:04:59.456: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 21 18:04:59.456: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 21 18:04:59.456: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 21 18:04:59.456: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 21 18:04:59.456: INFO: multus-5fbcs from openshift-multus started at 2020-08-21 14:24:42 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container kube-multus ready: true, restart count 0
Aug 21 18:04:59.456: INFO: cluster-monitoring-operator-5b5659466f-lbtcv from openshift-monitoring started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Aug 21 18:04:59.456: INFO: service-ca-operator-694cfbf5d5-vxbp2 from openshift-service-ca-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container operator ready: true, restart count 0
Aug 21 18:04:59.456: INFO: ingress-operator-695bc545b9-ps8wd from openshift-ingress-operator started at 2020-08-21 14:25:29 +0000 UTC (2 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container ingress-operator ready: true, restart count 0
Aug 21 18:04:59.456: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 21 18:04:59.456: INFO: openshift-kube-proxy-j677g from openshift-kube-proxy started at 2020-08-21 14:24:48 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 21 18:04:59.456: INFO: olm-operator-b5f57cdbb-nw9x4 from openshift-operator-lifecycle-manager started at 2020-08-21 14:25:30 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container olm-operator ready: true, restart count 0
Aug 21 18:04:59.456: INFO: dns-default-qhhzk from openshift-dns started at 2020-08-21 14:28:16 +0000 UTC (2 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container dns ready: true, restart count 0
Aug 21 18:04:59.456: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 21 18:04:59.456: INFO: ibm-cloud-provider-ip-169-60-87-181-7df9c474cf-v8lnf from ibm-system started at 2020-08-21 14:30:19 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container ibm-cloud-provider-ip-169-60-87-181 ready: true, restart count 0
Aug 21 18:04:59.456: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-08-21 14:34:17 +0000 UTC (3 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container alertmanager ready: true, restart count 0
Aug 21 18:04:59.456: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 21 18:04:59.456: INFO: 	Container config-reloader ready: true, restart count 0
Aug 21 18:04:59.456: INFO: openshift-service-catalog-apiserver-operator-78b9dddb6f-jxgbd from openshift-service-catalog-apiserver-operator started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container operator ready: true, restart count 1
Aug 21 18:04:59.456: INFO: catalog-operator-85f6c659cc-dqh5m from openshift-operator-lifecycle-manager started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 21 18:04:59.456: INFO: certified-operators-6b6b9f965f-fthsb from openshift-marketplace started at 2020-08-21 16:28:56 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container certified-operators ready: true, restart count 0
Aug 21 18:04:59.456: INFO: packageserver-7f69bd69d9-xtr6s from openshift-operator-lifecycle-manager started at 2020-08-21 17:26:25 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container packageserver ready: true, restart count 0
Aug 21 18:04:59.456: INFO: redhat-operators-6d986fdd47-pbjr6 from openshift-marketplace started at 2020-08-21 17:31:57 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container redhat-operators ready: true, restart count 0
Aug 21 18:04:59.456: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-08-21 17:32:11 +0000 UTC (3 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container alertmanager ready: true, restart count 0
Aug 21 18:04:59.456: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 21 18:04:59.456: INFO: 	Container config-reloader ready: true, restart count 0
Aug 21 18:04:59.456: INFO: ibm-master-proxy-static-10.188.240.230 from kube-system started at 2020-08-21 14:24:17 +0000 UTC (2 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 21 18:04:59.456: INFO: 	Container pause ready: true, restart count 0
Aug 21 18:04:59.456: INFO: ibmcloud-block-storage-driver-jzb5f from kube-system started at 2020-08-21 14:24:22 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 21 18:04:59.456: INFO: marketplace-operator-6957767d58-5m7kk from openshift-marketplace started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container marketplace-operator ready: true, restart count 0
Aug 21 18:04:59.456: INFO: downloads-678f5d6564-78bw4 from openshift-console started at 2020-08-21 14:25:29 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container download-server ready: true, restart count 0
Aug 21 18:04:59.456: INFO: grafana-c9c7455d7-2bzzz from openshift-monitoring started at 2020-08-21 17:26:24 +0000 UTC (2 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container grafana ready: true, restart count 0
Aug 21 18:04:59.456: INFO: 	Container grafana-proxy ready: true, restart count 0
Aug 21 18:04:59.456: INFO: sonobuoy-e2e-job-3399d07d932e4f9d from sonobuoy started at 2020-08-21 15:39:10 +0000 UTC (2 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container e2e ready: true, restart count 0
Aug 21 18:04:59.456: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 21 18:04:59.456: INFO: sonobuoy-systemd-logs-daemon-set-5cea973cc7904f8c-rjxm6 from sonobuoy started at 2020-08-21 15:39:10 +0000 UTC (2 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Aug 21 18:04:59.456: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 21 18:04:59.456: INFO: community-operators-68975cd7c8-h7glt from openshift-marketplace started at 2020-08-21 17:31:57 +0000 UTC (1 container statuses recorded)
Aug 21 18:04:59.456: INFO: 	Container community-operators ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node 10.188.240.202
STEP: verifying the node has the label node 10.188.240.222
STEP: verifying the node has the label node 10.188.240.230
Aug 21 18:04:59.673: INFO: Pod calico-kube-controllers-79d75767dd-bznnm requesting resource cpu=0m on Node 10.188.240.230
Aug 21 18:04:59.673: INFO: Pod calico-node-lfw9m requesting resource cpu=0m on Node 10.188.240.202
Aug 21 18:04:59.673: INFO: Pod calico-node-rw476 requesting resource cpu=0m on Node 10.188.240.230
Aug 21 18:04:59.673: INFO: Pod calico-node-s8hvd requesting resource cpu=0m on Node 10.188.240.222
Aug 21 18:04:59.673: INFO: Pod calico-typha-6c986fbc8c-d5w6g requesting resource cpu=0m on Node 10.188.240.222
Aug 21 18:04:59.673: INFO: Pod calico-typha-6c986fbc8c-mtt9h requesting resource cpu=0m on Node 10.188.240.202
Aug 21 18:04:59.673: INFO: Pod calico-typha-6c986fbc8c-rw4j9 requesting resource cpu=0m on Node 10.188.240.230
Aug 21 18:04:59.673: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.188.240.222
Aug 21 18:04:59.673: INFO: Pod ibm-cloud-provider-ip-169-60-87-181-7df9c474cf-gjd6l requesting resource cpu=5m on Node 10.188.240.222
Aug 21 18:04:59.673: INFO: Pod ibm-cloud-provider-ip-169-60-87-181-7df9c474cf-v8lnf requesting resource cpu=5m on Node 10.188.240.230
Aug 21 18:04:59.674: INFO: Pod ibm-file-plugin-6c96899f79-5r8lq requesting resource cpu=50m on Node 10.188.240.230
Aug 21 18:04:59.674: INFO: Pod ibm-keepalived-watcher-9tt65 requesting resource cpu=5m on Node 10.188.240.230
Aug 21 18:04:59.674: INFO: Pod ibm-keepalived-watcher-hgfkl requesting resource cpu=5m on Node 10.188.240.202
Aug 21 18:04:59.674: INFO: Pod ibm-keepalived-watcher-pqhzw requesting resource cpu=5m on Node 10.188.240.222
Aug 21 18:04:59.674: INFO: Pod ibm-master-proxy-static-10.188.240.202 requesting resource cpu=25m on Node 10.188.240.202
Aug 21 18:04:59.674: INFO: Pod ibm-master-proxy-static-10.188.240.222 requesting resource cpu=25m on Node 10.188.240.222
Aug 21 18:04:59.674: INFO: Pod ibm-master-proxy-static-10.188.240.230 requesting resource cpu=25m on Node 10.188.240.230
Aug 21 18:04:59.674: INFO: Pod ibm-storage-watcher-68df9b45c4-9rvr8 requesting resource cpu=50m on Node 10.188.240.230
Aug 21 18:04:59.674: INFO: Pod ibmcloud-block-storage-driver-88f6v requesting resource cpu=50m on Node 10.188.240.222
Aug 21 18:04:59.674: INFO: Pod ibmcloud-block-storage-driver-jzb5f requesting resource cpu=50m on Node 10.188.240.230
Aug 21 18:04:59.674: INFO: Pod ibmcloud-block-storage-driver-wmdvn requesting resource cpu=50m on Node 10.188.240.202
Aug 21 18:04:59.674: INFO: Pod ibmcloud-block-storage-plugin-68d5c65db9-clx4d requesting resource cpu=50m on Node 10.188.240.230
Aug 21 18:04:59.674: INFO: Pod vpn-69dd866c84-4s9m7 requesting resource cpu=5m on Node 10.188.240.222
Aug 21 18:04:59.674: INFO: Pod cluster-node-tuning-operator-b5f884945-f92th requesting resource cpu=10m on Node 10.188.240.230
Aug 21 18:04:59.674: INFO: Pod tuned-bh9bt requesting resource cpu=10m on Node 10.188.240.230
Aug 21 18:04:59.674: INFO: Pod tuned-hfsdd requesting resource cpu=10m on Node 10.188.240.202
Aug 21 18:04:59.674: INFO: Pod tuned-xffjb requesting resource cpu=10m on Node 10.188.240.222
Aug 21 18:04:59.674: INFO: Pod cluster-samples-operator-55944b8f44-hpv4t requesting resource cpu=20m on Node 10.188.240.222
Aug 21 18:04:59.675: INFO: Pod cluster-storage-operator-557b75f8d5-bpz57 requesting resource cpu=10m on Node 10.188.240.230
Aug 21 18:04:59.675: INFO: Pod console-operator-9878d4766-tfdhf requesting resource cpu=10m on Node 10.188.240.230
Aug 21 18:04:59.675: INFO: Pod console-5bf7799b6-64fjd requesting resource cpu=10m on Node 10.188.240.230
Aug 21 18:04:59.675: INFO: Pod console-5bf7799b6-qbrjl requesting resource cpu=10m on Node 10.188.240.222
Aug 21 18:04:59.675: INFO: Pod downloads-678f5d6564-78bw4 requesting resource cpu=10m on Node 10.188.240.230
Aug 21 18:04:59.675: INFO: Pod downloads-678f5d6564-sxpw2 requesting resource cpu=10m on Node 10.188.240.230
Aug 21 18:04:59.675: INFO: Pod dns-operator-6f9cf66db7-2qjt6 requesting resource cpu=20m on Node 10.188.240.230
Aug 21 18:04:59.675: INFO: Pod dns-default-qhhzk requesting resource cpu=110m on Node 10.188.240.230
Aug 21 18:04:59.675: INFO: Pod dns-default-xn4tw requesting resource cpu=110m on Node 10.188.240.202
Aug 21 18:04:59.675: INFO: Pod dns-default-xxclv requesting resource cpu=110m on Node 10.188.240.222
Aug 21 18:04:59.675: INFO: Pod cluster-image-registry-operator-6cfd58b66c-rsxhh requesting resource cpu=20m on Node 10.188.240.230
Aug 21 18:04:59.675: INFO: Pod image-registry-86f8b76dcb-c9ht9 requesting resource cpu=100m on Node 10.188.240.230
Aug 21 18:04:59.675: INFO: Pod node-ca-2zlsz requesting resource cpu=10m on Node 10.188.240.202
Aug 21 18:04:59.675: INFO: Pod node-ca-7vtw7 requesting resource cpu=10m on Node 10.188.240.230
Aug 21 18:04:59.675: INFO: Pod node-ca-wn9jz requesting resource cpu=10m on Node 10.188.240.222
Aug 21 18:04:59.675: INFO: Pod ingress-operator-695bc545b9-ps8wd requesting resource cpu=20m on Node 10.188.240.230
Aug 21 18:04:59.675: INFO: Pod router-default-79bfbd48f7-8pr9z requesting resource cpu=100m on Node 10.188.240.230
Aug 21 18:04:59.675: INFO: Pod router-default-79bfbd48f7-wq29r requesting resource cpu=100m on Node 10.188.240.222
Aug 21 18:04:59.675: INFO: Pod openshift-kube-proxy-dqhxm requesting resource cpu=100m on Node 10.188.240.202
Aug 21 18:04:59.675: INFO: Pod openshift-kube-proxy-j677g requesting resource cpu=100m on Node 10.188.240.230
Aug 21 18:04:59.675: INFO: Pod openshift-kube-proxy-w7zk7 requesting resource cpu=100m on Node 10.188.240.222
Aug 21 18:04:59.676: INFO: Pod certified-operators-6b6b9f965f-fthsb requesting resource cpu=10m on Node 10.188.240.230
Aug 21 18:04:59.676: INFO: Pod community-operators-68975cd7c8-h7glt requesting resource cpu=10m on Node 10.188.240.230
Aug 21 18:04:59.676: INFO: Pod marketplace-operator-6957767d58-5m7kk requesting resource cpu=10m on Node 10.188.240.230
Aug 21 18:04:59.676: INFO: Pod redhat-operators-6d986fdd47-pbjr6 requesting resource cpu=10m on Node 10.188.240.230
Aug 21 18:04:59.676: INFO: Pod alertmanager-main-0 requesting resource cpu=6m on Node 10.188.240.222
Aug 21 18:04:59.676: INFO: Pod alertmanager-main-1 requesting resource cpu=6m on Node 10.188.240.230
Aug 21 18:04:59.676: INFO: Pod alertmanager-main-2 requesting resource cpu=6m on Node 10.188.240.230
Aug 21 18:04:59.676: INFO: Pod cluster-monitoring-operator-5b5659466f-lbtcv requesting resource cpu=10m on Node 10.188.240.230
Aug 21 18:04:59.676: INFO: Pod grafana-c9c7455d7-2bzzz requesting resource cpu=5m on Node 10.188.240.230
Aug 21 18:04:59.676: INFO: Pod kube-state-metrics-c5f65645-vgmjg requesting resource cpu=4m on Node 10.188.240.222
Aug 21 18:04:59.676: INFO: Pod node-exporter-67tp6 requesting resource cpu=9m on Node 10.188.240.222
Aug 21 18:04:59.676: INFO: Pod node-exporter-jkv5q requesting resource cpu=9m on Node 10.188.240.230
Aug 21 18:04:59.676: INFO: Pod node-exporter-vw6s8 requesting resource cpu=9m on Node 10.188.240.202
Aug 21 18:04:59.676: INFO: Pod openshift-state-metrics-5849d797d8-lj8k9 requesting resource cpu=3m on Node 10.188.240.230
Aug 21 18:04:59.676: INFO: Pod prometheus-adapter-5697b6dddd-hpnk5 requesting resource cpu=1m on Node 10.188.240.230
Aug 21 18:04:59.676: INFO: Pod prometheus-adapter-5697b6dddd-jmhzs requesting resource cpu=1m on Node 10.188.240.222
Aug 21 18:04:59.676: INFO: Pod prometheus-k8s-0 requesting resource cpu=76m on Node 10.188.240.222
Aug 21 18:04:59.676: INFO: Pod prometheus-k8s-1 requesting resource cpu=76m on Node 10.188.240.230
Aug 21 18:04:59.676: INFO: Pod prometheus-operator-56d9d699cb-bvsq5 requesting resource cpu=5m on Node 10.188.240.222
Aug 21 18:04:59.676: INFO: Pod telemeter-client-6fdb57d68d-f82cq requesting resource cpu=3m on Node 10.188.240.222
Aug 21 18:04:59.676: INFO: Pod thanos-querier-5bcc6bd6c4-6wnxk requesting resource cpu=8m on Node 10.188.240.222
Aug 21 18:04:59.676: INFO: Pod thanos-querier-5bcc6bd6c4-7f4tt requesting resource cpu=8m on Node 10.188.240.222
Aug 21 18:04:59.677: INFO: Pod multus-4xs2x requesting resource cpu=10m on Node 10.188.240.222
Aug 21 18:04:59.677: INFO: Pod multus-59vjb requesting resource cpu=10m on Node 10.188.240.202
Aug 21 18:04:59.677: INFO: Pod multus-5fbcs requesting resource cpu=10m on Node 10.188.240.230
Aug 21 18:04:59.677: INFO: Pod multus-admission-controller-9x9x2 requesting resource cpu=10m on Node 10.188.240.230
Aug 21 18:04:59.677: INFO: Pod multus-admission-controller-t7nxb requesting resource cpu=10m on Node 10.188.240.202
Aug 21 18:04:59.677: INFO: Pod multus-admission-controller-tmrh2 requesting resource cpu=10m on Node 10.188.240.222
Aug 21 18:04:59.677: INFO: Pod network-operator-7986644c85-l5gh7 requesting resource cpu=10m on Node 10.188.240.222
Aug 21 18:04:59.677: INFO: Pod catalog-operator-85f6c659cc-dqh5m requesting resource cpu=10m on Node 10.188.240.230
Aug 21 18:04:59.677: INFO: Pod olm-operator-b5f57cdbb-nw9x4 requesting resource cpu=10m on Node 10.188.240.230
Aug 21 18:04:59.677: INFO: Pod packageserver-7f69bd69d9-d8d9l requesting resource cpu=10m on Node 10.188.240.222
Aug 21 18:04:59.677: INFO: Pod packageserver-7f69bd69d9-xtr6s requesting resource cpu=10m on Node 10.188.240.230
Aug 21 18:04:59.677: INFO: Pod service-ca-operator-694cfbf5d5-vxbp2 requesting resource cpu=10m on Node 10.188.240.230
Aug 21 18:04:59.677: INFO: Pod apiservice-cabundle-injector-594fd4555f-5fdl8 requesting resource cpu=10m on Node 10.188.240.222
Aug 21 18:04:59.677: INFO: Pod configmap-cabundle-injector-8446d4b88f-lc4xk requesting resource cpu=10m on Node 10.188.240.222
Aug 21 18:04:59.677: INFO: Pod service-serving-cert-signer-7879bf8d9f-nsbnq requesting resource cpu=10m on Node 10.188.240.230
Aug 21 18:04:59.677: INFO: Pod openshift-service-catalog-apiserver-operator-78b9dddb6f-jxgbd requesting resource cpu=0m on Node 10.188.240.230
Aug 21 18:04:59.677: INFO: Pod openshift-service-catalog-controller-manager-operator-5496stt64 requesting resource cpu=10m on Node 10.188.240.230
Aug 21 18:04:59.677: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.188.240.230
Aug 21 18:04:59.677: INFO: Pod sonobuoy-e2e-job-3399d07d932e4f9d requesting resource cpu=0m on Node 10.188.240.230
Aug 21 18:04:59.677: INFO: Pod sonobuoy-systemd-logs-daemon-set-5cea973cc7904f8c-m7w4l requesting resource cpu=0m on Node 10.188.240.222
Aug 21 18:04:59.677: INFO: Pod sonobuoy-systemd-logs-daemon-set-5cea973cc7904f8c-nfmgc requesting resource cpu=0m on Node 10.188.240.202
Aug 21 18:04:59.678: INFO: Pod sonobuoy-systemd-logs-daemon-set-5cea973cc7904f8c-rjxm6 requesting resource cpu=0m on Node 10.188.240.230
Aug 21 18:04:59.678: INFO: Pod tigera-operator-679798d94d-twz9q requesting resource cpu=100m on Node 10.188.240.222
STEP: Starting Pods to consume most of the cluster CPU.
Aug 21 18:04:59.678: INFO: Creating a pod which consumes cpu=2499m on Node 10.188.240.202
Aug 21 18:04:59.730: INFO: Creating a pod which consumes cpu=2226m on Node 10.188.240.222
Aug 21 18:04:59.770: INFO: Creating a pod which consumes cpu=2022m on Node 10.188.240.230
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0c5728e5-d655-49d5-82af-c89166bb3f10.162d5aa3343fe580], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6197/filler-pod-0c5728e5-d655-49d5-82af-c89166bb3f10 to 10.188.240.222]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0c5728e5-d655-49d5-82af-c89166bb3f10.162d5aa378c250bc], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0c5728e5-d655-49d5-82af-c89166bb3f10.162d5aa386be5d32], Reason = [Created], Message = [Created container filler-pod-0c5728e5-d655-49d5-82af-c89166bb3f10]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0c5728e5-d655-49d5-82af-c89166bb3f10.162d5aa3896755cd], Reason = [Started], Message = [Started container filler-pod-0c5728e5-d655-49d5-82af-c89166bb3f10]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-32d4edf1-47b6-493e-9832-33c10cfe1a9c.162d5aa336a0ef1a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6197/filler-pod-32d4edf1-47b6-493e-9832-33c10cfe1a9c to 10.188.240.230]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-32d4edf1-47b6-493e-9832-33c10cfe1a9c.162d5aa385955ed4], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-32d4edf1-47b6-493e-9832-33c10cfe1a9c.162d5aa3967ebd23], Reason = [Created], Message = [Created container filler-pod-32d4edf1-47b6-493e-9832-33c10cfe1a9c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-32d4edf1-47b6-493e-9832-33c10cfe1a9c.162d5aa399d456db], Reason = [Started], Message = [Started container filler-pod-32d4edf1-47b6-493e-9832-33c10cfe1a9c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-be2a8a68-4856-48be-a3d3-a9efb5a7a155.162d5aa331cadcf8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6197/filler-pod-be2a8a68-4856-48be-a3d3-a9efb5a7a155 to 10.188.240.202]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-be2a8a68-4856-48be-a3d3-a9efb5a7a155.162d5aa3718d80e8], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-be2a8a68-4856-48be-a3d3-a9efb5a7a155.162d5aa37c91fa81], Reason = [Created], Message = [Created container filler-pod-be2a8a68-4856-48be-a3d3-a9efb5a7a155]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-be2a8a68-4856-48be-a3d3-a9efb5a7a155.162d5aa37ea70ef9], Reason = [Started], Message = [Started container filler-pod-be2a8a68-4856-48be-a3d3-a9efb5a7a155]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.162d5aa42b975f1e], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.188.240.202
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.188.240.222
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.188.240.230
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 18:05:05.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6197" for this suite.
Aug 21 18:05:13.206: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:05:15.428: INFO: namespace sched-pred-6197 deletion completed in 10.274040981s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:16.626 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 18:05:15.428: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 18:05:15.608: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Aug 21 18:05:25.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 --namespace=crd-publish-openapi-2776 create -f -'
Aug 21 18:05:26.223: INFO: stderr: ""
Aug 21 18:05:26.223: INFO: stdout: "e2e-test-crd-publish-openapi-4014-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 21 18:05:26.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 --namespace=crd-publish-openapi-2776 delete e2e-test-crd-publish-openapi-4014-crds test-foo'
Aug 21 18:05:26.454: INFO: stderr: ""
Aug 21 18:05:26.454: INFO: stdout: "e2e-test-crd-publish-openapi-4014-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Aug 21 18:05:26.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 --namespace=crd-publish-openapi-2776 apply -f -'
Aug 21 18:05:26.963: INFO: stderr: ""
Aug 21 18:05:26.963: INFO: stdout: "e2e-test-crd-publish-openapi-4014-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 21 18:05:26.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 --namespace=crd-publish-openapi-2776 delete e2e-test-crd-publish-openapi-4014-crds test-foo'
Aug 21 18:05:27.179: INFO: stderr: ""
Aug 21 18:05:27.179: INFO: stdout: "e2e-test-crd-publish-openapi-4014-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Aug 21 18:05:27.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 --namespace=crd-publish-openapi-2776 create -f -'
Aug 21 18:05:27.737: INFO: rc: 1
Aug 21 18:05:27.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 --namespace=crd-publish-openapi-2776 apply -f -'
Aug 21 18:05:28.151: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Aug 21 18:05:28.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 --namespace=crd-publish-openapi-2776 create -f -'
Aug 21 18:05:28.770: INFO: rc: 1
Aug 21 18:05:28.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 --namespace=crd-publish-openapi-2776 apply -f -'
Aug 21 18:05:29.417: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Aug 21 18:05:29.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 explain e2e-test-crd-publish-openapi-4014-crds'
Aug 21 18:05:30.032: INFO: stderr: ""
Aug 21 18:05:30.032: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4014-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Aug 21 18:05:30.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 explain e2e-test-crd-publish-openapi-4014-crds.metadata'
Aug 21 18:05:30.472: INFO: stderr: ""
Aug 21 18:05:30.472: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4014-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Aug 21 18:05:30.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 explain e2e-test-crd-publish-openapi-4014-crds.spec'
Aug 21 18:05:31.122: INFO: stderr: ""
Aug 21 18:05:31.122: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4014-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Aug 21 18:05:31.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 explain e2e-test-crd-publish-openapi-4014-crds.spec.bars'
Aug 21 18:05:31.655: INFO: stderr: ""
Aug 21 18:05:31.655: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4014-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Aug 21 18:05:31.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 explain e2e-test-crd-publish-openapi-4014-crds.spec.bars2'
Aug 21 18:05:32.286: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 18:05:41.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2776" for this suite.
Aug 21 18:05:49.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:05:52.094: INFO: namespace crd-publish-openapi-2776 deletion completed in 10.575532861s

• [SLOW TEST:36.667 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 18:05:52.098: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 18:05:52.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 version'
Aug 21 18:05:52.465: INFO: stderr: ""
Aug 21 18:05:52.465: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.2\", GitCommit:\"c97fe5036ef3df2967d086711e6c0c405941e14b\", GitTreeState:\"clean\", BuildDate:\"2019-10-15T19:18:23Z\", GoVersion:\"go1.12.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16+\", GitVersion:\"v1.16.2+554af56\", GitCommit:\"554af56\", GitTreeState:\"clean\", BuildDate:\"2020-07-27T23:14:32Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 18:05:52.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7043" for this suite.
Aug 21 18:06:00.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:06:02.672: INFO: namespace kubectl-7043 deletion completed in 10.163744565s

• [SLOW TEST:10.574 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 18:06:02.672: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2926.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2926.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 21 18:06:07.107: INFO: DNS probes using dns-2926/dns-test-f425ec37-9a2d-4650-95d4-77c4c5f46fb0 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 18:06:07.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2926" for this suite.
Aug 21 18:06:15.247: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:06:17.360: INFO: namespace dns-2926 deletion completed in 10.170548892s

• [SLOW TEST:14.688 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 18:06:17.360: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 18:06:17.579: INFO: Creating ReplicaSet my-hostname-basic-aac79b36-1a60-4772-b7a3-ce0e81c5dbcd
Aug 21 18:06:17.612: INFO: Pod name my-hostname-basic-aac79b36-1a60-4772-b7a3-ce0e81c5dbcd: Found 0 pods out of 1
Aug 21 18:06:22.625: INFO: Pod name my-hostname-basic-aac79b36-1a60-4772-b7a3-ce0e81c5dbcd: Found 1 pods out of 1
Aug 21 18:06:22.625: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-aac79b36-1a60-4772-b7a3-ce0e81c5dbcd" is running
Aug 21 18:06:22.641: INFO: Pod "my-hostname-basic-aac79b36-1a60-4772-b7a3-ce0e81c5dbcd-rfz7f" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-21 18:06:17 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-21 18:06:19 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-21 18:06:19 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-21 18:06:17 +0000 UTC Reason: Message:}])
Aug 21 18:06:22.641: INFO: Trying to dial the pod
Aug 21 18:06:27.693: INFO: Controller my-hostname-basic-aac79b36-1a60-4772-b7a3-ce0e81c5dbcd: Got expected result from replica 1 [my-hostname-basic-aac79b36-1a60-4772-b7a3-ce0e81c5dbcd-rfz7f]: "my-hostname-basic-aac79b36-1a60-4772-b7a3-ce0e81c5dbcd-rfz7f", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 18:06:27.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1790" for this suite.
Aug 21 18:06:35.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:06:37.880: INFO: namespace replicaset-1790 deletion completed in 10.158566844s

• [SLOW TEST:20.520 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 18:06:37.881: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3773.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3773.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3773.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3773.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3773.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3773.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3773.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3773.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3773.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3773.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3773.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3773.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3773.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3773.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3773.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3773.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3773.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3773.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 21 18:06:42.210: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3773.svc.cluster.local from pod dns-3773/dns-test-8a7fae23-12f1-484a-9188-b155ced9b0d9: the server could not find the requested resource (get pods dns-test-8a7fae23-12f1-484a-9188-b155ced9b0d9)
Aug 21 18:06:42.226: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3773.svc.cluster.local from pod dns-3773/dns-test-8a7fae23-12f1-484a-9188-b155ced9b0d9: the server could not find the requested resource (get pods dns-test-8a7fae23-12f1-484a-9188-b155ced9b0d9)
Aug 21 18:06:42.347: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3773.svc.cluster.local from pod dns-3773/dns-test-8a7fae23-12f1-484a-9188-b155ced9b0d9: the server could not find the requested resource (get pods dns-test-8a7fae23-12f1-484a-9188-b155ced9b0d9)
Aug 21 18:06:42.372: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3773.svc.cluster.local from pod dns-3773/dns-test-8a7fae23-12f1-484a-9188-b155ced9b0d9: the server could not find the requested resource (get pods dns-test-8a7fae23-12f1-484a-9188-b155ced9b0d9)
Aug 21 18:06:42.390: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3773.svc.cluster.local from pod dns-3773/dns-test-8a7fae23-12f1-484a-9188-b155ced9b0d9: the server could not find the requested resource (get pods dns-test-8a7fae23-12f1-484a-9188-b155ced9b0d9)
Aug 21 18:06:42.446: INFO: Lookups using dns-3773/dns-test-8a7fae23-12f1-484a-9188-b155ced9b0d9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3773.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3773.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3773.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3773.svc.cluster.local jessie_udp@dns-test-service-2.dns-3773.svc.cluster.local]

Aug 21 18:06:47.679: INFO: DNS probes using dns-3773/dns-test-8a7fae23-12f1-484a-9188-b155ced9b0d9 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 18:06:47.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3773" for this suite.
Aug 21 18:06:55.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:06:57.942: INFO: namespace dns-3773 deletion completed in 10.137268519s

• [SLOW TEST:20.062 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 18:06:57.943: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-6368
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6368
STEP: Creating statefulset with conflicting port in namespace statefulset-6368
STEP: Waiting until pod test-pod will start running in namespace statefulset-6368
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6368
Aug 21 18:07:00.321: INFO: Observed stateful pod in namespace: statefulset-6368, name: ss-0, uid: 769db0c3-e041-48a5-b438-3ba855ea6480, status phase: Pending. Waiting for statefulset controller to delete.
Aug 21 18:07:00.349: INFO: Observed stateful pod in namespace: statefulset-6368, name: ss-0, uid: 769db0c3-e041-48a5-b438-3ba855ea6480, status phase: Failed. Waiting for statefulset controller to delete.
Aug 21 18:07:00.368: INFO: Observed stateful pod in namespace: statefulset-6368, name: ss-0, uid: 769db0c3-e041-48a5-b438-3ba855ea6480, status phase: Failed. Waiting for statefulset controller to delete.
Aug 21 18:07:00.382: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6368
STEP: Removing pod with conflicting port in namespace statefulset-6368
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6368 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Aug 21 18:07:04.483: INFO: Deleting all statefulset in ns statefulset-6368
Aug 21 18:07:04.497: INFO: Scaling statefulset ss to 0
Aug 21 18:07:14.553: INFO: Waiting for statefulset status.replicas updated to 0
Aug 21 18:07:14.564: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 18:07:14.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6368" for this suite.
Aug 21 18:07:24.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:07:26.753: INFO: namespace statefulset-6368 deletion completed in 12.111295399s

• [SLOW TEST:28.810 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 18:07:26.754: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 18:07:27.008: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-09eb94ae-29af-447d-a01b-1f1f0b880a4a" in namespace "security-context-test-9905" to be "success or failure"
Aug 21 18:07:27.019: INFO: Pod "busybox-privileged-false-09eb94ae-29af-447d-a01b-1f1f0b880a4a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.721665ms
Aug 21 18:07:29.036: INFO: Pod "busybox-privileged-false-09eb94ae-29af-447d-a01b-1f1f0b880a4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027755856s
Aug 21 18:07:29.036: INFO: Pod "busybox-privileged-false-09eb94ae-29af-447d-a01b-1f1f0b880a4a" satisfied condition "success or failure"
Aug 21 18:07:29.096: INFO: Got logs for pod "busybox-privileged-false-09eb94ae-29af-447d-a01b-1f1f0b880a4a": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 18:07:29.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9905" for this suite.
Aug 21 18:07:37.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:07:39.615: INFO: namespace security-context-test-9905 deletion completed in 10.491069407s

• [SLOW TEST:12.862 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 18:07:39.617: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 21 18:07:40.371: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 21 18:07:42.409: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733630060, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733630060, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733630060, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733630060, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 21 18:07:45.465: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Aug 21 18:07:49.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-192619208 attach --namespace=webhook-3613 to-be-attached-pod -i -c=container1'
Aug 21 18:07:49.866: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 18:07:49.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3613" for this suite.
Aug 21 18:07:59.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:08:02.115: INFO: namespace webhook-3613 deletion completed in 12.185242663s
STEP: Destroying namespace "webhook-3613-markers" for this suite.
Aug 21 18:08:10.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:08:12.270: INFO: namespace webhook-3613-markers deletion completed in 10.154612361s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:32.729 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 18:08:12.347: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 21 18:08:12.511: INFO: Creating deployment "webserver-deployment"
Aug 21 18:08:12.530: INFO: Waiting for observed generation 1
Aug 21 18:08:14.563: INFO: Waiting for all required pods to come up
Aug 21 18:08:14.581: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Aug 21 18:08:16.633: INFO: Waiting for deployment "webserver-deployment" to complete
Aug 21 18:08:16.662: INFO: Updating deployment "webserver-deployment" with a non-existent image
Aug 21 18:08:16.744: INFO: Updating deployment webserver-deployment
Aug 21 18:08:16.744: INFO: Waiting for observed generation 2
Aug 21 18:08:18.774: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 21 18:08:18.786: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 21 18:08:18.803: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 21 18:08:18.851: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 21 18:08:18.851: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 21 18:08:18.867: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 21 18:08:18.888: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Aug 21 18:08:18.889: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Aug 21 18:08:18.925: INFO: Updating deployment webserver-deployment
Aug 21 18:08:18.925: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Aug 21 18:08:18.948: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 21 18:08:20.969: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Aug 21 18:08:21.011: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1592 /apis/apps/v1/namespaces/deployment-1592/deployments/webserver-deployment 909f09f4-68e1-4238-aca5-bcd309a7ab85 101895 3 2020-08-21 18:08:12 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0007d2078 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-08-21 18:08:18 +0000 UTC,LastTransitionTime:2020-08-21 18:08:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-08-21 18:08:19 +0000 UTC,LastTransitionTime:2020-08-21 18:08:12 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Aug 21 18:08:21.024: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-1592 /apis/apps/v1/namespaces/deployment-1592/replicasets/webserver-deployment-c7997dcc8 f05e6133-d842-42ba-9747-427e3fd7e9fb 101891 3 2020-08-21 18:08:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 909f09f4-68e1-4238-aca5-bcd309a7ab85 0xc002fcc6a7 0xc002fcc6a8}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002fcc718 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 21 18:08:21.024: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Aug 21 18:08:21.025: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-1592 /apis/apps/v1/namespaces/deployment-1592/replicasets/webserver-deployment-595b5b9587 109239c1-baa2-45f0-b505-82fa8840f249 101837 3 2020-08-21 18:08:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 909f09f4-68e1-4238-aca5-bcd309a7ab85 0xc002fcc5a7 0xc002fcc5a8}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002fcc628 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Aug 21 18:08:21.052: INFO: Pod "webserver-deployment-595b5b9587-25g49" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-25g49 webserver-deployment-595b5b9587- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-595b5b9587-25g49 7217d36d-43fb-4e49-a9a4-051a4e608eb4 101866 0 2020-08-21 18:08:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 109239c1-baa2-45f0-b505-82fa8840f249 0xc0031d0f47 0xc0031d0f48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.222,PodIP:,StartTime:2020-08-21 18:08:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.053: INFO: Pod "webserver-deployment-595b5b9587-2w5mh" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2w5mh webserver-deployment-595b5b9587- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-595b5b9587-2w5mh 52fbe7a2-e5ae-40b1-9e89-5ea741936171 101636 0 2020-08-21 18:08:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.172.44/32 cni.projectcalico.org/podIPs:172.30.172.44/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.172.44"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 109239c1-baa2-45f0-b505-82fa8840f249 0xc0031d10e7 0xc0031d10e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.222,PodIP:172.30.172.44,StartTime:2020-08-21 18:08:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-21 18:08:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://b82a72d2d5f81939625811b563f9a52fa17e078ba0fd4984c6bf8be4e056d511,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.172.44,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.053: INFO: Pod "webserver-deployment-595b5b9587-4kgn5" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4kgn5 webserver-deployment-595b5b9587- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-595b5b9587-4kgn5 7571f730-c061-4756-8276-ba84716910ea 101975 0 2020-08-21 18:08:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.172.43/32 cni.projectcalico.org/podIPs:172.30.172.43/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.172.43"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 109239c1-baa2-45f0-b505-82fa8840f249 0xc0031d12a7 0xc0031d12a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.222,PodIP:,StartTime:2020-08-21 18:08:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.053: INFO: Pod "webserver-deployment-595b5b9587-4xpkv" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4xpkv webserver-deployment-595b5b9587- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-595b5b9587-4xpkv 2341d915-e911-4b84-b992-05f1732ff08b 101660 0 2020-08-21 18:08:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.174.107/32 cni.projectcalico.org/podIPs:172.30.174.107/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.174.107"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 109239c1-baa2-45f0-b505-82fa8840f249 0xc0031d1427 0xc0031d1428}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.230,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.230,PodIP:172.30.174.107,StartTime:2020-08-21 18:08:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-21 18:08:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://50b3f7ac4d710f93a2e416a1b8d41deff4a18f2cfa7f8a605778d6cb5fe33aae,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.174.107,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.054: INFO: Pod "webserver-deployment-595b5b9587-68ms2" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-68ms2 webserver-deployment-595b5b9587- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-595b5b9587-68ms2 6bcc8e62-5eb4-476a-b61d-28cc15608619 101633 0 2020-08-21 18:08:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.172.42/32 cni.projectcalico.org/podIPs:172.30.172.42/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.172.42"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 109239c1-baa2-45f0-b505-82fa8840f249 0xc0031d15e7 0xc0031d15e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.222,PodIP:172.30.172.42,StartTime:2020-08-21 18:08:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-21 18:08:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://e87fb35116126dfdb16c9182581df810d67f51be6f2abcdcdfa6ddbda759a02a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.172.42,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.055: INFO: Pod "webserver-deployment-595b5b9587-9lpn6" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-9lpn6 webserver-deployment-595b5b9587- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-595b5b9587-9lpn6 717da398-61d8-442f-a0a1-39d1c368e9e9 101878 0 2020-08-21 18:08:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 109239c1-baa2-45f0-b505-82fa8840f249 0xc0031d1787 0xc0031d1788}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.222,PodIP:,StartTime:2020-08-21 18:08:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.055: INFO: Pod "webserver-deployment-595b5b9587-bs4tb" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bs4tb webserver-deployment-595b5b9587- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-595b5b9587-bs4tb 23f039d8-89bd-48af-9071-86465e341a35 101638 0 2020-08-21 18:08:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.172.40/32 cni.projectcalico.org/podIPs:172.30.172.40/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.172.40"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 109239c1-baa2-45f0-b505-82fa8840f249 0xc0031d1927 0xc0031d1928}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.222,PodIP:172.30.172.40,StartTime:2020-08-21 18:08:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-21 18:08:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://42c14e3e491a7b1208114a1693c4ea54901d2364aab38a41b7acfe02d1afb64d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.172.40,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.055: INFO: Pod "webserver-deployment-595b5b9587-fjg2r" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-fjg2r webserver-deployment-595b5b9587- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-595b5b9587-fjg2r 65364310-0ac4-4824-817a-1ba39605d2c5 101877 0 2020-08-21 18:08:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 109239c1-baa2-45f0-b505-82fa8840f249 0xc0031d1ad7 0xc0031d1ad8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.202,PodIP:,StartTime:2020-08-21 18:08:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.057: INFO: Pod "webserver-deployment-595b5b9587-gsqf2" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-gsqf2 webserver-deployment-595b5b9587- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-595b5b9587-gsqf2 4633f321-9d7c-427c-96ba-ea10c68c51af 101928 0 2020-08-21 18:08:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.245.83/32 cni.projectcalico.org/podIPs:172.30.245.83/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.245.83"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 109239c1-baa2-45f0-b505-82fa8840f249 0xc0031d1c77 0xc0031d1c78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.202,PodIP:,StartTime:2020-08-21 18:08:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.057: INFO: Pod "webserver-deployment-595b5b9587-h57bs" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-h57bs webserver-deployment-595b5b9587- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-595b5b9587-h57bs 680f8a23-b8fa-4092-8b78-9bf73801661e 101976 0 2020-08-21 18:08:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.245.84/32 cni.projectcalico.org/podIPs:172.30.245.84/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.245.84"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 109239c1-baa2-45f0-b505-82fa8840f249 0xc0031d1e27 0xc0031d1e28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.202,PodIP:,StartTime:2020-08-21 18:08:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.057: INFO: Pod "webserver-deployment-595b5b9587-js7bk" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-js7bk webserver-deployment-595b5b9587- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-595b5b9587-js7bk 9eefe808-463d-4812-95f6-51bb700bf6bf 101925 0 2020-08-21 18:08:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.174.112/32 cni.projectcalico.org/podIPs:172.30.174.112/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.174.112"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 109239c1-baa2-45f0-b505-82fa8840f249 0xc0031d1fa7 0xc0031d1fa8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.230,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.230,PodIP:,StartTime:2020-08-21 18:08:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.058: INFO: Pod "webserver-deployment-595b5b9587-jv5qd" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-jv5qd webserver-deployment-595b5b9587- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-595b5b9587-jv5qd a33f3171-b398-446c-8011-11d761e1e772 101847 0 2020-08-21 18:08:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 109239c1-baa2-45f0-b505-82fa8840f249 0xc00058a467 0xc00058a468}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.202,PodIP:,StartTime:2020-08-21 18:08:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.058: INFO: Pod "webserver-deployment-595b5b9587-ktl5g" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-ktl5g webserver-deployment-595b5b9587- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-595b5b9587-ktl5g da4ba86c-2ae2-46a5-9d01-0de60abf0bd4 101856 0 2020-08-21 18:08:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 109239c1-baa2-45f0-b505-82fa8840f249 0xc00058aa77 0xc00058aa78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.222,PodIP:,StartTime:2020-08-21 18:08:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.058: INFO: Pod "webserver-deployment-595b5b9587-m5z7v" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-m5z7v webserver-deployment-595b5b9587- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-595b5b9587-m5z7v d3a343f3-44d5-4a3b-a094-2c0bbe3d8a1b 101645 0 2020-08-21 18:08:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.245.75/32 cni.projectcalico.org/podIPs:172.30.245.75/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.245.75"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 109239c1-baa2-45f0-b505-82fa8840f249 0xc00058b107 0xc00058b108}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.202,PodIP:172.30.245.75,StartTime:2020-08-21 18:08:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-21 18:08:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://d8c702ae9c83f224e3123332e2acb5862dea15448988a01e7cbcdc35bc5a508e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.245.75,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.058: INFO: Pod "webserver-deployment-595b5b9587-m9qgs" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-m9qgs webserver-deployment-595b5b9587- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-595b5b9587-m9qgs abec8986-77f9-45d8-bfef-890bdbdf0149 101652 0 2020-08-21 18:08:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.245.124/32 cni.projectcalico.org/podIPs:172.30.245.124/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.245.124"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 109239c1-baa2-45f0-b505-82fa8840f249 0xc00058b897 0xc00058b898}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.202,PodIP:172.30.245.124,StartTime:2020-08-21 18:08:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-21 18:08:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://a406fa8273a857b3a4ad3f5bf825763ee45198a526725e23384e16f92cad110a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.245.124,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.059: INFO: Pod "webserver-deployment-595b5b9587-mp596" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mp596 webserver-deployment-595b5b9587- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-595b5b9587-mp596 d21235d0-d912-45c2-af66-0748e443bdd6 101986 0 2020-08-21 18:08:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.245.90/32 cni.projectcalico.org/podIPs:172.30.245.90/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.245.90"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 109239c1-baa2-45f0-b505-82fa8840f249 0xc000054577 0xc000054578}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.202,PodIP:,StartTime:2020-08-21 18:08:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.059: INFO: Pod "webserver-deployment-595b5b9587-pdw9m" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-pdw9m webserver-deployment-595b5b9587- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-595b5b9587-pdw9m da9f0325-ba4d-48b1-ae2f-58f8e1f0bd5f 101842 0 2020-08-21 18:08:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 109239c1-baa2-45f0-b505-82fa8840f249 0xc0000548b7 0xc0000548b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.222,PodIP:,StartTime:2020-08-21 18:08:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.059: INFO: Pod "webserver-deployment-595b5b9587-t5c88" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-t5c88 webserver-deployment-595b5b9587- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-595b5b9587-t5c88 19c61b22-35cc-4260-b78b-5d969dc1d8df 101647 0 2020-08-21 18:08:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.245.74/32 cni.projectcalico.org/podIPs:172.30.245.74/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.245.74"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 109239c1-baa2-45f0-b505-82fa8840f249 0xc000054f67 0xc000054f68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.202,PodIP:172.30.245.74,StartTime:2020-08-21 18:08:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-21 18:08:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://36c0a4104e5677c294ee9cfc64e61371e513998e4a6205d46d4e8b3e771d2234,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.245.74,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.060: INFO: Pod "webserver-deployment-595b5b9587-tgzgd" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-tgzgd webserver-deployment-595b5b9587- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-595b5b9587-tgzgd d2077621-309d-4674-9045-3a7ce83aab26 101659 0 2020-08-21 18:08:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.245.127/32 cni.projectcalico.org/podIPs:172.30.245.127/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.245.127"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 109239c1-baa2-45f0-b505-82fa8840f249 0xc000055827 0xc000055828}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.202,PodIP:172.30.245.127,StartTime:2020-08-21 18:08:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-21 18:08:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://841e84601e389740dcb6e6f9dabb1f563c764960ff0abba69a713fe25d43e50b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.245.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.060: INFO: Pod "webserver-deployment-595b5b9587-tsp7c" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-tsp7c webserver-deployment-595b5b9587- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-595b5b9587-tsp7c 58774139-1581-4c68-a869-669046d3dff3 101950 0 2020-08-21 18:08:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.245.86/32 cni.projectcalico.org/podIPs:172.30.245.86/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.245.86"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 109239c1-baa2-45f0-b505-82fa8840f249 0xc000055b77 0xc000055b78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.202,PodIP:,StartTime:2020-08-21 18:08:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.060: INFO: Pod "webserver-deployment-c7997dcc8-7jfs9" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-7jfs9 webserver-deployment-c7997dcc8- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-c7997dcc8-7jfs9 eb017f09-f407-435a-b05d-9112cf07a343 101916 0 2020-08-21 18:08:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.245.66/32 cni.projectcalico.org/podIPs:172.30.245.66/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.245.66"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 f05e6133-d842-42ba-9747-427e3fd7e9fb 0xc00035e0b7 0xc00035e0b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.202,PodIP:172.30.245.66,StartTime:2020-08-21 18:08:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.245.66,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.061: INFO: Pod "webserver-deployment-c7997dcc8-bpnzv" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-bpnzv webserver-deployment-c7997dcc8- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-c7997dcc8-bpnzv ff7de39c-ca9d-4d58-9b49-99457e30fdd1 101906 0 2020-08-21 18:08:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.172.45/32 cni.projectcalico.org/podIPs:172.30.172.45/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.172.45"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 f05e6133-d842-42ba-9747-427e3fd7e9fb 0xc00035f017 0xc00035f018}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.222,PodIP:172.30.172.45,StartTime:2020-08-21 18:08:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.172.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.061: INFO: Pod "webserver-deployment-c7997dcc8-bqwwd" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-bqwwd webserver-deployment-c7997dcc8- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-c7997dcc8-bqwwd 3a23d15c-d5e9-428c-916b-a441e378cd78 101923 0 2020-08-21 18:08:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.174.108/32 cni.projectcalico.org/podIPs:172.30.174.108/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.174.108"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 f05e6133-d842-42ba-9747-427e3fd7e9fb 0xc00035fce7 0xc00035fce8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.230,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.230,PodIP:172.30.174.108,StartTime:2020-08-21 18:08:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.174.108,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.061: INFO: Pod "webserver-deployment-c7997dcc8-dskqn" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-dskqn webserver-deployment-c7997dcc8- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-c7997dcc8-dskqn ab28e7e8-322c-43d5-bb2e-94f574f6a455 101912 0 2020-08-21 18:08:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.245.79/32 cni.projectcalico.org/podIPs:172.30.245.79/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.245.79"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 f05e6133-d842-42ba-9747-427e3fd7e9fb 0xc0004a1d27 0xc0004a1d28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.202,PodIP:172.30.245.79,StartTime:2020-08-21 18:08:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.245.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.061: INFO: Pod "webserver-deployment-c7997dcc8-h6fs6" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-h6fs6 webserver-deployment-c7997dcc8- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-c7997dcc8-h6fs6 f93c9272-30ca-4f43-94b6-c17bf504e717 101899 0 2020-08-21 18:08:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 f05e6133-d842-42ba-9747-427e3fd7e9fb 0xc002ad4017 0xc002ad4018}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.222,PodIP:,StartTime:2020-08-21 18:08:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.062: INFO: Pod "webserver-deployment-c7997dcc8-k9kps" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-k9kps webserver-deployment-c7997dcc8- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-c7997dcc8-k9kps 3863c404-c0e8-4d91-a4d7-da6b5afaf3ff 101881 0 2020-08-21 18:08:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 f05e6133-d842-42ba-9747-427e3fd7e9fb 0xc002ad41b7 0xc002ad41b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.222,PodIP:,StartTime:2020-08-21 18:08:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.062: INFO: Pod "webserver-deployment-c7997dcc8-qcrz7" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qcrz7 webserver-deployment-c7997dcc8- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-c7997dcc8-qcrz7 03d70136-2ca8-46ba-b0f9-2c0a9928bd9a 101909 0 2020-08-21 18:08:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.245.72/32 cni.projectcalico.org/podIPs:172.30.245.72/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.245.72"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 f05e6133-d842-42ba-9747-427e3fd7e9fb 0xc002ad4377 0xc002ad4378}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.202,PodIP:172.30.245.72,StartTime:2020-08-21 18:08:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.245.72,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.062: INFO: Pod "webserver-deployment-c7997dcc8-qph8n" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qph8n webserver-deployment-c7997dcc8- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-c7997dcc8-qph8n 496556bd-f232-4b16-8115-72a902962b8d 101898 0 2020-08-21 18:08:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 f05e6133-d842-42ba-9747-427e3fd7e9fb 0xc002ad4547 0xc002ad4548}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.202,PodIP:,StartTime:2020-08-21 18:08:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.062: INFO: Pod "webserver-deployment-c7997dcc8-v7f49" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-v7f49 webserver-deployment-c7997dcc8- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-c7997dcc8-v7f49 0b084e39-c87d-44f0-9420-51f5cba1795b 101962 0 2020-08-21 18:08:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.245.91/32 cni.projectcalico.org/podIPs:172.30.245.91/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.245.91"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 f05e6133-d842-42ba-9747-427e3fd7e9fb 0xc002ad4707 0xc002ad4708}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.202,PodIP:,StartTime:2020-08-21 18:08:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.063: INFO: Pod "webserver-deployment-c7997dcc8-x4lv7" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-x4lv7 webserver-deployment-c7997dcc8- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-c7997dcc8-x4lv7 29aa593e-ce65-4bc8-8038-78364820f52b 101887 0 2020-08-21 18:08:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 f05e6133-d842-42ba-9747-427e3fd7e9fb 0xc002ad48b7 0xc002ad48b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.222,PodIP:,StartTime:2020-08-21 18:08:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.063: INFO: Pod "webserver-deployment-c7997dcc8-xffvr" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-xffvr webserver-deployment-c7997dcc8- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-c7997dcc8-xffvr e2bb1913-a8e4-44cb-b8ec-5b35d9115fae 101817 0 2020-08-21 18:08:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 f05e6133-d842-42ba-9747-427e3fd7e9fb 0xc002ad4a77 0xc002ad4a78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.222,PodIP:,StartTime:2020-08-21 18:08:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.063: INFO: Pod "webserver-deployment-c7997dcc8-z86h6" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-z86h6 webserver-deployment-c7997dcc8- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-c7997dcc8-z86h6 2830242c-99d0-4d3a-a745-8fc537435ce4 101936 0 2020-08-21 18:08:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.174.111/32 cni.projectcalico.org/podIPs:172.30.174.111/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.174.111"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 f05e6133-d842-42ba-9747-427e3fd7e9fb 0xc002ad4c17 0xc002ad4c18}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.230,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.230,PodIP:,StartTime:2020-08-21 18:08:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 21 18:08:21.064: INFO: Pod "webserver-deployment-c7997dcc8-zgklh" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-zgklh webserver-deployment-c7997dcc8- deployment-1592 /api/v1/namespaces/deployment-1592/pods/webserver-deployment-c7997dcc8-zgklh 39f6a0bc-ae51-41ab-a9d4-5d9e5e363507 101896 0 2020-08-21 18:08:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 f05e6133-d842-42ba-9747-427e3fd7e9fb 0xc002ad4db7 0xc002ad4db8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7jbfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7jbfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7jbfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.188.240.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fd4pk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-21 18:08:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.188.240.222,PodIP:,StartTime:2020-08-21 18:08:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 18:08:21.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1592" for this suite.
Aug 21 18:08:33.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:08:35.319: INFO: namespace deployment-1592 deletion completed in 14.232842951s

• [SLOW TEST:22.973 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 18:08:35.321: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 18:08:35.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6936" for this suite.
Aug 21 18:08:43.699: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:08:45.769: INFO: namespace services-6936 deletion completed in 10.146555247s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:10.448 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 18:08:45.769: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Aug 21 18:08:46.042: INFO: Waiting up to 5m0s for pod "downward-api-6273f578-fd2b-4e81-ab8f-2fe8d990d490" in namespace "downward-api-5352" to be "success or failure"
Aug 21 18:08:46.052: INFO: Pod "downward-api-6273f578-fd2b-4e81-ab8f-2fe8d990d490": Phase="Pending", Reason="", readiness=false. Elapsed: 10.408475ms
Aug 21 18:08:48.065: INFO: Pod "downward-api-6273f578-fd2b-4e81-ab8f-2fe8d990d490": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023345378s
STEP: Saw pod success
Aug 21 18:08:48.065: INFO: Pod "downward-api-6273f578-fd2b-4e81-ab8f-2fe8d990d490" satisfied condition "success or failure"
Aug 21 18:08:48.078: INFO: Trying to get logs from node 10.188.240.202 pod downward-api-6273f578-fd2b-4e81-ab8f-2fe8d990d490 container dapi-container: <nil>
STEP: delete the pod
Aug 21 18:08:48.157: INFO: Waiting for pod downward-api-6273f578-fd2b-4e81-ab8f-2fe8d990d490 to disappear
Aug 21 18:08:48.169: INFO: Pod downward-api-6273f578-fd2b-4e81-ab8f-2fe8d990d490 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 18:08:48.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5352" for this suite.
Aug 21 18:08:56.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:08:58.321: INFO: namespace downward-api-5352 deletion completed in 10.129112423s

• [SLOW TEST:12.552 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 18:08:58.321: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 21 18:08:58.580: INFO: Waiting up to 5m0s for pod "downwardapi-volume-54f960ae-7bfb-416e-9ed2-680ac34076c0" in namespace "projected-7952" to be "success or failure"
Aug 21 18:08:58.591: INFO: Pod "downwardapi-volume-54f960ae-7bfb-416e-9ed2-680ac34076c0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.372712ms
Aug 21 18:09:00.603: INFO: Pod "downwardapi-volume-54f960ae-7bfb-416e-9ed2-680ac34076c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023042374s
STEP: Saw pod success
Aug 21 18:09:00.603: INFO: Pod "downwardapi-volume-54f960ae-7bfb-416e-9ed2-680ac34076c0" satisfied condition "success or failure"
Aug 21 18:09:00.614: INFO: Trying to get logs from node 10.188.240.202 pod downwardapi-volume-54f960ae-7bfb-416e-9ed2-680ac34076c0 container client-container: <nil>
STEP: delete the pod
Aug 21 18:09:00.704: INFO: Waiting for pod downwardapi-volume-54f960ae-7bfb-416e-9ed2-680ac34076c0 to disappear
Aug 21 18:09:00.717: INFO: Pod downwardapi-volume-54f960ae-7bfb-416e-9ed2-680ac34076c0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 18:09:00.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7952" for this suite.
Aug 21 18:09:08.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:09:11.254: INFO: namespace projected-7952 deletion completed in 10.49489537s

• [SLOW TEST:12.932 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 21 18:09:11.254: INFO: >>> kubeConfig: /tmp/kubeconfig-192619208
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-ef0c4963-f89d-4ce1-aa43-9ea994581487
STEP: Creating a pod to test consume configMaps
Aug 21 18:09:11.519: INFO: Waiting up to 5m0s for pod "pod-configmaps-ad73c66c-81a5-474b-bee7-6f745469e456" in namespace "configmap-4897" to be "success or failure"
Aug 21 18:09:11.530: INFO: Pod "pod-configmaps-ad73c66c-81a5-474b-bee7-6f745469e456": Phase="Pending", Reason="", readiness=false. Elapsed: 9.974146ms
Aug 21 18:09:13.545: INFO: Pod "pod-configmaps-ad73c66c-81a5-474b-bee7-6f745469e456": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024841526s
Aug 21 18:09:15.557: INFO: Pod "pod-configmaps-ad73c66c-81a5-474b-bee7-6f745469e456": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036882258s
STEP: Saw pod success
Aug 21 18:09:15.557: INFO: Pod "pod-configmaps-ad73c66c-81a5-474b-bee7-6f745469e456" satisfied condition "success or failure"
Aug 21 18:09:15.568: INFO: Trying to get logs from node 10.188.240.202 pod pod-configmaps-ad73c66c-81a5-474b-bee7-6f745469e456 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 21 18:09:15.651: INFO: Waiting for pod pod-configmaps-ad73c66c-81a5-474b-bee7-6f745469e456 to disappear
Aug 21 18:09:15.665: INFO: Pod pod-configmaps-ad73c66c-81a5-474b-bee7-6f745469e456 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 21 18:09:15.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4897" for this suite.
Aug 21 18:09:23.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 21 18:09:25.866: INFO: namespace configmap-4897 deletion completed in 10.161176059s

• [SLOW TEST:14.612 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSAug 21 18:09:25.866: INFO: Running AfterSuite actions on all nodes
Aug 21 18:09:25.866: INFO: Running AfterSuite actions on node 1
Aug 21 18:09:25.866: INFO: Skipping dumping logs from cluster

Ran 276 of 4897 Specs in 8982.878 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Pending | 4621 Skipped
PASS

Ginkgo ran 1 suite in 2h29m44.790804519s
Test Suite Passed
