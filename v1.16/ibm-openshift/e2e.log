I1026 18:21:26.108493      23 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-668520336
I1026 18:21:26.108912      23 e2e.go:92] Starting e2e run "d1b6ffe8-28ca-4ed3-80d4-f18a1a22efac" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1603736484 - Will randomize all specs
Will run 276 of 4897 specs

Oct 26 18:21:26.124: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 18:21:26.130: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Oct 26 18:21:26.197: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Oct 26 18:21:26.261: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Oct 26 18:21:26.261: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Oct 26 18:21:26.261: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Oct 26 18:21:26.280: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Oct 26 18:21:26.280: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
Oct 26 18:21:26.280: INFO: e2e test version: v1.16.2
Oct 26 18:21:26.285: INFO: kube-apiserver version: v1.16.2+417b9fd
Oct 26 18:21:26.285: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 18:21:26.300: INFO: Cluster IP family: ipv4
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:21:26.301: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename job
Oct 26 18:21:26.479: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Oct 26 18:21:41.063: INFO: Successfully updated pod "adopt-release-jwtft"
STEP: Checking that the Job readopts the Pod
Oct 26 18:21:41.063: INFO: Waiting up to 15m0s for pod "adopt-release-jwtft" in namespace "job-7066" to be "adopted"
Oct 26 18:21:41.073: INFO: Pod "adopt-release-jwtft": Phase="Running", Reason="", readiness=true. Elapsed: 9.710813ms
Oct 26 18:21:43.087: INFO: Pod "adopt-release-jwtft": Phase="Running", Reason="", readiness=true. Elapsed: 2.023613822s
Oct 26 18:21:43.087: INFO: Pod "adopt-release-jwtft" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Oct 26 18:21:43.633: INFO: Successfully updated pod "adopt-release-jwtft"
STEP: Checking that the Job releases the Pod
Oct 26 18:21:43.633: INFO: Waiting up to 15m0s for pod "adopt-release-jwtft" in namespace "job-7066" to be "released"
Oct 26 18:21:43.644: INFO: Pod "adopt-release-jwtft": Phase="Running", Reason="", readiness=true. Elapsed: 10.281862ms
Oct 26 18:21:45.655: INFO: Pod "adopt-release-jwtft": Phase="Running", Reason="", readiness=true. Elapsed: 2.021216715s
Oct 26 18:21:45.655: INFO: Pod "adopt-release-jwtft" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:21:45.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7066" for this suite.
Oct 26 18:22:35.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:22:37.758: INFO: namespace job-7066 deletion completed in 52.088851684s

• [SLOW TEST:71.458 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:22:37.758: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Oct 26 18:22:37.983: INFO: Waiting up to 5m0s for pod "downward-api-e27ad2ca-11b4-443b-a479-2f3c20fc3235" in namespace "downward-api-8580" to be "success or failure"
Oct 26 18:22:37.994: INFO: Pod "downward-api-e27ad2ca-11b4-443b-a479-2f3c20fc3235": Phase="Pending", Reason="", readiness=false. Elapsed: 10.680622ms
Oct 26 18:22:40.006: INFO: Pod "downward-api-e27ad2ca-11b4-443b-a479-2f3c20fc3235": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022812484s
STEP: Saw pod success
Oct 26 18:22:40.006: INFO: Pod "downward-api-e27ad2ca-11b4-443b-a479-2f3c20fc3235" satisfied condition "success or failure"
Oct 26 18:22:40.017: INFO: Trying to get logs from node 10.123.240.172 pod downward-api-e27ad2ca-11b4-443b-a479-2f3c20fc3235 container dapi-container: <nil>
STEP: delete the pod
Oct 26 18:22:40.179: INFO: Waiting for pod downward-api-e27ad2ca-11b4-443b-a479-2f3c20fc3235 to disappear
Oct 26 18:22:40.191: INFO: Pod downward-api-e27ad2ca-11b4-443b-a479-2f3c20fc3235 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:22:40.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8580" for this suite.
Oct 26 18:22:48.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:22:50.352: INFO: namespace downward-api-8580 deletion completed in 10.142968104s

• [SLOW TEST:12.594 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:22:50.354: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 18:22:50.535: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:22:53.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2088" for this suite.
Oct 26 18:23:41.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:23:43.158: INFO: namespace pods-2088 deletion completed in 50.093715237s

• [SLOW TEST:52.805 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:23:43.159: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Oct 26 18:23:54.041: INFO: Successfully updated pod "pod-update-activedeadlineseconds-5e64df2a-ab27-4e12-9f1c-82481a1ef7fc"
Oct 26 18:23:54.041: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5e64df2a-ab27-4e12-9f1c-82481a1ef7fc" in namespace "pods-5522" to be "terminated due to deadline exceeded"
Oct 26 18:23:54.052: INFO: Pod "pod-update-activedeadlineseconds-5e64df2a-ab27-4e12-9f1c-82481a1ef7fc": Phase="Running", Reason="", readiness=true. Elapsed: 11.121833ms
Oct 26 18:23:56.062: INFO: Pod "pod-update-activedeadlineseconds-5e64df2a-ab27-4e12-9f1c-82481a1ef7fc": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.021700362s
Oct 26 18:23:56.062: INFO: Pod "pod-update-activedeadlineseconds-5e64df2a-ab27-4e12-9f1c-82481a1ef7fc" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:23:56.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5522" for this suite.
Oct 26 18:24:04.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:24:06.125: INFO: namespace pods-5522 deletion completed in 10.026953499s

• [SLOW TEST:22.967 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:24:06.127: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-402f74bb-6dbc-4721-81f0-c9b82bf84edd
STEP: Creating a pod to test consume secrets
Oct 26 18:24:06.323: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a3ec4acd-1d91-47c2-a23d-b318b392cc67" in namespace "projected-507" to be "success or failure"
Oct 26 18:24:06.336: INFO: Pod "pod-projected-secrets-a3ec4acd-1d91-47c2-a23d-b318b392cc67": Phase="Pending", Reason="", readiness=false. Elapsed: 12.073358ms
Oct 26 18:24:08.376: INFO: Pod "pod-projected-secrets-a3ec4acd-1d91-47c2-a23d-b318b392cc67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052001631s
Oct 26 18:24:10.412: INFO: Pod "pod-projected-secrets-a3ec4acd-1d91-47c2-a23d-b318b392cc67": Phase="Pending", Reason="", readiness=false. Elapsed: 4.088838652s
Oct 26 18:24:12.424: INFO: Pod "pod-projected-secrets-a3ec4acd-1d91-47c2-a23d-b318b392cc67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.100541541s
STEP: Saw pod success
Oct 26 18:24:12.424: INFO: Pod "pod-projected-secrets-a3ec4acd-1d91-47c2-a23d-b318b392cc67" satisfied condition "success or failure"
Oct 26 18:24:12.435: INFO: Trying to get logs from node 10.123.240.172 pod pod-projected-secrets-a3ec4acd-1d91-47c2-a23d-b318b392cc67 container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 26 18:24:12.517: INFO: Waiting for pod pod-projected-secrets-a3ec4acd-1d91-47c2-a23d-b318b392cc67 to disappear
Oct 26 18:24:12.527: INFO: Pod pod-projected-secrets-a3ec4acd-1d91-47c2-a23d-b318b392cc67 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:24:12.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-507" for this suite.
Oct 26 18:24:20.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:24:22.682: INFO: namespace projected-507 deletion completed in 10.131570422s

• [SLOW TEST:16.556 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:24:22.683: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:24:22.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9213" for this suite.
Oct 26 18:24:31.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:24:33.090: INFO: namespace resourcequota-9213 deletion completed in 10.118617382s

• [SLOW TEST:10.407 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:24:33.090: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Oct 26 18:24:33.349: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1411 /api/v1/namespaces/watch-1411/configmaps/e2e-watch-test-resource-version 738f8ace-8ede-4e36-a230-d734fb4d884d 39967 0 2020-10-26 18:24:33 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Oct 26 18:24:33.349: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1411 /api/v1/namespaces/watch-1411/configmaps/e2e-watch-test-resource-version 738f8ace-8ede-4e36-a230-d734fb4d884d 39969 0 2020-10-26 18:24:33 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:24:33.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1411" for this suite.
Oct 26 18:24:41.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:24:43.372: INFO: namespace watch-1411 deletion completed in 9.988786622s

• [SLOW TEST:10.282 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:24:43.373: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Oct 26 18:24:43.550: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Oct 26 18:24:43.620: INFO: Waiting for terminating namespaces to be deleted...
Oct 26 18:24:43.636: INFO: 
Logging pods the kubelet thinks is on node 10.123.240.172 before test
Oct 26 18:24:43.719: INFO: cluster-storage-operator-6c6dd4b587-52s64 from openshift-cluster-storage-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.719: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Oct 26 18:24:43.719: INFO: openshift-service-catalog-apiserver-operator-d6cf765ff-k9snw from openshift-service-catalog-apiserver-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.719: INFO: 	Container operator ready: true, restart count 1
Oct 26 18:24:43.719: INFO: sonobuoy from sonobuoy started at 2020-10-26 18:20:41 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.719: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Oct 26 18:24:43.719: INFO: prometheus-adapter-6cccbf8dbb-4bpjw from openshift-monitoring started at 2020-10-26 17:05:48 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.719: INFO: 	Container prometheus-adapter ready: true, restart count 0
Oct 26 18:24:43.719: INFO: ibmcloud-block-storage-driver-fm8d2 from kube-system started at 2020-10-26 16:56:55 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.720: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Oct 26 18:24:43.720: INFO: console-operator-7f9f78ff66-kf42x from openshift-console-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.720: INFO: 	Container console-operator ready: true, restart count 1
Oct 26 18:24:43.720: INFO: calico-kube-controllers-599969f895-cnxrs from calico-system started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.720: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Oct 26 18:24:43.720: INFO: multus-admission-controller-4dblm from openshift-multus started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.720: INFO: 	Container multus-admission-controller ready: true, restart count 0
Oct 26 18:24:43.720: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-10-26 17:05:50 +0000 UTC (3 container statuses recorded)
Oct 26 18:24:43.720: INFO: 	Container alertmanager ready: true, restart count 0
Oct 26 18:24:43.720: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 26 18:24:43.721: INFO: 	Container config-reloader ready: true, restart count 0
Oct 26 18:24:43.721: INFO: ibm-master-proxy-static-10.123.240.172 from kube-system started at 2020-10-26 16:56:50 +0000 UTC (2 container statuses recorded)
Oct 26 18:24:43.721: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 18:24:43.721: INFO: 	Container pause ready: true, restart count 0
Oct 26 18:24:43.721: INFO: multus-wk7tv from openshift-multus started at 2020-10-26 16:57:19 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.721: INFO: 	Container kube-multus ready: true, restart count 0
Oct 26 18:24:43.721: INFO: openshift-kube-proxy-k67gq from openshift-kube-proxy started at 2020-10-26 16:57:26 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.721: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 26 18:24:43.721: INFO: cluster-node-tuning-operator-86b7f98f7b-qjdxp from openshift-cluster-node-tuning-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.721: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Oct 26 18:24:43.722: INFO: node-exporter-pbqtq from openshift-monitoring started at 2020-10-26 16:59:03 +0000 UTC (2 container statuses recorded)
Oct 26 18:24:43.722: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 18:24:43.722: INFO: 	Container node-exporter ready: true, restart count 0
Oct 26 18:24:43.722: INFO: calico-typha-6b7867b64d-wwhp2 from calico-system started at 2020-10-26 17:00:10 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.722: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 18:24:43.722: INFO: ibm-keepalived-watcher-rdht2 from kube-system started at 2020-10-26 16:56:52 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.722: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 18:24:43.722: INFO: marketplace-operator-c74f66688-865bs from openshift-marketplace started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.722: INFO: 	Container marketplace-operator ready: true, restart count 0
Oct 26 18:24:43.722: INFO: openshift-service-catalog-controller-manager-operator-6fcbsks6s from openshift-service-catalog-controller-manager-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.723: INFO: 	Container operator ready: true, restart count 1
Oct 26 18:24:43.723: INFO: cluster-monitoring-operator-77bbbf9cb7-sd49n from openshift-monitoring started at 2020-10-26 16:58:24 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.723: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Oct 26 18:24:43.723: INFO: tuned-f8xvt from openshift-cluster-node-tuning-operator started at 2020-10-26 17:00:14 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.723: INFO: 	Container tuned ready: true, restart count 0
Oct 26 18:24:43.723: INFO: ibm-cloud-provider-ip-149-81-128-30-5bdf48d794-vdds8 from ibm-system started at 2020-10-26 17:10:29 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.723: INFO: 	Container ibm-cloud-provider-ip-149-81-128-30 ready: true, restart count 0
Oct 26 18:24:43.723: INFO: sonobuoy-systemd-logs-daemon-set-a94f7e2648a3464d-9nhrv from sonobuoy started at 2020-10-26 18:20:50 +0000 UTC (2 container statuses recorded)
Oct 26 18:24:43.723: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 26 18:24:43.723: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 26 18:24:43.723: INFO: calico-node-kchsj from calico-system started at 2020-10-26 16:58:05 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.724: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 18:24:43.724: INFO: olm-operator-cbd8cfd65-5hqds from openshift-operator-lifecycle-manager started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.724: INFO: 	Container olm-operator ready: true, restart count 0
Oct 26 18:24:43.724: INFO: service-ca-operator-54f4b4db4-wvhxp from openshift-service-ca-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.724: INFO: 	Container operator ready: true, restart count 0
Oct 26 18:24:43.724: INFO: downloads-56f66db77f-f9rtp from openshift-console started at 2020-10-26 16:58:23 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.724: INFO: 	Container download-server ready: true, restart count 0
Oct 26 18:24:43.724: INFO: downloads-56f66db77f-qk97h from openshift-console started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.724: INFO: 	Container download-server ready: true, restart count 0
Oct 26 18:24:43.724: INFO: ibmcloud-block-storage-plugin-79495594d5-7m8wf from kube-system started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.725: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Oct 26 18:24:43.725: INFO: ibm-storage-watcher-7fc85c5589-t79km from kube-system started at 2020-10-26 16:58:23 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.725: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Oct 26 18:24:43.725: INFO: ingress-operator-549d4c77b5-8mj9f from openshift-ingress-operator started at 2020-10-26 16:58:24 +0000 UTC (2 container statuses recorded)
Oct 26 18:24:43.725: INFO: 	Container ingress-operator ready: true, restart count 0
Oct 26 18:24:43.725: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 18:24:43.725: INFO: network-operator-5647cdbff6-qn2hq from openshift-network-operator started at 2020-10-26 16:56:55 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.725: INFO: 	Container network-operator ready: true, restart count 0
Oct 26 18:24:43.725: INFO: tigera-operator-798cfbf7dd-x8hf6 from tigera-operator started at 2020-10-26 16:56:55 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.725: INFO: 	Container tigera-operator ready: true, restart count 2
Oct 26 18:24:43.726: INFO: dns-operator-5bd9bd8fcd-gj6cw from openshift-dns-operator started at 2020-10-26 16:58:22 +0000 UTC (2 container statuses recorded)
Oct 26 18:24:43.726: INFO: 	Container dns-operator ready: true, restart count 0
Oct 26 18:24:43.726: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 18:24:43.726: INFO: cluster-image-registry-operator-854d785579-78wmp from openshift-image-registry started at 2020-10-26 16:58:22 +0000 UTC (2 container statuses recorded)
Oct 26 18:24:43.726: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Oct 26 18:24:43.726: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Oct 26 18:24:43.726: INFO: node-ca-rhts4 from openshift-image-registry started at 2020-10-26 16:59:40 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.726: INFO: 	Container node-ca ready: true, restart count 0
Oct 26 18:24:43.727: INFO: catalog-operator-7bf86b4f96-kpnmt from openshift-operator-lifecycle-manager started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.727: INFO: 	Container catalog-operator ready: true, restart count 0
Oct 26 18:24:43.727: INFO: ibm-file-plugin-6b86cbfbc6-vw94d from kube-system started at 2020-10-26 16:58:23 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.727: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Oct 26 18:24:43.727: INFO: cluster-samples-operator-644946789c-zw8th from openshift-cluster-samples-operator started at 2020-10-26 17:01:55 +0000 UTC (2 container statuses recorded)
Oct 26 18:24:43.727: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Oct 26 18:24:43.727: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Oct 26 18:24:43.727: INFO: dns-default-qnchn from openshift-dns started at 2020-10-26 16:59:44 +0000 UTC (2 container statuses recorded)
Oct 26 18:24:43.727: INFO: 	Container dns ready: true, restart count 0
Oct 26 18:24:43.727: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 26 18:24:43.728: INFO: 
Logging pods the kubelet thinks is on node 10.123.240.174 before test
Oct 26 18:24:43.826: INFO: ibm-master-proxy-static-10.123.240.174 from kube-system started at 2020-10-26 16:57:35 +0000 UTC (2 container statuses recorded)
Oct 26 18:24:43.826: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 18:24:43.826: INFO: 	Container pause ready: true, restart count 0
Oct 26 18:24:43.826: INFO: node-exporter-hpt5d from openshift-monitoring started at 2020-10-26 16:59:04 +0000 UTC (2 container statuses recorded)
Oct 26 18:24:43.827: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 18:24:43.827: INFO: 	Container node-exporter ready: true, restart count 0
Oct 26 18:24:43.827: INFO: image-registry-57486bbffb-v7qmg from openshift-image-registry started at 2020-10-26 17:01:54 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.827: INFO: 	Container registry ready: true, restart count 0
Oct 26 18:24:43.827: INFO: configmap-cabundle-injector-6676dbc567-zdqgt from openshift-service-ca started at 2020-10-26 16:59:02 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.827: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Oct 26 18:24:43.827: INFO: prometheus-operator-d5df96f57-bthbz from openshift-monitoring started at 2020-10-26 17:05:34 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.827: INFO: 	Container prometheus-operator ready: true, restart count 0
Oct 26 18:24:43.827: INFO: thanos-querier-85d68cbb66-5hkh8 from openshift-monitoring started at 2020-10-26 17:06:37 +0000 UTC (4 container statuses recorded)
Oct 26 18:24:43.827: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 18:24:43.828: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 26 18:24:43.828: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 26 18:24:43.828: INFO: 	Container thanos-querier ready: true, restart count 0
Oct 26 18:24:43.828: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-10-26 17:06:52 +0000 UTC (7 container statuses recorded)
Oct 26 18:24:43.828: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 18:24:43.828: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 26 18:24:43.828: INFO: 	Container prometheus ready: true, restart count 1
Oct 26 18:24:43.828: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Oct 26 18:24:43.828: INFO: 	Container prometheus-proxy ready: true, restart count 0
Oct 26 18:24:43.828: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Oct 26 18:24:43.829: INFO: 	Container thanos-sidecar ready: true, restart count 0
Oct 26 18:24:43.829: INFO: certified-operators-59fc9cd9b5-84rhs from openshift-marketplace started at 2020-10-26 18:00:44 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.829: INFO: 	Container certified-operators ready: true, restart count 0
Oct 26 18:24:43.829: INFO: console-77979dd75-lnchz from openshift-console started at 2020-10-26 17:01:28 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.829: INFO: 	Container console ready: true, restart count 0
Oct 26 18:24:43.829: INFO: service-serving-cert-signer-6d656c4cf7-l9qz5 from openshift-service-ca started at 2020-10-26 16:59:01 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.829: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Oct 26 18:24:43.829: INFO: calico-node-jfjg2 from calico-system started at 2020-10-26 16:58:05 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.829: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 18:24:43.829: INFO: apiservice-cabundle-injector-566d6d6dc7-lv6bf from openshift-service-ca started at 2020-10-26 16:59:02 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.830: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Oct 26 18:24:43.830: INFO: node-ca-lbgdr from openshift-image-registry started at 2020-10-26 16:59:40 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.830: INFO: 	Container node-ca ready: true, restart count 0
Oct 26 18:24:43.830: INFO: ibmcloud-block-storage-driver-s88xs from kube-system started at 2020-10-26 16:57:41 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.830: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Oct 26 18:24:43.830: INFO: calico-typha-6b7867b64d-j6hnv from calico-system started at 2020-10-26 16:58:05 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.830: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 18:24:43.830: INFO: kube-state-metrics-6bb5fc9995-w9q9p from openshift-monitoring started at 2020-10-26 16:59:03 +0000 UTC (3 container statuses recorded)
Oct 26 18:24:43.830: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Oct 26 18:24:43.830: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Oct 26 18:24:43.831: INFO: 	Container kube-state-metrics ready: true, restart count 0
Oct 26 18:24:43.831: INFO: tuned-nlq8g from openshift-cluster-node-tuning-operator started at 2020-10-26 17:00:14 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.831: INFO: 	Container tuned ready: true, restart count 0
Oct 26 18:24:43.831: INFO: sonobuoy-systemd-logs-daemon-set-a94f7e2648a3464d-wf2jr from sonobuoy started at 2020-10-26 18:20:51 +0000 UTC (2 container statuses recorded)
Oct 26 18:24:43.831: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 26 18:24:43.831: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 26 18:24:43.831: INFO: vpn-68d7d4d68d-clwcb from kube-system started at 2020-10-26 17:05:39 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.831: INFO: 	Container vpn ready: true, restart count 0
Oct 26 18:24:43.831: INFO: grafana-f6757cb99-n9dgx from openshift-monitoring started at 2020-10-26 17:05:54 +0000 UTC (2 container statuses recorded)
Oct 26 18:24:43.831: INFO: 	Container grafana ready: true, restart count 0
Oct 26 18:24:43.832: INFO: 	Container grafana-proxy ready: true, restart count 0
Oct 26 18:24:43.832: INFO: multus-wz2pl from openshift-multus started at 2020-10-26 16:57:37 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.832: INFO: 	Container kube-multus ready: true, restart count 0
Oct 26 18:24:43.832: INFO: ibm-keepalived-watcher-nxrtl from kube-system started at 2020-10-26 16:57:37 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.832: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 18:24:43.832: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-10-26 17:06:05 +0000 UTC (3 container statuses recorded)
Oct 26 18:24:43.832: INFO: 	Container alertmanager ready: true, restart count 0
Oct 26 18:24:43.832: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 26 18:24:43.832: INFO: 	Container config-reloader ready: true, restart count 0
Oct 26 18:24:43.832: INFO: router-default-56c7ff9d54-tz4cl from openshift-ingress started at 2020-10-26 17:00:10 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.833: INFO: 	Container router ready: true, restart count 0
Oct 26 18:24:43.833: INFO: redhat-operators-688fdf87f8-nfrk9 from openshift-marketplace started at 2020-10-26 17:00:16 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.833: INFO: 	Container redhat-operators ready: true, restart count 0
Oct 26 18:24:43.833: INFO: multus-admission-controller-f46wp from openshift-multus started at 2020-10-26 16:58:27 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.833: INFO: 	Container multus-admission-controller ready: true, restart count 0
Oct 26 18:24:43.833: INFO: registry-pvc-permissions-9fhwg from openshift-image-registry started at 2020-10-26 17:01:54 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.833: INFO: 	Container pvc-permissions ready: false, restart count 0
Oct 26 18:24:43.833: INFO: prometheus-adapter-6cccbf8dbb-97f5w from openshift-monitoring started at 2020-10-26 17:05:48 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.833: INFO: 	Container prometheus-adapter ready: true, restart count 0
Oct 26 18:24:43.833: INFO: community-operators-bd644f5c5-wb2l9 from openshift-marketplace started at 2020-10-26 17:00:16 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.833: INFO: 	Container community-operators ready: true, restart count 0
Oct 26 18:24:43.834: INFO: packageserver-f4c8dc585-2wlqw from openshift-operator-lifecycle-manager started at 2020-10-26 17:05:42 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.834: INFO: 	Container packageserver ready: true, restart count 0
Oct 26 18:24:43.834: INFO: openshift-kube-proxy-rs72s from openshift-kube-proxy started at 2020-10-26 16:57:37 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.834: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 26 18:24:43.834: INFO: dns-default-2hlsj from openshift-dns started at 2020-10-26 16:59:44 +0000 UTC (2 container statuses recorded)
Oct 26 18:24:43.834: INFO: 	Container dns ready: true, restart count 0
Oct 26 18:24:43.834: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 26 18:24:43.834: INFO: 
Logging pods the kubelet thinks is on node 10.123.240.178 before test
Oct 26 18:24:43.950: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-10-26 17:06:10 +0000 UTC (3 container statuses recorded)
Oct 26 18:24:43.950: INFO: 	Container alertmanager ready: true, restart count 0
Oct 26 18:24:43.951: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 26 18:24:43.951: INFO: 	Container config-reloader ready: true, restart count 0
Oct 26 18:24:43.951: INFO: console-77979dd75-z44rw from openshift-console started at 2020-10-26 17:01:17 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.951: INFO: 	Container console ready: true, restart count 0
Oct 26 18:24:43.951: INFO: telemeter-client-7c4c649567-kt5q9 from openshift-monitoring started at 2020-10-26 17:05:45 +0000 UTC (3 container statuses recorded)
Oct 26 18:24:43.951: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 18:24:43.951: INFO: 	Container reload ready: true, restart count 0
Oct 26 18:24:43.951: INFO: 	Container telemeter-client ready: true, restart count 0
Oct 26 18:24:43.951: INFO: ibm-master-proxy-static-10.123.240.178 from kube-system started at 2020-10-26 16:57:44 +0000 UTC (2 container statuses recorded)
Oct 26 18:24:43.951: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 18:24:43.952: INFO: 	Container pause ready: true, restart count 0
Oct 26 18:24:43.952: INFO: calico-node-d227t from calico-system started at 2020-10-26 16:58:05 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.952: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 18:24:43.952: INFO: thanos-querier-85d68cbb66-c2qgp from openshift-monitoring started at 2020-10-26 17:06:30 +0000 UTC (4 container statuses recorded)
Oct 26 18:24:43.952: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 18:24:43.952: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 26 18:24:43.952: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 26 18:24:43.952: INFO: 	Container thanos-querier ready: true, restart count 0
Oct 26 18:24:43.952: INFO: ibmcloud-block-storage-driver-2pnvt from kube-system started at 2020-10-26 16:57:55 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.952: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Oct 26 18:24:43.953: INFO: ibm-cloud-provider-ip-149-81-128-30-5bdf48d794-wkzz2 from ibm-system started at 2020-10-26 17:10:34 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.953: INFO: 	Container ibm-cloud-provider-ip-149-81-128-30 ready: true, restart count 0
Oct 26 18:24:43.953: INFO: node-ca-hsxg7 from openshift-image-registry started at 2020-10-26 16:59:40 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.953: INFO: 	Container node-ca ready: true, restart count 0
Oct 26 18:24:43.953: INFO: sonobuoy-e2e-job-6258e003556947b6 from sonobuoy started at 2020-10-26 18:20:50 +0000 UTC (2 container statuses recorded)
Oct 26 18:24:43.953: INFO: 	Container e2e ready: true, restart count 0
Oct 26 18:24:43.953: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 26 18:24:43.953: INFO: sonobuoy-systemd-logs-daemon-set-a94f7e2648a3464d-lj6zg from sonobuoy started at 2020-10-26 18:20:51 +0000 UTC (2 container statuses recorded)
Oct 26 18:24:43.953: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 26 18:24:43.953: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 26 18:24:43.954: INFO: multus-nht6l from openshift-multus started at 2020-10-26 16:57:47 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.954: INFO: 	Container kube-multus ready: true, restart count 0
Oct 26 18:24:43.954: INFO: multus-admission-controller-f5hdf from openshift-multus started at 2020-10-26 16:58:57 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.954: INFO: 	Container multus-admission-controller ready: true, restart count 0
Oct 26 18:24:43.954: INFO: packageserver-f4c8dc585-xsnzh from openshift-operator-lifecycle-manager started at 2020-10-26 17:05:38 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.954: INFO: 	Container packageserver ready: true, restart count 0
Oct 26 18:24:43.954: INFO: openshift-state-metrics-6888cfb99c-d9fvz from openshift-monitoring started at 2020-10-26 16:59:03 +0000 UTC (3 container statuses recorded)
Oct 26 18:24:43.954: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Oct 26 18:24:43.954: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Oct 26 18:24:43.954: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Oct 26 18:24:43.955: INFO: router-default-56c7ff9d54-nxdpr from openshift-ingress started at 2020-10-26 17:00:10 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.955: INFO: 	Container router ready: true, restart count 0
Oct 26 18:24:43.955: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-10-26 17:06:44 +0000 UTC (7 container statuses recorded)
Oct 26 18:24:43.955: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 18:24:43.955: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 26 18:24:43.955: INFO: 	Container prometheus ready: true, restart count 1
Oct 26 18:24:43.955: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Oct 26 18:24:43.955: INFO: 	Container prometheus-proxy ready: true, restart count 0
Oct 26 18:24:43.955: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Oct 26 18:24:43.955: INFO: 	Container thanos-sidecar ready: true, restart count 0
Oct 26 18:24:43.956: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-10-26 17:00:45 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.956: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Oct 26 18:24:43.956: INFO: ibm-keepalived-watcher-zr4ms from kube-system started at 2020-10-26 16:57:47 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.956: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 18:24:43.956: INFO: openshift-kube-proxy-v9jgr from openshift-kube-proxy started at 2020-10-26 16:57:47 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.956: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 26 18:24:43.956: INFO: calico-typha-6b7867b64d-bjwpm from calico-system started at 2020-10-26 17:00:10 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.956: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 18:24:43.956: INFO: node-exporter-vw6ds from openshift-monitoring started at 2020-10-26 16:59:04 +0000 UTC (2 container statuses recorded)
Oct 26 18:24:43.956: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 18:24:43.956: INFO: 	Container node-exporter ready: true, restart count 0
Oct 26 18:24:43.957: INFO: dns-default-qn8jr from openshift-dns started at 2020-10-26 16:59:44 +0000 UTC (2 container statuses recorded)
Oct 26 18:24:43.957: INFO: 	Container dns ready: true, restart count 0
Oct 26 18:24:43.957: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 26 18:24:43.957: INFO: tuned-2ksq9 from openshift-cluster-node-tuning-operator started at 2020-10-26 17:00:14 +0000 UTC (1 container statuses recorded)
Oct 26 18:24:43.957: INFO: 	Container tuned ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-21fcce03-9da4-4a2b-9dba-490816703846 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-21fcce03-9da4-4a2b-9dba-490816703846 off the node 10.123.240.172
STEP: verifying the node doesn't have the label kubernetes.io/e2e-21fcce03-9da4-4a2b-9dba-490816703846
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:29:50.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4711" for this suite.
Oct 26 18:30:02.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:30:04.404: INFO: namespace sched-pred-4711 deletion completed in 14.050334627s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:321.032 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:30:04.404: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Oct 26 18:30:04.541: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Oct 26 18:30:04.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 create -f - --namespace=kubectl-1782'
Oct 26 18:30:05.357: INFO: stderr: ""
Oct 26 18:30:05.357: INFO: stdout: "service/redis-slave created\n"
Oct 26 18:30:05.357: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Oct 26 18:30:05.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 create -f - --namespace=kubectl-1782'
Oct 26 18:30:06.100: INFO: stderr: ""
Oct 26 18:30:06.100: INFO: stdout: "service/redis-master created\n"
Oct 26 18:30:06.100: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Oct 26 18:30:06.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 create -f - --namespace=kubectl-1782'
Oct 26 18:30:06.790: INFO: stderr: ""
Oct 26 18:30:06.790: INFO: stdout: "service/frontend created\n"
Oct 26 18:30:06.790: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Oct 26 18:30:06.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 create -f - --namespace=kubectl-1782'
Oct 26 18:30:07.444: INFO: stderr: ""
Oct 26 18:30:07.444: INFO: stdout: "deployment.apps/frontend created\n"
Oct 26 18:30:07.444: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Oct 26 18:30:07.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 create -f - --namespace=kubectl-1782'
Oct 26 18:30:08.167: INFO: stderr: ""
Oct 26 18:30:08.167: INFO: stdout: "deployment.apps/redis-master created\n"
Oct 26 18:30:08.167: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Oct 26 18:30:08.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 create -f - --namespace=kubectl-1782'
Oct 26 18:30:08.815: INFO: stderr: ""
Oct 26 18:30:08.815: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Oct 26 18:30:08.815: INFO: Waiting for all frontend pods to be Running.
Oct 26 18:30:28.867: INFO: Waiting for frontend to serve content.
Oct 26 18:30:33.913: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Oct 26 18:30:38.959: INFO: Trying to add a new entry to the guestbook.
Oct 26 18:30:39.019: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Oct 26 18:30:39.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 delete --grace-period=0 --force -f - --namespace=kubectl-1782'
Oct 26 18:30:39.282: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 26 18:30:39.282: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Oct 26 18:30:39.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 delete --grace-period=0 --force -f - --namespace=kubectl-1782'
Oct 26 18:30:39.500: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 26 18:30:39.500: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Oct 26 18:30:39.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 delete --grace-period=0 --force -f - --namespace=kubectl-1782'
Oct 26 18:30:39.714: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 26 18:30:39.715: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Oct 26 18:30:39.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 delete --grace-period=0 --force -f - --namespace=kubectl-1782'
Oct 26 18:30:39.910: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 26 18:30:39.910: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Oct 26 18:30:39.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 delete --grace-period=0 --force -f - --namespace=kubectl-1782'
Oct 26 18:30:40.091: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 26 18:30:40.092: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Oct 26 18:30:40.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 delete --grace-period=0 --force -f - --namespace=kubectl-1782'
Oct 26 18:30:40.277: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 26 18:30:40.277: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:30:40.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1782" for this suite.
Oct 26 18:30:56.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:30:58.484: INFO: namespace kubectl-1782 deletion completed in 18.186554558s

• [SLOW TEST:54.079 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:30:58.484: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Oct 26 18:31:03.345: INFO: Successfully updated pod "annotationupdate044c7957-13cb-49a6-9a7b-da7c6547f7f3"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:31:05.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1665" for this suite.
Oct 26 18:31:37.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:31:39.517: INFO: namespace downward-api-1665 deletion completed in 34.082446243s

• [SLOW TEST:41.033 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:31:39.518: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-21147d32-6e60-4117-acac-22a767d1e928
STEP: Creating a pod to test consume secrets
Oct 26 18:31:39.773: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-775813f0-4e96-4da5-870c-43eb7a0840b7" in namespace "projected-8080" to be "success or failure"
Oct 26 18:31:39.786: INFO: Pod "pod-projected-secrets-775813f0-4e96-4da5-870c-43eb7a0840b7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.208535ms
Oct 26 18:31:41.800: INFO: Pod "pod-projected-secrets-775813f0-4e96-4da5-870c-43eb7a0840b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026889938s
STEP: Saw pod success
Oct 26 18:31:41.800: INFO: Pod "pod-projected-secrets-775813f0-4e96-4da5-870c-43eb7a0840b7" satisfied condition "success or failure"
Oct 26 18:31:41.815: INFO: Trying to get logs from node 10.123.240.172 pod pod-projected-secrets-775813f0-4e96-4da5-870c-43eb7a0840b7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 26 18:31:41.882: INFO: Waiting for pod pod-projected-secrets-775813f0-4e96-4da5-870c-43eb7a0840b7 to disappear
Oct 26 18:31:41.893: INFO: Pod pod-projected-secrets-775813f0-4e96-4da5-870c-43eb7a0840b7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:31:41.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8080" for this suite.
Oct 26 18:31:49.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:31:51.982: INFO: namespace projected-8080 deletion completed in 10.062849867s

• [SLOW TEST:12.465 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:31:51.983: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 18:31:52.265: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Oct 26 18:31:52.318: INFO: Number of nodes with available pods: 0
Oct 26 18:31:52.318: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 18:31:53.353: INFO: Number of nodes with available pods: 0
Oct 26 18:31:53.353: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 18:31:54.351: INFO: Number of nodes with available pods: 0
Oct 26 18:31:54.351: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 18:31:55.369: INFO: Number of nodes with available pods: 0
Oct 26 18:31:55.369: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 18:31:56.358: INFO: Number of nodes with available pods: 0
Oct 26 18:31:56.358: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 18:31:57.347: INFO: Number of nodes with available pods: 0
Oct 26 18:31:57.347: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 18:31:58.349: INFO: Number of nodes with available pods: 0
Oct 26 18:31:58.349: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 18:31:59.351: INFO: Number of nodes with available pods: 0
Oct 26 18:31:59.351: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 18:32:00.354: INFO: Number of nodes with available pods: 0
Oct 26 18:32:00.354: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 18:32:01.371: INFO: Number of nodes with available pods: 0
Oct 26 18:32:01.371: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 18:32:02.353: INFO: Number of nodes with available pods: 1
Oct 26 18:32:02.353: INFO: Node 10.123.240.174 is running more than one daemon pod
Oct 26 18:32:03.347: INFO: Number of nodes with available pods: 2
Oct 26 18:32:03.347: INFO: Node 10.123.240.178 is running more than one daemon pod
Oct 26 18:32:04.349: INFO: Number of nodes with available pods: 3
Oct 26 18:32:04.349: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Oct 26 18:32:04.467: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:04.467: INFO: Wrong image for pod: daemon-set-h5wh6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:04.467: INFO: Wrong image for pod: daemon-set-lkgvn. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:05.499: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:05.499: INFO: Wrong image for pod: daemon-set-h5wh6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:05.499: INFO: Wrong image for pod: daemon-set-lkgvn. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:06.512: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:06.512: INFO: Wrong image for pod: daemon-set-h5wh6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:06.512: INFO: Wrong image for pod: daemon-set-lkgvn. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:06.512: INFO: Pod daemon-set-lkgvn is not available
Oct 26 18:32:07.500: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:07.500: INFO: Wrong image for pod: daemon-set-h5wh6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:07.500: INFO: Wrong image for pod: daemon-set-lkgvn. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:07.500: INFO: Pod daemon-set-lkgvn is not available
Oct 26 18:32:08.502: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:08.502: INFO: Wrong image for pod: daemon-set-h5wh6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:08.502: INFO: Wrong image for pod: daemon-set-lkgvn. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:08.502: INFO: Pod daemon-set-lkgvn is not available
Oct 26 18:32:09.503: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:09.503: INFO: Wrong image for pod: daemon-set-h5wh6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:09.503: INFO: Wrong image for pod: daemon-set-lkgvn. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:09.503: INFO: Pod daemon-set-lkgvn is not available
Oct 26 18:32:10.501: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:10.501: INFO: Wrong image for pod: daemon-set-h5wh6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:10.501: INFO: Wrong image for pod: daemon-set-lkgvn. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:10.501: INFO: Pod daemon-set-lkgvn is not available
Oct 26 18:32:11.502: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:11.502: INFO: Wrong image for pod: daemon-set-h5wh6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:11.502: INFO: Pod daemon-set-mmtxg is not available
Oct 26 18:32:12.501: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:12.501: INFO: Wrong image for pod: daemon-set-h5wh6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:13.506: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:13.506: INFO: Wrong image for pod: daemon-set-h5wh6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:14.500: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:14.500: INFO: Wrong image for pod: daemon-set-h5wh6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:15.500: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:15.500: INFO: Wrong image for pod: daemon-set-h5wh6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:15.500: INFO: Pod daemon-set-h5wh6 is not available
Oct 26 18:32:16.499: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:16.499: INFO: Wrong image for pod: daemon-set-h5wh6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:16.499: INFO: Pod daemon-set-h5wh6 is not available
Oct 26 18:32:17.501: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:17.501: INFO: Wrong image for pod: daemon-set-h5wh6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:17.501: INFO: Pod daemon-set-h5wh6 is not available
Oct 26 18:32:18.500: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:18.500: INFO: Wrong image for pod: daemon-set-h5wh6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:18.500: INFO: Pod daemon-set-h5wh6 is not available
Oct 26 18:32:19.506: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:19.506: INFO: Wrong image for pod: daemon-set-h5wh6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:19.506: INFO: Pod daemon-set-h5wh6 is not available
Oct 26 18:32:20.500: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:20.500: INFO: Wrong image for pod: daemon-set-h5wh6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:20.500: INFO: Pod daemon-set-h5wh6 is not available
Oct 26 18:32:21.510: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:21.510: INFO: Wrong image for pod: daemon-set-h5wh6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:21.510: INFO: Pod daemon-set-h5wh6 is not available
Oct 26 18:32:22.502: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:22.502: INFO: Wrong image for pod: daemon-set-h5wh6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:22.502: INFO: Pod daemon-set-h5wh6 is not available
Oct 26 18:32:23.500: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:23.500: INFO: Wrong image for pod: daemon-set-h5wh6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:23.500: INFO: Pod daemon-set-h5wh6 is not available
Oct 26 18:32:24.500: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:24.500: INFO: Pod daemon-set-lx7zx is not available
Oct 26 18:32:25.501: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:25.501: INFO: Pod daemon-set-lx7zx is not available
Oct 26 18:32:26.502: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:26.502: INFO: Pod daemon-set-lx7zx is not available
Oct 26 18:32:27.504: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:27.504: INFO: Pod daemon-set-lx7zx is not available
Oct 26 18:32:28.500: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:28.501: INFO: Pod daemon-set-lx7zx is not available
Oct 26 18:32:29.500: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:29.500: INFO: Pod daemon-set-lx7zx is not available
Oct 26 18:32:30.504: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:30.504: INFO: Pod daemon-set-lx7zx is not available
Oct 26 18:32:31.499: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:31.499: INFO: Pod daemon-set-lx7zx is not available
Oct 26 18:32:32.501: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:32.501: INFO: Pod daemon-set-lx7zx is not available
Oct 26 18:32:33.501: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:33.501: INFO: Pod daemon-set-lx7zx is not available
Oct 26 18:32:34.507: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:35.500: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:35.500: INFO: Pod daemon-set-f8k7d is not available
Oct 26 18:32:36.501: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:36.501: INFO: Pod daemon-set-f8k7d is not available
Oct 26 18:32:37.500: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:37.500: INFO: Pod daemon-set-f8k7d is not available
Oct 26 18:32:38.500: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:38.500: INFO: Pod daemon-set-f8k7d is not available
Oct 26 18:32:39.501: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:39.501: INFO: Pod daemon-set-f8k7d is not available
Oct 26 18:32:40.499: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:40.499: INFO: Pod daemon-set-f8k7d is not available
Oct 26 18:32:41.502: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:41.502: INFO: Pod daemon-set-f8k7d is not available
Oct 26 18:32:42.500: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:42.500: INFO: Pod daemon-set-f8k7d is not available
Oct 26 18:32:43.501: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:43.501: INFO: Pod daemon-set-f8k7d is not available
Oct 26 18:32:44.502: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:44.502: INFO: Pod daemon-set-f8k7d is not available
Oct 26 18:32:45.503: INFO: Wrong image for pod: daemon-set-f8k7d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 18:32:45.503: INFO: Pod daemon-set-f8k7d is not available
Oct 26 18:32:46.500: INFO: Pod daemon-set-dm8fx is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Oct 26 18:32:46.548: INFO: Number of nodes with available pods: 2
Oct 26 18:32:46.548: INFO: Node 10.123.240.174 is running more than one daemon pod
Oct 26 18:32:47.575: INFO: Number of nodes with available pods: 3
Oct 26 18:32:47.575: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6872, will wait for the garbage collector to delete the pods
Oct 26 18:32:47.718: INFO: Deleting DaemonSet.extensions daemon-set took: 25.247351ms
Oct 26 18:32:48.319: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.309512ms
Oct 26 18:33:00.631: INFO: Number of nodes with available pods: 0
Oct 26 18:33:00.631: INFO: Number of running nodes: 0, number of available pods: 0
Oct 26 18:33:00.642: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6872/daemonsets","resourceVersion":"42654"},"items":null}

Oct 26 18:33:00.653: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6872/pods","resourceVersion":"42654"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:33:00.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6872" for this suite.
Oct 26 18:33:08.785: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:33:10.791: INFO: namespace daemonsets-6872 deletion completed in 10.067665667s

• [SLOW TEST:78.808 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:33:10.791: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Oct 26 18:33:10.923: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 18:33:20.171: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:33:55.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6475" for this suite.
Oct 26 18:34:03.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:34:05.121: INFO: namespace crd-publish-openapi-6475 deletion completed in 10.062845932s

• [SLOW TEST:54.330 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:34:05.125: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1026 18:34:11.447146      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Oct 26 18:34:11.447: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:34:11.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4305" for this suite.
Oct 26 18:34:19.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:34:21.511: INFO: namespace gc-4305 deletion completed in 10.042089808s

• [SLOW TEST:16.386 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:34:21.511: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-whw2
STEP: Creating a pod to test atomic-volume-subpath
Oct 26 18:34:21.713: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-whw2" in namespace "subpath-1366" to be "success or failure"
Oct 26 18:34:21.724: INFO: Pod "pod-subpath-test-configmap-whw2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.121947ms
Oct 26 18:34:23.737: INFO: Pod "pod-subpath-test-configmap-whw2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023930983s
Oct 26 18:34:25.749: INFO: Pod "pod-subpath-test-configmap-whw2": Phase="Running", Reason="", readiness=true. Elapsed: 4.036010158s
Oct 26 18:34:27.760: INFO: Pod "pod-subpath-test-configmap-whw2": Phase="Running", Reason="", readiness=true. Elapsed: 6.046767825s
Oct 26 18:34:29.770: INFO: Pod "pod-subpath-test-configmap-whw2": Phase="Running", Reason="", readiness=true. Elapsed: 8.057328978s
Oct 26 18:34:31.784: INFO: Pod "pod-subpath-test-configmap-whw2": Phase="Running", Reason="", readiness=true. Elapsed: 10.071351386s
Oct 26 18:34:33.796: INFO: Pod "pod-subpath-test-configmap-whw2": Phase="Running", Reason="", readiness=true. Elapsed: 12.083166236s
Oct 26 18:34:35.808: INFO: Pod "pod-subpath-test-configmap-whw2": Phase="Running", Reason="", readiness=true. Elapsed: 14.094827974s
Oct 26 18:34:37.818: INFO: Pod "pod-subpath-test-configmap-whw2": Phase="Running", Reason="", readiness=true. Elapsed: 16.105646435s
Oct 26 18:34:39.829: INFO: Pod "pod-subpath-test-configmap-whw2": Phase="Running", Reason="", readiness=true. Elapsed: 18.116701828s
Oct 26 18:34:41.842: INFO: Pod "pod-subpath-test-configmap-whw2": Phase="Running", Reason="", readiness=true. Elapsed: 20.12916993s
Oct 26 18:34:43.854: INFO: Pod "pod-subpath-test-configmap-whw2": Phase="Running", Reason="", readiness=true. Elapsed: 22.14167351s
Oct 26 18:34:45.866: INFO: Pod "pod-subpath-test-configmap-whw2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.153112206s
STEP: Saw pod success
Oct 26 18:34:45.866: INFO: Pod "pod-subpath-test-configmap-whw2" satisfied condition "success or failure"
Oct 26 18:34:45.877: INFO: Trying to get logs from node 10.123.240.172 pod pod-subpath-test-configmap-whw2 container test-container-subpath-configmap-whw2: <nil>
STEP: delete the pod
Oct 26 18:34:45.959: INFO: Waiting for pod pod-subpath-test-configmap-whw2 to disappear
Oct 26 18:34:45.969: INFO: Pod pod-subpath-test-configmap-whw2 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-whw2
Oct 26 18:34:45.969: INFO: Deleting pod "pod-subpath-test-configmap-whw2" in namespace "subpath-1366"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:34:45.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1366" for this suite.
Oct 26 18:34:54.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:34:56.040: INFO: namespace subpath-1366 deletion completed in 10.032689915s

• [SLOW TEST:34.529 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:34:56.041: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-857b5be2-e466-41be-b535-168ca70decdb in namespace container-probe-4608
Oct 26 18:35:04.269: INFO: Started pod liveness-857b5be2-e466-41be-b535-168ca70decdb in namespace container-probe-4608
STEP: checking the pod's current state and verifying that restartCount is present
Oct 26 18:35:04.283: INFO: Initial restart count of pod liveness-857b5be2-e466-41be-b535-168ca70decdb is 0
Oct 26 18:35:14.362: INFO: Restart count of pod container-probe-4608/liveness-857b5be2-e466-41be-b535-168ca70decdb is now 1 (10.078977638s elapsed)
Oct 26 18:35:34.495: INFO: Restart count of pod container-probe-4608/liveness-857b5be2-e466-41be-b535-168ca70decdb is now 2 (30.212246325s elapsed)
Oct 26 18:35:54.616: INFO: Restart count of pod container-probe-4608/liveness-857b5be2-e466-41be-b535-168ca70decdb is now 3 (50.333603003s elapsed)
Oct 26 18:36:14.736: INFO: Restart count of pod container-probe-4608/liveness-857b5be2-e466-41be-b535-168ca70decdb is now 4 (1m10.452897899s elapsed)
Oct 26 18:37:15.164: INFO: Restart count of pod container-probe-4608/liveness-857b5be2-e466-41be-b535-168ca70decdb is now 5 (2m10.881001186s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:37:15.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4608" for this suite.
Oct 26 18:37:23.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:37:25.271: INFO: namespace container-probe-4608 deletion completed in 9.993627314s

• [SLOW TEST:149.231 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:37:25.272: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-e79d623a-c9f3-40d6-8572-5ea2d0b981d0
STEP: Creating a pod to test consume secrets
Oct 26 18:37:25.489: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-129c74bb-0339-4bd4-bfe3-7d6329677cc9" in namespace "projected-1423" to be "success or failure"
Oct 26 18:37:25.499: INFO: Pod "pod-projected-secrets-129c74bb-0339-4bd4-bfe3-7d6329677cc9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.28631ms
Oct 26 18:37:27.514: INFO: Pod "pod-projected-secrets-129c74bb-0339-4bd4-bfe3-7d6329677cc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024632517s
Oct 26 18:37:29.526: INFO: Pod "pod-projected-secrets-129c74bb-0339-4bd4-bfe3-7d6329677cc9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036146004s
Oct 26 18:37:31.563: INFO: Pod "pod-projected-secrets-129c74bb-0339-4bd4-bfe3-7d6329677cc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.073267367s
STEP: Saw pod success
Oct 26 18:37:31.563: INFO: Pod "pod-projected-secrets-129c74bb-0339-4bd4-bfe3-7d6329677cc9" satisfied condition "success or failure"
Oct 26 18:37:31.573: INFO: Trying to get logs from node 10.123.240.178 pod pod-projected-secrets-129c74bb-0339-4bd4-bfe3-7d6329677cc9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 26 18:37:31.666: INFO: Waiting for pod pod-projected-secrets-129c74bb-0339-4bd4-bfe3-7d6329677cc9 to disappear
Oct 26 18:37:31.678: INFO: Pod pod-projected-secrets-129c74bb-0339-4bd4-bfe3-7d6329677cc9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:37:31.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1423" for this suite.
Oct 26 18:37:39.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:37:41.788: INFO: namespace projected-1423 deletion completed in 10.082197301s

• [SLOW TEST:16.516 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:37:41.789: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-91ff24e2-7aa1-4ee5-9dae-f24ca4573364
STEP: Creating a pod to test consume configMaps
Oct 26 18:37:41.999: INFO: Waiting up to 5m0s for pod "pod-configmaps-cb93c449-22dd-493d-94a0-c803dc6db4ce" in namespace "configmap-9767" to be "success or failure"
Oct 26 18:37:42.008: INFO: Pod "pod-configmaps-cb93c449-22dd-493d-94a0-c803dc6db4ce": Phase="Pending", Reason="", readiness=false. Elapsed: 8.912052ms
Oct 26 18:37:44.022: INFO: Pod "pod-configmaps-cb93c449-22dd-493d-94a0-c803dc6db4ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023357193s
Oct 26 18:37:46.033: INFO: Pod "pod-configmaps-cb93c449-22dd-493d-94a0-c803dc6db4ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034437496s
STEP: Saw pod success
Oct 26 18:37:46.033: INFO: Pod "pod-configmaps-cb93c449-22dd-493d-94a0-c803dc6db4ce" satisfied condition "success or failure"
Oct 26 18:37:46.043: INFO: Trying to get logs from node 10.123.240.172 pod pod-configmaps-cb93c449-22dd-493d-94a0-c803dc6db4ce container configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 18:37:46.138: INFO: Waiting for pod pod-configmaps-cb93c449-22dd-493d-94a0-c803dc6db4ce to disappear
Oct 26 18:37:46.150: INFO: Pod pod-configmaps-cb93c449-22dd-493d-94a0-c803dc6db4ce no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:37:46.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9767" for this suite.
Oct 26 18:37:54.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:37:56.256: INFO: namespace configmap-9767 deletion completed in 10.076772363s

• [SLOW TEST:14.467 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:37:56.256: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-98a3ed0c-efbf-44f0-bdf9-3a4f70e0ce41
STEP: Creating a pod to test consume secrets
Oct 26 18:37:56.490: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0bb0a398-d7e0-43ba-8a64-77fa348d867f" in namespace "projected-9174" to be "success or failure"
Oct 26 18:37:56.505: INFO: Pod "pod-projected-secrets-0bb0a398-d7e0-43ba-8a64-77fa348d867f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.060874ms
Oct 26 18:37:58.516: INFO: Pod "pod-projected-secrets-0bb0a398-d7e0-43ba-8a64-77fa348d867f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026158955s
Oct 26 18:38:00.529: INFO: Pod "pod-projected-secrets-0bb0a398-d7e0-43ba-8a64-77fa348d867f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038571816s
STEP: Saw pod success
Oct 26 18:38:00.529: INFO: Pod "pod-projected-secrets-0bb0a398-d7e0-43ba-8a64-77fa348d867f" satisfied condition "success or failure"
Oct 26 18:38:00.543: INFO: Trying to get logs from node 10.123.240.178 pod pod-projected-secrets-0bb0a398-d7e0-43ba-8a64-77fa348d867f container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 26 18:38:00.604: INFO: Waiting for pod pod-projected-secrets-0bb0a398-d7e0-43ba-8a64-77fa348d867f to disappear
Oct 26 18:38:00.615: INFO: Pod pod-projected-secrets-0bb0a398-d7e0-43ba-8a64-77fa348d867f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:38:00.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9174" for this suite.
Oct 26 18:38:08.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:38:10.729: INFO: namespace projected-9174 deletion completed in 10.087070366s

• [SLOW TEST:14.472 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:38:10.729: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7057
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-7057
I1026 18:38:11.001654      23 runners.go:184] Created replication controller with name: externalname-service, namespace: services-7057, replica count: 2
I1026 18:38:14.052223      23 runners.go:184] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1026 18:38:17.052479      23 runners.go:184] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 26 18:38:20.052: INFO: Creating new exec pod
I1026 18:38:20.052823      23 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 26 18:38:23.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=services-7057 execpod7lvlq -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Oct 26 18:38:23.574: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Oct 26 18:38:23.575: INFO: stdout: ""
Oct 26 18:38:23.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=services-7057 execpod7lvlq -- /bin/sh -x -c nc -zv -t -w 2 172.21.0.172 80'
Oct 26 18:38:23.960: INFO: stderr: "+ nc -zv -t -w 2 172.21.0.172 80\nConnection to 172.21.0.172 80 port [tcp/http] succeeded!\n"
Oct 26 18:38:23.960: INFO: stdout: ""
Oct 26 18:38:23.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=services-7057 execpod7lvlq -- /bin/sh -x -c nc -zv -t -w 2 10.123.240.172 31881'
Oct 26 18:38:24.295: INFO: stderr: "+ nc -zv -t -w 2 10.123.240.172 31881\nConnection to 10.123.240.172 31881 port [tcp/31881] succeeded!\n"
Oct 26 18:38:24.295: INFO: stdout: ""
Oct 26 18:38:24.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=services-7057 execpod7lvlq -- /bin/sh -x -c nc -zv -t -w 2 10.123.240.174 31881'
Oct 26 18:38:24.918: INFO: stderr: "+ nc -zv -t -w 2 10.123.240.174 31881\nConnection to 10.123.240.174 31881 port [tcp/31881] succeeded!\n"
Oct 26 18:38:24.918: INFO: stdout: ""
Oct 26 18:38:24.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=services-7057 execpod7lvlq -- /bin/sh -x -c nc -zv -t -w 2 149.81.70.29 31881'
Oct 26 18:38:26.054: INFO: stderr: "+ nc -zv -t -w 2 149.81.70.29 31881\nConnection to 149.81.70.29 31881 port [tcp/31881] succeeded!\n"
Oct 26 18:38:26.054: INFO: stdout: ""
Oct 26 18:38:26.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=services-7057 execpod7lvlq -- /bin/sh -x -c nc -zv -t -w 2 149.81.70.30 31881'
Oct 26 18:38:26.410: INFO: stderr: "+ nc -zv -t -w 2 149.81.70.30 31881\nConnection to 149.81.70.30 31881 port [tcp/31881] succeeded!\n"
Oct 26 18:38:26.410: INFO: stdout: ""
Oct 26 18:38:26.410: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:38:26.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7057" for this suite.
Oct 26 18:38:34.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:38:36.786: INFO: namespace services-7057 deletion completed in 10.265320307s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:26.057 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:38:36.787: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Oct 26 18:38:41.615: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4995 pod-service-account-8797f55a-19c2-498b-92f6-0562c2829128 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Oct 26 18:38:42.021: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4995 pod-service-account-8797f55a-19c2-498b-92f6-0562c2829128 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Oct 26 18:38:42.785: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4995 pod-service-account-8797f55a-19c2-498b-92f6-0562c2829128 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:38:43.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4995" for this suite.
Oct 26 18:38:51.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:38:53.273: INFO: namespace svcaccounts-4995 deletion completed in 10.072407944s

• [SLOW TEST:16.486 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:38:53.273: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-5ae8ef1f-3b76-496c-a9d8-5160b5900216
STEP: Creating a pod to test consume secrets
Oct 26 18:38:53.485: INFO: Waiting up to 5m0s for pod "pod-secrets-ddbb6e64-223c-40ce-9762-f0ec016b43fa" in namespace "secrets-6303" to be "success or failure"
Oct 26 18:38:53.499: INFO: Pod "pod-secrets-ddbb6e64-223c-40ce-9762-f0ec016b43fa": Phase="Pending", Reason="", readiness=false. Elapsed: 14.422964ms
Oct 26 18:38:55.510: INFO: Pod "pod-secrets-ddbb6e64-223c-40ce-9762-f0ec016b43fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025144355s
Oct 26 18:38:57.522: INFO: Pod "pod-secrets-ddbb6e64-223c-40ce-9762-f0ec016b43fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037075293s
STEP: Saw pod success
Oct 26 18:38:57.522: INFO: Pod "pod-secrets-ddbb6e64-223c-40ce-9762-f0ec016b43fa" satisfied condition "success or failure"
Oct 26 18:38:57.533: INFO: Trying to get logs from node 10.123.240.172 pod pod-secrets-ddbb6e64-223c-40ce-9762-f0ec016b43fa container secret-volume-test: <nil>
STEP: delete the pod
Oct 26 18:38:57.594: INFO: Waiting for pod pod-secrets-ddbb6e64-223c-40ce-9762-f0ec016b43fa to disappear
Oct 26 18:38:57.607: INFO: Pod pod-secrets-ddbb6e64-223c-40ce-9762-f0ec016b43fa no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:38:57.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6303" for this suite.
Oct 26 18:39:05.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:39:07.696: INFO: namespace secrets-6303 deletion completed in 10.058288303s

• [SLOW TEST:14.423 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:39:07.696: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Oct 26 18:39:07.872: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-668520336 proxy --unix-socket=/tmp/kubectl-proxy-unix842900399/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:39:07.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8815" for this suite.
Oct 26 18:39:16.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:39:18.068: INFO: namespace kubectl-8815 deletion completed in 10.080152056s

• [SLOW TEST:10.373 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:39:18.073: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Oct 26 18:39:21.376: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:39:21.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8538" for this suite.
Oct 26 18:39:29.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:39:31.664: INFO: namespace container-runtime-8538 deletion completed in 10.165621697s

• [SLOW TEST:13.591 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:39:31.665: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:39:34.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8849" for this suite.
Oct 26 18:39:51.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:39:53.133: INFO: namespace replication-controller-8849 deletion completed in 18.152040272s

• [SLOW TEST:21.469 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:39:53.133: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 18:39:53.611: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 18:39:55.650: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739334393, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739334393, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739334393, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739334393, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 18:39:58.693: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 18:39:58.706: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3553-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:40:00.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1648" for this suite.
Oct 26 18:40:08.351: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:40:10.383: INFO: namespace webhook-1648 deletion completed in 10.086888082s
STEP: Destroying namespace "webhook-1648-markers" for this suite.
Oct 26 18:40:18.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:40:20.353: INFO: namespace webhook-1648-markers deletion completed in 9.969538591s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:27.272 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:40:20.406: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 18:40:21.352: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 18:40:23.385: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739334421, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739334421, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739334421, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739334421, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 18:40:26.435: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:40:26.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1430" for this suite.
Oct 26 18:40:34.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:40:36.876: INFO: namespace webhook-1430 deletion completed in 10.109281507s
STEP: Destroying namespace "webhook-1430-markers" for this suite.
Oct 26 18:40:44.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:40:46.930: INFO: namespace webhook-1430-markers deletion completed in 10.053466429s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:26.580 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:40:46.986: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:40:58.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7386" for this suite.
Oct 26 18:41:06.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:41:08.535: INFO: namespace resourcequota-7386 deletion completed in 10.214855729s

• [SLOW TEST:21.549 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:41:08.538: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
Oct 26 18:41:08.711: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 26 18:42:08.888: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 18:42:08.901: INFO: Starting informer...
STEP: Starting pod...
Oct 26 18:42:09.167: INFO: Pod is running on 10.123.240.178. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Oct 26 18:42:09.210: INFO: Pod wasn't evicted. Proceeding
Oct 26 18:42:09.210: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Oct 26 18:43:24.352: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:43:24.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-4865" for this suite.
Oct 26 18:43:56.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:43:58.415: INFO: namespace taint-single-pod-4865 deletion completed in 34.044739888s

• [SLOW TEST:169.878 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:43:58.416: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 18:43:59.568: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 18:44:01.603: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739334639, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739334639, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739334639, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739334639, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 18:44:04.649: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:44:15.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-186" for this suite.
Oct 26 18:44:23.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:44:25.140: INFO: namespace webhook-186 deletion completed in 10.068439514s
STEP: Destroying namespace "webhook-186-markers" for this suite.
Oct 26 18:44:33.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:44:35.245: INFO: namespace webhook-186-markers deletion completed in 10.104662236s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:36.883 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:44:35.299: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 18:44:36.171: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 18:44:38.203: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739334676, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739334676, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739334676, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739334676, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 18:44:41.245: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:44:41.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5102" for this suite.
Oct 26 18:44:57.543: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:44:59.504: INFO: namespace webhook-5102 deletion completed in 18.005383911s
STEP: Destroying namespace "webhook-5102-markers" for this suite.
Oct 26 18:45:07.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:45:09.563: INFO: namespace webhook-5102-markers deletion completed in 10.059520178s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:34.342 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:45:09.643: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:45:26.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1350" for this suite.
Oct 26 18:45:35.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:45:37.061: INFO: namespace resourcequota-1350 deletion completed in 10.088274179s

• [SLOW TEST:27.418 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:45:37.062: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-4356b8c5-261d-4625-a3bd-3fa2bd37327b
STEP: Creating a pod to test consume secrets
Oct 26 18:45:37.284: INFO: Waiting up to 5m0s for pod "pod-secrets-05fec8bd-149d-4210-8910-ef7ea9b57c26" in namespace "secrets-5249" to be "success or failure"
Oct 26 18:45:37.293: INFO: Pod "pod-secrets-05fec8bd-149d-4210-8910-ef7ea9b57c26": Phase="Pending", Reason="", readiness=false. Elapsed: 9.221458ms
Oct 26 18:45:39.304: INFO: Pod "pod-secrets-05fec8bd-149d-4210-8910-ef7ea9b57c26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019800752s
STEP: Saw pod success
Oct 26 18:45:39.304: INFO: Pod "pod-secrets-05fec8bd-149d-4210-8910-ef7ea9b57c26" satisfied condition "success or failure"
Oct 26 18:45:39.314: INFO: Trying to get logs from node 10.123.240.178 pod pod-secrets-05fec8bd-149d-4210-8910-ef7ea9b57c26 container secret-env-test: <nil>
STEP: delete the pod
Oct 26 18:45:39.403: INFO: Waiting for pod pod-secrets-05fec8bd-149d-4210-8910-ef7ea9b57c26 to disappear
Oct 26 18:45:39.414: INFO: Pod pod-secrets-05fec8bd-149d-4210-8910-ef7ea9b57c26 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:45:39.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5249" for this suite.
Oct 26 18:45:47.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:45:49.547: INFO: namespace secrets-5249 deletion completed in 10.105624338s

• [SLOW TEST:12.486 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:45:49.548: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Oct 26 18:45:49.723: INFO: Waiting up to 5m0s for pod "pod-7f73113a-ae69-4ccc-b585-2b7ed13eef2b" in namespace "emptydir-1990" to be "success or failure"
Oct 26 18:45:49.737: INFO: Pod "pod-7f73113a-ae69-4ccc-b585-2b7ed13eef2b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.183893ms
Oct 26 18:45:51.758: INFO: Pod "pod-7f73113a-ae69-4ccc-b585-2b7ed13eef2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034674818s
Oct 26 18:45:53.770: INFO: Pod "pod-7f73113a-ae69-4ccc-b585-2b7ed13eef2b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046965372s
Oct 26 18:45:55.782: INFO: Pod "pod-7f73113a-ae69-4ccc-b585-2b7ed13eef2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.058106699s
STEP: Saw pod success
Oct 26 18:45:55.782: INFO: Pod "pod-7f73113a-ae69-4ccc-b585-2b7ed13eef2b" satisfied condition "success or failure"
Oct 26 18:45:55.794: INFO: Trying to get logs from node 10.123.240.178 pod pod-7f73113a-ae69-4ccc-b585-2b7ed13eef2b container test-container: <nil>
STEP: delete the pod
Oct 26 18:45:55.857: INFO: Waiting for pod pod-7f73113a-ae69-4ccc-b585-2b7ed13eef2b to disappear
Oct 26 18:45:55.868: INFO: Pod pod-7f73113a-ae69-4ccc-b585-2b7ed13eef2b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:45:55.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1990" for this suite.
Oct 26 18:46:03.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:46:05.948: INFO: namespace emptydir-1990 deletion completed in 10.035474884s

• [SLOW TEST:16.400 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:46:05.948: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:46:10.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-265" for this suite.
Oct 26 18:46:18.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:46:20.203: INFO: namespace kubelet-test-265 deletion completed in 10.026677872s

• [SLOW TEST:14.255 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:46:20.204: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Oct 26 18:46:24.573: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-668520336 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Oct 26 18:46:34.912: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:46:34.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8354" for this suite.
Oct 26 18:46:42.979: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:46:44.966: INFO: namespace pods-8354 deletion completed in 10.027245874s

• [SLOW TEST:24.762 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:46:44.969: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:47:01.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2043" for this suite.
Oct 26 18:47:09.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:47:11.591: INFO: namespace resourcequota-2043 deletion completed in 10.054582012s

• [SLOW TEST:26.622 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:47:11.591: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-c5efb605-5496-47d6-a3a2-9197d2e0f8ef
STEP: Creating a pod to test consume configMaps
Oct 26 18:47:11.793: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e1001aea-1cfc-43c2-bbd3-7424bd41b80a" in namespace "projected-92" to be "success or failure"
Oct 26 18:47:11.805: INFO: Pod "pod-projected-configmaps-e1001aea-1cfc-43c2-bbd3-7424bd41b80a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.964833ms
Oct 26 18:47:13.820: INFO: Pod "pod-projected-configmaps-e1001aea-1cfc-43c2-bbd3-7424bd41b80a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026395964s
STEP: Saw pod success
Oct 26 18:47:13.820: INFO: Pod "pod-projected-configmaps-e1001aea-1cfc-43c2-bbd3-7424bd41b80a" satisfied condition "success or failure"
Oct 26 18:47:13.830: INFO: Trying to get logs from node 10.123.240.178 pod pod-projected-configmaps-e1001aea-1cfc-43c2-bbd3-7424bd41b80a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 18:47:13.913: INFO: Waiting for pod pod-projected-configmaps-e1001aea-1cfc-43c2-bbd3-7424bd41b80a to disappear
Oct 26 18:47:13.926: INFO: Pod pod-projected-configmaps-e1001aea-1cfc-43c2-bbd3-7424bd41b80a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:47:13.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-92" for this suite.
Oct 26 18:47:22.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:47:23.957: INFO: namespace projected-92 deletion completed in 9.998738352s

• [SLOW TEST:12.365 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:47:23.957: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-eb1b09fe-4e94-4944-9437-d303473bc788
Oct 26 18:47:24.153: INFO: Pod name my-hostname-basic-eb1b09fe-4e94-4944-9437-d303473bc788: Found 0 pods out of 1
Oct 26 18:47:29.167: INFO: Pod name my-hostname-basic-eb1b09fe-4e94-4944-9437-d303473bc788: Found 1 pods out of 1
Oct 26 18:47:29.167: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-eb1b09fe-4e94-4944-9437-d303473bc788" are running
Oct 26 18:47:29.178: INFO: Pod "my-hostname-basic-eb1b09fe-4e94-4944-9437-d303473bc788-b7wmm" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-10-26 18:47:24 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-10-26 18:47:26 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-10-26 18:47:26 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-10-26 18:47:24 +0000 UTC Reason: Message:}])
Oct 26 18:47:29.178: INFO: Trying to dial the pod
Oct 26 18:47:34.221: INFO: Controller my-hostname-basic-eb1b09fe-4e94-4944-9437-d303473bc788: Got expected result from replica 1 [my-hostname-basic-eb1b09fe-4e94-4944-9437-d303473bc788-b7wmm]: "my-hostname-basic-eb1b09fe-4e94-4944-9437-d303473bc788-b7wmm", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:47:34.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9631" for this suite.
Oct 26 18:47:42.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:47:44.240: INFO: namespace replication-controller-9631 deletion completed in 9.99793885s

• [SLOW TEST:20.283 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:47:44.240: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 18:47:44.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 version'
Oct 26 18:47:44.530: INFO: stderr: ""
Oct 26 18:47:44.530: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.2\", GitCommit:\"c97fe5036ef3df2967d086711e6c0c405941e14b\", GitTreeState:\"clean\", BuildDate:\"2019-10-15T19:18:23Z\", GoVersion:\"go1.12.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16+\", GitVersion:\"v1.16.2+417b9fd\", GitCommit:\"417b9fd\", GitTreeState:\"clean\", BuildDate:\"2020-09-15T16:05:56Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:47:44.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5215" for this suite.
Oct 26 18:47:52.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:47:54.652: INFO: namespace kubectl-5215 deletion completed in 10.101572798s

• [SLOW TEST:10.412 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:47:54.652: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 18:47:54.874: INFO: (0) /api/v1/nodes/10.123.240.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 50.231767ms)
Oct 26 18:47:54.892: INFO: (1) /api/v1/nodes/10.123.240.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 17.377526ms)
Oct 26 18:47:54.911: INFO: (2) /api/v1/nodes/10.123.240.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 18.931337ms)
Oct 26 18:47:54.929: INFO: (3) /api/v1/nodes/10.123.240.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 18.467479ms)
Oct 26 18:47:54.947: INFO: (4) /api/v1/nodes/10.123.240.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 17.699959ms)
Oct 26 18:47:54.965: INFO: (5) /api/v1/nodes/10.123.240.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 17.935642ms)
Oct 26 18:47:54.983: INFO: (6) /api/v1/nodes/10.123.240.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 17.806764ms)
Oct 26 18:47:55.000: INFO: (7) /api/v1/nodes/10.123.240.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 17.284395ms)
Oct 26 18:47:55.021: INFO: (8) /api/v1/nodes/10.123.240.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.1807ms)
Oct 26 18:47:55.038: INFO: (9) /api/v1/nodes/10.123.240.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 16.99827ms)
Oct 26 18:47:55.057: INFO: (10) /api/v1/nodes/10.123.240.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 19.16391ms)
Oct 26 18:47:55.075: INFO: (11) /api/v1/nodes/10.123.240.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 18.329956ms)
Oct 26 18:47:55.093: INFO: (12) /api/v1/nodes/10.123.240.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 17.254177ms)
Oct 26 18:47:55.118: INFO: (13) /api/v1/nodes/10.123.240.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 24.681719ms)
Oct 26 18:47:55.140: INFO: (14) /api/v1/nodes/10.123.240.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 22.194652ms)
Oct 26 18:47:55.162: INFO: (15) /api/v1/nodes/10.123.240.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 22.221296ms)
Oct 26 18:47:55.181: INFO: (16) /api/v1/nodes/10.123.240.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 18.179865ms)
Oct 26 18:47:55.201: INFO: (17) /api/v1/nodes/10.123.240.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.67703ms)
Oct 26 18:47:55.219: INFO: (18) /api/v1/nodes/10.123.240.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 17.547446ms)
Oct 26 18:47:55.243: INFO: (19) /api/v1/nodes/10.123.240.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 24.137358ms)
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:47:55.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5128" for this suite.
Oct 26 18:48:03.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:48:04.290: INFO: namespace proxy-5128 deletion completed in 9.007261879s

• [SLOW TEST:9.638 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:48:04.290: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Oct 26 18:48:04.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 --namespace=kubectl-9433 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Oct 26 18:48:07.065: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Oct 26 18:48:07.065: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:48:09.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9433" for this suite.
Oct 26 18:48:17.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:48:19.181: INFO: namespace kubectl-9433 deletion completed in 10.064950011s

• [SLOW TEST:14.891 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:48:19.182: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-1113
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct 26 18:48:19.340: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Oct 26 18:48:47.738: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.84.61 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1113 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 18:48:47.738: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 18:48:48.967: INFO: Found all expected endpoints: [netserver-0]
Oct 26 18:48:48.981: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.101.234 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1113 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 18:48:48.981: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 18:48:50.381: INFO: Found all expected endpoints: [netserver-1]
Oct 26 18:48:50.391: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.37.225 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1113 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 18:48:50.391: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 18:48:51.599: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:48:51.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1113" for this suite.
Oct 26 18:48:59.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:49:01.741: INFO: namespace pod-network-test-1113 deletion completed in 10.111494746s

• [SLOW TEST:42.559 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:49:01.744: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 18:49:01.921: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-00861832-8176-488c-9395-ccb926bb6048
STEP: Creating configMap with name cm-test-opt-upd-78f7b270-7754-4239-8c15-713b3308fd7a
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-00861832-8176-488c-9395-ccb926bb6048
STEP: Updating configmap cm-test-opt-upd-78f7b270-7754-4239-8c15-713b3308fd7a
STEP: Creating configMap with name cm-test-opt-create-98cc92d0-c714-40ab-a3f9-13511fa32bca
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:50:17.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5645" for this suite.
Oct 26 18:50:33.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:50:35.398: INFO: namespace configmap-5645 deletion completed in 18.039081279s

• [SLOW TEST:93.655 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:50:35.399: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Oct 26 18:50:37.654: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:50:37.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7209" for this suite.
Oct 26 18:50:45.773: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:50:47.807: INFO: namespace container-runtime-7209 deletion completed in 10.07806361s

• [SLOW TEST:12.408 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:50:47.807: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-kstr
STEP: Creating a pod to test atomic-volume-subpath
Oct 26 18:50:48.054: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-kstr" in namespace "subpath-6546" to be "success or failure"
Oct 26 18:50:48.065: INFO: Pod "pod-subpath-test-secret-kstr": Phase="Pending", Reason="", readiness=false. Elapsed: 10.765938ms
Oct 26 18:50:50.079: INFO: Pod "pod-subpath-test-secret-kstr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024646333s
Oct 26 18:50:52.089: INFO: Pod "pod-subpath-test-secret-kstr": Phase="Running", Reason="", readiness=true. Elapsed: 4.035157246s
Oct 26 18:50:54.101: INFO: Pod "pod-subpath-test-secret-kstr": Phase="Running", Reason="", readiness=true. Elapsed: 6.047085123s
Oct 26 18:50:56.114: INFO: Pod "pod-subpath-test-secret-kstr": Phase="Running", Reason="", readiness=true. Elapsed: 8.059748988s
Oct 26 18:50:58.127: INFO: Pod "pod-subpath-test-secret-kstr": Phase="Running", Reason="", readiness=true. Elapsed: 10.072590637s
Oct 26 18:51:00.138: INFO: Pod "pod-subpath-test-secret-kstr": Phase="Running", Reason="", readiness=true. Elapsed: 12.084387415s
Oct 26 18:51:02.150: INFO: Pod "pod-subpath-test-secret-kstr": Phase="Running", Reason="", readiness=true. Elapsed: 14.096381349s
Oct 26 18:51:04.162: INFO: Pod "pod-subpath-test-secret-kstr": Phase="Running", Reason="", readiness=true. Elapsed: 16.107952753s
Oct 26 18:51:06.172: INFO: Pod "pod-subpath-test-secret-kstr": Phase="Running", Reason="", readiness=true. Elapsed: 18.118315648s
Oct 26 18:51:08.183: INFO: Pod "pod-subpath-test-secret-kstr": Phase="Running", Reason="", readiness=true. Elapsed: 20.129072037s
Oct 26 18:51:10.195: INFO: Pod "pod-subpath-test-secret-kstr": Phase="Running", Reason="", readiness=true. Elapsed: 22.140644758s
Oct 26 18:51:12.208: INFO: Pod "pod-subpath-test-secret-kstr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.153577213s
STEP: Saw pod success
Oct 26 18:51:12.208: INFO: Pod "pod-subpath-test-secret-kstr" satisfied condition "success or failure"
Oct 26 18:51:12.218: INFO: Trying to get logs from node 10.123.240.178 pod pod-subpath-test-secret-kstr container test-container-subpath-secret-kstr: <nil>
STEP: delete the pod
Oct 26 18:51:12.332: INFO: Waiting for pod pod-subpath-test-secret-kstr to disappear
Oct 26 18:51:12.347: INFO: Pod pod-subpath-test-secret-kstr no longer exists
STEP: Deleting pod pod-subpath-test-secret-kstr
Oct 26 18:51:12.348: INFO: Deleting pod "pod-subpath-test-secret-kstr" in namespace "subpath-6546"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:51:12.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6546" for this suite.
Oct 26 18:51:20.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:51:22.457: INFO: namespace subpath-6546 deletion completed in 10.05611101s

• [SLOW TEST:34.650 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:51:22.458: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:51:29.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2202" for this suite.
Oct 26 18:51:37.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:51:39.734: INFO: namespace resourcequota-2202 deletion completed in 10.040593005s

• [SLOW TEST:17.277 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:51:39.739: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 18:51:39.907: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-c7f7f446-6031-4720-9f9a-ae55c78dc3a2
STEP: Creating secret with name s-test-opt-upd-4aa66942-2b4b-4016-9e67-27988c2eeb18
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-c7f7f446-6031-4720-9f9a-ae55c78dc3a2
STEP: Updating secret s-test-opt-upd-4aa66942-2b4b-4016-9e67-27988c2eeb18
STEP: Creating secret with name s-test-opt-create-2e41e5ef-2891-4da7-807a-1ff6633940ec
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:53:11.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8393" for this suite.
Oct 26 18:53:43.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:53:45.728: INFO: namespace projected-8393 deletion completed in 34.105490189s

• [SLOW TEST:125.989 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:53:45.728: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Oct 26 18:53:48.599: INFO: Successfully updated pod "annotationupdate7d7eb998-d4ae-41f5-bb9d-b93b5032cbb6"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:53:50.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2774" for this suite.
Oct 26 18:54:04.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:54:06.789: INFO: namespace projected-2774 deletion completed in 16.107412584s

• [SLOW TEST:21.060 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:54:06.789: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Oct 26 18:54:06.973: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d746272c-3ad9-4aae-aa3d-13352ad6afff" in namespace "downward-api-4078" to be "success or failure"
Oct 26 18:54:06.993: INFO: Pod "downwardapi-volume-d746272c-3ad9-4aae-aa3d-13352ad6afff": Phase="Pending", Reason="", readiness=false. Elapsed: 18.105839ms
Oct 26 18:54:09.005: INFO: Pod "downwardapi-volume-d746272c-3ad9-4aae-aa3d-13352ad6afff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030608541s
Oct 26 18:54:11.027: INFO: Pod "downwardapi-volume-d746272c-3ad9-4aae-aa3d-13352ad6afff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052611979s
STEP: Saw pod success
Oct 26 18:54:11.027: INFO: Pod "downwardapi-volume-d746272c-3ad9-4aae-aa3d-13352ad6afff" satisfied condition "success or failure"
Oct 26 18:54:11.040: INFO: Trying to get logs from node 10.123.240.178 pod downwardapi-volume-d746272c-3ad9-4aae-aa3d-13352ad6afff container client-container: <nil>
STEP: delete the pod
Oct 26 18:54:11.110: INFO: Waiting for pod downwardapi-volume-d746272c-3ad9-4aae-aa3d-13352ad6afff to disappear
Oct 26 18:54:11.120: INFO: Pod downwardapi-volume-d746272c-3ad9-4aae-aa3d-13352ad6afff no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:54:11.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4078" for this suite.
Oct 26 18:54:19.198: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:54:21.224: INFO: namespace downward-api-4078 deletion completed in 10.072409592s

• [SLOW TEST:14.435 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:54:21.227: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Oct 26 18:54:21.847: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Oct 26 18:54:23.899: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739335261, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739335261, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739335261, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739335261, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 18:54:26.954: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 18:54:26.967: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:54:28.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3114" for this suite.
Oct 26 18:54:36.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:54:38.457: INFO: namespace crd-webhook-3114 deletion completed in 10.102286052s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:17.288 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:54:38.517: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Oct 26 18:54:42.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec pod-sharedvolume-732b420e-1050-4a38-b97f-40477531a252 -c busybox-main-container --namespace=emptydir-7643 -- cat /usr/share/volumeshare/shareddata.txt'
Oct 26 18:54:43.229: INFO: stderr: ""
Oct 26 18:54:43.229: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:54:43.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7643" for this suite.
Oct 26 18:54:51.306: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:54:53.355: INFO: namespace emptydir-7643 deletion completed in 10.09049469s

• [SLOW TEST:14.838 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:54:53.355: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-78802c2b-fc4e-4afc-9141-8ee5624e0dce
STEP: Creating a pod to test consume configMaps
Oct 26 18:54:53.564: INFO: Waiting up to 5m0s for pod "pod-configmaps-5909e2f4-d108-418e-b01f-0aa6558caba2" in namespace "configmap-5020" to be "success or failure"
Oct 26 18:54:53.574: INFO: Pod "pod-configmaps-5909e2f4-d108-418e-b01f-0aa6558caba2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.715316ms
Oct 26 18:54:55.586: INFO: Pod "pod-configmaps-5909e2f4-d108-418e-b01f-0aa6558caba2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022314273s
STEP: Saw pod success
Oct 26 18:54:55.586: INFO: Pod "pod-configmaps-5909e2f4-d108-418e-b01f-0aa6558caba2" satisfied condition "success or failure"
Oct 26 18:54:55.597: INFO: Trying to get logs from node 10.123.240.178 pod pod-configmaps-5909e2f4-d108-418e-b01f-0aa6558caba2 container configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 18:54:55.656: INFO: Waiting for pod pod-configmaps-5909e2f4-d108-418e-b01f-0aa6558caba2 to disappear
Oct 26 18:54:55.671: INFO: Pod pod-configmaps-5909e2f4-d108-418e-b01f-0aa6558caba2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:54:55.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5020" for this suite.
Oct 26 18:55:03.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:55:05.782: INFO: namespace configmap-5020 deletion completed in 10.07906616s

• [SLOW TEST:12.426 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:55:05.783: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9774.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9774.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9774.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9774.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9774.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9774.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9774.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9774.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9774.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9774.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9774.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9774.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9774.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9774.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9774.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9774.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9774.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9774.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 26 18:55:24.067: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9774.svc.cluster.local from pod dns-9774/dns-test-72d2de8c-0def-436f-bfa4-6eb1914e8ac8: the server could not find the requested resource (get pods dns-test-72d2de8c-0def-436f-bfa4-6eb1914e8ac8)
Oct 26 18:55:24.084: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9774.svc.cluster.local from pod dns-9774/dns-test-72d2de8c-0def-436f-bfa4-6eb1914e8ac8: the server could not find the requested resource (get pods dns-test-72d2de8c-0def-436f-bfa4-6eb1914e8ac8)
Oct 26 18:55:24.284: INFO: Lookups using dns-9774/dns-test-72d2de8c-0def-436f-bfa4-6eb1914e8ac8 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9774.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9774.svc.cluster.local]

Oct 26 18:55:29.543: INFO: DNS probes using dns-9774/dns-test-72d2de8c-0def-436f-bfa4-6eb1914e8ac8 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:55:29.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9774" for this suite.
Oct 26 18:55:37.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:55:39.715: INFO: namespace dns-9774 deletion completed in 10.0531924s

• [SLOW TEST:33.933 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:55:39.716: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-72760885-f171-4051-aaf3-0d437a43d6cd
STEP: Creating a pod to test consume secrets
Oct 26 18:55:39.945: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-70fff64a-dc7f-4cbd-b0af-fb39c002afb0" in namespace "projected-3506" to be "success or failure"
Oct 26 18:55:39.956: INFO: Pod "pod-projected-secrets-70fff64a-dc7f-4cbd-b0af-fb39c002afb0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.715619ms
Oct 26 18:55:41.967: INFO: Pod "pod-projected-secrets-70fff64a-dc7f-4cbd-b0af-fb39c002afb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021933429s
Oct 26 18:55:43.978: INFO: Pod "pod-projected-secrets-70fff64a-dc7f-4cbd-b0af-fb39c002afb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033554507s
STEP: Saw pod success
Oct 26 18:55:43.978: INFO: Pod "pod-projected-secrets-70fff64a-dc7f-4cbd-b0af-fb39c002afb0" satisfied condition "success or failure"
Oct 26 18:55:43.989: INFO: Trying to get logs from node 10.123.240.178 pod pod-projected-secrets-70fff64a-dc7f-4cbd-b0af-fb39c002afb0 container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 26 18:55:44.049: INFO: Waiting for pod pod-projected-secrets-70fff64a-dc7f-4cbd-b0af-fb39c002afb0 to disappear
Oct 26 18:55:44.065: INFO: Pod pod-projected-secrets-70fff64a-dc7f-4cbd-b0af-fb39c002afb0 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:55:44.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3506" for this suite.
Oct 26 18:55:52.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:55:54.157: INFO: namespace projected-3506 deletion completed in 10.061063285s

• [SLOW TEST:14.442 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:55:54.159: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:55:54.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6684" for this suite.
Oct 26 18:56:02.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:56:04.488: INFO: namespace services-6684 deletion completed in 10.138842024s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:10.330 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:56:04.489: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 18:56:04.651: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Oct 26 18:56:14.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 --namespace=crd-publish-openapi-653 create -f -'
Oct 26 18:56:15.515: INFO: stderr: ""
Oct 26 18:56:15.515: INFO: stdout: "e2e-test-crd-publish-openapi-7178-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Oct 26 18:56:15.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 --namespace=crd-publish-openapi-653 delete e2e-test-crd-publish-openapi-7178-crds test-cr'
Oct 26 18:56:15.762: INFO: stderr: ""
Oct 26 18:56:15.762: INFO: stdout: "e2e-test-crd-publish-openapi-7178-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Oct 26 18:56:15.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 --namespace=crd-publish-openapi-653 apply -f -'
Oct 26 18:56:16.415: INFO: stderr: ""
Oct 26 18:56:16.415: INFO: stdout: "e2e-test-crd-publish-openapi-7178-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Oct 26 18:56:16.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 --namespace=crd-publish-openapi-653 delete e2e-test-crd-publish-openapi-7178-crds test-cr'
Oct 26 18:56:16.573: INFO: stderr: ""
Oct 26 18:56:16.573: INFO: stdout: "e2e-test-crd-publish-openapi-7178-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Oct 26 18:56:16.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 explain e2e-test-crd-publish-openapi-7178-crds'
Oct 26 18:56:16.926: INFO: stderr: ""
Oct 26 18:56:16.926: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7178-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:56:28.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-653" for this suite.
Oct 26 18:56:36.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:56:38.675: INFO: namespace crd-publish-openapi-653 deletion completed in 10.020323835s

• [SLOW TEST:34.186 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:56:38.675: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Oct 26 18:56:39.904: INFO: Waiting up to 5m0s for pod "client-containers-58975293-79c3-44e8-8e9f-61dcd3118651" in namespace "containers-2144" to be "success or failure"
Oct 26 18:56:39.914: INFO: Pod "client-containers-58975293-79c3-44e8-8e9f-61dcd3118651": Phase="Pending", Reason="", readiness=false. Elapsed: 9.92455ms
Oct 26 18:56:41.929: INFO: Pod "client-containers-58975293-79c3-44e8-8e9f-61dcd3118651": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024505723s
STEP: Saw pod success
Oct 26 18:56:41.929: INFO: Pod "client-containers-58975293-79c3-44e8-8e9f-61dcd3118651" satisfied condition "success or failure"
Oct 26 18:56:41.951: INFO: Trying to get logs from node 10.123.240.178 pod client-containers-58975293-79c3-44e8-8e9f-61dcd3118651 container test-container: <nil>
STEP: delete the pod
Oct 26 18:56:42.051: INFO: Waiting for pod client-containers-58975293-79c3-44e8-8e9f-61dcd3118651 to disappear
Oct 26 18:56:42.066: INFO: Pod client-containers-58975293-79c3-44e8-8e9f-61dcd3118651 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:56:42.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2144" for this suite.
Oct 26 18:56:50.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:56:52.134: INFO: namespace containers-2144 deletion completed in 10.03843462s

• [SLOW TEST:13.459 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:56:52.135: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:56:52.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5208" for this suite.
Oct 26 18:57:00.369: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:57:02.438: INFO: namespace custom-resource-definition-5208 deletion completed in 10.110528096s

• [SLOW TEST:10.303 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:57:02.438: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Oct 26 18:57:02.676: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85b3ccbd-65f1-47e4-b141-6d00601f4edb" in namespace "downward-api-6832" to be "success or failure"
Oct 26 18:57:02.689: INFO: Pod "downwardapi-volume-85b3ccbd-65f1-47e4-b141-6d00601f4edb": Phase="Pending", Reason="", readiness=false. Elapsed: 13.188862ms
Oct 26 18:57:04.702: INFO: Pod "downwardapi-volume-85b3ccbd-65f1-47e4-b141-6d00601f4edb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026193055s
STEP: Saw pod success
Oct 26 18:57:04.702: INFO: Pod "downwardapi-volume-85b3ccbd-65f1-47e4-b141-6d00601f4edb" satisfied condition "success or failure"
Oct 26 18:57:04.714: INFO: Trying to get logs from node 10.123.240.178 pod downwardapi-volume-85b3ccbd-65f1-47e4-b141-6d00601f4edb container client-container: <nil>
STEP: delete the pod
Oct 26 18:57:04.778: INFO: Waiting for pod downwardapi-volume-85b3ccbd-65f1-47e4-b141-6d00601f4edb to disappear
Oct 26 18:57:04.789: INFO: Pod downwardapi-volume-85b3ccbd-65f1-47e4-b141-6d00601f4edb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:57:04.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6832" for this suite.
Oct 26 18:57:12.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:57:14.848: INFO: namespace downward-api-6832 deletion completed in 10.008870195s

• [SLOW TEST:12.410 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:57:14.852: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Oct 26 18:57:15.070: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3470f839-9fc5-4ff2-ae13-2162ead0dafa" in namespace "projected-7525" to be "success or failure"
Oct 26 18:57:15.082: INFO: Pod "downwardapi-volume-3470f839-9fc5-4ff2-ae13-2162ead0dafa": Phase="Pending", Reason="", readiness=false. Elapsed: 12.438456ms
Oct 26 18:57:17.095: INFO: Pod "downwardapi-volume-3470f839-9fc5-4ff2-ae13-2162ead0dafa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025145337s
STEP: Saw pod success
Oct 26 18:57:17.095: INFO: Pod "downwardapi-volume-3470f839-9fc5-4ff2-ae13-2162ead0dafa" satisfied condition "success or failure"
Oct 26 18:57:17.107: INFO: Trying to get logs from node 10.123.240.178 pod downwardapi-volume-3470f839-9fc5-4ff2-ae13-2162ead0dafa container client-container: <nil>
STEP: delete the pod
Oct 26 18:57:17.167: INFO: Waiting for pod downwardapi-volume-3470f839-9fc5-4ff2-ae13-2162ead0dafa to disappear
Oct 26 18:57:17.182: INFO: Pod downwardapi-volume-3470f839-9fc5-4ff2-ae13-2162ead0dafa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:57:17.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7525" for this suite.
Oct 26 18:57:25.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:57:27.232: INFO: namespace projected-7525 deletion completed in 10.027117211s

• [SLOW TEST:12.381 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:57:27.233: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 18:57:27.378: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:57:31.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5111" for this suite.
Oct 26 18:58:17.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:58:19.609: INFO: namespace pods-5111 deletion completed in 48.045314565s

• [SLOW TEST:52.377 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:58:19.609: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Oct 26 18:58:19.781: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:58:24.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-123" for this suite.
Oct 26 18:58:32.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:58:34.115: INFO: namespace init-container-123 deletion completed in 10.015481469s

• [SLOW TEST:14.505 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:58:34.115: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Oct 26 18:58:34.314: INFO: Waiting up to 5m0s for pod "downwardapi-volume-acb73515-e7dc-453b-9c61-83424c76d578" in namespace "projected-2736" to be "success or failure"
Oct 26 18:58:34.333: INFO: Pod "downwardapi-volume-acb73515-e7dc-453b-9c61-83424c76d578": Phase="Pending", Reason="", readiness=false. Elapsed: 19.172428ms
Oct 26 18:58:36.346: INFO: Pod "downwardapi-volume-acb73515-e7dc-453b-9c61-83424c76d578": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031954797s
STEP: Saw pod success
Oct 26 18:58:36.346: INFO: Pod "downwardapi-volume-acb73515-e7dc-453b-9c61-83424c76d578" satisfied condition "success or failure"
Oct 26 18:58:36.356: INFO: Trying to get logs from node 10.123.240.178 pod downwardapi-volume-acb73515-e7dc-453b-9c61-83424c76d578 container client-container: <nil>
STEP: delete the pod
Oct 26 18:58:36.418: INFO: Waiting for pod downwardapi-volume-acb73515-e7dc-453b-9c61-83424c76d578 to disappear
Oct 26 18:58:36.428: INFO: Pod downwardapi-volume-acb73515-e7dc-453b-9c61-83424c76d578 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:58:36.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2736" for this suite.
Oct 26 18:58:44.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:58:46.504: INFO: namespace projected-2736 deletion completed in 10.053867307s

• [SLOW TEST:12.389 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:58:46.506: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-290fc550-8e60-43d2-a801-a5d2009f2d57
STEP: Creating a pod to test consume configMaps
Oct 26 18:58:47.807: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5aa1a3ab-b2b7-42ff-9330-8b279ddff3f9" in namespace "projected-3439" to be "success or failure"
Oct 26 18:58:47.830: INFO: Pod "pod-projected-configmaps-5aa1a3ab-b2b7-42ff-9330-8b279ddff3f9": Phase="Pending", Reason="", readiness=false. Elapsed: 23.169347ms
Oct 26 18:58:49.841: INFO: Pod "pod-projected-configmaps-5aa1a3ab-b2b7-42ff-9330-8b279ddff3f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03340834s
Oct 26 18:58:51.851: INFO: Pod "pod-projected-configmaps-5aa1a3ab-b2b7-42ff-9330-8b279ddff3f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044352588s
STEP: Saw pod success
Oct 26 18:58:51.852: INFO: Pod "pod-projected-configmaps-5aa1a3ab-b2b7-42ff-9330-8b279ddff3f9" satisfied condition "success or failure"
Oct 26 18:58:51.862: INFO: Trying to get logs from node 10.123.240.178 pod pod-projected-configmaps-5aa1a3ab-b2b7-42ff-9330-8b279ddff3f9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 18:58:51.920: INFO: Waiting for pod pod-projected-configmaps-5aa1a3ab-b2b7-42ff-9330-8b279ddff3f9 to disappear
Oct 26 18:58:51.931: INFO: Pod pod-projected-configmaps-5aa1a3ab-b2b7-42ff-9330-8b279ddff3f9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:58:51.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3439" for this suite.
Oct 26 18:59:00.003: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:59:02.193: INFO: namespace projected-3439 deletion completed in 10.233116871s

• [SLOW TEST:15.687 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:59:02.193: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Oct 26 18:59:04.986: INFO: Successfully updated pod "labelsupdatea8331fc2-47c7-42e2-9a11-cc35c2db748d"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:59:07.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5420" for this suite.
Oct 26 18:59:21.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:59:23.206: INFO: namespace projected-5420 deletion completed in 16.073335012s

• [SLOW TEST:21.013 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:59:23.208: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 18:59:23.419: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-19f9b2c2-be61-4ca6-b7cb-45684ff19c2c" in namespace "security-context-test-9284" to be "success or failure"
Oct 26 18:59:23.432: INFO: Pod "alpine-nnp-false-19f9b2c2-be61-4ca6-b7cb-45684ff19c2c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.411144ms
Oct 26 18:59:25.443: INFO: Pod "alpine-nnp-false-19f9b2c2-be61-4ca6-b7cb-45684ff19c2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02383189s
Oct 26 18:59:27.456: INFO: Pod "alpine-nnp-false-19f9b2c2-be61-4ca6-b7cb-45684ff19c2c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037409022s
Oct 26 18:59:29.470: INFO: Pod "alpine-nnp-false-19f9b2c2-be61-4ca6-b7cb-45684ff19c2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050625389s
Oct 26 18:59:29.470: INFO: Pod "alpine-nnp-false-19f9b2c2-be61-4ca6-b7cb-45684ff19c2c" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:59:29.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9284" for this suite.
Oct 26 18:59:37.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:59:39.596: INFO: namespace security-context-test-9284 deletion completed in 10.043121631s

• [SLOW TEST:16.388 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:59:39.598: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-a6b66547-843d-492f-b188-83f7c956cafd
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:59:39.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8965" for this suite.
Oct 26 18:59:47.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 18:59:49.867: INFO: namespace configmap-8965 deletion completed in 10.08008367s

• [SLOW TEST:10.269 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 18:59:49.867: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 18:59:50.926: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 18:59:52.960: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739335590, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739335590, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739335591, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739335590, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 18:59:55.999: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 18:59:56.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2112" for this suite.
Oct 26 19:00:04.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:00:06.658: INFO: namespace webhook-2112 deletion completed in 10.038726796s
STEP: Destroying namespace "webhook-2112-markers" for this suite.
Oct 26 19:00:14.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:00:16.707: INFO: namespace webhook-2112-markers deletion completed in 10.049508029s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:26.899 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:00:16.767: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Oct 26 19:00:47.567: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1026 19:00:47.567244      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:00:47.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7275" for this suite.
Oct 26 19:00:55.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:00:57.736: INFO: namespace gc-7275 deletion completed in 10.14285662s

• [SLOW TEST:40.969 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:00:57.741: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:01:05.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4801" for this suite.
Oct 26 19:01:13.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:01:15.470: INFO: namespace namespaces-4801 deletion completed in 10.077829778s
STEP: Destroying namespace "nsdeletetest-2692" for this suite.
Oct 26 19:01:15.480: INFO: Namespace nsdeletetest-2692 was already deleted
STEP: Destroying namespace "nsdeletetest-2016" for this suite.
Oct 26 19:01:23.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:01:25.559: INFO: namespace nsdeletetest-2016 deletion completed in 10.079044676s

• [SLOW TEST:27.818 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:01:25.559: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2035.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2035.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2035.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2035.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 26 19:01:29.849: INFO: DNS probes using dns-test-774e3f73-7ad8-421e-8f75-ecfcfdbcdefc succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2035.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2035.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2035.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2035.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 26 19:01:48.059: INFO: DNS probes using dns-test-8c1b37a9-92c3-405f-a238-61332b0ee544 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2035.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2035.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2035.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2035.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 26 19:01:52.266: INFO: DNS probes using dns-test-3d6a6cd1-9d29-4d92-9a09-5cc0f8aaa909 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:01:52.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2035" for this suite.
Oct 26 19:02:00.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:02:02.541: INFO: namespace dns-2035 deletion completed in 10.141854335s

• [SLOW TEST:36.982 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:02:02.541: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-836b0e26-bb00-4838-b39c-6a7beca5b87f
STEP: Creating a pod to test consume secrets
Oct 26 19:02:02.802: INFO: Waiting up to 5m0s for pod "pod-secrets-f68cdd34-575b-4488-8906-d2c01b7c979e" in namespace "secrets-5067" to be "success or failure"
Oct 26 19:02:02.822: INFO: Pod "pod-secrets-f68cdd34-575b-4488-8906-d2c01b7c979e": Phase="Pending", Reason="", readiness=false. Elapsed: 19.640076ms
Oct 26 19:02:04.836: INFO: Pod "pod-secrets-f68cdd34-575b-4488-8906-d2c01b7c979e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033076328s
STEP: Saw pod success
Oct 26 19:02:04.836: INFO: Pod "pod-secrets-f68cdd34-575b-4488-8906-d2c01b7c979e" satisfied condition "success or failure"
Oct 26 19:02:04.846: INFO: Trying to get logs from node 10.123.240.178 pod pod-secrets-f68cdd34-575b-4488-8906-d2c01b7c979e container secret-volume-test: <nil>
STEP: delete the pod
Oct 26 19:02:04.957: INFO: Waiting for pod pod-secrets-f68cdd34-575b-4488-8906-d2c01b7c979e to disappear
Oct 26 19:02:04.966: INFO: Pod pod-secrets-f68cdd34-575b-4488-8906-d2c01b7c979e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:02:04.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5067" for this suite.
Oct 26 19:02:13.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:02:15.044: INFO: namespace secrets-5067 deletion completed in 10.048858931s

• [SLOW TEST:12.503 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:02:15.048: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Oct 26 19:02:15.267: INFO: Waiting up to 5m0s for pod "downwardapi-volume-78ff9b8f-9b9e-480b-ab3a-bafa8b78619a" in namespace "projected-2095" to be "success or failure"
Oct 26 19:02:15.278: INFO: Pod "downwardapi-volume-78ff9b8f-9b9e-480b-ab3a-bafa8b78619a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.924764ms
Oct 26 19:02:17.298: INFO: Pod "downwardapi-volume-78ff9b8f-9b9e-480b-ab3a-bafa8b78619a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030686686s
Oct 26 19:02:19.309: INFO: Pod "downwardapi-volume-78ff9b8f-9b9e-480b-ab3a-bafa8b78619a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042463648s
STEP: Saw pod success
Oct 26 19:02:19.310: INFO: Pod "downwardapi-volume-78ff9b8f-9b9e-480b-ab3a-bafa8b78619a" satisfied condition "success or failure"
Oct 26 19:02:19.321: INFO: Trying to get logs from node 10.123.240.178 pod downwardapi-volume-78ff9b8f-9b9e-480b-ab3a-bafa8b78619a container client-container: <nil>
STEP: delete the pod
Oct 26 19:02:19.388: INFO: Waiting for pod downwardapi-volume-78ff9b8f-9b9e-480b-ab3a-bafa8b78619a to disappear
Oct 26 19:02:19.397: INFO: Pod downwardapi-volume-78ff9b8f-9b9e-480b-ab3a-bafa8b78619a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:02:19.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2095" for this suite.
Oct 26 19:02:27.488: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:02:29.494: INFO: namespace projected-2095 deletion completed in 10.059888338s

• [SLOW TEST:14.447 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:02:29.495: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Oct 26 19:02:29.708: INFO: Pod name pod-release: Found 0 pods out of 1
Oct 26 19:02:34.722: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:02:35.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1663" for this suite.
Oct 26 19:02:43.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:02:45.882: INFO: namespace replication-controller-1663 deletion completed in 10.045908049s

• [SLOW TEST:16.388 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:02:45.883: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Oct 26 19:02:46.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-7389'
Oct 26 19:02:46.244: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Oct 26 19:02:46.244: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Oct 26 19:02:46.260: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Oct 26 19:02:46.316: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Oct 26 19:02:46.345: INFO: scanned /root for discovery docs: <nil>
Oct 26 19:02:46.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-7389'
Oct 26 19:02:59.871: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Oct 26 19:02:59.871: INFO: stdout: "Created e2e-test-httpd-rc-f7eed31c7a685f6c2dbce77b74b1f07b\nScaling up e2e-test-httpd-rc-f7eed31c7a685f6c2dbce77b74b1f07b from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-f7eed31c7a685f6c2dbce77b74b1f07b up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-f7eed31c7a685f6c2dbce77b74b1f07b to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Oct 26 19:02:59.871: INFO: stdout: "Created e2e-test-httpd-rc-f7eed31c7a685f6c2dbce77b74b1f07b\nScaling up e2e-test-httpd-rc-f7eed31c7a685f6c2dbce77b74b1f07b from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-f7eed31c7a685f6c2dbce77b74b1f07b up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-f7eed31c7a685f6c2dbce77b74b1f07b to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Oct 26 19:02:59.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-7389'
Oct 26 19:03:00.016: INFO: stderr: ""
Oct 26 19:03:00.016: INFO: stdout: "e2e-test-httpd-rc-f7eed31c7a685f6c2dbce77b74b1f07b-slp2n e2e-test-httpd-rc-l4njm "
STEP: Replicas for run=e2e-test-httpd-rc: expected=1 actual=2
Oct 26 19:03:05.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-7389'
Oct 26 19:03:05.161: INFO: stderr: ""
Oct 26 19:03:05.161: INFO: stdout: "e2e-test-httpd-rc-f7eed31c7a685f6c2dbce77b74b1f07b-slp2n "
Oct 26 19:03:05.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods e2e-test-httpd-rc-f7eed31c7a685f6c2dbce77b74b1f07b-slp2n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7389'
Oct 26 19:03:05.374: INFO: stderr: ""
Oct 26 19:03:05.374: INFO: stdout: "true"
Oct 26 19:03:05.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods e2e-test-httpd-rc-f7eed31c7a685f6c2dbce77b74b1f07b-slp2n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7389'
Oct 26 19:03:05.521: INFO: stderr: ""
Oct 26 19:03:05.521: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Oct 26 19:03:05.521: INFO: e2e-test-httpd-rc-f7eed31c7a685f6c2dbce77b74b1f07b-slp2n is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Oct 26 19:03:05.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 delete rc e2e-test-httpd-rc --namespace=kubectl-7389'
Oct 26 19:03:05.685: INFO: stderr: ""
Oct 26 19:03:05.685: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:03:05.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7389" for this suite.
Oct 26 19:03:19.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:03:21.842: INFO: namespace kubectl-7389 deletion completed in 16.12965398s

• [SLOW TEST:35.960 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:03:21.843: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Oct 26 19:03:26.134: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:03:26.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9289" for this suite.
Oct 26 19:03:34.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:03:36.253: INFO: namespace container-runtime-9289 deletion completed in 10.028315356s

• [SLOW TEST:14.410 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:03:36.253: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Oct 26 19:03:36.387: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Oct 26 19:03:36.476: INFO: Waiting for terminating namespaces to be deleted...
Oct 26 19:03:36.493: INFO: 
Logging pods the kubelet thinks is on node 10.123.240.172 before test
Oct 26 19:03:36.622: INFO: cluster-storage-operator-6c6dd4b587-52s64 from openshift-cluster-storage-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.622: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Oct 26 19:03:36.623: INFO: openshift-service-catalog-apiserver-operator-d6cf765ff-k9snw from openshift-service-catalog-apiserver-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container operator ready: true, restart count 1
Oct 26 19:03:36.623: INFO: sonobuoy from sonobuoy started at 2020-10-26 18:20:41 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Oct 26 19:03:36.623: INFO: multus-admission-controller-4dblm from openshift-multus started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container multus-admission-controller ready: true, restart count 0
Oct 26 19:03:36.623: INFO: prometheus-adapter-6cccbf8dbb-4bpjw from openshift-monitoring started at 2020-10-26 17:05:48 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container prometheus-adapter ready: true, restart count 0
Oct 26 19:03:36.623: INFO: ibmcloud-block-storage-driver-fm8d2 from kube-system started at 2020-10-26 16:56:55 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Oct 26 19:03:36.623: INFO: console-operator-7f9f78ff66-kf42x from openshift-console-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container console-operator ready: true, restart count 1
Oct 26 19:03:36.623: INFO: calico-kube-controllers-599969f895-cnxrs from calico-system started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Oct 26 19:03:36.623: INFO: cluster-node-tuning-operator-86b7f98f7b-qjdxp from openshift-cluster-node-tuning-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Oct 26 19:03:36.623: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-10-26 17:05:50 +0000 UTC (3 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container alertmanager ready: true, restart count 0
Oct 26 19:03:36.623: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 26 19:03:36.623: INFO: 	Container config-reloader ready: true, restart count 0
Oct 26 19:03:36.623: INFO: ibm-master-proxy-static-10.123.240.172 from kube-system started at 2020-10-26 16:56:50 +0000 UTC (2 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 19:03:36.623: INFO: 	Container pause ready: true, restart count 0
Oct 26 19:03:36.623: INFO: multus-wk7tv from openshift-multus started at 2020-10-26 16:57:19 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container kube-multus ready: true, restart count 0
Oct 26 19:03:36.623: INFO: openshift-kube-proxy-k67gq from openshift-kube-proxy started at 2020-10-26 16:57:26 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 26 19:03:36.623: INFO: cluster-monitoring-operator-77bbbf9cb7-sd49n from openshift-monitoring started at 2020-10-26 16:58:24 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Oct 26 19:03:36.623: INFO: node-exporter-pbqtq from openshift-monitoring started at 2020-10-26 16:59:03 +0000 UTC (2 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:03:36.623: INFO: 	Container node-exporter ready: true, restart count 0
Oct 26 19:03:36.623: INFO: calico-typha-6b7867b64d-wwhp2 from calico-system started at 2020-10-26 17:00:10 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 19:03:36.623: INFO: ibm-keepalived-watcher-rdht2 from kube-system started at 2020-10-26 16:56:52 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 19:03:36.623: INFO: marketplace-operator-c74f66688-865bs from openshift-marketplace started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container marketplace-operator ready: true, restart count 0
Oct 26 19:03:36.623: INFO: openshift-service-catalog-controller-manager-operator-6fcbsks6s from openshift-service-catalog-controller-manager-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container operator ready: true, restart count 1
Oct 26 19:03:36.623: INFO: tuned-f8xvt from openshift-cluster-node-tuning-operator started at 2020-10-26 17:00:14 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container tuned ready: true, restart count 0
Oct 26 19:03:36.623: INFO: ibm-cloud-provider-ip-149-81-128-30-5bdf48d794-vdds8 from ibm-system started at 2020-10-26 17:10:29 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container ibm-cloud-provider-ip-149-81-128-30 ready: true, restart count 0
Oct 26 19:03:36.623: INFO: packageserver-d5dbff54b-xqj44 from openshift-operator-lifecycle-manager started at 2020-10-26 18:42:11 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container packageserver ready: true, restart count 0
Oct 26 19:03:36.623: INFO: downloads-56f66db77f-f9rtp from openshift-console started at 2020-10-26 16:58:23 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container download-server ready: true, restart count 0
Oct 26 19:03:36.623: INFO: sonobuoy-systemd-logs-daemon-set-a94f7e2648a3464d-9nhrv from sonobuoy started at 2020-10-26 18:20:50 +0000 UTC (2 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 26 19:03:36.623: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 26 19:03:36.623: INFO: telemeter-client-7c4c649567-vp6j2 from openshift-monitoring started at 2020-10-26 18:42:09 +0000 UTC (3 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:03:36.623: INFO: 	Container reload ready: true, restart count 0
Oct 26 19:03:36.623: INFO: 	Container telemeter-client ready: true, restart count 0
Oct 26 19:03:36.623: INFO: calico-node-kchsj from calico-system started at 2020-10-26 16:58:05 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 19:03:36.623: INFO: olm-operator-cbd8cfd65-5hqds from openshift-operator-lifecycle-manager started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container olm-operator ready: true, restart count 0
Oct 26 19:03:36.623: INFO: service-ca-operator-54f4b4db4-wvhxp from openshift-service-ca-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container operator ready: true, restart count 0
Oct 26 19:03:36.623: INFO: cluster-image-registry-operator-854d785579-78wmp from openshift-image-registry started at 2020-10-26 16:58:22 +0000 UTC (2 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Oct 26 19:03:36.623: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Oct 26 19:03:36.623: INFO: downloads-56f66db77f-qk97h from openshift-console started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container download-server ready: true, restart count 0
Oct 26 19:03:36.623: INFO: ibmcloud-block-storage-plugin-79495594d5-7m8wf from kube-system started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Oct 26 19:03:36.623: INFO: ibm-storage-watcher-7fc85c5589-t79km from kube-system started at 2020-10-26 16:58:23 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Oct 26 19:03:36.623: INFO: ingress-operator-549d4c77b5-8mj9f from openshift-ingress-operator started at 2020-10-26 16:58:24 +0000 UTC (2 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container ingress-operator ready: true, restart count 0
Oct 26 19:03:36.623: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:03:36.623: INFO: network-operator-5647cdbff6-qn2hq from openshift-network-operator started at 2020-10-26 16:56:55 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container network-operator ready: true, restart count 0
Oct 26 19:03:36.623: INFO: tigera-operator-798cfbf7dd-x8hf6 from tigera-operator started at 2020-10-26 16:56:55 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container tigera-operator ready: true, restart count 2
Oct 26 19:03:36.623: INFO: dns-operator-5bd9bd8fcd-gj6cw from openshift-dns-operator started at 2020-10-26 16:58:22 +0000 UTC (2 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container dns-operator ready: true, restart count 0
Oct 26 19:03:36.623: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:03:36.623: INFO: node-ca-rhts4 from openshift-image-registry started at 2020-10-26 16:59:40 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container node-ca ready: true, restart count 0
Oct 26 19:03:36.623: INFO: dns-default-qnchn from openshift-dns started at 2020-10-26 16:59:44 +0000 UTC (2 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container dns ready: true, restart count 0
Oct 26 19:03:36.623: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 26 19:03:36.623: INFO: router-default-56c7ff9d54-2kbrj from openshift-ingress started at 2020-10-26 18:42:09 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container router ready: true, restart count 0
Oct 26 19:03:36.623: INFO: catalog-operator-7bf86b4f96-kpnmt from openshift-operator-lifecycle-manager started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container catalog-operator ready: true, restart count 0
Oct 26 19:03:36.623: INFO: ibm-file-plugin-6b86cbfbc6-vw94d from kube-system started at 2020-10-26 16:58:23 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Oct 26 19:03:36.623: INFO: cluster-samples-operator-644946789c-zw8th from openshift-cluster-samples-operator started at 2020-10-26 17:01:55 +0000 UTC (2 container statuses recorded)
Oct 26 19:03:36.623: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Oct 26 19:03:36.623: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Oct 26 19:03:36.623: INFO: 
Logging pods the kubelet thinks is on node 10.123.240.174 before test
Oct 26 19:03:36.739: INFO: multus-admission-controller-f46wp from openshift-multus started at 2020-10-26 16:58:27 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.739: INFO: 	Container multus-admission-controller ready: true, restart count 0
Oct 26 19:03:36.739: INFO: ibm-cloud-provider-ip-149-81-128-30-5bdf48d794-9qftf from ibm-system started at 2020-10-26 18:42:09 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.739: INFO: 	Container ibm-cloud-provider-ip-149-81-128-30 ready: true, restart count 0
Oct 26 19:03:36.739: INFO: router-default-56c7ff9d54-tz4cl from openshift-ingress started at 2020-10-26 17:00:10 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.739: INFO: 	Container router ready: true, restart count 0
Oct 26 19:03:36.739: INFO: redhat-operators-688fdf87f8-nfrk9 from openshift-marketplace started at 2020-10-26 17:00:16 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.739: INFO: 	Container redhat-operators ready: true, restart count 0
Oct 26 19:03:36.739: INFO: registry-pvc-permissions-9fhwg from openshift-image-registry started at 2020-10-26 17:01:54 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container pvc-permissions ready: false, restart count 0
Oct 26 19:03:36.740: INFO: prometheus-adapter-6cccbf8dbb-97f5w from openshift-monitoring started at 2020-10-26 17:05:48 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container prometheus-adapter ready: true, restart count 0
Oct 26 19:03:36.740: INFO: openshift-kube-proxy-rs72s from openshift-kube-proxy started at 2020-10-26 16:57:37 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 26 19:03:36.740: INFO: dns-default-2hlsj from openshift-dns started at 2020-10-26 16:59:44 +0000 UTC (2 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container dns ready: true, restart count 0
Oct 26 19:03:36.740: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 26 19:03:36.740: INFO: community-operators-bd644f5c5-wb2l9 from openshift-marketplace started at 2020-10-26 17:00:16 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container community-operators ready: true, restart count 0
Oct 26 19:03:36.740: INFO: ibm-master-proxy-static-10.123.240.174 from kube-system started at 2020-10-26 16:57:35 +0000 UTC (2 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 19:03:36.740: INFO: 	Container pause ready: true, restart count 0
Oct 26 19:03:36.740: INFO: node-exporter-hpt5d from openshift-monitoring started at 2020-10-26 16:59:04 +0000 UTC (2 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:03:36.740: INFO: 	Container node-exporter ready: true, restart count 0
Oct 26 19:03:36.740: INFO: configmap-cabundle-injector-6676dbc567-zdqgt from openshift-service-ca started at 2020-10-26 16:59:02 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Oct 26 19:03:36.740: INFO: image-registry-57486bbffb-v7qmg from openshift-image-registry started at 2020-10-26 17:01:54 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container registry ready: true, restart count 0
Oct 26 19:03:36.740: INFO: console-77979dd75-lnchz from openshift-console started at 2020-10-26 17:01:28 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container console ready: true, restart count 0
Oct 26 19:03:36.740: INFO: service-serving-cert-signer-6d656c4cf7-l9qz5 from openshift-service-ca started at 2020-10-26 16:59:01 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Oct 26 19:03:36.740: INFO: prometheus-operator-d5df96f57-bthbz from openshift-monitoring started at 2020-10-26 17:05:34 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container prometheus-operator ready: true, restart count 0
Oct 26 19:03:36.740: INFO: thanos-querier-85d68cbb66-5hkh8 from openshift-monitoring started at 2020-10-26 17:06:37 +0000 UTC (4 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:03:36.740: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 26 19:03:36.740: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 26 19:03:36.740: INFO: 	Container thanos-querier ready: true, restart count 0
Oct 26 19:03:36.740: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-10-26 17:06:52 +0000 UTC (7 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:03:36.740: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 26 19:03:36.740: INFO: 	Container prometheus ready: true, restart count 1
Oct 26 19:03:36.740: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Oct 26 19:03:36.740: INFO: 	Container prometheus-proxy ready: true, restart count 0
Oct 26 19:03:36.740: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Oct 26 19:03:36.740: INFO: 	Container thanos-sidecar ready: true, restart count 0
Oct 26 19:03:36.740: INFO: certified-operators-59fc9cd9b5-84rhs from openshift-marketplace started at 2020-10-26 18:00:44 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container certified-operators ready: true, restart count 0
Oct 26 19:03:36.740: INFO: ibmcloud-block-storage-driver-s88xs from kube-system started at 2020-10-26 16:57:41 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Oct 26 19:03:36.740: INFO: calico-typha-6b7867b64d-j6hnv from calico-system started at 2020-10-26 16:58:05 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 19:03:36.740: INFO: calico-node-jfjg2 from calico-system started at 2020-10-26 16:58:05 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 19:03:36.740: INFO: apiservice-cabundle-injector-566d6d6dc7-lv6bf from openshift-service-ca started at 2020-10-26 16:59:02 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Oct 26 19:03:36.740: INFO: node-ca-lbgdr from openshift-image-registry started at 2020-10-26 16:59:40 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container node-ca ready: true, restart count 0
Oct 26 19:03:36.740: INFO: grafana-f6757cb99-n9dgx from openshift-monitoring started at 2020-10-26 17:05:54 +0000 UTC (2 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container grafana ready: true, restart count 0
Oct 26 19:03:36.740: INFO: 	Container grafana-proxy ready: true, restart count 0
Oct 26 19:03:36.740: INFO: multus-wz2pl from openshift-multus started at 2020-10-26 16:57:37 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container kube-multus ready: true, restart count 0
Oct 26 19:03:36.740: INFO: ibm-keepalived-watcher-nxrtl from kube-system started at 2020-10-26 16:57:37 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 19:03:36.740: INFO: kube-state-metrics-6bb5fc9995-w9q9p from openshift-monitoring started at 2020-10-26 16:59:03 +0000 UTC (3 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Oct 26 19:03:36.740: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Oct 26 19:03:36.740: INFO: 	Container kube-state-metrics ready: true, restart count 0
Oct 26 19:03:36.740: INFO: tuned-nlq8g from openshift-cluster-node-tuning-operator started at 2020-10-26 17:00:14 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container tuned ready: true, restart count 0
Oct 26 19:03:36.740: INFO: sonobuoy-systemd-logs-daemon-set-a94f7e2648a3464d-wf2jr from sonobuoy started at 2020-10-26 18:20:51 +0000 UTC (2 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 26 19:03:36.740: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 26 19:03:36.740: INFO: vpn-68d7d4d68d-clwcb from kube-system started at 2020-10-26 17:05:39 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container vpn ready: true, restart count 0
Oct 26 19:03:36.740: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-10-26 17:06:05 +0000 UTC (3 container statuses recorded)
Oct 26 19:03:36.740: INFO: 	Container alertmanager ready: true, restart count 0
Oct 26 19:03:36.740: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 26 19:03:36.740: INFO: 	Container config-reloader ready: true, restart count 0
Oct 26 19:03:36.740: INFO: 
Logging pods the kubelet thinks is on node 10.123.240.178 before test
Oct 26 19:03:36.798: INFO: openshift-kube-proxy-v9jgr from openshift-kube-proxy started at 2020-10-26 16:57:47 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.798: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 26 19:03:36.798: INFO: calico-typha-6b7867b64d-bjwpm from calico-system started at 2020-10-26 17:00:10 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.798: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 19:03:36.798: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-10-26 18:42:54 +0000 UTC (7 container statuses recorded)
Oct 26 19:03:36.798: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:03:36.798: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 26 19:03:36.798: INFO: 	Container prometheus ready: true, restart count 1
Oct 26 19:03:36.798: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Oct 26 19:03:36.798: INFO: 	Container prometheus-proxy ready: true, restart count 0
Oct 26 19:03:36.798: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Oct 26 19:03:36.798: INFO: 	Container thanos-sidecar ready: true, restart count 0
Oct 26 19:03:36.798: INFO: ibm-keepalived-watcher-zr4ms from kube-system started at 2020-10-26 16:57:47 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.798: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 19:03:36.798: INFO: dns-default-qn8jr from openshift-dns started at 2020-10-26 16:59:44 +0000 UTC (2 container statuses recorded)
Oct 26 19:03:36.798: INFO: 	Container dns ready: true, restart count 0
Oct 26 19:03:36.798: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 26 19:03:36.798: INFO: tuned-2ksq9 from openshift-cluster-node-tuning-operator started at 2020-10-26 17:00:14 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.798: INFO: 	Container tuned ready: true, restart count 0
Oct 26 19:03:36.798: INFO: node-exporter-vw6ds from openshift-monitoring started at 2020-10-26 16:59:04 +0000 UTC (2 container statuses recorded)
Oct 26 19:03:36.798: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:03:36.798: INFO: 	Container node-exporter ready: true, restart count 0
Oct 26 19:03:36.798: INFO: calico-node-d227t from calico-system started at 2020-10-26 16:58:05 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.798: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 19:03:36.798: INFO: multus-admission-controller-qxdxf from openshift-multus started at 2020-10-26 18:42:44 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.798: INFO: 	Container multus-admission-controller ready: true, restart count 0
Oct 26 19:03:36.798: INFO: ibm-master-proxy-static-10.123.240.178 from kube-system started at 2020-10-26 16:57:44 +0000 UTC (2 container statuses recorded)
Oct 26 19:03:36.798: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 19:03:36.798: INFO: 	Container pause ready: true, restart count 0
Oct 26 19:03:36.798: INFO: node-ca-hsxg7 from openshift-image-registry started at 2020-10-26 16:59:40 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.798: INFO: 	Container node-ca ready: true, restart count 0
Oct 26 19:03:36.798: INFO: packageserver-d5dbff54b-q2kg6 from openshift-operator-lifecycle-manager started at 2020-10-26 18:42:16 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.798: INFO: 	Container packageserver ready: true, restart count 0
Oct 26 19:03:36.798: INFO: ibmcloud-block-storage-driver-2pnvt from kube-system started at 2020-10-26 16:57:55 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.798: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Oct 26 19:03:36.798: INFO: console-77979dd75-sj7zj from openshift-console started at 2020-10-26 18:42:09 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.798: INFO: 	Container console ready: true, restart count 0
Oct 26 19:03:36.798: INFO: openshift-state-metrics-6888cfb99c-snkdc from openshift-monitoring started at 2020-10-26 18:42:09 +0000 UTC (3 container statuses recorded)
Oct 26 19:03:36.798: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Oct 26 19:03:36.798: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Oct 26 19:03:36.798: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Oct 26 19:03:36.798: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-10-26 18:42:24 +0000 UTC (3 container statuses recorded)
Oct 26 19:03:36.798: INFO: 	Container alertmanager ready: true, restart count 0
Oct 26 19:03:36.798: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 26 19:03:36.798: INFO: 	Container config-reloader ready: true, restart count 0
Oct 26 19:03:36.798: INFO: sonobuoy-e2e-job-6258e003556947b6 from sonobuoy started at 2020-10-26 18:20:50 +0000 UTC (2 container statuses recorded)
Oct 26 19:03:36.798: INFO: 	Container e2e ready: true, restart count 0
Oct 26 19:03:36.798: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 26 19:03:36.798: INFO: sonobuoy-systemd-logs-daemon-set-a94f7e2648a3464d-lj6zg from sonobuoy started at 2020-10-26 18:20:51 +0000 UTC (2 container statuses recorded)
Oct 26 19:03:36.798: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 26 19:03:36.798: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 26 19:03:36.798: INFO: thanos-querier-85d68cbb66-lgj99 from openshift-monitoring started at 2020-10-26 18:42:09 +0000 UTC (4 container statuses recorded)
Oct 26 19:03:36.798: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:03:36.798: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 26 19:03:36.798: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 26 19:03:36.798: INFO: 	Container thanos-querier ready: true, restart count 0
Oct 26 19:03:36.798: INFO: multus-nht6l from openshift-multus started at 2020-10-26 16:57:47 +0000 UTC (1 container statuses recorded)
Oct 26 19:03:36.798: INFO: 	Container kube-multus ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e39bc95f-fc0c-40ff-9955-37da0ec94594 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-e39bc95f-fc0c-40ff-9955-37da0ec94594 off the node 10.123.240.178
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e39bc95f-fc0c-40ff-9955-37da0ec94594
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:03:43.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4997" for this suite.
Oct 26 19:03:55.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:03:57.178: INFO: namespace sched-pred-4997 deletion completed in 14.045079253s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:20.925 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:03:57.178: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-2hss
STEP: Creating a pod to test atomic-volume-subpath
Oct 26 19:03:57.409: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-2hss" in namespace "subpath-9589" to be "success or failure"
Oct 26 19:03:57.420: INFO: Pod "pod-subpath-test-downwardapi-2hss": Phase="Pending", Reason="", readiness=false. Elapsed: 11.051193ms
Oct 26 19:03:59.433: INFO: Pod "pod-subpath-test-downwardapi-2hss": Phase="Running", Reason="", readiness=true. Elapsed: 2.023886347s
Oct 26 19:04:01.445: INFO: Pod "pod-subpath-test-downwardapi-2hss": Phase="Running", Reason="", readiness=true. Elapsed: 4.036633124s
Oct 26 19:04:03.458: INFO: Pod "pod-subpath-test-downwardapi-2hss": Phase="Running", Reason="", readiness=true. Elapsed: 6.04899176s
Oct 26 19:04:05.471: INFO: Pod "pod-subpath-test-downwardapi-2hss": Phase="Running", Reason="", readiness=true. Elapsed: 8.062330374s
Oct 26 19:04:07.482: INFO: Pod "pod-subpath-test-downwardapi-2hss": Phase="Running", Reason="", readiness=true. Elapsed: 10.073562096s
Oct 26 19:04:09.497: INFO: Pod "pod-subpath-test-downwardapi-2hss": Phase="Running", Reason="", readiness=true. Elapsed: 12.087852299s
Oct 26 19:04:11.535: INFO: Pod "pod-subpath-test-downwardapi-2hss": Phase="Running", Reason="", readiness=true. Elapsed: 14.126143648s
Oct 26 19:04:13.546: INFO: Pod "pod-subpath-test-downwardapi-2hss": Phase="Running", Reason="", readiness=true. Elapsed: 16.136975715s
Oct 26 19:04:15.561: INFO: Pod "pod-subpath-test-downwardapi-2hss": Phase="Running", Reason="", readiness=true. Elapsed: 18.152207441s
Oct 26 19:04:17.587: INFO: Pod "pod-subpath-test-downwardapi-2hss": Phase="Running", Reason="", readiness=true. Elapsed: 20.178172799s
Oct 26 19:04:19.599: INFO: Pod "pod-subpath-test-downwardapi-2hss": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.190055379s
STEP: Saw pod success
Oct 26 19:04:19.599: INFO: Pod "pod-subpath-test-downwardapi-2hss" satisfied condition "success or failure"
Oct 26 19:04:19.609: INFO: Trying to get logs from node 10.123.240.178 pod pod-subpath-test-downwardapi-2hss container test-container-subpath-downwardapi-2hss: <nil>
STEP: delete the pod
Oct 26 19:04:19.688: INFO: Waiting for pod pod-subpath-test-downwardapi-2hss to disappear
Oct 26 19:04:19.701: INFO: Pod pod-subpath-test-downwardapi-2hss no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-2hss
Oct 26 19:04:19.701: INFO: Deleting pod "pod-subpath-test-downwardapi-2hss" in namespace "subpath-9589"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:04:19.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9589" for this suite.
Oct 26 19:04:27.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:04:29.791: INFO: namespace subpath-9589 deletion completed in 10.062571712s

• [SLOW TEST:32.613 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:04:29.793: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-22ae6598-34da-4b20-9f69-c6bb01879245
STEP: Creating a pod to test consume configMaps
Oct 26 19:04:30.005: INFO: Waiting up to 5m0s for pod "pod-configmaps-6659fe85-cc6e-4697-848c-6a7543a7c0a6" in namespace "configmap-5230" to be "success or failure"
Oct 26 19:04:30.024: INFO: Pod "pod-configmaps-6659fe85-cc6e-4697-848c-6a7543a7c0a6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.304064ms
Oct 26 19:04:32.036: INFO: Pod "pod-configmaps-6659fe85-cc6e-4697-848c-6a7543a7c0a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03064231s
Oct 26 19:04:34.047: INFO: Pod "pod-configmaps-6659fe85-cc6e-4697-848c-6a7543a7c0a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041990008s
STEP: Saw pod success
Oct 26 19:04:34.048: INFO: Pod "pod-configmaps-6659fe85-cc6e-4697-848c-6a7543a7c0a6" satisfied condition "success or failure"
Oct 26 19:04:34.061: INFO: Trying to get logs from node 10.123.240.178 pod pod-configmaps-6659fe85-cc6e-4697-848c-6a7543a7c0a6 container configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 19:04:34.131: INFO: Waiting for pod pod-configmaps-6659fe85-cc6e-4697-848c-6a7543a7c0a6 to disappear
Oct 26 19:04:34.141: INFO: Pod pod-configmaps-6659fe85-cc6e-4697-848c-6a7543a7c0a6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:04:34.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5230" for this suite.
Oct 26 19:04:42.198: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:04:44.177: INFO: namespace configmap-5230 deletion completed in 10.021543868s

• [SLOW TEST:14.384 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:04:44.178: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:04:46.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2179" for this suite.
Oct 26 19:05:34.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:05:36.597: INFO: namespace kubelet-test-2179 deletion completed in 50.135589655s

• [SLOW TEST:52.419 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:05:36.597: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Oct 26 19:05:36.814: INFO: Waiting up to 5m0s for pod "var-expansion-0b41d450-3b8d-46da-9785-99bd12e61c12" in namespace "var-expansion-2575" to be "success or failure"
Oct 26 19:05:36.829: INFO: Pod "var-expansion-0b41d450-3b8d-46da-9785-99bd12e61c12": Phase="Pending", Reason="", readiness=false. Elapsed: 14.686955ms
Oct 26 19:05:38.841: INFO: Pod "var-expansion-0b41d450-3b8d-46da-9785-99bd12e61c12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02640641s
STEP: Saw pod success
Oct 26 19:05:38.841: INFO: Pod "var-expansion-0b41d450-3b8d-46da-9785-99bd12e61c12" satisfied condition "success or failure"
Oct 26 19:05:38.850: INFO: Trying to get logs from node 10.123.240.178 pod var-expansion-0b41d450-3b8d-46da-9785-99bd12e61c12 container dapi-container: <nil>
STEP: delete the pod
Oct 26 19:05:38.918: INFO: Waiting for pod var-expansion-0b41d450-3b8d-46da-9785-99bd12e61c12 to disappear
Oct 26 19:05:38.926: INFO: Pod var-expansion-0b41d450-3b8d-46da-9785-99bd12e61c12 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:05:38.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2575" for this suite.
Oct 26 19:05:46.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:05:49.105: INFO: namespace var-expansion-2575 deletion completed in 10.147629798s

• [SLOW TEST:12.508 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:05:49.106: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Oct 26 19:05:49.320: INFO: Waiting up to 5m0s for pod "pod-137e6fdf-b66d-4d8d-8e47-11f116097547" in namespace "emptydir-2778" to be "success or failure"
Oct 26 19:05:49.340: INFO: Pod "pod-137e6fdf-b66d-4d8d-8e47-11f116097547": Phase="Pending", Reason="", readiness=false. Elapsed: 19.110606ms
Oct 26 19:05:51.352: INFO: Pod "pod-137e6fdf-b66d-4d8d-8e47-11f116097547": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031690734s
Oct 26 19:05:53.364: INFO: Pod "pod-137e6fdf-b66d-4d8d-8e47-11f116097547": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04368671s
STEP: Saw pod success
Oct 26 19:05:53.364: INFO: Pod "pod-137e6fdf-b66d-4d8d-8e47-11f116097547" satisfied condition "success or failure"
Oct 26 19:05:53.374: INFO: Trying to get logs from node 10.123.240.178 pod pod-137e6fdf-b66d-4d8d-8e47-11f116097547 container test-container: <nil>
STEP: delete the pod
Oct 26 19:05:53.432: INFO: Waiting for pod pod-137e6fdf-b66d-4d8d-8e47-11f116097547 to disappear
Oct 26 19:05:53.443: INFO: Pod pod-137e6fdf-b66d-4d8d-8e47-11f116097547 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:05:53.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2778" for this suite.
Oct 26 19:06:01.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:06:03.531: INFO: namespace emptydir-2778 deletion completed in 10.063793162s

• [SLOW TEST:14.425 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:06:03.533: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 19:06:04.584: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 19:06:06.620: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739335964, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739335964, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739335964, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739335964, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 19:06:09.668: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:06:09.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4995" for this suite.
Oct 26 19:06:17.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:06:19.967: INFO: namespace webhook-4995 deletion completed in 10.23361695s
STEP: Destroying namespace "webhook-4995-markers" for this suite.
Oct 26 19:06:28.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:06:30.026: INFO: namespace webhook-4995-markers deletion completed in 10.058877498s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:26.552 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:06:30.086: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Oct 26 19:06:30.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 create -f - --namespace=kubectl-1509'
Oct 26 19:06:31.156: INFO: stderr: ""
Oct 26 19:06:31.156: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 26 19:06:31.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1509'
Oct 26 19:06:31.339: INFO: stderr: ""
Oct 26 19:06:31.339: INFO: stdout: "update-demo-nautilus-4xggc update-demo-nautilus-zr7jn "
Oct 26 19:06:31.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-nautilus-4xggc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1509'
Oct 26 19:06:31.485: INFO: stderr: ""
Oct 26 19:06:31.485: INFO: stdout: ""
Oct 26 19:06:31.485: INFO: update-demo-nautilus-4xggc is created but not running
Oct 26 19:06:36.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1509'
Oct 26 19:06:36.649: INFO: stderr: ""
Oct 26 19:06:36.649: INFO: stdout: "update-demo-nautilus-4xggc update-demo-nautilus-zr7jn "
Oct 26 19:06:36.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-nautilus-4xggc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1509'
Oct 26 19:06:36.854: INFO: stderr: ""
Oct 26 19:06:36.854: INFO: stdout: "true"
Oct 26 19:06:36.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-nautilus-4xggc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1509'
Oct 26 19:06:37.030: INFO: stderr: ""
Oct 26 19:06:37.030: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct 26 19:06:37.030: INFO: validating pod update-demo-nautilus-4xggc
Oct 26 19:06:37.056: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 26 19:06:37.056: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 26 19:06:37.056: INFO: update-demo-nautilus-4xggc is verified up and running
Oct 26 19:06:37.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-nautilus-zr7jn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1509'
Oct 26 19:06:37.209: INFO: stderr: ""
Oct 26 19:06:37.209: INFO: stdout: "true"
Oct 26 19:06:37.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-nautilus-zr7jn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1509'
Oct 26 19:06:37.357: INFO: stderr: ""
Oct 26 19:06:37.357: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct 26 19:06:37.357: INFO: validating pod update-demo-nautilus-zr7jn
Oct 26 19:06:37.385: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 26 19:06:37.385: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 26 19:06:37.385: INFO: update-demo-nautilus-zr7jn is verified up and running
STEP: scaling down the replication controller
Oct 26 19:06:37.390: INFO: scanned /root for discovery docs: <nil>
Oct 26 19:06:37.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-1509'
Oct 26 19:06:38.603: INFO: stderr: ""
Oct 26 19:06:38.603: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 26 19:06:38.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1509'
Oct 26 19:06:38.752: INFO: stderr: ""
Oct 26 19:06:38.752: INFO: stdout: "update-demo-nautilus-4xggc update-demo-nautilus-zr7jn "
STEP: Replicas for name=update-demo: expected=1 actual=2
Oct 26 19:06:43.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1509'
Oct 26 19:06:43.905: INFO: stderr: ""
Oct 26 19:06:43.905: INFO: stdout: "update-demo-nautilus-4xggc update-demo-nautilus-zr7jn "
STEP: Replicas for name=update-demo: expected=1 actual=2
Oct 26 19:06:48.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1509'
Oct 26 19:06:49.050: INFO: stderr: ""
Oct 26 19:06:49.050: INFO: stdout: "update-demo-nautilus-4xggc "
Oct 26 19:06:49.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-nautilus-4xggc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1509'
Oct 26 19:06:49.215: INFO: stderr: ""
Oct 26 19:06:49.215: INFO: stdout: "true"
Oct 26 19:06:49.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-nautilus-4xggc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1509'
Oct 26 19:06:49.474: INFO: stderr: ""
Oct 26 19:06:49.474: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct 26 19:06:49.474: INFO: validating pod update-demo-nautilus-4xggc
Oct 26 19:06:49.494: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 26 19:06:49.494: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 26 19:06:49.494: INFO: update-demo-nautilus-4xggc is verified up and running
STEP: scaling up the replication controller
Oct 26 19:06:49.499: INFO: scanned /root for discovery docs: <nil>
Oct 26 19:06:49.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-1509'
Oct 26 19:06:50.693: INFO: stderr: ""
Oct 26 19:06:50.693: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 26 19:06:50.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1509'
Oct 26 19:06:50.850: INFO: stderr: ""
Oct 26 19:06:50.850: INFO: stdout: "update-demo-nautilus-4xggc update-demo-nautilus-g52nv "
Oct 26 19:06:50.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-nautilus-4xggc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1509'
Oct 26 19:06:51.038: INFO: stderr: ""
Oct 26 19:06:51.038: INFO: stdout: "true"
Oct 26 19:06:51.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-nautilus-4xggc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1509'
Oct 26 19:06:51.204: INFO: stderr: ""
Oct 26 19:06:51.204: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct 26 19:06:51.204: INFO: validating pod update-demo-nautilus-4xggc
Oct 26 19:06:51.227: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 26 19:06:51.227: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 26 19:06:51.227: INFO: update-demo-nautilus-4xggc is verified up and running
Oct 26 19:06:51.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-nautilus-g52nv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1509'
Oct 26 19:06:51.370: INFO: stderr: ""
Oct 26 19:06:51.370: INFO: stdout: ""
Oct 26 19:06:51.370: INFO: update-demo-nautilus-g52nv is created but not running
Oct 26 19:06:56.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1509'
Oct 26 19:06:56.525: INFO: stderr: ""
Oct 26 19:06:56.525: INFO: stdout: "update-demo-nautilus-4xggc update-demo-nautilus-g52nv "
Oct 26 19:06:56.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-nautilus-4xggc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1509'
Oct 26 19:06:56.663: INFO: stderr: ""
Oct 26 19:06:56.663: INFO: stdout: "true"
Oct 26 19:06:56.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-nautilus-4xggc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1509'
Oct 26 19:06:56.806: INFO: stderr: ""
Oct 26 19:06:56.806: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct 26 19:06:56.806: INFO: validating pod update-demo-nautilus-4xggc
Oct 26 19:06:56.825: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 26 19:06:56.825: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 26 19:06:56.825: INFO: update-demo-nautilus-4xggc is verified up and running
Oct 26 19:06:56.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-nautilus-g52nv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1509'
Oct 26 19:06:56.978: INFO: stderr: ""
Oct 26 19:06:56.978: INFO: stdout: "true"
Oct 26 19:06:56.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-nautilus-g52nv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1509'
Oct 26 19:06:57.121: INFO: stderr: ""
Oct 26 19:06:57.121: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct 26 19:06:57.121: INFO: validating pod update-demo-nautilus-g52nv
Oct 26 19:06:57.145: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 26 19:06:57.145: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 26 19:06:57.145: INFO: update-demo-nautilus-g52nv is verified up and running
STEP: using delete to clean up resources
Oct 26 19:06:57.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 delete --grace-period=0 --force -f - --namespace=kubectl-1509'
Oct 26 19:06:57.295: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 26 19:06:57.295: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Oct 26 19:06:57.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1509'
Oct 26 19:06:57.495: INFO: stderr: "No resources found in kubectl-1509 namespace.\n"
Oct 26 19:06:57.495: INFO: stdout: ""
Oct 26 19:06:57.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods -l name=update-demo --namespace=kubectl-1509 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Oct 26 19:06:57.657: INFO: stderr: ""
Oct 26 19:06:57.657: INFO: stdout: "update-demo-nautilus-4xggc\nupdate-demo-nautilus-g52nv\n"
Oct 26 19:06:58.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1509'
Oct 26 19:06:58.323: INFO: stderr: "No resources found in kubectl-1509 namespace.\n"
Oct 26 19:06:58.323: INFO: stdout: ""
Oct 26 19:06:58.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods -l name=update-demo --namespace=kubectl-1509 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Oct 26 19:06:58.485: INFO: stderr: ""
Oct 26 19:06:58.485: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:06:58.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1509" for this suite.
Oct 26 19:07:14.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:07:16.513: INFO: namespace kubectl-1509 deletion completed in 17.996406499s

• [SLOW TEST:46.428 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:07:16.513: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-cc5045be-1796-4c38-89f6-cd2ad1dbb6ef
STEP: Creating secret with name secret-projected-all-test-volume-c96bbe24-4f07-4e5c-892e-5cd00302c6c5
STEP: Creating a pod to test Check all projections for projected volume plugin
Oct 26 19:07:16.734: INFO: Waiting up to 5m0s for pod "projected-volume-a4ec2ac3-f42f-4be3-b868-2233ba529d2e" in namespace "projected-822" to be "success or failure"
Oct 26 19:07:16.744: INFO: Pod "projected-volume-a4ec2ac3-f42f-4be3-b868-2233ba529d2e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.799661ms
Oct 26 19:07:18.755: INFO: Pod "projected-volume-a4ec2ac3-f42f-4be3-b868-2233ba529d2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021039688s
STEP: Saw pod success
Oct 26 19:07:18.755: INFO: Pod "projected-volume-a4ec2ac3-f42f-4be3-b868-2233ba529d2e" satisfied condition "success or failure"
Oct 26 19:07:18.765: INFO: Trying to get logs from node 10.123.240.178 pod projected-volume-a4ec2ac3-f42f-4be3-b868-2233ba529d2e container projected-all-volume-test: <nil>
STEP: delete the pod
Oct 26 19:07:18.841: INFO: Waiting for pod projected-volume-a4ec2ac3-f42f-4be3-b868-2233ba529d2e to disappear
Oct 26 19:07:18.852: INFO: Pod projected-volume-a4ec2ac3-f42f-4be3-b868-2233ba529d2e no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:07:18.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-822" for this suite.
Oct 26 19:07:26.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:07:28.879: INFO: namespace projected-822 deletion completed in 9.998760686s

• [SLOW TEST:12.366 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:07:28.879: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-e0c11b73-fd69-4caf-b6be-13c5b1b66adb
STEP: Creating a pod to test consume configMaps
Oct 26 19:07:29.081: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ea295dbe-5760-4afd-a1b1-9b06ab9ada06" in namespace "projected-545" to be "success or failure"
Oct 26 19:07:29.091: INFO: Pod "pod-projected-configmaps-ea295dbe-5760-4afd-a1b1-9b06ab9ada06": Phase="Pending", Reason="", readiness=false. Elapsed: 10.002398ms
Oct 26 19:07:31.106: INFO: Pod "pod-projected-configmaps-ea295dbe-5760-4afd-a1b1-9b06ab9ada06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025592711s
STEP: Saw pod success
Oct 26 19:07:31.107: INFO: Pod "pod-projected-configmaps-ea295dbe-5760-4afd-a1b1-9b06ab9ada06" satisfied condition "success or failure"
Oct 26 19:07:31.120: INFO: Trying to get logs from node 10.123.240.178 pod pod-projected-configmaps-ea295dbe-5760-4afd-a1b1-9b06ab9ada06 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 19:07:31.183: INFO: Waiting for pod pod-projected-configmaps-ea295dbe-5760-4afd-a1b1-9b06ab9ada06 to disappear
Oct 26 19:07:31.195: INFO: Pod pod-projected-configmaps-ea295dbe-5760-4afd-a1b1-9b06ab9ada06 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:07:31.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-545" for this suite.
Oct 26 19:07:39.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:07:41.255: INFO: namespace projected-545 deletion completed in 10.03021795s

• [SLOW TEST:12.375 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:07:41.255: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:07:52.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6900" for this suite.
Oct 26 19:08:00.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:08:02.632: INFO: namespace resourcequota-6900 deletion completed in 10.034131841s

• [SLOW TEST:21.377 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:08:02.632: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:08:19.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-434" for this suite.
Oct 26 19:08:27.162: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:08:29.138: INFO: namespace resourcequota-434 deletion completed in 10.017844935s

• [SLOW TEST:26.506 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:08:29.138: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-qqbw
STEP: Creating a pod to test atomic-volume-subpath
Oct 26 19:08:29.413: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-qqbw" in namespace "subpath-1036" to be "success or failure"
Oct 26 19:08:29.422: INFO: Pod "pod-subpath-test-projected-qqbw": Phase="Pending", Reason="", readiness=false. Elapsed: 9.105007ms
Oct 26 19:08:31.435: INFO: Pod "pod-subpath-test-projected-qqbw": Phase="Running", Reason="", readiness=true. Elapsed: 2.022444565s
Oct 26 19:08:33.447: INFO: Pod "pod-subpath-test-projected-qqbw": Phase="Running", Reason="", readiness=true. Elapsed: 4.034367752s
Oct 26 19:08:35.459: INFO: Pod "pod-subpath-test-projected-qqbw": Phase="Running", Reason="", readiness=true. Elapsed: 6.046089975s
Oct 26 19:08:37.472: INFO: Pod "pod-subpath-test-projected-qqbw": Phase="Running", Reason="", readiness=true. Elapsed: 8.059504218s
Oct 26 19:08:39.484: INFO: Pod "pod-subpath-test-projected-qqbw": Phase="Running", Reason="", readiness=true. Elapsed: 10.071471347s
Oct 26 19:08:41.496: INFO: Pod "pod-subpath-test-projected-qqbw": Phase="Running", Reason="", readiness=true. Elapsed: 12.083307499s
Oct 26 19:08:43.507: INFO: Pod "pod-subpath-test-projected-qqbw": Phase="Running", Reason="", readiness=true. Elapsed: 14.094631052s
Oct 26 19:08:45.520: INFO: Pod "pod-subpath-test-projected-qqbw": Phase="Running", Reason="", readiness=true. Elapsed: 16.107140275s
Oct 26 19:08:47.532: INFO: Pod "pod-subpath-test-projected-qqbw": Phase="Running", Reason="", readiness=true. Elapsed: 18.118723244s
Oct 26 19:08:49.549: INFO: Pod "pod-subpath-test-projected-qqbw": Phase="Running", Reason="", readiness=true. Elapsed: 20.135943155s
Oct 26 19:08:51.561: INFO: Pod "pod-subpath-test-projected-qqbw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.147853811s
STEP: Saw pod success
Oct 26 19:08:51.561: INFO: Pod "pod-subpath-test-projected-qqbw" satisfied condition "success or failure"
Oct 26 19:08:51.572: INFO: Trying to get logs from node 10.123.240.178 pod pod-subpath-test-projected-qqbw container test-container-subpath-projected-qqbw: <nil>
STEP: delete the pod
Oct 26 19:08:51.633: INFO: Waiting for pod pod-subpath-test-projected-qqbw to disappear
Oct 26 19:08:51.647: INFO: Pod pod-subpath-test-projected-qqbw no longer exists
STEP: Deleting pod pod-subpath-test-projected-qqbw
Oct 26 19:08:51.647: INFO: Deleting pod "pod-subpath-test-projected-qqbw" in namespace "subpath-1036"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:08:51.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1036" for this suite.
Oct 26 19:08:59.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:09:01.841: INFO: namespace subpath-1036 deletion completed in 10.159182327s

• [SLOW TEST:32.703 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:09:01.843: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-4698/configmap-test-364de3e1-4e5d-47d0-80f2-e9779a521e16
STEP: Creating a pod to test consume configMaps
Oct 26 19:09:02.062: INFO: Waiting up to 5m0s for pod "pod-configmaps-f9f47382-b0b6-4bc6-86ed-881bab923493" in namespace "configmap-4698" to be "success or failure"
Oct 26 19:09:02.074: INFO: Pod "pod-configmaps-f9f47382-b0b6-4bc6-86ed-881bab923493": Phase="Pending", Reason="", readiness=false. Elapsed: 12.291208ms
Oct 26 19:09:04.085: INFO: Pod "pod-configmaps-f9f47382-b0b6-4bc6-86ed-881bab923493": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023352878s
STEP: Saw pod success
Oct 26 19:09:04.085: INFO: Pod "pod-configmaps-f9f47382-b0b6-4bc6-86ed-881bab923493" satisfied condition "success or failure"
Oct 26 19:09:04.100: INFO: Trying to get logs from node 10.123.240.178 pod pod-configmaps-f9f47382-b0b6-4bc6-86ed-881bab923493 container env-test: <nil>
STEP: delete the pod
Oct 26 19:09:04.167: INFO: Waiting for pod pod-configmaps-f9f47382-b0b6-4bc6-86ed-881bab923493 to disappear
Oct 26 19:09:04.178: INFO: Pod pod-configmaps-f9f47382-b0b6-4bc6-86ed-881bab923493 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:09:04.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4698" for this suite.
Oct 26 19:09:12.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:09:14.317: INFO: namespace configmap-4698 deletion completed in 10.103542416s

• [SLOW TEST:12.475 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:09:14.318: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-a6568a8c-3d60-443a-996e-2775b8e4b9b9
STEP: Creating a pod to test consume secrets
Oct 26 19:09:14.561: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3fb2cf34-eef7-4c27-8e2e-3f043a6a9731" in namespace "projected-2556" to be "success or failure"
Oct 26 19:09:14.571: INFO: Pod "pod-projected-secrets-3fb2cf34-eef7-4c27-8e2e-3f043a6a9731": Phase="Pending", Reason="", readiness=false. Elapsed: 10.357278ms
Oct 26 19:09:16.585: INFO: Pod "pod-projected-secrets-3fb2cf34-eef7-4c27-8e2e-3f043a6a9731": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02352709s
Oct 26 19:09:18.603: INFO: Pod "pod-projected-secrets-3fb2cf34-eef7-4c27-8e2e-3f043a6a9731": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041693905s
STEP: Saw pod success
Oct 26 19:09:18.603: INFO: Pod "pod-projected-secrets-3fb2cf34-eef7-4c27-8e2e-3f043a6a9731" satisfied condition "success or failure"
Oct 26 19:09:18.614: INFO: Trying to get logs from node 10.123.240.178 pod pod-projected-secrets-3fb2cf34-eef7-4c27-8e2e-3f043a6a9731 container secret-volume-test: <nil>
STEP: delete the pod
Oct 26 19:09:18.672: INFO: Waiting for pod pod-projected-secrets-3fb2cf34-eef7-4c27-8e2e-3f043a6a9731 to disappear
Oct 26 19:09:18.683: INFO: Pod pod-projected-secrets-3fb2cf34-eef7-4c27-8e2e-3f043a6a9731 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:09:18.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2556" for this suite.
Oct 26 19:09:26.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:09:28.765: INFO: namespace projected-2556 deletion completed in 10.044638737s

• [SLOW TEST:14.448 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:09:28.765: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Oct 26 19:09:28.953: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d455954f-a977-4334-8ddf-23599185e328" in namespace "projected-8963" to be "success or failure"
Oct 26 19:09:28.966: INFO: Pod "downwardapi-volume-d455954f-a977-4334-8ddf-23599185e328": Phase="Pending", Reason="", readiness=false. Elapsed: 12.903503ms
Oct 26 19:09:30.978: INFO: Pod "downwardapi-volume-d455954f-a977-4334-8ddf-23599185e328": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024872156s
Oct 26 19:09:32.989: INFO: Pod "downwardapi-volume-d455954f-a977-4334-8ddf-23599185e328": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036232604s
STEP: Saw pod success
Oct 26 19:09:32.989: INFO: Pod "downwardapi-volume-d455954f-a977-4334-8ddf-23599185e328" satisfied condition "success or failure"
Oct 26 19:09:33.003: INFO: Trying to get logs from node 10.123.240.178 pod downwardapi-volume-d455954f-a977-4334-8ddf-23599185e328 container client-container: <nil>
STEP: delete the pod
Oct 26 19:09:33.077: INFO: Waiting for pod downwardapi-volume-d455954f-a977-4334-8ddf-23599185e328 to disappear
Oct 26 19:09:33.088: INFO: Pod downwardapi-volume-d455954f-a977-4334-8ddf-23599185e328 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:09:33.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8963" for this suite.
Oct 26 19:09:41.166: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:09:43.194: INFO: namespace projected-8963 deletion completed in 10.069039635s

• [SLOW TEST:14.428 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:09:43.194: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 19:09:43.415: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Creating first CR 
Oct 26 19:09:44.126: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-10-26T19:09:44Z generation:1 name:name1 resourceVersion:59422 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:46e349f5-57d0-4949-9adb-fd9d870a6cd9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Oct 26 19:09:54.143: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-10-26T19:09:54Z generation:1 name:name2 resourceVersion:59461 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:905d356f-785f-4bbf-8cb4-fe180bc4de37] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Oct 26 19:10:04.161: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-10-26T19:09:44Z generation:2 name:name1 resourceVersion:59498 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:46e349f5-57d0-4949-9adb-fd9d870a6cd9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Oct 26 19:10:14.179: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-10-26T19:09:54Z generation:2 name:name2 resourceVersion:59537 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:905d356f-785f-4bbf-8cb4-fe180bc4de37] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Oct 26 19:10:24.207: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-10-26T19:09:44Z generation:2 name:name1 resourceVersion:59573 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:46e349f5-57d0-4949-9adb-fd9d870a6cd9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Oct 26 19:10:34.236: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-10-26T19:09:54Z generation:2 name:name2 resourceVersion:59609 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:905d356f-785f-4bbf-8cb4-fe180bc4de37] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:10:44.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-1548" for this suite.
Oct 26 19:10:52.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:10:54.984: INFO: namespace crd-watch-1548 deletion completed in 10.180673673s

• [SLOW TEST:71.791 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:10:54.986: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Oct 26 19:10:55.142: INFO: namespace kubectl-9177
Oct 26 19:10:55.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 create -f - --namespace=kubectl-9177'
Oct 26 19:10:57.086: INFO: stderr: ""
Oct 26 19:10:57.086: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Oct 26 19:10:58.105: INFO: Selector matched 1 pods for map[app:redis]
Oct 26 19:10:58.105: INFO: Found 0 / 1
Oct 26 19:10:59.099: INFO: Selector matched 1 pods for map[app:redis]
Oct 26 19:10:59.099: INFO: Found 0 / 1
Oct 26 19:11:00.099: INFO: Selector matched 1 pods for map[app:redis]
Oct 26 19:11:00.099: INFO: Found 0 / 1
Oct 26 19:11:01.102: INFO: Selector matched 1 pods for map[app:redis]
Oct 26 19:11:01.102: INFO: Found 1 / 1
Oct 26 19:11:01.102: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Oct 26 19:11:01.114: INFO: Selector matched 1 pods for map[app:redis]
Oct 26 19:11:01.114: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Oct 26 19:11:01.114: INFO: wait on redis-master startup in kubectl-9177 
Oct 26 19:11:01.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 logs redis-master-7djg8 redis-master --namespace=kubectl-9177'
Oct 26 19:11:01.295: INFO: stderr: ""
Oct 26 19:11:01.295: INFO: stdout: "1:C 26 Oct 2020 19:10:59.503 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 26 Oct 2020 19:10:59.504 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 26 Oct 2020 19:10:59.504 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 26 Oct 2020 19:10:59.507 * Running mode=standalone, port=6379.\n1:M 26 Oct 2020 19:10:59.508 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 26 Oct 2020 19:10:59.508 # Server initialized\n1:M 26 Oct 2020 19:10:59.509 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 26 Oct 2020 19:10:59.509 * Ready to accept connections\n"
STEP: exposing RC
Oct 26 19:11:01.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-9177'
Oct 26 19:11:01.492: INFO: stderr: ""
Oct 26 19:11:01.492: INFO: stdout: "service/rm2 exposed\n"
Oct 26 19:11:01.507: INFO: Service rm2 in namespace kubectl-9177 found.
STEP: exposing service
Oct 26 19:11:03.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-9177'
Oct 26 19:11:03.759: INFO: stderr: ""
Oct 26 19:11:03.759: INFO: stdout: "service/rm3 exposed\n"
Oct 26 19:11:03.770: INFO: Service rm3 in namespace kubectl-9177 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:11:05.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9177" for this suite.
Oct 26 19:11:19.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:11:21.823: INFO: namespace kubectl-9177 deletion completed in 15.987258092s

• [SLOW TEST:26.837 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:11:21.827: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Oct 26 19:11:22.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-573'
Oct 26 19:11:22.257: INFO: stderr: ""
Oct 26 19:11:22.257: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Oct 26 19:11:22.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 delete pods e2e-test-httpd-pod --namespace=kubectl-573'
Oct 26 19:11:34.072: INFO: stderr: ""
Oct 26 19:11:34.072: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:11:34.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-573" for this suite.
Oct 26 19:11:42.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:11:44.196: INFO: namespace kubectl-573 deletion completed in 10.083044756s

• [SLOW TEST:22.369 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:11:44.196: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:11:48.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4039" for this suite.
Oct 26 19:12:38.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:12:40.592: INFO: namespace kubelet-test-4039 deletion completed in 52.068213308s

• [SLOW TEST:56.396 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:12:40.593: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-8743, will wait for the garbage collector to delete the pods
Oct 26 19:12:44.880: INFO: Deleting Job.batch foo took: 26.122299ms
Oct 26 19:12:45.481: INFO: Terminating Job.batch foo pods took: 600.47594ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:13:24.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8743" for this suite.
Oct 26 19:13:32.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:13:34.405: INFO: namespace job-8743 deletion completed in 9.989864048s

• [SLOW TEST:53.813 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:13:34.405: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 19:13:34.574: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-06070f85-390e-4288-83e6-4aba873029b6
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-06070f85-390e-4288-83e6-4aba873029b6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:14:55.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1963" for this suite.
Oct 26 19:15:27.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:15:29.962: INFO: namespace configmap-1963 deletion completed in 34.023903926s

• [SLOW TEST:115.557 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:15:29.962: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Oct 26 19:15:30.176: INFO: Waiting up to 5m0s for pod "var-expansion-ccb61bad-6a42-423a-841d-556c5f03e3fb" in namespace "var-expansion-3523" to be "success or failure"
Oct 26 19:15:30.187: INFO: Pod "var-expansion-ccb61bad-6a42-423a-841d-556c5f03e3fb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.24022ms
Oct 26 19:15:32.198: INFO: Pod "var-expansion-ccb61bad-6a42-423a-841d-556c5f03e3fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022267098s
Oct 26 19:15:34.210: INFO: Pod "var-expansion-ccb61bad-6a42-423a-841d-556c5f03e3fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034546175s
STEP: Saw pod success
Oct 26 19:15:34.210: INFO: Pod "var-expansion-ccb61bad-6a42-423a-841d-556c5f03e3fb" satisfied condition "success or failure"
Oct 26 19:15:34.220: INFO: Trying to get logs from node 10.123.240.178 pod var-expansion-ccb61bad-6a42-423a-841d-556c5f03e3fb container dapi-container: <nil>
STEP: delete the pod
Oct 26 19:15:34.278: INFO: Waiting for pod var-expansion-ccb61bad-6a42-423a-841d-556c5f03e3fb to disappear
Oct 26 19:15:34.288: INFO: Pod var-expansion-ccb61bad-6a42-423a-841d-556c5f03e3fb no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:15:34.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3523" for this suite.
Oct 26 19:15:42.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:15:44.392: INFO: namespace var-expansion-3523 deletion completed in 10.086165533s

• [SLOW TEST:14.430 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:15:44.394: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:15:47.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5785" for this suite.
Oct 26 19:15:55.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:15:57.837: INFO: namespace emptydir-wrapper-5785 deletion completed in 9.983146841s

• [SLOW TEST:13.443 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:15:57.837: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Oct 26 19:15:58.031: INFO: Waiting up to 5m0s for pod "pod-962e566b-7e2f-4259-b045-a2cbb075ea86" in namespace "emptydir-4759" to be "success or failure"
Oct 26 19:15:58.040: INFO: Pod "pod-962e566b-7e2f-4259-b045-a2cbb075ea86": Phase="Pending", Reason="", readiness=false. Elapsed: 8.904397ms
Oct 26 19:16:00.052: INFO: Pod "pod-962e566b-7e2f-4259-b045-a2cbb075ea86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020756202s
Oct 26 19:16:02.066: INFO: Pod "pod-962e566b-7e2f-4259-b045-a2cbb075ea86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034724132s
STEP: Saw pod success
Oct 26 19:16:02.066: INFO: Pod "pod-962e566b-7e2f-4259-b045-a2cbb075ea86" satisfied condition "success or failure"
Oct 26 19:16:02.077: INFO: Trying to get logs from node 10.123.240.178 pod pod-962e566b-7e2f-4259-b045-a2cbb075ea86 container test-container: <nil>
STEP: delete the pod
Oct 26 19:16:02.137: INFO: Waiting for pod pod-962e566b-7e2f-4259-b045-a2cbb075ea86 to disappear
Oct 26 19:16:02.149: INFO: Pod pod-962e566b-7e2f-4259-b045-a2cbb075ea86 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:16:02.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4759" for this suite.
Oct 26 19:16:10.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:16:12.198: INFO: namespace emptydir-4759 deletion completed in 10.019952428s

• [SLOW TEST:14.361 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:16:12.198: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-f89bd58a-1138-4774-9b6f-5e125c2d472d
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:16:12.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9136" for this suite.
Oct 26 19:16:20.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:16:22.421: INFO: namespace secrets-9136 deletion completed in 10.058588999s

• [SLOW TEST:10.223 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:16:22.421: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Oct 26 19:16:22.712: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5b236d4d-7f3c-467b-8bd1-5578b58d5c52" in namespace "downward-api-2918" to be "success or failure"
Oct 26 19:16:22.722: INFO: Pod "downwardapi-volume-5b236d4d-7f3c-467b-8bd1-5578b58d5c52": Phase="Pending", Reason="", readiness=false. Elapsed: 9.910672ms
Oct 26 19:16:24.735: INFO: Pod "downwardapi-volume-5b236d4d-7f3c-467b-8bd1-5578b58d5c52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023587871s
Oct 26 19:16:26.751: INFO: Pod "downwardapi-volume-5b236d4d-7f3c-467b-8bd1-5578b58d5c52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038877672s
STEP: Saw pod success
Oct 26 19:16:26.751: INFO: Pod "downwardapi-volume-5b236d4d-7f3c-467b-8bd1-5578b58d5c52" satisfied condition "success or failure"
Oct 26 19:16:26.764: INFO: Trying to get logs from node 10.123.240.178 pod downwardapi-volume-5b236d4d-7f3c-467b-8bd1-5578b58d5c52 container client-container: <nil>
STEP: delete the pod
Oct 26 19:16:26.839: INFO: Waiting for pod downwardapi-volume-5b236d4d-7f3c-467b-8bd1-5578b58d5c52 to disappear
Oct 26 19:16:26.850: INFO: Pod downwardapi-volume-5b236d4d-7f3c-467b-8bd1-5578b58d5c52 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:16:26.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2918" for this suite.
Oct 26 19:16:34.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:16:36.995: INFO: namespace downward-api-2918 deletion completed in 10.110496348s

• [SLOW TEST:14.574 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:16:37.004: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Oct 26 19:16:37.239: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc5c0057-c392-4fb0-86b0-2bddfb483a67" in namespace "downward-api-186" to be "success or failure"
Oct 26 19:16:37.249: INFO: Pod "downwardapi-volume-fc5c0057-c392-4fb0-86b0-2bddfb483a67": Phase="Pending", Reason="", readiness=false. Elapsed: 9.41065ms
Oct 26 19:16:39.259: INFO: Pod "downwardapi-volume-fc5c0057-c392-4fb0-86b0-2bddfb483a67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019966962s
Oct 26 19:16:41.271: INFO: Pod "downwardapi-volume-fc5c0057-c392-4fb0-86b0-2bddfb483a67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031253725s
STEP: Saw pod success
Oct 26 19:16:41.271: INFO: Pod "downwardapi-volume-fc5c0057-c392-4fb0-86b0-2bddfb483a67" satisfied condition "success or failure"
Oct 26 19:16:41.280: INFO: Trying to get logs from node 10.123.240.178 pod downwardapi-volume-fc5c0057-c392-4fb0-86b0-2bddfb483a67 container client-container: <nil>
STEP: delete the pod
Oct 26 19:16:41.339: INFO: Waiting for pod downwardapi-volume-fc5c0057-c392-4fb0-86b0-2bddfb483a67 to disappear
Oct 26 19:16:41.349: INFO: Pod downwardapi-volume-fc5c0057-c392-4fb0-86b0-2bddfb483a67 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:16:41.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-186" for this suite.
Oct 26 19:16:49.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:16:51.500: INFO: namespace downward-api-186 deletion completed in 10.120649633s

• [SLOW TEST:14.496 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:16:51.500: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 19:16:51.658: INFO: Creating deployment "webserver-deployment"
Oct 26 19:16:51.675: INFO: Waiting for observed generation 1
Oct 26 19:16:53.701: INFO: Waiting for all required pods to come up
Oct 26 19:16:53.721: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Oct 26 19:16:55.765: INFO: Waiting for deployment "webserver-deployment" to complete
Oct 26 19:16:55.785: INFO: Updating deployment "webserver-deployment" with a non-existent image
Oct 26 19:16:55.861: INFO: Updating deployment webserver-deployment
Oct 26 19:16:55.861: INFO: Waiting for observed generation 2
Oct 26 19:16:57.882: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Oct 26 19:16:57.892: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Oct 26 19:16:57.902: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Oct 26 19:16:57.936: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Oct 26 19:16:57.936: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Oct 26 19:16:57.947: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Oct 26 19:16:57.969: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Oct 26 19:16:57.969: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Oct 26 19:16:57.997: INFO: Updating deployment webserver-deployment
Oct 26 19:16:57.997: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Oct 26 19:16:58.023: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Oct 26 19:16:58.037: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Oct 26 19:16:58.059: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6705 /apis/apps/v1/namespaces/deployment-6705/deployments/webserver-deployment a084ba9a-ac51-4190-9530-723e4969f42a 62235 3 2020-10-26 19:16:51 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002a779d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-10-26 19:16:56 +0000 UTC,LastTransitionTime:2020-10-26 19:16:51 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-10-26 19:16:58 +0000 UTC,LastTransitionTime:2020-10-26 19:16:58 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Oct 26 19:16:58.071: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-6705 /apis/apps/v1/namespaces/deployment-6705/replicasets/webserver-deployment-c7997dcc8 74cfb903-d91e-477c-ab15-22ae85c55c4f 62232 3 2020-10-26 19:16:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment a084ba9a-ac51-4190-9530-723e4969f42a 0xc0034bf117 0xc0034bf118}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0034bf188 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 26 19:16:58.071: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Oct 26 19:16:58.071: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-6705 /apis/apps/v1/namespaces/deployment-6705/replicasets/webserver-deployment-595b5b9587 d08d7359-8de0-4548-b9fe-6e55cadb3efe 62230 3 2020-10-26 19:16:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment a084ba9a-ac51-4190-9530-723e4969f42a 0xc0034bf057 0xc0034bf058}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0034bf0b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Oct 26 19:16:58.099: INFO: Pod "webserver-deployment-595b5b9587-5v65v" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5v65v webserver-deployment-595b5b9587- deployment-6705 /api/v1/namespaces/deployment-6705/pods/webserver-deployment-595b5b9587-5v65v 4c366cdf-7a00-4473-bf7b-389c0971406e 62127 0 2020-10-26 19:16:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.84.14/32 cni.projectcalico.org/podIPs:172.30.84.14/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.84.14"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 d08d7359-8de0-4548-b9fe-6e55cadb3efe 0xc002a77e37 0xc002a77e38}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pmk2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pmk2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pmk2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.240.174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9z6vm,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.240.174,PodIP:172.30.84.14,StartTime:2020-10-26 19:16:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-26 19:16:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://364a99a3ead2f1fbb7236f4d84b764a6ffec45ce30f0131163c772219e9eb68d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.84.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 19:16:58.099: INFO: Pod "webserver-deployment-595b5b9587-68b26" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-68b26 webserver-deployment-595b5b9587- deployment-6705 /api/v1/namespaces/deployment-6705/pods/webserver-deployment-595b5b9587-68b26 1220c179-f21b-4ea6-8a7f-d1c054eab9a4 62240 0 2020-10-26 19:16:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 d08d7359-8de0-4548-b9fe-6e55cadb3efe 0xc002a77fd7 0xc002a77fd8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pmk2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pmk2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pmk2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9z6vm,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 19:16:58.100: INFO: Pod "webserver-deployment-595b5b9587-bqbh8" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bqbh8 webserver-deployment-595b5b9587- deployment-6705 /api/v1/namespaces/deployment-6705/pods/webserver-deployment-595b5b9587-bqbh8 c92b57fb-ba0d-45c4-8621-13d49c32e569 62129 0 2020-10-26 19:16:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.37.231/32 cni.projectcalico.org/podIPs:172.30.37.231/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.37.231"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 d08d7359-8de0-4548-b9fe-6e55cadb3efe 0xc002e22117 0xc002e22118}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pmk2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pmk2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pmk2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.240.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9z6vm,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.240.178,PodIP:172.30.37.231,StartTime:2020-10-26 19:16:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-26 19:16:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://467dddf568447705def1544829947991d9403c88d54045c35ded1df94dc0f6e1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.37.231,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 19:16:58.100: INFO: Pod "webserver-deployment-595b5b9587-dqrrq" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-dqrrq webserver-deployment-595b5b9587- deployment-6705 /api/v1/namespaces/deployment-6705/pods/webserver-deployment-595b5b9587-dqrrq 220cc265-2e15-4b39-9ea5-8e75ebf3c6ee 62089 0 2020-10-26 19:16:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.84.13/32 cni.projectcalico.org/podIPs:172.30.84.13/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.84.13"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 d08d7359-8de0-4548-b9fe-6e55cadb3efe 0xc002e222d7 0xc002e222d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pmk2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pmk2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pmk2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.240.174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9z6vm,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.240.174,PodIP:172.30.84.13,StartTime:2020-10-26 19:16:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-26 19:16:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://72c7fdce05967a4ae019955c411bdfbd1f046b32c2d8108a5c926eb93b0e1d0b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.84.13,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 19:16:58.100: INFO: Pod "webserver-deployment-595b5b9587-h725m" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-h725m webserver-deployment-595b5b9587- deployment-6705 /api/v1/namespaces/deployment-6705/pods/webserver-deployment-595b5b9587-h725m 5da592c6-6ad3-4122-b8d4-fb3b5fffb796 62118 0 2020-10-26 19:16:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.101.230/32 cni.projectcalico.org/podIPs:172.30.101.230/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.101.230"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 d08d7359-8de0-4548-b9fe-6e55cadb3efe 0xc002e22477 0xc002e22478}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pmk2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pmk2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pmk2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.240.172,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9z6vm,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.240.172,PodIP:172.30.101.230,StartTime:2020-10-26 19:16:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-26 19:16:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://5e57e2f4e925b60233b0f0faf39ea67cbde9751c3ed0820a16d286efe941cd0a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.101.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 19:16:58.100: INFO: Pod "webserver-deployment-595b5b9587-lt2m2" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-lt2m2 webserver-deployment-595b5b9587- deployment-6705 /api/v1/namespaces/deployment-6705/pods/webserver-deployment-595b5b9587-lt2m2 e6d9f851-04e0-4f57-8de2-0e809ef8f96a 62133 0 2020-10-26 19:16:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.37.230/32 cni.projectcalico.org/podIPs:172.30.37.230/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.37.230"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 d08d7359-8de0-4548-b9fe-6e55cadb3efe 0xc002e22637 0xc002e22638}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pmk2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pmk2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pmk2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.240.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9z6vm,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.240.178,PodIP:172.30.37.230,StartTime:2020-10-26 19:16:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-26 19:16:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://39ef40f4862fd8cb06e218ad87785d2fd2347de58fc89c3846865b30d3e75442,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.37.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 19:16:58.100: INFO: Pod "webserver-deployment-595b5b9587-p4fvr" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-p4fvr webserver-deployment-595b5b9587- deployment-6705 /api/v1/namespaces/deployment-6705/pods/webserver-deployment-595b5b9587-p4fvr 84db262f-7a81-4988-a8bb-281dd548aa93 62239 0 2020-10-26 19:16:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 d08d7359-8de0-4548-b9fe-6e55cadb3efe 0xc002e227d7 0xc002e227d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pmk2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pmk2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pmk2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.240.174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9z6vm,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 19:16:58.101: INFO: Pod "webserver-deployment-595b5b9587-qqzc6" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qqzc6 webserver-deployment-595b5b9587- deployment-6705 /api/v1/namespaces/deployment-6705/pods/webserver-deployment-595b5b9587-qqzc6 34b72e19-755f-4f6b-bc84-c2eaf76ace39 62099 0 2020-10-26 19:16:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.37.229/32 cni.projectcalico.org/podIPs:172.30.37.229/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.37.229"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 d08d7359-8de0-4548-b9fe-6e55cadb3efe 0xc002e22930 0xc002e22931}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pmk2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pmk2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pmk2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.240.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9z6vm,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.240.178,PodIP:172.30.37.229,StartTime:2020-10-26 19:16:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-26 19:16:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://6103e278fb1e0c938440627020df29b9a7e95d2ab0afc7359b8970f841252e0d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.37.229,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 19:16:58.101: INFO: Pod "webserver-deployment-595b5b9587-vjpdd" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-vjpdd webserver-deployment-595b5b9587- deployment-6705 /api/v1/namespaces/deployment-6705/pods/webserver-deployment-595b5b9587-vjpdd 5d7c0edb-59db-410b-9171-de29f74f6a45 62114 0 2020-10-26 19:16:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.101.232/32 cni.projectcalico.org/podIPs:172.30.101.232/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.101.232"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 d08d7359-8de0-4548-b9fe-6e55cadb3efe 0xc002e22ac7 0xc002e22ac8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pmk2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pmk2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pmk2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.240.172,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9z6vm,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.240.172,PodIP:172.30.101.232,StartTime:2020-10-26 19:16:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-26 19:16:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://fa1197f2fd7c7e9a816fbc7cbfd5135dafd82b93e579548d2566b9e8d5062c4a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.101.232,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 19:16:58.101: INFO: Pod "webserver-deployment-595b5b9587-wnrqv" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-wnrqv webserver-deployment-595b5b9587- deployment-6705 /api/v1/namespaces/deployment-6705/pods/webserver-deployment-595b5b9587-wnrqv ffe3ee46-d2e9-46bd-8f24-87e054c676be 62111 0 2020-10-26 19:16:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.101.236/32 cni.projectcalico.org/podIPs:172.30.101.236/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.101.236"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 d08d7359-8de0-4548-b9fe-6e55cadb3efe 0xc002e22c67 0xc002e22c68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pmk2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pmk2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pmk2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.240.172,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9z6vm,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.240.172,PodIP:172.30.101.236,StartTime:2020-10-26 19:16:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-26 19:16:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://9c82b11dd5f496b673c50849e37fae3e5acec4988fd8cf1a19b3d7246e7edc03,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.101.236,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 19:16:58.101: INFO: Pod "webserver-deployment-c7997dcc8-8jsjw" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-8jsjw webserver-deployment-c7997dcc8- deployment-6705 /api/v1/namespaces/deployment-6705/pods/webserver-deployment-c7997dcc8-8jsjw c7bac52c-0ac3-4a68-a149-8be03a5fd949 62211 0 2020-10-26 19:16:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.37.227/32 cni.projectcalico.org/podIPs:172.30.37.227/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.37.227"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 74cfb903-d91e-477c-ab15-22ae85c55c4f 0xc002e22e27 0xc002e22e28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pmk2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pmk2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pmk2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.240.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9z6vm,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.240.178,PodIP:,StartTime:2020-10-26 19:16:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 19:16:58.101: INFO: Pod "webserver-deployment-c7997dcc8-bdpk4" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-bdpk4 webserver-deployment-c7997dcc8- deployment-6705 /api/v1/namespaces/deployment-6705/pods/webserver-deployment-c7997dcc8-bdpk4 0f4e7952-23d3-4ab5-a265-b5b0da62a38e 62213 0 2020-10-26 19:16:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.84.29/32 cni.projectcalico.org/podIPs:172.30.84.29/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.84.29"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 74cfb903-d91e-477c-ab15-22ae85c55c4f 0xc002e22fe7 0xc002e22fe8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pmk2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pmk2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pmk2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.240.174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9z6vm,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.240.174,PodIP:,StartTime:2020-10-26 19:16:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 19:16:58.101: INFO: Pod "webserver-deployment-c7997dcc8-fjmmn" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-fjmmn webserver-deployment-c7997dcc8- deployment-6705 /api/v1/namespaces/deployment-6705/pods/webserver-deployment-c7997dcc8-fjmmn 7883281b-ed88-4649-a266-cbadc6a24540 62224 0 2020-10-26 19:16:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.84.30/32 cni.projectcalico.org/podIPs:172.30.84.30/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.84.30"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 74cfb903-d91e-477c-ab15-22ae85c55c4f 0xc002e231a7 0xc002e231a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pmk2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pmk2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pmk2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.240.174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9z6vm,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.240.174,PodIP:,StartTime:2020-10-26 19:16:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 19:16:58.102: INFO: Pod "webserver-deployment-c7997dcc8-hp9vn" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-hp9vn webserver-deployment-c7997dcc8- deployment-6705 /api/v1/namespaces/deployment-6705/pods/webserver-deployment-c7997dcc8-hp9vn 5e41caef-7fcc-40ec-ac8d-03d3aa1ec47b 62207 0 2020-10-26 19:16:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.101.227/32 cni.projectcalico.org/podIPs:172.30.101.227/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.101.227"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 74cfb903-d91e-477c-ab15-22ae85c55c4f 0xc002e23347 0xc002e23348}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pmk2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pmk2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pmk2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.240.172,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9z6vm,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.240.172,PodIP:,StartTime:2020-10-26 19:16:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 19:16:58.102: INFO: Pod "webserver-deployment-c7997dcc8-pjfz4" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-pjfz4 webserver-deployment-c7997dcc8- deployment-6705 /api/v1/namespaces/deployment-6705/pods/webserver-deployment-c7997dcc8-pjfz4 0f235832-90b3-443a-9f24-3d37ac5b7374 62226 0 2020-10-26 19:16:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.37.232/32 cni.projectcalico.org/podIPs:172.30.37.232/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.37.232"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 74cfb903-d91e-477c-ab15-22ae85c55c4f 0xc002e23507 0xc002e23508}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pmk2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pmk2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pmk2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.240.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9z6vm,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:16:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.240.178,PodIP:,StartTime:2020-10-26 19:16:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:16:58.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6705" for this suite.
Oct 26 19:17:08.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:17:10.185: INFO: namespace deployment-6705 deletion completed in 12.05529566s

• [SLOW TEST:18.685 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:17:10.186: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 19:17:10.430: INFO: Pod name rollover-pod: Found 0 pods out of 1
Oct 26 19:17:15.446: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Oct 26 19:17:15.446: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Oct 26 19:17:17.457: INFO: Creating deployment "test-rollover-deployment"
Oct 26 19:17:17.482: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Oct 26 19:17:19.504: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Oct 26 19:17:19.525: INFO: Ensure that both replica sets have 1 created replica
Oct 26 19:17:19.548: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Oct 26 19:17:19.575: INFO: Updating deployment test-rollover-deployment
Oct 26 19:17:19.575: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Oct 26 19:17:21.602: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Oct 26 19:17:21.624: INFO: Make sure deployment "test-rollover-deployment" is complete
Oct 26 19:17:21.651: INFO: all replica sets need to contain the pod-template-hash label
Oct 26 19:17:21.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336637, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336637, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336641, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336637, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 19:17:23.672: INFO: all replica sets need to contain the pod-template-hash label
Oct 26 19:17:23.672: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336637, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336637, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336641, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336637, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 19:17:25.681: INFO: all replica sets need to contain the pod-template-hash label
Oct 26 19:17:25.681: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336637, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336637, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336641, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336637, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 19:17:27.675: INFO: all replica sets need to contain the pod-template-hash label
Oct 26 19:17:27.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336637, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336637, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336641, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336637, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 19:17:29.673: INFO: all replica sets need to contain the pod-template-hash label
Oct 26 19:17:29.673: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336637, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336637, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336641, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336637, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 19:17:31.695: INFO: 
Oct 26 19:17:31.695: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336637, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336637, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336651, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336637, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 19:17:33.673: INFO: 
Oct 26 19:17:33.673: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Oct 26 19:17:33.705: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-1375 /apis/apps/v1/namespaces/deployment-1375/deployments/test-rollover-deployment db355b8c-bd40-48a1-b193-d4947203b8eb 62831 2 2020-10-26 19:17:17 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0033a2fb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-10-26 19:17:17 +0000 UTC,LastTransitionTime:2020-10-26 19:17:17 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2020-10-26 19:17:31 +0000 UTC,LastTransitionTime:2020-10-26 19:17:17 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Oct 26 19:17:33.716: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-1375 /apis/apps/v1/namespaces/deployment-1375/replicasets/test-rollover-deployment-7d7dc6548c b5d00e03-93bb-49b9-9799-5a94c6c7ca0b 62819 2 2020-10-26 19:17:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment db355b8c-bd40-48a1-b193-d4947203b8eb 0xc001f7b747 0xc001f7b748}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001f7b7b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Oct 26 19:17:33.716: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Oct 26 19:17:33.716: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1375 /apis/apps/v1/namespaces/deployment-1375/replicasets/test-rollover-controller bba9339a-52ee-4fca-900b-e2b6a2224028 62829 2 2020-10-26 19:17:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment db355b8c-bd40-48a1-b193-d4947203b8eb 0xc001f7b677 0xc001f7b678}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001f7b6d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 26 19:17:33.716: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-1375 /apis/apps/v1/namespaces/deployment-1375/replicasets/test-rollover-deployment-f6c94f66c fb44ecbb-fd2a-4207-ad86-12448089bd4d 62753 2 2020-10-26 19:17:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment db355b8c-bd40-48a1-b193-d4947203b8eb 0xc001f7b820 0xc001f7b821}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001f7b898 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 26 19:17:33.728: INFO: Pod "test-rollover-deployment-7d7dc6548c-fcprc" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-fcprc test-rollover-deployment-7d7dc6548c- deployment-1375 /api/v1/namespaces/deployment-1375/pods/test-rollover-deployment-7d7dc6548c-fcprc 7297ce51-3746-4c98-a677-f642b98f3783 62782 0 2020-10-26 19:17:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[cni.projectcalico.org/podIP:172.30.101.243/32 cni.projectcalico.org/podIPs:172.30.101.243/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.101.243"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c b5d00e03-93bb-49b9-9799-5a94c6c7ca0b 0xc0024d3f57 0xc0024d3f58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-l9jls,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-l9jls,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-l9jls,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.240.172,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-qp8b8,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:17:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:17:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:17:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:17:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.240.172,PodIP:172.30.101.243,StartTime:2020-10-26 19:17:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-26 19:17:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://0bdd03f52b78c102486f810cfb974765f738093133a0f06684359c9286bc1582,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.101.243,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:17:33.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1375" for this suite.
Oct 26 19:17:41.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:17:43.792: INFO: namespace deployment-1375 deletion completed in 10.041054213s

• [SLOW TEST:33.607 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:17:43.793: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 19:17:43.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 create -f - --namespace=kubectl-4559'
Oct 26 19:17:44.814: INFO: stderr: ""
Oct 26 19:17:44.814: INFO: stdout: "replicationcontroller/redis-master created\n"
Oct 26 19:17:44.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 create -f - --namespace=kubectl-4559'
Oct 26 19:17:45.483: INFO: stderr: ""
Oct 26 19:17:45.483: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Oct 26 19:17:46.507: INFO: Selector matched 1 pods for map[app:redis]
Oct 26 19:17:46.507: INFO: Found 0 / 1
Oct 26 19:17:47.498: INFO: Selector matched 1 pods for map[app:redis]
Oct 26 19:17:47.498: INFO: Found 1 / 1
Oct 26 19:17:47.498: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Oct 26 19:17:47.510: INFO: Selector matched 1 pods for map[app:redis]
Oct 26 19:17:47.510: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Oct 26 19:17:47.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 describe pod redis-master-zjwxg --namespace=kubectl-4559'
Oct 26 19:17:47.799: INFO: stderr: ""
Oct 26 19:17:47.800: INFO: stdout: "Name:         redis-master-zjwxg\nNamespace:    kubectl-4559\nPriority:     0\nNode:         10.123.240.178/10.123.240.178\nStart Time:   Mon, 26 Oct 2020 19:17:44 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 172.30.37.238/32\n              cni.projectcalico.org/podIPs: 172.30.37.238/32\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"k8s-pod-network\",\n                    \"ips\": [\n                        \"172.30.37.238\"\n                    ],\n                    \"dns\": {}\n                }]\n              openshift.io/scc: privileged\nStatus:       Running\nIP:           172.30.37.238\nIPs:\n  IP:           172.30.37.238\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   cri-o://c48bd18df6c11aff3512b6f870cd6f6015b8911a8eb12eea0cb1ca4d328c5934\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 26 Oct 2020 19:17:46 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-7bsvj (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-7bsvj:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-7bsvj\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                     Message\n  ----    ------     ----       ----                     -------\n  Normal  Scheduled  <unknown>  default-scheduler        Successfully assigned kubectl-4559/redis-master-zjwxg to 10.123.240.178\n  Normal  Pulled     1s         kubelet, 10.123.240.178  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    1s         kubelet, 10.123.240.178  Created container redis-master\n  Normal  Started    1s         kubelet, 10.123.240.178  Started container redis-master\n"
Oct 26 19:17:47.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 describe rc redis-master --namespace=kubectl-4559'
Oct 26 19:17:48.015: INFO: stderr: ""
Oct 26 19:17:48.015: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-4559\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-zjwxg\n"
Oct 26 19:17:48.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 describe service redis-master --namespace=kubectl-4559'
Oct 26 19:17:48.204: INFO: stderr: ""
Oct 26 19:17:48.204: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-4559\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.21.62.70\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.30.37.238:6379\nSession Affinity:  None\nEvents:            <none>\n"
Oct 26 19:17:48.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 describe node 10.123.240.172'
Oct 26 19:17:48.729: INFO: stderr: ""
Oct 26 19:17:48.730: INFO: stdout: "Name:               10.123.240.172\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-de\n                    failure-domain.beta.kubernetes.io/zone=fra05\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=149.81.70.29\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.123.240.172\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_7_64\n                    ibm-cloud.kubernetes.io/region=eu-de\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-bubfm2ef0iic4vgllb5g-kubee2epvgd-default-000002fa\n                    ibm-cloud.kubernetes.io/worker-pool-id=bubfm2ef0iic4vgllb5g-51c27f5\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.3.38_1542_openshift\n                    ibm-cloud.kubernetes.io/zone=fra05\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.123.240.172\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    node.openshift.io/os_id=rhel\n                    privateVLAN=2723042\n                    publicVLAN=2723040\n                    topology.kubernetes.io/region=eu-de\n                    topology.kubernetes.io/zone=fra05\nAnnotations:        projectcalico.org/IPv4Address: 10.123.240.172/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.101.192\nCreationTimestamp:  Mon, 26 Oct 2020 16:56:51 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 26 Oct 2020 16:58:15 +0000   Mon, 26 Oct 2020 16:58:15 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 26 Oct 2020 19:17:24 +0000   Mon, 26 Oct 2020 16:56:51 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 26 Oct 2020 19:17:24 +0000   Mon, 26 Oct 2020 16:56:51 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 26 Oct 2020 19:17:24 +0000   Mon, 26 Oct 2020 16:56:51 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 26 Oct 2020 19:17:24 +0000   Mon, 26 Oct 2020 16:58:22 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.123.240.172\n  ExternalIP:  149.81.70.29\n  Hostname:    10.123.240.172\nCapacity:\n cpu:                4\n ephemeral-storage:  103078840Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16260868Ki\n pods:               110\nAllocatable:\n cpu:                3910m\n ephemeral-storage:  100275095474\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             13484804Ki\n pods:               110\nSystem Info:\n Machine ID:                                             51a37e0e4842485d8f2720da8fd36e1d\n System UUID:                                            51575C2E-3F32-988A-88EA-CAB700622D71\n Boot ID:                                                3d10aa05-a4be-4af3-9fd8-7ac343715535\n Kernel Version:                                         3.10.0-1160.2.1.el7.x86_64\n OS Image:                                               Red Hat\n Operating System:                                       linux\n Architecture:                                           amd64\n Container Runtime Version:                              cri-o://1.16.6-18.rhaos4.3.git538d861.el7\n Kubelet Version:                                        v1.16.2+417b9fd\n Kube-Proxy Version:                                     v1.16.2+417b9fd\nProviderID:                                              ibm://fee034388aa6435883a1f720010ab3a2///bubfm2ef0iic4vgllb5g/kube-bubfm2ef0iic4vgllb5g-kubee2epvgd-default-000002fa\nNon-terminated Pods:                                     (42 in total)\n  Namespace                                              Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                                              ----                                                               ------------  ----------  ---------------  -------------  ---\n  calico-system                                          calico-kube-controllers-599969f895-cnxrs                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         139m\n  calico-system                                          calico-node-kchsj                                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         139m\n  calico-system                                          calico-typha-6b7867b64d-wwhp2                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         137m\n  ibm-system                                             ibm-cloud-provider-ip-149-81-128-30-5bdf48d794-vdds8               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         127m\n  kube-system                                            ibm-file-plugin-6b86cbfbc6-vw94d                                   50m (1%)      200m (5%)   100Mi (0%)       200Mi (1%)     147m\n  kube-system                                            ibm-keepalived-watcher-rdht2                                       5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         140m\n  kube-system                                            ibm-master-proxy-static-10.123.240.172                             25m (0%)      300m (7%)   32M (0%)         512M (3%)      140m\n  kube-system                                            ibm-storage-watcher-7fc85c5589-t79km                               50m (1%)      200m (5%)   100Mi (0%)       200Mi (1%)     147m\n  kube-system                                            ibmcloud-block-storage-driver-fm8d2                                50m (1%)      300m (7%)   100Mi (0%)       300Mi (2%)     140m\n  kube-system                                            ibmcloud-block-storage-plugin-79495594d5-7m8wf                     50m (1%)      300m (7%)   100Mi (0%)       300Mi (2%)     147m\n  openshift-cluster-node-tuning-operator                 cluster-node-tuning-operator-86b7f98f7b-qjdxp                      10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         153m\n  openshift-cluster-node-tuning-operator                 tuned-f8xvt                                                        10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         137m\n  openshift-cluster-samples-operator                     cluster-samples-operator-644946789c-zw8th                          20m (0%)      0 (0%)      0 (0%)           0 (0%)         135m\n  openshift-cluster-storage-operator                     cluster-storage-operator-6c6dd4b587-52s64                          10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         153m\n  openshift-console-operator                             console-operator-7f9f78ff66-kf42x                                  10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         153m\n  openshift-console                                      downloads-56f66db77f-f9rtp                                         10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         153m\n  openshift-console                                      downloads-56f66db77f-qk97h                                         10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         153m\n  openshift-dns-operator                                 dns-operator-5bd9bd8fcd-gj6cw                                      20m (0%)      0 (0%)      40Mi (0%)        0 (0%)         153m\n  openshift-dns                                          dns-default-qnchn                                                  110m (2%)     0 (0%)      70Mi (0%)        512Mi (3%)     138m\n  openshift-image-registry                               cluster-image-registry-operator-854d785579-78wmp                   20m (0%)      0 (0%)      0 (0%)           0 (0%)         153m\n  openshift-image-registry                               node-ca-rhts4                                                      10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         138m\n  openshift-ingress-operator                             ingress-operator-549d4c77b5-8mj9f                                  20m (0%)      0 (0%)      40Mi (0%)        0 (0%)         153m\n  openshift-ingress                                      router-default-56c7ff9d54-2kbrj                                    100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         35m\n  openshift-kube-proxy                                   openshift-kube-proxy-k67gq                                         100m (2%)     0 (0%)      200Mi (1%)       0 (0%)         140m\n  openshift-marketplace                                  marketplace-operator-c74f66688-865bs                               10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         153m\n  openshift-monitoring                                   alertmanager-main-2                                                6m (0%)       0 (0%)      220Mi (1%)       0 (0%)         131m\n  openshift-monitoring                                   cluster-monitoring-operator-77bbbf9cb7-sd49n                       10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         153m\n  openshift-monitoring                                   node-exporter-pbqtq                                                9m (0%)       0 (0%)      210Mi (1%)       0 (0%)         138m\n  openshift-monitoring                                   prometheus-adapter-6cccbf8dbb-4bpjw                                1m (0%)       0 (0%)      20Mi (0%)        0 (0%)         132m\n  openshift-monitoring                                   telemeter-client-7c4c649567-vp6j2                                  3m (0%)       0 (0%)      20Mi (0%)        0 (0%)         35m\n  openshift-multus                                       multus-admission-controller-4dblm                                  10m (0%)      0 (0%)      0 (0%)           0 (0%)         139m\n  openshift-multus                                       multus-wk7tv                                                       10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         140m\n  openshift-network-operator                             network-operator-5647cdbff6-qn2hq                                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         153m\n  openshift-operator-lifecycle-manager                   catalog-operator-7bf86b4f96-kpnmt                                  10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         153m\n  openshift-operator-lifecycle-manager                   olm-operator-cbd8cfd65-5hqds                                       10m (0%)      0 (0%)      160Mi (1%)       0 (0%)         153m\n  openshift-operator-lifecycle-manager                   packageserver-d5dbff54b-xqj44                                      10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         35m\n  openshift-service-ca-operator                          service-ca-operator-54f4b4db4-wvhxp                                10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         153m\n  openshift-service-catalog-apiserver-operator           openshift-service-catalog-apiserver-operator-d6cf765ff-k9snw       0 (0%)        0 (0%)      50Mi (0%)        0 (0%)         153m\n  openshift-service-catalog-controller-manager-operator  openshift-service-catalog-controller-manager-operator-6fcbsks6s    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         153m\n  sonobuoy                                               sonobuoy                                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         57m\n  sonobuoy                                               sonobuoy-systemd-logs-daemon-set-a94f7e2648a3464d-9nhrv            0 (0%)        0 (0%)      0 (0%)           0 (0%)         56m\n  tigera-operator                                        tigera-operator-798cfbf7dd-x8hf6                                   100m (2%)     0 (0%)      40Mi (0%)        0 (0%)         148m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                914m (23%)       1300m (33%)\n  memory             2699794Ki (20%)  2048288Ki (15%)\n  ephemeral-storage  0 (0%)           0 (0%)\nEvents:\n  Type    Reason                   Age                  From                        Message\n  ----    ------                   ----                 ----                        -------\n  Normal  Starting                 140m                 kubelet, 10.123.240.172     Starting kubelet.\n  Normal  NodeAllocatableEnforced  140m                 kubelet, 10.123.240.172     Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientPID     140m (x7 over 140m)  kubelet, 10.123.240.172     Node 10.123.240.172 status is now: NodeHasSufficientPID\n  Normal  NodeHasSufficientMemory  140m (x8 over 140m)  kubelet, 10.123.240.172     Node 10.123.240.172 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    140m (x8 over 140m)  kubelet, 10.123.240.172     Node 10.123.240.172 status is now: NodeHasNoDiskPressure\n  Normal  Starting                 140m                 kube-proxy, 10.123.240.172  Starting kube-proxy.\n"
Oct 26 19:17:48.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 describe namespace kubectl-4559'
Oct 26 19:17:48.920: INFO: stderr: ""
Oct 26 19:17:48.920: INFO: stdout: "Name:         kubectl-4559\nLabels:       e2e-framework=kubectl\n              e2e-run=d1b6ffe8-28ca-4ed3-80d4-f18a1a22efac\nAnnotations:  openshift.io/sa.scc.mcs: s0:c52,c14\n              openshift.io/sa.scc.supplemental-groups: 1002680000/10000\n              openshift.io/sa.scc.uid-range: 1002680000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:17:48.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4559" for this suite.
Oct 26 19:18:20.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:18:22.990: INFO: namespace kubectl-4559 deletion completed in 34.042246328s

• [SLOW TEST:39.197 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:18:22.990: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-8ac07eec-ed98-4750-9188-1e20bf379c39
STEP: Creating a pod to test consume configMaps
Oct 26 19:18:23.181: INFO: Waiting up to 5m0s for pod "pod-configmaps-239de891-36bd-494f-8b4d-75e44b8e31a9" in namespace "configmap-347" to be "success or failure"
Oct 26 19:18:23.201: INFO: Pod "pod-configmaps-239de891-36bd-494f-8b4d-75e44b8e31a9": Phase="Pending", Reason="", readiness=false. Elapsed: 19.933157ms
Oct 26 19:18:25.214: INFO: Pod "pod-configmaps-239de891-36bd-494f-8b4d-75e44b8e31a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032106228s
Oct 26 19:18:27.234: INFO: Pod "pod-configmaps-239de891-36bd-494f-8b4d-75e44b8e31a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052303455s
STEP: Saw pod success
Oct 26 19:18:27.234: INFO: Pod "pod-configmaps-239de891-36bd-494f-8b4d-75e44b8e31a9" satisfied condition "success or failure"
Oct 26 19:18:27.245: INFO: Trying to get logs from node 10.123.240.178 pod pod-configmaps-239de891-36bd-494f-8b4d-75e44b8e31a9 container configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 19:18:27.344: INFO: Waiting for pod pod-configmaps-239de891-36bd-494f-8b4d-75e44b8e31a9 to disappear
Oct 26 19:18:27.361: INFO: Pod pod-configmaps-239de891-36bd-494f-8b4d-75e44b8e31a9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:18:27.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-347" for this suite.
Oct 26 19:18:35.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:18:37.470: INFO: namespace configmap-347 deletion completed in 10.054430884s

• [SLOW TEST:14.480 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:18:37.474: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 19:18:38.226: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 19:18:40.266: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336718, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336718, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336718, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739336718, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 19:18:43.318: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:18:43.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2997" for this suite.
Oct 26 19:18:51.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:18:53.703: INFO: namespace webhook-2997 deletion completed in 10.070977554s
STEP: Destroying namespace "webhook-2997-markers" for this suite.
Oct 26 19:19:01.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:19:03.820: INFO: namespace webhook-2997-markers deletion completed in 10.117107184s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:26.410 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:19:03.884: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Oct 26 19:19:06.179: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:19:06.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3333" for this suite.
Oct 26 19:19:14.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:19:16.288: INFO: namespace container-runtime-3333 deletion completed in 10.026038558s

• [SLOW TEST:12.404 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:19:16.289: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Oct 26 19:19:16.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-4949'
Oct 26 19:19:16.690: INFO: stderr: ""
Oct 26 19:19:16.690: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Oct 26 19:19:21.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pod e2e-test-httpd-pod --namespace=kubectl-4949 -o json'
Oct 26 19:19:21.893: INFO: stderr: ""
Oct 26 19:19:21.893: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.30.37.240/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.37.240/32\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.37.240\\\"\\n    ],\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2020-10-26T19:19:16Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4949\",\n        \"resourceVersion\": \"63780\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-4949/pods/e2e-test-httpd-pod\",\n        \"uid\": \"aa6dce6f-4988-468c-9e45-2544420f0df3\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-xzsh9\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-q7nqd\"\n            }\n        ],\n        \"nodeName\": \"10.123.240.178\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c52,c39\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-xzsh9\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-xzsh9\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-10-26T19:19:16Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-10-26T19:19:19Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-10-26T19:19:19Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-10-26T19:19:16Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://06b2fab5337beac2df2253a91a77816ba3f94a4d3592b45052dc0f453ecb3afc\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-10-26T19:19:18Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.123.240.178\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.37.240\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.37.240\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-10-26T19:19:16Z\"\n    }\n}\n"
STEP: replace the image in the pod
Oct 26 19:19:21.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 replace -f - --namespace=kubectl-4949'
Oct 26 19:19:22.566: INFO: stderr: ""
Oct 26 19:19:22.566: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Oct 26 19:19:22.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 delete pods e2e-test-httpd-pod --namespace=kubectl-4949'
Oct 26 19:19:24.486: INFO: stderr: ""
Oct 26 19:19:24.486: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:19:24.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4949" for this suite.
Oct 26 19:19:32.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:19:34.636: INFO: namespace kubectl-4949 deletion completed in 10.116149699s

• [SLOW TEST:18.347 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:19:34.636: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Oct 26 19:19:35.956: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:19:35.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1026 19:19:35.955964      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-4260" for this suite.
Oct 26 19:19:44.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:19:46.037: INFO: namespace gc-4260 deletion completed in 10.046893929s

• [SLOW TEST:11.400 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:19:46.037: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Oct 26 19:19:46.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-5920'
Oct 26 19:19:46.356: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Oct 26 19:19:46.356: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Oct 26 19:19:46.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 delete jobs e2e-test-httpd-job --namespace=kubectl-5920'
Oct 26 19:19:46.546: INFO: stderr: ""
Oct 26 19:19:46.546: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:19:46.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5920" for this suite.
Oct 26 19:19:54.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:19:56.654: INFO: namespace kubectl-5920 deletion completed in 10.086080745s

• [SLOW TEST:10.617 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:19:56.654: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-4901
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-4901
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4901
Oct 26 19:19:56.892: INFO: Found 0 stateful pods, waiting for 1
Oct 26 19:20:06.905: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Oct 26 19:20:06.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 26 19:20:07.347: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 26 19:20:07.347: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 26 19:20:07.347: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 26 19:20:07.358: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Oct 26 19:20:17.374: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Oct 26 19:20:17.374: INFO: Waiting for statefulset status.replicas updated to 0
Oct 26 19:20:17.426: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Oct 26 19:20:17.426: INFO: ss-0  10.123.240.178  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:19:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:19:56 +0000 UTC  }]
Oct 26 19:20:17.426: INFO: 
Oct 26 19:20:17.426: INFO: StatefulSet ss has not reached scale 3, at 1
Oct 26 19:20:18.439: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986524448s
Oct 26 19:20:19.450: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.974330481s
Oct 26 19:20:20.462: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.963139665s
Oct 26 19:20:21.475: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.951246309s
Oct 26 19:20:22.487: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.938116329s
Oct 26 19:20:23.500: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.925782303s
Oct 26 19:20:24.512: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.913234396s
Oct 26 19:20:25.525: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.900911002s
Oct 26 19:20:26.537: INFO: Verifying statefulset ss doesn't scale past 3 for another 887.618721ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4901
Oct 26 19:20:27.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:20:27.963: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 26 19:20:27.963: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 26 19:20:27.963: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 26 19:20:27.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:20:28.351: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Oct 26 19:20:28.352: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 26 19:20:28.352: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 26 19:20:28.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:20:28.678: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Oct 26 19:20:28.678: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 26 19:20:28.678: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 26 19:20:28.691: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Oct 26 19:20:38.703: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 26 19:20:38.703: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 26 19:20:38.703: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Oct 26 19:20:38.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 26 19:20:39.109: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 26 19:20:39.109: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 26 19:20:39.109: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 26 19:20:39.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 26 19:20:39.531: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 26 19:20:39.531: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 26 19:20:39.531: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 26 19:20:39.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 26 19:20:39.896: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 26 19:20:39.896: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 26 19:20:39.896: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 26 19:20:39.896: INFO: Waiting for statefulset status.replicas updated to 0
Oct 26 19:20:39.907: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Oct 26 19:20:49.930: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Oct 26 19:20:49.930: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Oct 26 19:20:49.930: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Oct 26 19:20:49.967: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Oct 26 19:20:49.967: INFO: ss-0  10.123.240.178  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:19:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:19:56 +0000 UTC  }]
Oct 26 19:20:49.967: INFO: ss-1  10.123.240.172  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:17 +0000 UTC  }]
Oct 26 19:20:49.967: INFO: ss-2  10.123.240.174  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:17 +0000 UTC  }]
Oct 26 19:20:49.967: INFO: 
Oct 26 19:20:49.967: INFO: StatefulSet ss has not reached scale 0, at 3
Oct 26 19:20:50.979: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Oct 26 19:20:50.979: INFO: ss-0  10.123.240.178  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:19:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:19:56 +0000 UTC  }]
Oct 26 19:20:50.979: INFO: ss-1  10.123.240.172  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:17 +0000 UTC  }]
Oct 26 19:20:50.979: INFO: ss-2  10.123.240.174  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:17 +0000 UTC  }]
Oct 26 19:20:50.979: INFO: 
Oct 26 19:20:50.979: INFO: StatefulSet ss has not reached scale 0, at 3
Oct 26 19:20:51.998: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Oct 26 19:20:51.998: INFO: ss-0  10.123.240.178  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:19:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:19:56 +0000 UTC  }]
Oct 26 19:20:51.998: INFO: ss-1  10.123.240.172  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:17 +0000 UTC  }]
Oct 26 19:20:51.998: INFO: ss-2  10.123.240.174  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:17 +0000 UTC  }]
Oct 26 19:20:51.998: INFO: 
Oct 26 19:20:51.998: INFO: StatefulSet ss has not reached scale 0, at 3
Oct 26 19:20:53.013: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Oct 26 19:20:53.013: INFO: ss-0  10.123.240.178  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:19:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:19:56 +0000 UTC  }]
Oct 26 19:20:53.013: INFO: ss-1  10.123.240.172  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:17 +0000 UTC  }]
Oct 26 19:20:53.014: INFO: 
Oct 26 19:20:53.014: INFO: StatefulSet ss has not reached scale 0, at 2
Oct 26 19:20:54.025: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Oct 26 19:20:54.025: INFO: ss-0  10.123.240.178  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:19:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:19:56 +0000 UTC  }]
Oct 26 19:20:54.025: INFO: ss-1  10.123.240.172  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:17 +0000 UTC  }]
Oct 26 19:20:54.025: INFO: 
Oct 26 19:20:54.025: INFO: StatefulSet ss has not reached scale 0, at 2
Oct 26 19:20:55.039: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Oct 26 19:20:55.039: INFO: ss-1  10.123.240.172  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:17 +0000 UTC  }]
Oct 26 19:20:55.039: INFO: 
Oct 26 19:20:55.039: INFO: StatefulSet ss has not reached scale 0, at 1
Oct 26 19:20:56.053: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Oct 26 19:20:56.054: INFO: ss-1  10.123.240.172  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:17 +0000 UTC  }]
Oct 26 19:20:56.054: INFO: 
Oct 26 19:20:56.054: INFO: StatefulSet ss has not reached scale 0, at 1
Oct 26 19:20:57.066: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Oct 26 19:20:57.066: INFO: ss-1  10.123.240.172  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:17 +0000 UTC  }]
Oct 26 19:20:57.066: INFO: 
Oct 26 19:20:57.066: INFO: StatefulSet ss has not reached scale 0, at 1
Oct 26 19:20:58.091: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Oct 26 19:20:58.091: INFO: ss-1  10.123.240.172  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:17 +0000 UTC  }]
Oct 26 19:20:58.091: INFO: 
Oct 26 19:20:58.091: INFO: StatefulSet ss has not reached scale 0, at 1
Oct 26 19:20:59.102: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Oct 26 19:20:59.102: INFO: ss-1  10.123.240.172  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 19:20:17 +0000 UTC  }]
Oct 26 19:20:59.102: INFO: 
Oct 26 19:20:59.102: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4901
Oct 26 19:21:00.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:21:00.358: INFO: rc: 1
Oct 26 19:21:00.358: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  error: unable to upgrade connection: container not found ("webserver")
 [] <nil> 0xc008800ff0 exit status 1 <nil> <nil> true [0xc00358c108 0xc00358c168 0xc00358c1a0] [0xc00358c108 0xc00358c168 0xc00358c1a0] [0xc00358c158 0xc00358c190] [0x10efce0 0x10efce0] 0xc0035c6d20 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Oct 26 19:21:10.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:21:10.501: INFO: rc: 1
Oct 26 19:21:10.501: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0038ec330 exit status 1 <nil> <nil> true [0xc0037e61f8 0xc0037e6210 0xc0037e6238] [0xc0037e61f8 0xc0037e6210 0xc0037e6238] [0xc0037e6208 0xc0037e6220] [0x10efce0 0x10efce0] 0xc006db29c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:21:20.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:21:20.796: INFO: rc: 1
Oct 26 19:21:20.796: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0088012f0 exit status 1 <nil> <nil> true [0xc00358c1b0 0xc00358c1c8 0xc00358c1f8] [0xc00358c1b0 0xc00358c1c8 0xc00358c1f8] [0xc00358c1c0 0xc00358c1e8] [0x10efce0 0x10efce0] 0xc0035c71a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:21:30.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:21:30.951: INFO: rc: 1
Oct 26 19:21:30.951: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0088015f0 exit status 1 <nil> <nil> true [0xc00358c208 0xc00358c220 0xc00358c238] [0xc00358c208 0xc00358c220 0xc00358c238] [0xc00358c218 0xc00358c230] [0x10efce0 0x10efce0] 0xc0035c77a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:21:40.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:21:41.106: INFO: rc: 1
Oct 26 19:21:41.106: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0088018f0 exit status 1 <nil> <nil> true [0xc00358c240 0xc00358c258 0xc00358c290] [0xc00358c240 0xc00358c258 0xc00358c290] [0xc00358c250 0xc00358c288] [0x10efce0 0x10efce0] 0xc0035c7c20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:21:51.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:21:51.327: INFO: rc: 1
Oct 26 19:21:51.327: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0038ec690 exit status 1 <nil> <nil> true [0xc0037e6258 0xc0037e6290 0xc0037e62d0] [0xc0037e6258 0xc0037e6290 0xc0037e62d0] [0xc0037e6278 0xc0037e62c8] [0x10efce0 0x10efce0] 0xc006db2d80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:22:01.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:22:01.478: INFO: rc: 1
Oct 26 19:22:01.478: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc008801c20 exit status 1 <nil> <nil> true [0xc00358c2a0 0xc00358c2c8 0xc00358c300] [0xc00358c2a0 0xc00358c2c8 0xc00358c300] [0xc00358c2c0 0xc00358c2f8] [0x10efce0 0x10efce0] 0xc0035c7f80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:22:11.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:22:11.629: INFO: rc: 1
Oct 26 19:22:11.629: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc008801f80 exit status 1 <nil> <nil> true [0xc00358c308 0xc00358c340 0xc00358c370] [0xc00358c308 0xc00358c340 0xc00358c370] [0xc00358c338 0xc00358c360] [0x10efce0 0x10efce0] 0xc002930300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:22:21.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:22:21.777: INFO: rc: 1
Oct 26 19:22:21.778: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc007106c30 exit status 1 <nil> <nil> true [0xc00279c000 0xc00279c038 0xc00279c088] [0xc00279c000 0xc00279c038 0xc00279c088] [0xc00279c018 0xc00279c070] [0x10efce0 0x10efce0] 0xc0044f6420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:22:31.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:22:31.960: INFO: rc: 1
Oct 26 19:22:31.960: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0038ec9c0 exit status 1 <nil> <nil> true [0xc0037e62d8 0xc0037e62f0 0xc0037e6348] [0xc0037e62d8 0xc0037e62f0 0xc0037e6348] [0xc0037e62e8 0xc0037e6330] [0x10efce0 0x10efce0] 0xc006db30e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:22:41.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:22:42.114: INFO: rc: 1
Oct 26 19:22:42.114: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003882300 exit status 1 <nil> <nil> true [0xc00358c388 0xc00358c3c8 0xc00358c418] [0xc00358c388 0xc00358c3c8 0xc00358c418] [0xc00358c3a0 0xc00358c3f8] [0x10efce0 0x10efce0] 0xc0029306c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:22:52.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:22:52.278: INFO: rc: 1
Oct 26 19:22:52.278: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0038ecd20 exit status 1 <nil> <nil> true [0xc0037e6370 0xc0037e63b0 0xc0037e63d8] [0xc0037e6370 0xc0037e63b0 0xc0037e63d8] [0xc0037e6390 0xc0037e63d0] [0x10efce0 0x10efce0] 0xc006db3500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:23:02.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:23:02.424: INFO: rc: 1
Oct 26 19:23:02.424: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc008800330 exit status 1 <nil> <nil> true [0xc0037e6000 0xc0037e6060 0xc0037e6110] [0xc0037e6000 0xc0037e6060 0xc0037e6110] [0xc0037e6030 0xc0037e60f8] [0x10efce0 0x10efce0] 0xc0035c6720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:23:12.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:23:12.616: INFO: rc: 1
Oct 26 19:23:12.616: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0038ec2d0 exit status 1 <nil> <nil> true [0xc000b0c010 0xc000b0c070 0xc000b0c0a8] [0xc000b0c010 0xc000b0c070 0xc000b0c0a8] [0xc000b0c030 0xc000b0c090] [0x10efce0 0x10efce0] 0xc0029302a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:23:22.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:23:22.765: INFO: rc: 1
Oct 26 19:23:22.765: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc008800660 exit status 1 <nil> <nil> true [0xc0037e6118 0xc0037e6130 0xc0037e6148] [0xc0037e6118 0xc0037e6130 0xc0037e6148] [0xc0037e6128 0xc0037e6140] [0x10efce0 0x10efce0] 0xc0035c6f00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:23:32.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:23:32.914: INFO: rc: 1
Oct 26 19:23:32.915: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0038ec630 exit status 1 <nil> <nil> true [0xc000b0c0b8 0xc000b0c0e8 0xc000b0c130] [0xc000b0c0b8 0xc000b0c0e8 0xc000b0c130] [0xc000b0c0d8 0xc000b0c120] [0x10efce0 0x10efce0] 0xc002930660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:23:42.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:23:43.129: INFO: rc: 1
Oct 26 19:23:43.129: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc008800990 exit status 1 <nil> <nil> true [0xc0037e6150 0xc0037e61b0 0xc0037e61f8] [0xc0037e6150 0xc0037e61b0 0xc0037e61f8] [0xc0037e6190 0xc0037e61e0] [0x10efce0 0x10efce0] 0xc0035c72c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:23:53.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:23:53.288: INFO: rc: 1
Oct 26 19:23:53.288: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0038ec9f0 exit status 1 <nil> <nil> true [0xc000b0c140 0xc000b0c1d8 0xc000b0c208] [0xc000b0c140 0xc000b0c1d8 0xc000b0c208] [0xc000b0c1c8 0xc000b0c1f8] [0x10efce0 0x10efce0] 0xc002930a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:24:03.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:24:03.451: INFO: rc: 1
Oct 26 19:24:03.451: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0038ed260 exit status 1 <nil> <nil> true [0xc000b0c230 0xc000b0c248 0xc000b0c260] [0xc000b0c230 0xc000b0c248 0xc000b0c260] [0xc000b0c240 0xc000b0c258] [0x10efce0 0x10efce0] 0xc002930de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:24:13.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:24:13.607: INFO: rc: 1
Oct 26 19:24:13.607: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc008800ed0 exit status 1 <nil> <nil> true [0xc0037e6200 0xc0037e6218 0xc0037e6258] [0xc0037e6200 0xc0037e6218 0xc0037e6258] [0xc0037e6210 0xc0037e6238] [0x10efce0 0x10efce0] 0xc0035c7980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:24:23.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:24:23.768: INFO: rc: 1
Oct 26 19:24:23.768: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0038ed590 exit status 1 <nil> <nil> true [0xc000b0c268 0xc000b0c280 0xc000b0c298] [0xc000b0c268 0xc000b0c280 0xc000b0c298] [0xc000b0c278 0xc000b0c290] [0x10efce0 0x10efce0] 0xc002931200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:24:33.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:24:33.914: INFO: rc: 1
Oct 26 19:24:33.914: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0038822d0 exit status 1 <nil> <nil> true [0xc00279c000 0xc00279c038 0xc00279c088] [0xc00279c000 0xc00279c038 0xc00279c088] [0xc00279c018 0xc00279c070] [0x10efce0 0x10efce0] 0xc002e585a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:24:43.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:24:44.104: INFO: rc: 1
Oct 26 19:24:44.104: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc008801230 exit status 1 <nil> <nil> true [0xc0037e6270 0xc0037e62b0 0xc0037e62d8] [0xc0037e6270 0xc0037e62b0 0xc0037e62d8] [0xc0037e6290 0xc0037e62d0] [0x10efce0 0x10efce0] 0xc0035c7d40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:24:54.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:24:54.258: INFO: rc: 1
Oct 26 19:24:54.258: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc008801290 exit status 1 <nil> <nil> true [0xc00279c098 0xc00279c0e8 0xc00279c110] [0xc00279c098 0xc00279c0e8 0xc00279c110] [0xc00279c0c8 0xc00279c108] [0x10efce0 0x10efce0] 0xc0035c7e00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:25:04.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:25:04.423: INFO: rc: 1
Oct 26 19:25:04.424: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003882300 exit status 1 <nil> <nil> true [0xc00279c000 0xc00279c038 0xc00279c088] [0xc00279c000 0xc00279c038 0xc00279c088] [0xc00279c018 0xc00279c070] [0x10efce0 0x10efce0] 0xc0035c6720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:25:14.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:25:14.578: INFO: rc: 1
Oct 26 19:25:14.578: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0038ec300 exit status 1 <nil> <nil> true [0xc000b0c010 0xc000b0c070 0xc000b0c0a8] [0xc000b0c010 0xc000b0c070 0xc000b0c0a8] [0xc000b0c030 0xc000b0c090] [0x10efce0 0x10efce0] 0xc002e585a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:25:24.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:25:24.719: INFO: rc: 1
Oct 26 19:25:24.719: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0038ec660 exit status 1 <nil> <nil> true [0xc000b0c0b8 0xc000b0c0e8 0xc000b0c130] [0xc000b0c0b8 0xc000b0c0e8 0xc000b0c130] [0xc000b0c0d8 0xc000b0c120] [0x10efce0 0x10efce0] 0xc002e58b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:25:34.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:25:34.862: INFO: rc: 1
Oct 26 19:25:34.862: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0038ec990 exit status 1 <nil> <nil> true [0xc000b0c140 0xc000b0c1d8 0xc000b0c208] [0xc000b0c140 0xc000b0c1d8 0xc000b0c208] [0xc000b0c1c8 0xc000b0c1f8] [0x10efce0 0x10efce0] 0xc002e58fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:25:44.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:25:45.111: INFO: rc: 1
Oct 26 19:25:45.111: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0038eccf0 exit status 1 <nil> <nil> true [0xc000b0c230 0xc000b0c248 0xc000b0c260] [0xc000b0c230 0xc000b0c248 0xc000b0c260] [0xc000b0c240 0xc000b0c258] [0x10efce0 0x10efce0] 0xc002e59380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:25:55.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:25:55.266: INFO: rc: 1
Oct 26 19:25:55.266: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0038ed290 exit status 1 <nil> <nil> true [0xc000b0c268 0xc000b0c280 0xc000b0c298] [0xc000b0c268 0xc000b0c280 0xc000b0c298] [0xc000b0c278 0xc000b0c290] [0x10efce0 0x10efce0] 0xc002e596e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 26 19:26:05.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-4901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:26:05.420: INFO: rc: 1
Oct 26 19:26:05.421: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
Oct 26 19:26:05.421: INFO: Scaling statefulset ss to 0
Oct 26 19:26:05.459: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Oct 26 19:26:05.468: INFO: Deleting all statefulset in ns statefulset-4901
Oct 26 19:26:05.481: INFO: Scaling statefulset ss to 0
Oct 26 19:26:05.517: INFO: Waiting for statefulset status.replicas updated to 0
Oct 26 19:26:05.528: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:26:05.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4901" for this suite.
Oct 26 19:26:13.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:26:15.620: INFO: namespace statefulset-4901 deletion completed in 10.023691967s

• [SLOW TEST:378.966 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:26:15.621: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Oct 26 19:26:16.910: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e5aa8ae-0cd0-462e-8155-9ba098bab8b9" in namespace "projected-3222" to be "success or failure"
Oct 26 19:26:16.922: INFO: Pod "downwardapi-volume-1e5aa8ae-0cd0-462e-8155-9ba098bab8b9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.974691ms
Oct 26 19:26:18.933: INFO: Pod "downwardapi-volume-1e5aa8ae-0cd0-462e-8155-9ba098bab8b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023584403s
Oct 26 19:26:20.945: INFO: Pod "downwardapi-volume-1e5aa8ae-0cd0-462e-8155-9ba098bab8b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035655377s
STEP: Saw pod success
Oct 26 19:26:20.945: INFO: Pod "downwardapi-volume-1e5aa8ae-0cd0-462e-8155-9ba098bab8b9" satisfied condition "success or failure"
Oct 26 19:26:20.957: INFO: Trying to get logs from node 10.123.240.178 pod downwardapi-volume-1e5aa8ae-0cd0-462e-8155-9ba098bab8b9 container client-container: <nil>
STEP: delete the pod
Oct 26 19:26:21.063: INFO: Waiting for pod downwardapi-volume-1e5aa8ae-0cd0-462e-8155-9ba098bab8b9 to disappear
Oct 26 19:26:21.074: INFO: Pod downwardapi-volume-1e5aa8ae-0cd0-462e-8155-9ba098bab8b9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:26:21.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3222" for this suite.
Oct 26 19:26:29.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:26:31.112: INFO: namespace projected-3222 deletion completed in 10.02087979s

• [SLOW TEST:15.491 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:26:31.112: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Oct 26 19:26:31.304: INFO: Waiting up to 5m0s for pod "pod-56d60a9d-27b0-404c-a18e-79eb02a81b3e" in namespace "emptydir-5459" to be "success or failure"
Oct 26 19:26:31.314: INFO: Pod "pod-56d60a9d-27b0-404c-a18e-79eb02a81b3e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.388501ms
Oct 26 19:26:33.326: INFO: Pod "pod-56d60a9d-27b0-404c-a18e-79eb02a81b3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021540366s
Oct 26 19:26:35.352: INFO: Pod "pod-56d60a9d-27b0-404c-a18e-79eb02a81b3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048049629s
STEP: Saw pod success
Oct 26 19:26:35.352: INFO: Pod "pod-56d60a9d-27b0-404c-a18e-79eb02a81b3e" satisfied condition "success or failure"
Oct 26 19:26:35.363: INFO: Trying to get logs from node 10.123.240.178 pod pod-56d60a9d-27b0-404c-a18e-79eb02a81b3e container test-container: <nil>
STEP: delete the pod
Oct 26 19:26:35.428: INFO: Waiting for pod pod-56d60a9d-27b0-404c-a18e-79eb02a81b3e to disappear
Oct 26 19:26:35.439: INFO: Pod pod-56d60a9d-27b0-404c-a18e-79eb02a81b3e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:26:35.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5459" for this suite.
Oct 26 19:26:43.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:26:45.558: INFO: namespace emptydir-5459 deletion completed in 10.098617965s

• [SLOW TEST:14.446 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:26:45.560: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Oct 26 19:26:47.800: INFO: Pod pod-hostip-ca545954-3815-4254-865c-fb001fe0390b has hostIP: 10.123.240.178
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:26:47.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4813" for this suite.
Oct 26 19:27:19.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:27:21.814: INFO: namespace pods-4813 deletion completed in 33.994878274s

• [SLOW TEST:36.254 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:27:21.815: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:27:38.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1933" for this suite.
Oct 26 19:27:46.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:27:48.192: INFO: namespace resourcequota-1933 deletion completed in 10.073563812s

• [SLOW TEST:26.377 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:27:48.195: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Oct 26 19:27:48.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 api-versions'
Oct 26 19:27:48.507: INFO: stderr: ""
Oct 26 19:27:48.507: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1beta1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.openshift.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:27:48.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3565" for this suite.
Oct 26 19:27:56.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:27:58.561: INFO: namespace kubectl-3565 deletion completed in 10.017575271s

• [SLOW TEST:10.366 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:27:58.561: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Oct 26 19:27:58.770: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd381a07-8098-4b53-9a1d-6116f1e92783" in namespace "downward-api-7085" to be "success or failure"
Oct 26 19:27:58.781: INFO: Pod "downwardapi-volume-fd381a07-8098-4b53-9a1d-6116f1e92783": Phase="Pending", Reason="", readiness=false. Elapsed: 11.213054ms
Oct 26 19:28:00.795: INFO: Pod "downwardapi-volume-fd381a07-8098-4b53-9a1d-6116f1e92783": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025152252s
Oct 26 19:28:02.805: INFO: Pod "downwardapi-volume-fd381a07-8098-4b53-9a1d-6116f1e92783": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035519555s
STEP: Saw pod success
Oct 26 19:28:02.805: INFO: Pod "downwardapi-volume-fd381a07-8098-4b53-9a1d-6116f1e92783" satisfied condition "success or failure"
Oct 26 19:28:02.817: INFO: Trying to get logs from node 10.123.240.178 pod downwardapi-volume-fd381a07-8098-4b53-9a1d-6116f1e92783 container client-container: <nil>
STEP: delete the pod
Oct 26 19:28:02.879: INFO: Waiting for pod downwardapi-volume-fd381a07-8098-4b53-9a1d-6116f1e92783 to disappear
Oct 26 19:28:02.890: INFO: Pod downwardapi-volume-fd381a07-8098-4b53-9a1d-6116f1e92783 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:28:02.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7085" for this suite.
Oct 26 19:28:10.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:28:13.021: INFO: namespace downward-api-7085 deletion completed in 10.087763664s

• [SLOW TEST:14.461 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:28:13.022: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Oct 26 19:28:13.256: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Oct 26 19:28:49.374: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 19:28:59.098: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:29:35.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1286" for this suite.
Oct 26 19:29:43.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:29:45.175: INFO: namespace crd-publish-openapi-1286 deletion completed in 10.065584388s

• [SLOW TEST:92.153 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:29:45.180: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-6bc40db8-76ca-4fe1-9a7c-ab5a447d5f1e
STEP: Creating a pod to test consume secrets
Oct 26 19:29:45.436: INFO: Waiting up to 5m0s for pod "pod-secrets-7564e5a4-e342-4179-8b83-4ba89a401d63" in namespace "secrets-2094" to be "success or failure"
Oct 26 19:29:45.450: INFO: Pod "pod-secrets-7564e5a4-e342-4179-8b83-4ba89a401d63": Phase="Pending", Reason="", readiness=false. Elapsed: 14.052016ms
Oct 26 19:29:47.461: INFO: Pod "pod-secrets-7564e5a4-e342-4179-8b83-4ba89a401d63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025016076s
STEP: Saw pod success
Oct 26 19:29:47.462: INFO: Pod "pod-secrets-7564e5a4-e342-4179-8b83-4ba89a401d63" satisfied condition "success or failure"
Oct 26 19:29:47.473: INFO: Trying to get logs from node 10.123.240.178 pod pod-secrets-7564e5a4-e342-4179-8b83-4ba89a401d63 container secret-volume-test: <nil>
STEP: delete the pod
Oct 26 19:29:47.559: INFO: Waiting for pod pod-secrets-7564e5a4-e342-4179-8b83-4ba89a401d63 to disappear
Oct 26 19:29:47.569: INFO: Pod pod-secrets-7564e5a4-e342-4179-8b83-4ba89a401d63 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:29:47.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2094" for this suite.
Oct 26 19:29:55.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:29:57.646: INFO: namespace secrets-2094 deletion completed in 10.055730105s

• [SLOW TEST:12.466 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:29:57.647: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:30:08.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1821" for this suite.
Oct 26 19:30:17.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:30:19.052: INFO: namespace resourcequota-1821 deletion completed in 10.053207281s

• [SLOW TEST:21.405 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:30:19.052: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-b4863f01-377d-4430-a365-766fc8ee52b0 in namespace container-probe-576
Oct 26 19:30:23.266: INFO: Started pod test-webserver-b4863f01-377d-4430-a365-766fc8ee52b0 in namespace container-probe-576
STEP: checking the pod's current state and verifying that restartCount is present
Oct 26 19:30:23.276: INFO: Initial restart count of pod test-webserver-b4863f01-377d-4430-a365-766fc8ee52b0 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:34:24.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-576" for this suite.
Oct 26 19:34:32.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:34:34.899: INFO: namespace container-probe-576 deletion completed in 10.010068271s

• [SLOW TEST:255.848 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:34:34.900: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-2490/secret-test-e04420a1-6bb9-456e-a90a-091b698f3af5
STEP: Creating a pod to test consume secrets
Oct 26 19:34:36.118: INFO: Waiting up to 5m0s for pod "pod-configmaps-9cbd5b9f-a39b-44a0-86d3-2464c46aa0a2" in namespace "secrets-2490" to be "success or failure"
Oct 26 19:34:36.129: INFO: Pod "pod-configmaps-9cbd5b9f-a39b-44a0-86d3-2464c46aa0a2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.21518ms
Oct 26 19:34:38.140: INFO: Pod "pod-configmaps-9cbd5b9f-a39b-44a0-86d3-2464c46aa0a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022176033s
STEP: Saw pod success
Oct 26 19:34:38.140: INFO: Pod "pod-configmaps-9cbd5b9f-a39b-44a0-86d3-2464c46aa0a2" satisfied condition "success or failure"
Oct 26 19:34:38.149: INFO: Trying to get logs from node 10.123.240.178 pod pod-configmaps-9cbd5b9f-a39b-44a0-86d3-2464c46aa0a2 container env-test: <nil>
STEP: delete the pod
Oct 26 19:34:38.267: INFO: Waiting for pod pod-configmaps-9cbd5b9f-a39b-44a0-86d3-2464c46aa0a2 to disappear
Oct 26 19:34:38.276: INFO: Pod pod-configmaps-9cbd5b9f-a39b-44a0-86d3-2464c46aa0a2 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:34:38.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2490" for this suite.
Oct 26 19:34:46.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:34:48.353: INFO: namespace secrets-2490 deletion completed in 10.049808797s

• [SLOW TEST:13.453 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:34:48.353: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 19:34:48.636: INFO: Create a RollingUpdate DaemonSet
Oct 26 19:34:48.655: INFO: Check that daemon pods launch on every node of the cluster
Oct 26 19:34:48.695: INFO: Number of nodes with available pods: 0
Oct 26 19:34:48.695: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 19:34:49.726: INFO: Number of nodes with available pods: 0
Oct 26 19:34:49.727: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 19:34:50.727: INFO: Number of nodes with available pods: 1
Oct 26 19:34:50.727: INFO: Node 10.123.240.174 is running more than one daemon pod
Oct 26 19:34:51.727: INFO: Number of nodes with available pods: 3
Oct 26 19:34:51.727: INFO: Number of running nodes: 3, number of available pods: 3
Oct 26 19:34:51.728: INFO: Update the DaemonSet to trigger a rollout
Oct 26 19:34:51.817: INFO: Updating DaemonSet daemon-set
Oct 26 19:35:05.874: INFO: Roll back the DaemonSet before rollout is complete
Oct 26 19:35:05.905: INFO: Updating DaemonSet daemon-set
Oct 26 19:35:05.905: INFO: Make sure DaemonSet rollback is complete
Oct 26 19:35:05.925: INFO: Wrong image for pod: daemon-set-5bgxc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Oct 26 19:35:05.925: INFO: Pod daemon-set-5bgxc is not available
Oct 26 19:35:06.962: INFO: Wrong image for pod: daemon-set-5bgxc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Oct 26 19:35:06.963: INFO: Pod daemon-set-5bgxc is not available
Oct 26 19:35:07.965: INFO: Wrong image for pod: daemon-set-5bgxc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Oct 26 19:35:07.965: INFO: Pod daemon-set-5bgxc is not available
Oct 26 19:35:08.964: INFO: Pod daemon-set-cnx9p is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4957, will wait for the garbage collector to delete the pods
Oct 26 19:35:09.090: INFO: Deleting DaemonSet.extensions daemon-set took: 26.714348ms
Oct 26 19:35:09.591: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.305506ms
Oct 26 19:35:20.603: INFO: Number of nodes with available pods: 0
Oct 26 19:35:20.603: INFO: Number of running nodes: 0, number of available pods: 0
Oct 26 19:35:20.615: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4957/daemonsets","resourceVersion":"68871"},"items":null}

Oct 26 19:35:20.625: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4957/pods","resourceVersion":"68871"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:35:20.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4957" for this suite.
Oct 26 19:35:28.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:35:30.774: INFO: namespace daemonsets-4957 deletion completed in 10.07948564s

• [SLOW TEST:42.421 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:35:30.774: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Oct 26 19:35:30.960: INFO: Waiting up to 5m0s for pod "client-containers-1f4b2c4b-0747-47dc-b479-60400ac39f37" in namespace "containers-8444" to be "success or failure"
Oct 26 19:35:30.971: INFO: Pod "client-containers-1f4b2c4b-0747-47dc-b479-60400ac39f37": Phase="Pending", Reason="", readiness=false. Elapsed: 10.807466ms
Oct 26 19:35:32.984: INFO: Pod "client-containers-1f4b2c4b-0747-47dc-b479-60400ac39f37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02401402s
Oct 26 19:35:34.997: INFO: Pod "client-containers-1f4b2c4b-0747-47dc-b479-60400ac39f37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036575947s
STEP: Saw pod success
Oct 26 19:35:34.997: INFO: Pod "client-containers-1f4b2c4b-0747-47dc-b479-60400ac39f37" satisfied condition "success or failure"
Oct 26 19:35:35.007: INFO: Trying to get logs from node 10.123.240.178 pod client-containers-1f4b2c4b-0747-47dc-b479-60400ac39f37 container test-container: <nil>
STEP: delete the pod
Oct 26 19:35:35.083: INFO: Waiting for pod client-containers-1f4b2c4b-0747-47dc-b479-60400ac39f37 to disappear
Oct 26 19:35:35.093: INFO: Pod client-containers-1f4b2c4b-0747-47dc-b479-60400ac39f37 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:35:35.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8444" for this suite.
Oct 26 19:35:43.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:35:45.237: INFO: namespace containers-8444 deletion completed in 10.126423989s

• [SLOW TEST:14.463 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:35:45.237: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Oct 26 19:35:48.129: INFO: Successfully updated pod "pod-update-6b3d8fa5-04ef-4307-9d3d-3229603f7d99"
STEP: verifying the updated pod is in kubernetes
Oct 26 19:35:48.152: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:35:48.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4950" for this suite.
Oct 26 19:36:20.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:36:22.273: INFO: namespace pods-4950 deletion completed in 34.086312211s

• [SLOW TEST:37.036 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:36:22.274: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:36:30.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1636" for this suite.
Oct 26 19:36:38.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:36:40.508: INFO: namespace job-1636 deletion completed in 9.983215883s

• [SLOW TEST:18.234 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:36:40.509: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 19:36:40.650: INFO: Creating ReplicaSet my-hostname-basic-7adf7639-015c-4e23-83b7-1f69005ff524
Oct 26 19:36:40.676: INFO: Pod name my-hostname-basic-7adf7639-015c-4e23-83b7-1f69005ff524: Found 0 pods out of 1
Oct 26 19:36:45.689: INFO: Pod name my-hostname-basic-7adf7639-015c-4e23-83b7-1f69005ff524: Found 1 pods out of 1
Oct 26 19:36:45.689: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-7adf7639-015c-4e23-83b7-1f69005ff524" is running
Oct 26 19:36:45.699: INFO: Pod "my-hostname-basic-7adf7639-015c-4e23-83b7-1f69005ff524-rvx2m" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-10-26 19:36:40 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-10-26 19:36:42 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-10-26 19:36:42 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-10-26 19:36:40 +0000 UTC Reason: Message:}])
Oct 26 19:36:45.699: INFO: Trying to dial the pod
Oct 26 19:36:50.747: INFO: Controller my-hostname-basic-7adf7639-015c-4e23-83b7-1f69005ff524: Got expected result from replica 1 [my-hostname-basic-7adf7639-015c-4e23-83b7-1f69005ff524-rvx2m]: "my-hostname-basic-7adf7639-015c-4e23-83b7-1f69005ff524-rvx2m", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:36:50.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3679" for this suite.
Oct 26 19:36:58.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:37:00.824: INFO: namespace replicaset-3679 deletion completed in 10.037435894s

• [SLOW TEST:20.316 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:37:00.825: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 19:37:00.970: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Oct 26 19:37:09.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 --namespace=crd-publish-openapi-4946 create -f -'
Oct 26 19:37:10.845: INFO: stderr: ""
Oct 26 19:37:10.845: INFO: stdout: "e2e-test-crd-publish-openapi-573-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Oct 26 19:37:10.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 --namespace=crd-publish-openapi-4946 delete e2e-test-crd-publish-openapi-573-crds test-cr'
Oct 26 19:37:11.028: INFO: stderr: ""
Oct 26 19:37:11.028: INFO: stdout: "e2e-test-crd-publish-openapi-573-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Oct 26 19:37:11.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 --namespace=crd-publish-openapi-4946 apply -f -'
Oct 26 19:37:11.458: INFO: stderr: ""
Oct 26 19:37:11.458: INFO: stdout: "e2e-test-crd-publish-openapi-573-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Oct 26 19:37:11.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 --namespace=crd-publish-openapi-4946 delete e2e-test-crd-publish-openapi-573-crds test-cr'
Oct 26 19:37:11.703: INFO: stderr: ""
Oct 26 19:37:11.703: INFO: stdout: "e2e-test-crd-publish-openapi-573-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Oct 26 19:37:11.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 explain e2e-test-crd-publish-openapi-573-crds'
Oct 26 19:37:12.326: INFO: stderr: ""
Oct 26 19:37:12.326: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-573-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:37:21.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4946" for this suite.
Oct 26 19:37:29.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:37:31.702: INFO: namespace crd-publish-openapi-4946 deletion completed in 10.048136072s

• [SLOW TEST:30.877 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:37:31.702: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-2311
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2311
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2311
Oct 26 19:37:31.938: INFO: Found 0 stateful pods, waiting for 1
Oct 26 19:37:41.952: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Oct 26 19:37:41.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-2311 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 26 19:37:42.314: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 26 19:37:42.314: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 26 19:37:42.314: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 26 19:37:42.326: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Oct 26 19:37:52.340: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Oct 26 19:37:52.340: INFO: Waiting for statefulset status.replicas updated to 0
Oct 26 19:37:52.386: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999997219s
Oct 26 19:37:53.398: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.988252844s
Oct 26 19:37:54.408: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.976633728s
Oct 26 19:37:55.422: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.965961024s
Oct 26 19:37:56.433: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.952591219s
Oct 26 19:37:57.445: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.941352905s
Oct 26 19:37:58.456: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.929799861s
Oct 26 19:37:59.470: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.918099246s
Oct 26 19:38:00.483: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.904463447s
Oct 26 19:38:01.495: INFO: Verifying statefulset ss doesn't scale past 1 for another 891.689425ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2311
Oct 26 19:38:02.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-2311 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:38:02.905: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 26 19:38:02.905: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 26 19:38:02.905: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 26 19:38:02.918: INFO: Found 1 stateful pods, waiting for 3
Oct 26 19:38:12.930: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 26 19:38:12.930: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 26 19:38:12.930: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Oct 26 19:38:12.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-2311 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 26 19:38:13.350: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 26 19:38:13.350: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 26 19:38:13.350: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 26 19:38:13.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-2311 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 26 19:38:13.709: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 26 19:38:13.709: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 26 19:38:13.709: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 26 19:38:13.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-2311 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 26 19:38:14.040: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 26 19:38:14.040: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 26 19:38:14.040: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 26 19:38:14.040: INFO: Waiting for statefulset status.replicas updated to 0
Oct 26 19:38:14.056: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Oct 26 19:38:24.095: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Oct 26 19:38:24.095: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Oct 26 19:38:24.095: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Oct 26 19:38:24.159: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999996846s
Oct 26 19:38:25.172: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986905642s
Oct 26 19:38:26.183: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.974305609s
Oct 26 19:38:27.195: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.963205611s
Oct 26 19:38:28.209: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.951331686s
Oct 26 19:38:29.221: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.937585356s
Oct 26 19:38:30.234: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.924781534s
Oct 26 19:38:31.254: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.912100409s
Oct 26 19:38:32.268: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.892214249s
Oct 26 19:38:33.281: INFO: Verifying statefulset ss doesn't scale past 3 for another 878.12248ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2311
Oct 26 19:38:34.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-2311 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:38:34.845: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 26 19:38:34.846: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 26 19:38:34.846: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 26 19:38:34.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-2311 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:38:35.191: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 26 19:38:35.191: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 26 19:38:35.191: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 26 19:38:35.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-2311 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:38:35.589: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 26 19:38:35.589: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 26 19:38:35.589: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 26 19:38:35.589: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Oct 26 19:38:55.642: INFO: Deleting all statefulset in ns statefulset-2311
Oct 26 19:38:55.654: INFO: Scaling statefulset ss to 0
Oct 26 19:38:55.692: INFO: Waiting for statefulset status.replicas updated to 0
Oct 26 19:38:55.704: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:38:55.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2311" for this suite.
Oct 26 19:39:03.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:39:05.831: INFO: namespace statefulset-2311 deletion completed in 10.061173116s

• [SLOW TEST:94.129 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:39:05.832: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Oct 26 19:39:06.060: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9149 /api/v1/namespaces/watch-9149/configmaps/e2e-watch-test-configmap-a d69c8c84-3868-4cdf-bf3d-79ebf24e52b7 70548 0 2020-10-26 19:39:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Oct 26 19:39:06.060: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9149 /api/v1/namespaces/watch-9149/configmaps/e2e-watch-test-configmap-a d69c8c84-3868-4cdf-bf3d-79ebf24e52b7 70548 0 2020-10-26 19:39:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Oct 26 19:39:16.097: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9149 /api/v1/namespaces/watch-9149/configmaps/e2e-watch-test-configmap-a d69c8c84-3868-4cdf-bf3d-79ebf24e52b7 70603 0 2020-10-26 19:39:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Oct 26 19:39:16.098: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9149 /api/v1/namespaces/watch-9149/configmaps/e2e-watch-test-configmap-a d69c8c84-3868-4cdf-bf3d-79ebf24e52b7 70603 0 2020-10-26 19:39:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Oct 26 19:39:26.122: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9149 /api/v1/namespaces/watch-9149/configmaps/e2e-watch-test-configmap-a d69c8c84-3868-4cdf-bf3d-79ebf24e52b7 70646 0 2020-10-26 19:39:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Oct 26 19:39:26.123: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9149 /api/v1/namespaces/watch-9149/configmaps/e2e-watch-test-configmap-a d69c8c84-3868-4cdf-bf3d-79ebf24e52b7 70646 0 2020-10-26 19:39:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Oct 26 19:39:36.146: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9149 /api/v1/namespaces/watch-9149/configmaps/e2e-watch-test-configmap-a d69c8c84-3868-4cdf-bf3d-79ebf24e52b7 70691 0 2020-10-26 19:39:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Oct 26 19:39:36.146: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9149 /api/v1/namespaces/watch-9149/configmaps/e2e-watch-test-configmap-a d69c8c84-3868-4cdf-bf3d-79ebf24e52b7 70691 0 2020-10-26 19:39:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Oct 26 19:39:46.171: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9149 /api/v1/namespaces/watch-9149/configmaps/e2e-watch-test-configmap-b 66d44d9d-eca0-4b83-a3cb-f3bf53bcd482 70737 0 2020-10-26 19:39:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Oct 26 19:39:46.171: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9149 /api/v1/namespaces/watch-9149/configmaps/e2e-watch-test-configmap-b 66d44d9d-eca0-4b83-a3cb-f3bf53bcd482 70737 0 2020-10-26 19:39:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Oct 26 19:39:56.197: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9149 /api/v1/namespaces/watch-9149/configmaps/e2e-watch-test-configmap-b 66d44d9d-eca0-4b83-a3cb-f3bf53bcd482 70772 0 2020-10-26 19:39:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Oct 26 19:39:56.198: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9149 /api/v1/namespaces/watch-9149/configmaps/e2e-watch-test-configmap-b 66d44d9d-eca0-4b83-a3cb-f3bf53bcd482 70772 0 2020-10-26 19:39:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:40:06.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9149" for this suite.
Oct 26 19:40:14.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:40:16.235: INFO: namespace watch-9149 deletion completed in 10.010640188s

• [SLOW TEST:70.403 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:40:16.235: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-1650
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Oct 26 19:40:16.421: INFO: Found 0 stateful pods, waiting for 3
Oct 26 19:40:26.434: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 26 19:40:26.434: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 26 19:40:26.434: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Oct 26 19:40:26.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-1650 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 26 19:40:27.026: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 26 19:40:27.026: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 26 19:40:27.026: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Oct 26 19:40:37.129: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Oct 26 19:40:47.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-1650 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:40:47.533: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 26 19:40:47.533: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 26 19:40:47.533: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 26 19:40:57.603: INFO: Waiting for StatefulSet statefulset-1650/ss2 to complete update
Oct 26 19:40:57.603: INFO: Waiting for Pod statefulset-1650/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Oct 26 19:40:57.603: INFO: Waiting for Pod statefulset-1650/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Oct 26 19:41:07.630: INFO: Waiting for StatefulSet statefulset-1650/ss2 to complete update
Oct 26 19:41:07.630: INFO: Waiting for Pod statefulset-1650/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Oct 26 19:41:07.630: INFO: Waiting for Pod statefulset-1650/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Oct 26 19:41:17.626: INFO: Waiting for StatefulSet statefulset-1650/ss2 to complete update
Oct 26 19:41:17.626: INFO: Waiting for Pod statefulset-1650/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Oct 26 19:41:27.625: INFO: Waiting for StatefulSet statefulset-1650/ss2 to complete update
Oct 26 19:41:27.625: INFO: Waiting for Pod statefulset-1650/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Oct 26 19:41:37.627: INFO: Waiting for StatefulSet statefulset-1650/ss2 to complete update
STEP: Rolling back to a previous revision
Oct 26 19:41:47.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-1650 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 26 19:41:47.991: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 26 19:41:47.991: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 26 19:41:47.991: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 26 19:41:58.087: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Oct 26 19:42:08.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=statefulset-1650 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 19:42:08.503: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 26 19:42:08.503: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 26 19:42:08.503: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 26 19:42:38.584: INFO: Waiting for StatefulSet statefulset-1650/ss2 to complete update
Oct 26 19:42:38.584: INFO: Waiting for Pod statefulset-1650/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Oct 26 19:42:48.607: INFO: Deleting all statefulset in ns statefulset-1650
Oct 26 19:42:48.616: INFO: Scaling statefulset ss2 to 0
Oct 26 19:43:08.662: INFO: Waiting for statefulset status.replicas updated to 0
Oct 26 19:43:08.682: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:43:08.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1650" for this suite.
Oct 26 19:43:18.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:43:20.737: INFO: namespace statefulset-1650 deletion completed in 11.984805818s

• [SLOW TEST:184.502 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:43:20.740: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-289
STEP: creating replication controller nodeport-test in namespace services-289
I1026 19:43:20.943136      23 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-289, replica count: 2
Oct 26 19:43:23.997: INFO: Creating new exec pod
I1026 19:43:23.997026      23 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 26 19:43:27.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=services-289 execpoddxngv -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Oct 26 19:43:27.526: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Oct 26 19:43:27.526: INFO: stdout: ""
Oct 26 19:43:27.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=services-289 execpoddxngv -- /bin/sh -x -c nc -zv -t -w 2 172.21.87.241 80'
Oct 26 19:43:27.889: INFO: stderr: "+ nc -zv -t -w 2 172.21.87.241 80\nConnection to 172.21.87.241 80 port [tcp/http] succeeded!\n"
Oct 26 19:43:27.889: INFO: stdout: ""
Oct 26 19:43:27.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=services-289 execpoddxngv -- /bin/sh -x -c nc -zv -t -w 2 10.123.240.172 31704'
Oct 26 19:43:28.323: INFO: stderr: "+ nc -zv -t -w 2 10.123.240.172 31704\nConnection to 10.123.240.172 31704 port [tcp/31704] succeeded!\n"
Oct 26 19:43:28.323: INFO: stdout: ""
Oct 26 19:43:28.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=services-289 execpoddxngv -- /bin/sh -x -c nc -zv -t -w 2 10.123.240.174 31704'
Oct 26 19:43:28.679: INFO: stderr: "+ nc -zv -t -w 2 10.123.240.174 31704\nConnection to 10.123.240.174 31704 port [tcp/31704] succeeded!\n"
Oct 26 19:43:28.679: INFO: stdout: ""
Oct 26 19:43:28.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=services-289 execpoddxngv -- /bin/sh -x -c nc -zv -t -w 2 149.81.70.29 31704'
Oct 26 19:43:29.050: INFO: stderr: "+ nc -zv -t -w 2 149.81.70.29 31704\nConnection to 149.81.70.29 31704 port [tcp/31704] succeeded!\n"
Oct 26 19:43:29.050: INFO: stdout: ""
Oct 26 19:43:29.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=services-289 execpoddxngv -- /bin/sh -x -c nc -zv -t -w 2 149.81.70.30 31704'
Oct 26 19:43:29.485: INFO: stderr: "+ nc -zv -t -w 2 149.81.70.30 31704\nConnection to 149.81.70.30 31704 port [tcp/31704] succeeded!\n"
Oct 26 19:43:29.485: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:43:29.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-289" for this suite.
Oct 26 19:43:37.557: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:43:39.555: INFO: namespace services-289 deletion completed in 10.045211234s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:18.816 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:43:39.555: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 19:43:40.515: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 19:43:42.547: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739338220, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739338220, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739338220, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739338220, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 19:43:45.595: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Oct 26 19:43:49.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 attach --namespace=webhook-1451 to-be-attached-pod -i -c=container1'
Oct 26 19:43:50.034: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:43:50.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1451" for this suite.
Oct 26 19:43:58.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:44:00.208: INFO: namespace webhook-1451 deletion completed in 10.128800997s
STEP: Destroying namespace "webhook-1451-markers" for this suite.
Oct 26 19:44:08.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:44:10.300: INFO: namespace webhook-1451-markers deletion completed in 10.091872478s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:30.802 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:44:10.357: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 19:44:10.527: INFO: Creating deployment "test-recreate-deployment"
Oct 26 19:44:10.542: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Oct 26 19:44:10.564: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Oct 26 19:44:12.594: INFO: Waiting deployment "test-recreate-deployment" to complete
Oct 26 19:44:12.606: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739338250, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739338250, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739338250, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739338250, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 19:44:14.623: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Oct 26 19:44:14.655: INFO: Updating deployment test-recreate-deployment
Oct 26 19:44:14.655: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Oct 26 19:44:14.861: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-4149 /apis/apps/v1/namespaces/deployment-4149/deployments/test-recreate-deployment 0528e4d2-f2ca-407d-83c3-68e647c5a344 72651 2 2020-10-26 19:44:10 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0024d3258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-10-26 19:44:14 +0000 UTC,LastTransitionTime:2020-10-26 19:44:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-10-26 19:44:14 +0000 UTC,LastTransitionTime:2020-10-26 19:44:10 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Oct 26 19:44:14.873: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-4149 /apis/apps/v1/namespaces/deployment-4149/replicasets/test-recreate-deployment-5f94c574ff f811cdc3-2aba-4560-8b1e-6b8303d52350 72649 1 2020-10-26 19:44:14 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 0528e4d2-f2ca-407d-83c3-68e647c5a344 0xc001f7a707 0xc001f7a708}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001f7a768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 26 19:44:14.873: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Oct 26 19:44:14.873: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-4149 /apis/apps/v1/namespaces/deployment-4149/replicasets/test-recreate-deployment-68fc85c7bb 46cad6a4-d842-476a-a256-a2d2773e8618 72639 2 2020-10-26 19:44:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 0528e4d2-f2ca-407d-83c3-68e647c5a344 0xc001f7a7d7 0xc001f7a7d8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001f7a838 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 26 19:44:14.884: INFO: Pod "test-recreate-deployment-5f94c574ff-vm9tv" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-vm9tv test-recreate-deployment-5f94c574ff- deployment-4149 /api/v1/namespaces/deployment-4149/pods/test-recreate-deployment-5f94c574ff-vm9tv f0583dbc-8aa8-49d6-8f95-19ebb99c0536 72652 0 2020-10-26 19:44:14 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff f811cdc3-2aba-4560-8b1e-6b8303d52350 0xc001f7aca7 0xc001f7aca8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tft4c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tft4c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tft4c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.240.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-tgq6m,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:44:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:44:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:44:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:44:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.240.178,PodIP:,StartTime:2020-10-26 19:44:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:44:14.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4149" for this suite.
Oct 26 19:44:22.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:44:24.991: INFO: namespace deployment-4149 deletion completed in 10.065835926s

• [SLOW TEST:14.634 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:44:24.991: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-8673
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8673 to expose endpoints map[]
Oct 26 19:44:25.208: INFO: Get endpoints failed (14.221604ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Oct 26 19:44:26.220: INFO: successfully validated that service endpoint-test2 in namespace services-8673 exposes endpoints map[] (1.026293094s elapsed)
STEP: Creating pod pod1 in namespace services-8673
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8673 to expose endpoints map[pod1:[80]]
Oct 26 19:44:28.347: INFO: successfully validated that service endpoint-test2 in namespace services-8673 exposes endpoints map[pod1:[80]] (2.077640583s elapsed)
STEP: Creating pod pod2 in namespace services-8673
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8673 to expose endpoints map[pod1:[80] pod2:[80]]
Oct 26 19:44:30.496: INFO: successfully validated that service endpoint-test2 in namespace services-8673 exposes endpoints map[pod1:[80] pod2:[80]] (2.114228931s elapsed)
STEP: Deleting pod pod1 in namespace services-8673
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8673 to expose endpoints map[pod2:[80]]
Oct 26 19:44:30.544: INFO: successfully validated that service endpoint-test2 in namespace services-8673 exposes endpoints map[pod2:[80]] (24.253213ms elapsed)
STEP: Deleting pod pod2 in namespace services-8673
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8673 to expose endpoints map[]
Oct 26 19:44:31.587: INFO: successfully validated that service endpoint-test2 in namespace services-8673 exposes endpoints map[] (1.023572305s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:44:31.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8673" for this suite.
Oct 26 19:44:45.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:44:47.802: INFO: namespace services-8673 deletion completed in 16.10659924s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:22.810 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:44:47.806: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 19:44:48.474: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 19:44:51.543: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:44:51.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3883" for this suite.
Oct 26 19:44:59.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:45:01.946: INFO: namespace webhook-3883 deletion completed in 10.105293864s
STEP: Destroying namespace "webhook-3883-markers" for this suite.
Oct 26 19:45:09.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:45:11.990: INFO: namespace webhook-3883-markers deletion completed in 10.044499815s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:24.244 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:45:12.051: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 19:45:12.239: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-c3f992a4-42b9-45a7-a1e8-fcf8dc8e2ebf" in namespace "security-context-test-6751" to be "success or failure"
Oct 26 19:45:12.250: INFO: Pod "busybox-privileged-false-c3f992a4-42b9-45a7-a1e8-fcf8dc8e2ebf": Phase="Pending", Reason="", readiness=false. Elapsed: 11.535696ms
Oct 26 19:45:14.261: INFO: Pod "busybox-privileged-false-c3f992a4-42b9-45a7-a1e8-fcf8dc8e2ebf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022836558s
Oct 26 19:45:14.262: INFO: Pod "busybox-privileged-false-c3f992a4-42b9-45a7-a1e8-fcf8dc8e2ebf" satisfied condition "success or failure"
Oct 26 19:45:14.357: INFO: Got logs for pod "busybox-privileged-false-c3f992a4-42b9-45a7-a1e8-fcf8dc8e2ebf": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:45:14.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6751" for this suite.
Oct 26 19:45:22.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:45:24.463: INFO: namespace security-context-test-6751 deletion completed in 10.078063644s

• [SLOW TEST:12.412 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:45:24.464: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Oct 26 19:45:24.620: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:46:11.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7169" for this suite.
Oct 26 19:46:19.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:46:21.962: INFO: namespace crd-publish-openapi-7169 deletion completed in 10.142978303s

• [SLOW TEST:57.498 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:46:21.963: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Oct 26 19:46:22.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-9011'
Oct 26 19:46:22.401: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Oct 26 19:46:22.401: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Oct 26 19:46:26.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 delete deployment e2e-test-httpd-deployment --namespace=kubectl-9011'
Oct 26 19:46:26.613: INFO: stderr: ""
Oct 26 19:46:26.613: INFO: stdout: "deployment.extensions \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:46:26.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9011" for this suite.
Oct 26 19:46:40.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:46:42.729: INFO: namespace kubectl-9011 deletion completed in 16.094442514s

• [SLOW TEST:20.766 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:46:42.729: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Oct 26 19:46:42.886: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-668520336 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:46:42.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1716" for this suite.
Oct 26 19:46:51.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:46:53.056: INFO: namespace kubectl-1716 deletion completed in 10.030406136s

• [SLOW TEST:10.327 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:46:53.060: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Oct 26 19:46:53.205: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Oct 26 19:46:53.316: INFO: Waiting for terminating namespaces to be deleted...
Oct 26 19:46:53.332: INFO: 
Logging pods the kubelet thinks is on node 10.123.240.172 before test
Oct 26 19:46:53.439: INFO: console-operator-7f9f78ff66-kf42x from openshift-console-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.439: INFO: 	Container console-operator ready: true, restart count 1
Oct 26 19:46:53.440: INFO: calico-kube-controllers-599969f895-cnxrs from calico-system started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Oct 26 19:46:53.440: INFO: multus-admission-controller-4dblm from openshift-multus started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container multus-admission-controller ready: true, restart count 0
Oct 26 19:46:53.440: INFO: prometheus-adapter-6cccbf8dbb-4bpjw from openshift-monitoring started at 2020-10-26 17:05:48 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container prometheus-adapter ready: true, restart count 0
Oct 26 19:46:53.440: INFO: ibmcloud-block-storage-driver-fm8d2 from kube-system started at 2020-10-26 16:56:55 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Oct 26 19:46:53.440: INFO: multus-wk7tv from openshift-multus started at 2020-10-26 16:57:19 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container kube-multus ready: true, restart count 0
Oct 26 19:46:53.440: INFO: openshift-kube-proxy-k67gq from openshift-kube-proxy started at 2020-10-26 16:57:26 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 26 19:46:53.440: INFO: cluster-node-tuning-operator-86b7f98f7b-qjdxp from openshift-cluster-node-tuning-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Oct 26 19:46:53.440: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-10-26 17:05:50 +0000 UTC (3 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container alertmanager ready: true, restart count 0
Oct 26 19:46:53.440: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 26 19:46:53.440: INFO: 	Container config-reloader ready: true, restart count 0
Oct 26 19:46:53.440: INFO: ibm-master-proxy-static-10.123.240.172 from kube-system started at 2020-10-26 16:56:50 +0000 UTC (2 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 19:46:53.440: INFO: 	Container pause ready: true, restart count 0
Oct 26 19:46:53.440: INFO: marketplace-operator-c74f66688-865bs from openshift-marketplace started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container marketplace-operator ready: true, restart count 0
Oct 26 19:46:53.440: INFO: openshift-service-catalog-controller-manager-operator-6fcbsks6s from openshift-service-catalog-controller-manager-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container operator ready: true, restart count 1
Oct 26 19:46:53.440: INFO: cluster-monitoring-operator-77bbbf9cb7-sd49n from openshift-monitoring started at 2020-10-26 16:58:24 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Oct 26 19:46:53.440: INFO: node-exporter-pbqtq from openshift-monitoring started at 2020-10-26 16:59:03 +0000 UTC (2 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:46:53.440: INFO: 	Container node-exporter ready: true, restart count 0
Oct 26 19:46:53.440: INFO: calico-typha-6b7867b64d-wwhp2 from calico-system started at 2020-10-26 17:00:10 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 19:46:53.440: INFO: ibm-keepalived-watcher-rdht2 from kube-system started at 2020-10-26 16:56:52 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 19:46:53.440: INFO: ibm-cloud-provider-ip-149-81-128-30-5bdf48d794-vdds8 from ibm-system started at 2020-10-26 17:10:29 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container ibm-cloud-provider-ip-149-81-128-30 ready: true, restart count 0
Oct 26 19:46:53.440: INFO: packageserver-d5dbff54b-xqj44 from openshift-operator-lifecycle-manager started at 2020-10-26 18:42:11 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container packageserver ready: true, restart count 0
Oct 26 19:46:53.440: INFO: tuned-f8xvt from openshift-cluster-node-tuning-operator started at 2020-10-26 17:00:14 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container tuned ready: true, restart count 0
Oct 26 19:46:53.440: INFO: olm-operator-cbd8cfd65-5hqds from openshift-operator-lifecycle-manager started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container olm-operator ready: true, restart count 0
Oct 26 19:46:53.440: INFO: service-ca-operator-54f4b4db4-wvhxp from openshift-service-ca-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container operator ready: true, restart count 0
Oct 26 19:46:53.440: INFO: downloads-56f66db77f-f9rtp from openshift-console started at 2020-10-26 16:58:23 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container download-server ready: true, restart count 0
Oct 26 19:46:53.440: INFO: sonobuoy-systemd-logs-daemon-set-a94f7e2648a3464d-9nhrv from sonobuoy started at 2020-10-26 18:20:50 +0000 UTC (2 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Oct 26 19:46:53.440: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 26 19:46:53.440: INFO: telemeter-client-7c4c649567-vp6j2 from openshift-monitoring started at 2020-10-26 18:42:09 +0000 UTC (3 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:46:53.440: INFO: 	Container reload ready: true, restart count 0
Oct 26 19:46:53.440: INFO: 	Container telemeter-client ready: true, restart count 0
Oct 26 19:46:53.440: INFO: calico-node-kchsj from calico-system started at 2020-10-26 16:58:05 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 19:46:53.440: INFO: tigera-operator-798cfbf7dd-x8hf6 from tigera-operator started at 2020-10-26 16:56:55 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container tigera-operator ready: true, restart count 2
Oct 26 19:46:53.440: INFO: dns-operator-5bd9bd8fcd-gj6cw from openshift-dns-operator started at 2020-10-26 16:58:22 +0000 UTC (2 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container dns-operator ready: true, restart count 0
Oct 26 19:46:53.440: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:46:53.440: INFO: cluster-image-registry-operator-854d785579-78wmp from openshift-image-registry started at 2020-10-26 16:58:22 +0000 UTC (2 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Oct 26 19:46:53.440: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Oct 26 19:46:53.440: INFO: downloads-56f66db77f-qk97h from openshift-console started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container download-server ready: true, restart count 0
Oct 26 19:46:53.440: INFO: ibmcloud-block-storage-plugin-79495594d5-7m8wf from kube-system started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.440: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Oct 26 19:46:53.440: INFO: ibm-storage-watcher-7fc85c5589-t79km from kube-system started at 2020-10-26 16:58:23 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.441: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Oct 26 19:46:53.441: INFO: ingress-operator-549d4c77b5-8mj9f from openshift-ingress-operator started at 2020-10-26 16:58:24 +0000 UTC (2 container statuses recorded)
Oct 26 19:46:53.441: INFO: 	Container ingress-operator ready: true, restart count 0
Oct 26 19:46:53.441: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:46:53.441: INFO: network-operator-5647cdbff6-qn2hq from openshift-network-operator started at 2020-10-26 16:56:55 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.441: INFO: 	Container network-operator ready: true, restart count 0
Oct 26 19:46:53.441: INFO: node-ca-rhts4 from openshift-image-registry started at 2020-10-26 16:59:40 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.441: INFO: 	Container node-ca ready: true, restart count 0
Oct 26 19:46:53.441: INFO: ibm-file-plugin-6b86cbfbc6-vw94d from kube-system started at 2020-10-26 16:58:23 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.441: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Oct 26 19:46:53.441: INFO: cluster-samples-operator-644946789c-zw8th from openshift-cluster-samples-operator started at 2020-10-26 17:01:55 +0000 UTC (2 container statuses recorded)
Oct 26 19:46:53.441: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Oct 26 19:46:53.441: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Oct 26 19:46:53.441: INFO: dns-default-qnchn from openshift-dns started at 2020-10-26 16:59:44 +0000 UTC (2 container statuses recorded)
Oct 26 19:46:53.441: INFO: 	Container dns ready: true, restart count 0
Oct 26 19:46:53.441: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 26 19:46:53.441: INFO: router-default-56c7ff9d54-2kbrj from openshift-ingress started at 2020-10-26 18:42:09 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.441: INFO: 	Container router ready: true, restart count 0
Oct 26 19:46:53.441: INFO: catalog-operator-7bf86b4f96-kpnmt from openshift-operator-lifecycle-manager started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.441: INFO: 	Container catalog-operator ready: true, restart count 0
Oct 26 19:46:53.441: INFO: openshift-service-catalog-apiserver-operator-d6cf765ff-k9snw from openshift-service-catalog-apiserver-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.441: INFO: 	Container operator ready: true, restart count 1
Oct 26 19:46:53.441: INFO: sonobuoy from sonobuoy started at 2020-10-26 18:20:41 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.441: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Oct 26 19:46:53.441: INFO: cluster-storage-operator-6c6dd4b587-52s64 from openshift-cluster-storage-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.441: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Oct 26 19:46:53.441: INFO: 
Logging pods the kubelet thinks is on node 10.123.240.174 before test
Oct 26 19:46:53.556: INFO: openshift-kube-proxy-rs72s from openshift-kube-proxy started at 2020-10-26 16:57:37 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.556: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 26 19:46:53.556: INFO: dns-default-2hlsj from openshift-dns started at 2020-10-26 16:59:44 +0000 UTC (2 container statuses recorded)
Oct 26 19:46:53.556: INFO: 	Container dns ready: true, restart count 0
Oct 26 19:46:53.556: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 26 19:46:53.556: INFO: community-operators-bd644f5c5-wb2l9 from openshift-marketplace started at 2020-10-26 17:00:16 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.556: INFO: 	Container community-operators ready: true, restart count 0
Oct 26 19:46:53.556: INFO: ibm-master-proxy-static-10.123.240.174 from kube-system started at 2020-10-26 16:57:35 +0000 UTC (2 container statuses recorded)
Oct 26 19:46:53.556: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 19:46:53.556: INFO: 	Container pause ready: true, restart count 0
Oct 26 19:46:53.556: INFO: node-exporter-hpt5d from openshift-monitoring started at 2020-10-26 16:59:04 +0000 UTC (2 container statuses recorded)
Oct 26 19:46:53.557: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:46:53.557: INFO: 	Container node-exporter ready: true, restart count 0
Oct 26 19:46:53.557: INFO: configmap-cabundle-injector-6676dbc567-zdqgt from openshift-service-ca started at 2020-10-26 16:59:02 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.557: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Oct 26 19:46:53.557: INFO: image-registry-57486bbffb-v7qmg from openshift-image-registry started at 2020-10-26 17:01:54 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.557: INFO: 	Container registry ready: true, restart count 0
Oct 26 19:46:53.557: INFO: certified-operators-59fc9cd9b5-84rhs from openshift-marketplace started at 2020-10-26 18:00:44 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.557: INFO: 	Container certified-operators ready: true, restart count 0
Oct 26 19:46:53.557: INFO: console-77979dd75-lnchz from openshift-console started at 2020-10-26 17:01:28 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.557: INFO: 	Container console ready: true, restart count 0
Oct 26 19:46:53.557: INFO: service-serving-cert-signer-6d656c4cf7-l9qz5 from openshift-service-ca started at 2020-10-26 16:59:01 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.557: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Oct 26 19:46:53.557: INFO: prometheus-operator-d5df96f57-bthbz from openshift-monitoring started at 2020-10-26 17:05:34 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.557: INFO: 	Container prometheus-operator ready: true, restart count 0
Oct 26 19:46:53.558: INFO: thanos-querier-85d68cbb66-5hkh8 from openshift-monitoring started at 2020-10-26 17:06:37 +0000 UTC (4 container statuses recorded)
Oct 26 19:46:53.558: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:46:53.558: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 26 19:46:53.558: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 26 19:46:53.558: INFO: 	Container thanos-querier ready: true, restart count 0
Oct 26 19:46:53.558: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-10-26 17:06:52 +0000 UTC (7 container statuses recorded)
Oct 26 19:46:53.558: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:46:53.558: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 26 19:46:53.558: INFO: 	Container prometheus ready: true, restart count 1
Oct 26 19:46:53.558: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Oct 26 19:46:53.558: INFO: 	Container prometheus-proxy ready: true, restart count 0
Oct 26 19:46:53.558: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Oct 26 19:46:53.558: INFO: 	Container thanos-sidecar ready: true, restart count 0
Oct 26 19:46:53.558: INFO: ibmcloud-block-storage-driver-s88xs from kube-system started at 2020-10-26 16:57:41 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.558: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Oct 26 19:46:53.558: INFO: calico-typha-6b7867b64d-j6hnv from calico-system started at 2020-10-26 16:58:05 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.558: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 19:46:53.559: INFO: calico-node-jfjg2 from calico-system started at 2020-10-26 16:58:05 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.559: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 19:46:53.559: INFO: apiservice-cabundle-injector-566d6d6dc7-lv6bf from openshift-service-ca started at 2020-10-26 16:59:02 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.559: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Oct 26 19:46:53.559: INFO: node-ca-lbgdr from openshift-image-registry started at 2020-10-26 16:59:40 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.559: INFO: 	Container node-ca ready: true, restart count 0
Oct 26 19:46:53.559: INFO: vpn-68d7d4d68d-clwcb from kube-system started at 2020-10-26 17:05:39 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.559: INFO: 	Container vpn ready: true, restart count 0
Oct 26 19:46:53.559: INFO: grafana-f6757cb99-n9dgx from openshift-monitoring started at 2020-10-26 17:05:54 +0000 UTC (2 container statuses recorded)
Oct 26 19:46:53.559: INFO: 	Container grafana ready: true, restart count 0
Oct 26 19:46:53.559: INFO: 	Container grafana-proxy ready: true, restart count 0
Oct 26 19:46:53.559: INFO: multus-wz2pl from openshift-multus started at 2020-10-26 16:57:37 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.559: INFO: 	Container kube-multus ready: true, restart count 0
Oct 26 19:46:53.559: INFO: ibm-keepalived-watcher-nxrtl from kube-system started at 2020-10-26 16:57:37 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.559: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 19:46:53.559: INFO: kube-state-metrics-6bb5fc9995-w9q9p from openshift-monitoring started at 2020-10-26 16:59:03 +0000 UTC (3 container statuses recorded)
Oct 26 19:46:53.559: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Oct 26 19:46:53.560: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Oct 26 19:46:53.560: INFO: 	Container kube-state-metrics ready: true, restart count 0
Oct 26 19:46:53.560: INFO: tuned-nlq8g from openshift-cluster-node-tuning-operator started at 2020-10-26 17:00:14 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.560: INFO: 	Container tuned ready: true, restart count 0
Oct 26 19:46:53.560: INFO: sonobuoy-systemd-logs-daemon-set-a94f7e2648a3464d-wf2jr from sonobuoy started at 2020-10-26 18:20:51 +0000 UTC (2 container statuses recorded)
Oct 26 19:46:53.560: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Oct 26 19:46:53.560: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 26 19:46:53.560: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-10-26 17:06:05 +0000 UTC (3 container statuses recorded)
Oct 26 19:46:53.560: INFO: 	Container alertmanager ready: true, restart count 0
Oct 26 19:46:53.560: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 26 19:46:53.560: INFO: 	Container config-reloader ready: true, restart count 0
Oct 26 19:46:53.560: INFO: multus-admission-controller-f46wp from openshift-multus started at 2020-10-26 16:58:27 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.560: INFO: 	Container multus-admission-controller ready: true, restart count 0
Oct 26 19:46:53.560: INFO: ibm-cloud-provider-ip-149-81-128-30-5bdf48d794-9qftf from ibm-system started at 2020-10-26 18:42:09 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.560: INFO: 	Container ibm-cloud-provider-ip-149-81-128-30 ready: true, restart count 0
Oct 26 19:46:53.560: INFO: router-default-56c7ff9d54-tz4cl from openshift-ingress started at 2020-10-26 17:00:10 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.561: INFO: 	Container router ready: true, restart count 0
Oct 26 19:46:53.561: INFO: redhat-operators-688fdf87f8-nfrk9 from openshift-marketplace started at 2020-10-26 17:00:16 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.561: INFO: 	Container redhat-operators ready: true, restart count 0
Oct 26 19:46:53.561: INFO: registry-pvc-permissions-9fhwg from openshift-image-registry started at 2020-10-26 17:01:54 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.561: INFO: 	Container pvc-permissions ready: false, restart count 0
Oct 26 19:46:53.561: INFO: prometheus-adapter-6cccbf8dbb-97f5w from openshift-monitoring started at 2020-10-26 17:05:48 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.561: INFO: 	Container prometheus-adapter ready: true, restart count 0
Oct 26 19:46:53.561: INFO: 
Logging pods the kubelet thinks is on node 10.123.240.178 before test
Oct 26 19:46:53.635: INFO: openshift-state-metrics-6888cfb99c-snkdc from openshift-monitoring started at 2020-10-26 18:42:09 +0000 UTC (3 container statuses recorded)
Oct 26 19:46:53.635: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Oct 26 19:46:53.635: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Oct 26 19:46:53.635: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Oct 26 19:46:53.635: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-10-26 18:42:24 +0000 UTC (3 container statuses recorded)
Oct 26 19:46:53.635: INFO: 	Container alertmanager ready: true, restart count 0
Oct 26 19:46:53.635: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 26 19:46:53.635: INFO: 	Container config-reloader ready: true, restart count 0
Oct 26 19:46:53.635: INFO: sonobuoy-e2e-job-6258e003556947b6 from sonobuoy started at 2020-10-26 18:20:50 +0000 UTC (2 container statuses recorded)
Oct 26 19:46:53.635: INFO: 	Container e2e ready: true, restart count 0
Oct 26 19:46:53.635: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 26 19:46:53.636: INFO: console-77979dd75-sj7zj from openshift-console started at 2020-10-26 18:42:09 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.636: INFO: 	Container console ready: true, restart count 0
Oct 26 19:46:53.636: INFO: sonobuoy-systemd-logs-daemon-set-a94f7e2648a3464d-lj6zg from sonobuoy started at 2020-10-26 18:20:51 +0000 UTC (2 container statuses recorded)
Oct 26 19:46:53.636: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Oct 26 19:46:53.636: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 26 19:46:53.636: INFO: thanos-querier-85d68cbb66-lgj99 from openshift-monitoring started at 2020-10-26 18:42:09 +0000 UTC (4 container statuses recorded)
Oct 26 19:46:53.636: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:46:53.636: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 26 19:46:53.636: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 26 19:46:53.636: INFO: 	Container thanos-querier ready: true, restart count 0
Oct 26 19:46:53.636: INFO: multus-nht6l from openshift-multus started at 2020-10-26 16:57:47 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.636: INFO: 	Container kube-multus ready: true, restart count 0
Oct 26 19:46:53.636: INFO: calico-typha-6b7867b64d-bjwpm from calico-system started at 2020-10-26 17:00:10 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.636: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 19:46:53.636: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-10-26 18:42:54 +0000 UTC (7 container statuses recorded)
Oct 26 19:46:53.636: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:46:53.636: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 26 19:46:53.636: INFO: 	Container prometheus ready: true, restart count 1
Oct 26 19:46:53.636: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Oct 26 19:46:53.636: INFO: 	Container prometheus-proxy ready: true, restart count 0
Oct 26 19:46:53.636: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Oct 26 19:46:53.636: INFO: 	Container thanos-sidecar ready: true, restart count 0
Oct 26 19:46:53.636: INFO: ibm-keepalived-watcher-zr4ms from kube-system started at 2020-10-26 16:57:47 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.636: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 19:46:53.636: INFO: openshift-kube-proxy-v9jgr from openshift-kube-proxy started at 2020-10-26 16:57:47 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.636: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 26 19:46:53.636: INFO: tuned-2ksq9 from openshift-cluster-node-tuning-operator started at 2020-10-26 17:00:14 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.636: INFO: 	Container tuned ready: true, restart count 0
Oct 26 19:46:53.636: INFO: node-exporter-vw6ds from openshift-monitoring started at 2020-10-26 16:59:04 +0000 UTC (2 container statuses recorded)
Oct 26 19:46:53.636: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:46:53.636: INFO: 	Container node-exporter ready: true, restart count 0
Oct 26 19:46:53.636: INFO: dns-default-qn8jr from openshift-dns started at 2020-10-26 16:59:44 +0000 UTC (2 container statuses recorded)
Oct 26 19:46:53.636: INFO: 	Container dns ready: true, restart count 0
Oct 26 19:46:53.636: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 26 19:46:53.636: INFO: multus-admission-controller-qxdxf from openshift-multus started at 2020-10-26 18:42:44 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.636: INFO: 	Container multus-admission-controller ready: true, restart count 0
Oct 26 19:46:53.636: INFO: ibm-master-proxy-static-10.123.240.178 from kube-system started at 2020-10-26 16:57:44 +0000 UTC (2 container statuses recorded)
Oct 26 19:46:53.636: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 19:46:53.636: INFO: 	Container pause ready: true, restart count 0
Oct 26 19:46:53.636: INFO: calico-node-d227t from calico-system started at 2020-10-26 16:58:05 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.636: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 19:46:53.636: INFO: node-ca-hsxg7 from openshift-image-registry started at 2020-10-26 16:59:40 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.636: INFO: 	Container node-ca ready: true, restart count 0
Oct 26 19:46:53.636: INFO: packageserver-d5dbff54b-q2kg6 from openshift-operator-lifecycle-manager started at 2020-10-26 18:42:16 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.636: INFO: 	Container packageserver ready: true, restart count 0
Oct 26 19:46:53.636: INFO: ibmcloud-block-storage-driver-2pnvt from kube-system started at 2020-10-26 16:57:55 +0000 UTC (1 container statuses recorded)
Oct 26 19:46:53.636: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1641a2802ffe69c7], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1641a28031a0cd67], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:46:54.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6572" for this suite.
Oct 26 19:47:02.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:47:04.822: INFO: namespace sched-pred-6572 deletion completed in 10.009770128s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:11.763 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:47:04.822: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-9051
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9051 to expose endpoints map[]
Oct 26 19:47:05.024: INFO: Get endpoints failed (10.665806ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Oct 26 19:47:06.040: INFO: successfully validated that service multi-endpoint-test in namespace services-9051 exposes endpoints map[] (1.025917395s elapsed)
STEP: Creating pod pod1 in namespace services-9051
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9051 to expose endpoints map[pod1:[100]]
Oct 26 19:47:09.202: INFO: successfully validated that service multi-endpoint-test in namespace services-9051 exposes endpoints map[pod1:[100]] (3.117883264s elapsed)
STEP: Creating pod pod2 in namespace services-9051
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9051 to expose endpoints map[pod1:[100] pod2:[101]]
Oct 26 19:47:11.337: INFO: successfully validated that service multi-endpoint-test in namespace services-9051 exposes endpoints map[pod1:[100] pod2:[101]] (2.101646171s elapsed)
STEP: Deleting pod pod1 in namespace services-9051
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9051 to expose endpoints map[pod2:[101]]
Oct 26 19:47:12.409: INFO: successfully validated that service multi-endpoint-test in namespace services-9051 exposes endpoints map[pod2:[101]] (1.053929234s elapsed)
STEP: Deleting pod pod2 in namespace services-9051
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9051 to expose endpoints map[]
Oct 26 19:47:13.454: INFO: successfully validated that service multi-endpoint-test in namespace services-9051 exposes endpoints map[] (1.025209277s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:47:13.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9051" for this suite.
Oct 26 19:47:27.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:47:29.533: INFO: namespace services-9051 deletion completed in 15.998991714s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:24.711 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:47:29.534: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 19:47:29.689: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-52b09d22-fc8e-446d-adbd-ad4a572dca49
STEP: Creating configMap with name cm-test-opt-upd-27ba2bdb-0981-4bff-b68a-b32b1580170f
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-52b09d22-fc8e-446d-adbd-ad4a572dca49
STEP: Updating configmap cm-test-opt-upd-27ba2bdb-0981-4bff-b68a-b32b1580170f
STEP: Creating configMap with name cm-test-opt-create-e70c7dae-81cd-4f0b-98dc-7b3ac97dd6fd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:48:51.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7052" for this suite.
Oct 26 19:49:05.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:49:07.273: INFO: namespace projected-7052 deletion completed in 16.078832201s

• [SLOW TEST:97.740 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:49:07.273: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-5825
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct 26 19:49:07.409: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Oct 26 19:49:29.821: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.37.221:8080/dial?request=hostName&protocol=udp&host=172.30.84.44&port=8081&tries=1'] Namespace:pod-network-test-5825 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 19:49:29.821: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 19:49:30.067: INFO: Waiting for endpoints: map[]
Oct 26 19:49:30.077: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.37.221:8080/dial?request=hostName&protocol=udp&host=172.30.37.220&port=8081&tries=1'] Namespace:pod-network-test-5825 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 19:49:30.077: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 19:49:30.292: INFO: Waiting for endpoints: map[]
Oct 26 19:49:30.303: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.37.221:8080/dial?request=hostName&protocol=udp&host=172.30.101.202&port=8081&tries=1'] Namespace:pod-network-test-5825 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 19:49:30.303: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 19:49:30.541: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:49:30.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5825" for this suite.
Oct 26 19:49:38.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:49:40.638: INFO: namespace pod-network-test-5825 deletion completed in 10.077389829s

• [SLOW TEST:33.365 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:49:40.638: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Oct 26 19:49:40.790: INFO: PodSpec: initContainers in spec.initContainers
Oct 26 19:50:28.282: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-2556477f-a0e6-4c9f-92cd-44b0b847679a", GenerateName:"", Namespace:"init-container-6448", SelfLink:"/api/v1/namespaces/init-container-6448/pods/pod-init-2556477f-a0e6-4c9f-92cd-44b0b847679a", UID:"e6a40a1f-ddfe-43f5-8138-797e737300d8", ResourceVersion:"75185", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63739338580, loc:(*time.Location)(0x84c02a0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"790109879"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.30.37.222/32", "cni.projectcalico.org/podIPs":"172.30.37.222/32", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.37.222\"\n    ],\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-l9z47", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001ed2100), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-l9z47", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00a8160f0), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-l9z47", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00a816190), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-l9z47", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00a816050), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00c5400e8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.123.240.178", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc005c9a000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00c5401a0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00c5401c0)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00c5401dc), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00c5401e0), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739338580, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739338580, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739338580, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739338580, loc:(*time.Location)(0x84c02a0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.123.240.178", PodIP:"172.30.37.222", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.37.222"}}, StartTime:(*v1.Time)(0xc002c680c0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003546230)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0035462a0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"cri-o://1baf839cb37f6c587afe57bc0872a05af4419964b61f30c64c1b0a2fbeb95dc7", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002c68100), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002c680e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc00c540254)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:50:28.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6448" for this suite.
Oct 26 19:51:00.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:51:02.521: INFO: namespace init-container-6448 deletion completed in 34.212412088s

• [SLOW TEST:81.883 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:51:02.521: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Oct 26 19:51:06.956: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct 26 19:51:06.974: INFO: Pod pod-with-poststart-exec-hook still exists
Oct 26 19:51:08.974: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct 26 19:51:08.991: INFO: Pod pod-with-poststart-exec-hook still exists
Oct 26 19:51:10.974: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct 26 19:51:10.987: INFO: Pod pod-with-poststart-exec-hook still exists
Oct 26 19:51:12.974: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct 26 19:51:12.986: INFO: Pod pod-with-poststart-exec-hook still exists
Oct 26 19:51:14.974: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct 26 19:51:14.988: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:51:14.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4700" for this suite.
Oct 26 19:51:29.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:51:31.125: INFO: namespace container-lifecycle-hook-4700 deletion completed in 16.090070263s

• [SLOW TEST:28.603 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:51:31.125: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Oct 26 19:51:39.471: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct 26 19:51:39.483: INFO: Pod pod-with-prestop-http-hook still exists
Oct 26 19:51:41.484: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct 26 19:51:41.498: INFO: Pod pod-with-prestop-http-hook still exists
Oct 26 19:51:43.484: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct 26 19:51:43.496: INFO: Pod pod-with-prestop-http-hook still exists
Oct 26 19:51:45.484: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct 26 19:51:45.504: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:51:45.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8603" for this suite.
Oct 26 19:52:17.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:52:19.664: INFO: namespace container-lifecycle-hook-8603 deletion completed in 34.098948646s

• [SLOW TEST:48.540 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:52:19.665: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Oct 26 19:52:24.020: INFO: &Pod{ObjectMeta:{send-events-c6916890-b9c6-4ff2-8ada-5e6dad8d86fc  events-830 /api/v1/namespaces/events-830/pods/send-events-c6916890-b9c6-4ff2-8ada-5e6dad8d86fc 8bb4d3a7-29b1-40e6-8699-b199c29e238f 75889 0 2020-10-26 19:52:19 +0000 UTC <nil> <nil> map[name:foo time:920865758] map[cni.projectcalico.org/podIP:172.30.37.228/32 cni.projectcalico.org/podIPs:172.30.37.228/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.37.228"
    ],
    "dns": {}
}] openshift.io/scc:anyuid] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-txdxt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-txdxt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-txdxt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.240.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c56,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:52:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:52:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:52:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:52:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.240.178,PodIP:172.30.37.228,StartTime:2020-10-26 19:52:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-26 19:52:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:cri-o://584f1337a829485005e4fd20ddb6b354e2fd006b13599df2c079103307652d0f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.37.228,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Oct 26 19:52:26.035: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Oct 26 19:52:28.048: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:52:28.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-830" for this suite.
Oct 26 19:53:02.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:53:04.114: INFO: namespace events-830 deletion completed in 36.017835638s

• [SLOW TEST:44.450 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:53:04.116: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 19:53:04.821: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 19:53:06.858: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739338784, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739338784, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739338784, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739338784, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 19:53:09.902: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:53:22.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5599" for this suite.
Oct 26 19:53:30.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:53:32.467: INFO: namespace webhook-5599 deletion completed in 10.087594577s
STEP: Destroying namespace "webhook-5599-markers" for this suite.
Oct 26 19:53:40.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:53:42.540: INFO: namespace webhook-5599-markers deletion completed in 10.072966799s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:38.480 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:53:42.597: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-ea1d2c5e-990d-4ab9-a1a6-2cdabba2edd8
STEP: Creating a pod to test consume configMaps
Oct 26 19:53:42.792: INFO: Waiting up to 5m0s for pod "pod-configmaps-f416d0fd-8ff7-496d-9c0b-2f84c142b82c" in namespace "configmap-7192" to be "success or failure"
Oct 26 19:53:42.802: INFO: Pod "pod-configmaps-f416d0fd-8ff7-496d-9c0b-2f84c142b82c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.523738ms
Oct 26 19:53:44.821: INFO: Pod "pod-configmaps-f416d0fd-8ff7-496d-9c0b-2f84c142b82c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028600731s
Oct 26 19:53:46.832: INFO: Pod "pod-configmaps-f416d0fd-8ff7-496d-9c0b-2f84c142b82c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039595555s
STEP: Saw pod success
Oct 26 19:53:46.833: INFO: Pod "pod-configmaps-f416d0fd-8ff7-496d-9c0b-2f84c142b82c" satisfied condition "success or failure"
Oct 26 19:53:46.843: INFO: Trying to get logs from node 10.123.240.178 pod pod-configmaps-f416d0fd-8ff7-496d-9c0b-2f84c142b82c container configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 19:53:46.935: INFO: Waiting for pod pod-configmaps-f416d0fd-8ff7-496d-9c0b-2f84c142b82c to disappear
Oct 26 19:53:46.944: INFO: Pod pod-configmaps-f416d0fd-8ff7-496d-9c0b-2f84c142b82c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:53:46.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7192" for this suite.
Oct 26 19:53:55.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:53:57.162: INFO: namespace configmap-7192 deletion completed in 10.187867519s

• [SLOW TEST:14.565 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:53:57.162: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Oct 26 19:53:57.381: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2a44285a-183a-43ad-af5c-7ae6b21b5765" in namespace "downward-api-8955" to be "success or failure"
Oct 26 19:53:57.394: INFO: Pod "downwardapi-volume-2a44285a-183a-43ad-af5c-7ae6b21b5765": Phase="Pending", Reason="", readiness=false. Elapsed: 11.960131ms
Oct 26 19:53:59.406: INFO: Pod "downwardapi-volume-2a44285a-183a-43ad-af5c-7ae6b21b5765": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024452166s
STEP: Saw pod success
Oct 26 19:53:59.406: INFO: Pod "downwardapi-volume-2a44285a-183a-43ad-af5c-7ae6b21b5765" satisfied condition "success or failure"
Oct 26 19:53:59.417: INFO: Trying to get logs from node 10.123.240.178 pod downwardapi-volume-2a44285a-183a-43ad-af5c-7ae6b21b5765 container client-container: <nil>
STEP: delete the pod
Oct 26 19:53:59.484: INFO: Waiting for pod downwardapi-volume-2a44285a-183a-43ad-af5c-7ae6b21b5765 to disappear
Oct 26 19:53:59.496: INFO: Pod downwardapi-volume-2a44285a-183a-43ad-af5c-7ae6b21b5765 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:53:59.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8955" for this suite.
Oct 26 19:54:07.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:54:09.592: INFO: namespace downward-api-8955 deletion completed in 10.059810573s

• [SLOW TEST:12.430 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:54:09.593: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 19:54:09.722: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Oct 26 19:54:09.756: INFO: Pod name sample-pod: Found 0 pods out of 1
Oct 26 19:54:14.770: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Oct 26 19:54:14.770: INFO: Creating deployment "test-rolling-update-deployment"
Oct 26 19:54:14.787: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Oct 26 19:54:14.807: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Oct 26 19:54:16.834: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Oct 26 19:54:16.844: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739338854, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739338854, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739338854, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739338854, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 19:54:18.856: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Oct 26 19:54:18.889: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6417 /apis/apps/v1/namespaces/deployment-6417/deployments/test-rolling-update-deployment f362f15a-fce4-45bc-ab7f-62c3af51a277 76779 1 2020-10-26 19:54:14 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00412ef88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-10-26 19:54:14 +0000 UTC,LastTransitionTime:2020-10-26 19:54:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2020-10-26 19:54:17 +0000 UTC,LastTransitionTime:2020-10-26 19:54:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Oct 26 19:54:18.900: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-6417 /apis/apps/v1/namespaces/deployment-6417/replicasets/test-rolling-update-deployment-55d946486 9a39c864-f36b-42a3-a20c-6a6d55d4166d 76770 1 2020-10-26 19:54:14 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment f362f15a-fce4-45bc-ab7f-62c3af51a277 0xc0095c2620 0xc0095c2621}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0095c2688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Oct 26 19:54:18.900: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Oct 26 19:54:18.900: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6417 /apis/apps/v1/namespaces/deployment-6417/replicasets/test-rolling-update-controller 2fc9037c-6060-40dd-a9d4-6268965b1d46 76778 2 2020-10-26 19:54:09 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment f362f15a-fce4-45bc-ab7f-62c3af51a277 0xc0095c2557 0xc0095c2558}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0095c25b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 26 19:54:18.912: INFO: Pod "test-rolling-update-deployment-55d946486-l8jjf" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-l8jjf test-rolling-update-deployment-55d946486- deployment-6417 /api/v1/namespaces/deployment-6417/pods/test-rolling-update-deployment-55d946486-l8jjf 3cf78f93-95e6-4fa1-8313-74b7f34e427e 76769 0 2020-10-26 19:54:14 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[cni.projectcalico.org/podIP:172.30.37.231/32 cni.projectcalico.org/podIPs:172.30.37.231/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.37.231"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 9a39c864-f36b-42a3-a20c-6a6d55d4166d 0xc0095c2b40 0xc0095c2b41}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2kwc8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2kwc8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2kwc8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.240.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b5cjd,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:54:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:54:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:54:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 19:54:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.240.178,PodIP:172.30.37.231,StartTime:2020-10-26 19:54:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-26 19:54:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://fd44df8656368586bef52bd8dd52301cd9976d11c8f95bab1ff2643453b0a1e9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.37.231,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:54:18.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6417" for this suite.
Oct 26 19:54:26.979: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:54:28.991: INFO: namespace deployment-6417 deletion completed in 10.055801318s

• [SLOW TEST:19.399 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:54:28.992: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 19:54:29.244: INFO: (0) /api/v1/nodes/10.123.240.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 57.941618ms)
Oct 26 19:54:29.264: INFO: (1) /api/v1/nodes/10.123.240.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.295618ms)
Oct 26 19:54:29.285: INFO: (2) /api/v1/nodes/10.123.240.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.614859ms)
Oct 26 19:54:29.304: INFO: (3) /api/v1/nodes/10.123.240.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 19.087244ms)
Oct 26 19:54:29.325: INFO: (4) /api/v1/nodes/10.123.240.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.803867ms)
Oct 26 19:54:29.346: INFO: (5) /api/v1/nodes/10.123.240.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 21.541249ms)
Oct 26 19:54:29.366: INFO: (6) /api/v1/nodes/10.123.240.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 19.645452ms)
Oct 26 19:54:29.386: INFO: (7) /api/v1/nodes/10.123.240.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 19.59069ms)
Oct 26 19:54:29.420: INFO: (8) /api/v1/nodes/10.123.240.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 34.149964ms)
Oct 26 19:54:29.445: INFO: (9) /api/v1/nodes/10.123.240.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 24.681812ms)
Oct 26 19:54:29.472: INFO: (10) /api/v1/nodes/10.123.240.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 26.556783ms)
Oct 26 19:54:29.490: INFO: (11) /api/v1/nodes/10.123.240.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 18.337979ms)
Oct 26 19:54:29.511: INFO: (12) /api/v1/nodes/10.123.240.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 21.053532ms)
Oct 26 19:54:29.530: INFO: (13) /api/v1/nodes/10.123.240.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 18.640624ms)
Oct 26 19:54:29.552: INFO: (14) /api/v1/nodes/10.123.240.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 21.700809ms)
Oct 26 19:54:29.572: INFO: (15) /api/v1/nodes/10.123.240.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 19.40775ms)
Oct 26 19:54:29.588: INFO: (16) /api/v1/nodes/10.123.240.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 16.210992ms)
Oct 26 19:54:29.606: INFO: (17) /api/v1/nodes/10.123.240.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 18.056646ms)
Oct 26 19:54:29.623: INFO: (18) /api/v1/nodes/10.123.240.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 17.026493ms)
Oct 26 19:54:29.642: INFO: (19) /api/v1/nodes/10.123.240.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 19.176299ms)
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:54:29.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5041" for this suite.
Oct 26 19:54:37.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:54:38.746: INFO: namespace proxy-5041 deletion completed in 9.083602654s

• [SLOW TEST:9.755 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:54:38.747: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4673.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4673.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4673.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4673.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4673.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4673.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 26 19:54:43.186: INFO: DNS probes using dns-4673/dns-test-a0a8cd3c-a26c-4463-a910-bf865f10c393 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:54:43.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4673" for this suite.
Oct 26 19:54:51.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:54:53.242: INFO: namespace dns-4673 deletion completed in 9.986098115s

• [SLOW TEST:14.495 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:54:53.243: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-40df47f5-b9b8-4355-b8b6-831495b41dac
STEP: Creating a pod to test consume secrets
Oct 26 19:54:53.456: INFO: Waiting up to 5m0s for pod "pod-secrets-8606c9d8-169e-4b9f-945c-d4b13a27c8e5" in namespace "secrets-9258" to be "success or failure"
Oct 26 19:54:53.465: INFO: Pod "pod-secrets-8606c9d8-169e-4b9f-945c-d4b13a27c8e5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.372653ms
Oct 26 19:54:55.475: INFO: Pod "pod-secrets-8606c9d8-169e-4b9f-945c-d4b13a27c8e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019115939s
STEP: Saw pod success
Oct 26 19:54:55.475: INFO: Pod "pod-secrets-8606c9d8-169e-4b9f-945c-d4b13a27c8e5" satisfied condition "success or failure"
Oct 26 19:54:55.485: INFO: Trying to get logs from node 10.123.240.178 pod pod-secrets-8606c9d8-169e-4b9f-945c-d4b13a27c8e5 container secret-volume-test: <nil>
STEP: delete the pod
Oct 26 19:54:55.545: INFO: Waiting for pod pod-secrets-8606c9d8-169e-4b9f-945c-d4b13a27c8e5 to disappear
Oct 26 19:54:55.556: INFO: Pod pod-secrets-8606c9d8-169e-4b9f-945c-d4b13a27c8e5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:54:55.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9258" for this suite.
Oct 26 19:55:03.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:55:05.648: INFO: namespace secrets-9258 deletion completed in 10.069688894s

• [SLOW TEST:12.404 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:55:05.648: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:55:05.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5341" for this suite.
Oct 26 19:55:13.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:55:15.956: INFO: namespace kubelet-test-5341 deletion completed in 10.022879242s

• [SLOW TEST:10.308 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:55:15.956: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Oct 26 19:55:16.154: INFO: Waiting up to 5m0s for pod "downward-api-08e4011f-07fb-48c6-8143-0b7ca22a8a68" in namespace "downward-api-6401" to be "success or failure"
Oct 26 19:55:16.163: INFO: Pod "downward-api-08e4011f-07fb-48c6-8143-0b7ca22a8a68": Phase="Pending", Reason="", readiness=false. Elapsed: 9.285114ms
Oct 26 19:55:18.174: INFO: Pod "downward-api-08e4011f-07fb-48c6-8143-0b7ca22a8a68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020479183s
Oct 26 19:55:20.188: INFO: Pod "downward-api-08e4011f-07fb-48c6-8143-0b7ca22a8a68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034001014s
STEP: Saw pod success
Oct 26 19:55:20.188: INFO: Pod "downward-api-08e4011f-07fb-48c6-8143-0b7ca22a8a68" satisfied condition "success or failure"
Oct 26 19:55:20.198: INFO: Trying to get logs from node 10.123.240.178 pod downward-api-08e4011f-07fb-48c6-8143-0b7ca22a8a68 container dapi-container: <nil>
STEP: delete the pod
Oct 26 19:55:20.255: INFO: Waiting for pod downward-api-08e4011f-07fb-48c6-8143-0b7ca22a8a68 to disappear
Oct 26 19:55:20.266: INFO: Pod downward-api-08e4011f-07fb-48c6-8143-0b7ca22a8a68 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:55:20.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6401" for this suite.
Oct 26 19:55:28.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:55:30.374: INFO: namespace downward-api-6401 deletion completed in 10.087811815s

• [SLOW TEST:14.418 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:55:30.374: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Oct 26 19:55:30.546: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Oct 26 19:55:30.623: INFO: Waiting for terminating namespaces to be deleted...
Oct 26 19:55:30.644: INFO: 
Logging pods the kubelet thinks is on node 10.123.240.172 before test
Oct 26 19:55:30.734: INFO: dns-default-qnchn from openshift-dns started at 2020-10-26 16:59:44 +0000 UTC (2 container statuses recorded)
Oct 26 19:55:30.734: INFO: 	Container dns ready: true, restart count 0
Oct 26 19:55:30.734: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 26 19:55:30.734: INFO: router-default-56c7ff9d54-2kbrj from openshift-ingress started at 2020-10-26 18:42:09 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.734: INFO: 	Container router ready: true, restart count 0
Oct 26 19:55:30.734: INFO: catalog-operator-7bf86b4f96-kpnmt from openshift-operator-lifecycle-manager started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.734: INFO: 	Container catalog-operator ready: true, restart count 0
Oct 26 19:55:30.734: INFO: ibm-file-plugin-6b86cbfbc6-vw94d from kube-system started at 2020-10-26 16:58:23 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.734: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Oct 26 19:55:30.734: INFO: cluster-samples-operator-644946789c-zw8th from openshift-cluster-samples-operator started at 2020-10-26 17:01:55 +0000 UTC (2 container statuses recorded)
Oct 26 19:55:30.734: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Oct 26 19:55:30.734: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Oct 26 19:55:30.734: INFO: cluster-storage-operator-6c6dd4b587-52s64 from openshift-cluster-storage-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.734: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Oct 26 19:55:30.734: INFO: openshift-service-catalog-apiserver-operator-d6cf765ff-k9snw from openshift-service-catalog-apiserver-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.734: INFO: 	Container operator ready: true, restart count 1
Oct 26 19:55:30.734: INFO: sonobuoy from sonobuoy started at 2020-10-26 18:20:41 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.734: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Oct 26 19:55:30.734: INFO: multus-admission-controller-4dblm from openshift-multus started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.734: INFO: 	Container multus-admission-controller ready: true, restart count 0
Oct 26 19:55:30.734: INFO: prometheus-adapter-6cccbf8dbb-4bpjw from openshift-monitoring started at 2020-10-26 17:05:48 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.734: INFO: 	Container prometheus-adapter ready: true, restart count 0
Oct 26 19:55:30.734: INFO: ibmcloud-block-storage-driver-fm8d2 from kube-system started at 2020-10-26 16:56:55 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.734: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Oct 26 19:55:30.734: INFO: console-operator-7f9f78ff66-kf42x from openshift-console-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.734: INFO: 	Container console-operator ready: true, restart count 1
Oct 26 19:55:30.734: INFO: calico-kube-controllers-599969f895-cnxrs from calico-system started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.734: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Oct 26 19:55:30.734: INFO: cluster-node-tuning-operator-86b7f98f7b-qjdxp from openshift-cluster-node-tuning-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.734: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Oct 26 19:55:30.734: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-10-26 17:05:50 +0000 UTC (3 container statuses recorded)
Oct 26 19:55:30.734: INFO: 	Container alertmanager ready: true, restart count 0
Oct 26 19:55:30.734: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 26 19:55:30.734: INFO: 	Container config-reloader ready: true, restart count 0
Oct 26 19:55:30.734: INFO: ibm-master-proxy-static-10.123.240.172 from kube-system started at 2020-10-26 16:56:50 +0000 UTC (2 container statuses recorded)
Oct 26 19:55:30.734: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 19:55:30.734: INFO: 	Container pause ready: true, restart count 0
Oct 26 19:55:30.735: INFO: multus-wk7tv from openshift-multus started at 2020-10-26 16:57:19 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.735: INFO: 	Container kube-multus ready: true, restart count 0
Oct 26 19:55:30.735: INFO: openshift-kube-proxy-k67gq from openshift-kube-proxy started at 2020-10-26 16:57:26 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.735: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 26 19:55:30.735: INFO: cluster-monitoring-operator-77bbbf9cb7-sd49n from openshift-monitoring started at 2020-10-26 16:58:24 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.735: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Oct 26 19:55:30.735: INFO: node-exporter-pbqtq from openshift-monitoring started at 2020-10-26 16:59:03 +0000 UTC (2 container statuses recorded)
Oct 26 19:55:30.735: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:55:30.735: INFO: 	Container node-exporter ready: true, restart count 0
Oct 26 19:55:30.735: INFO: calico-typha-6b7867b64d-wwhp2 from calico-system started at 2020-10-26 17:00:10 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.735: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 19:55:30.735: INFO: ibm-keepalived-watcher-rdht2 from kube-system started at 2020-10-26 16:56:52 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.735: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 19:55:30.735: INFO: marketplace-operator-c74f66688-865bs from openshift-marketplace started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.735: INFO: 	Container marketplace-operator ready: true, restart count 0
Oct 26 19:55:30.735: INFO: openshift-service-catalog-controller-manager-operator-6fcbsks6s from openshift-service-catalog-controller-manager-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.735: INFO: 	Container operator ready: true, restart count 1
Oct 26 19:55:30.735: INFO: tuned-f8xvt from openshift-cluster-node-tuning-operator started at 2020-10-26 17:00:14 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.735: INFO: 	Container tuned ready: true, restart count 0
Oct 26 19:55:30.735: INFO: ibm-cloud-provider-ip-149-81-128-30-5bdf48d794-vdds8 from ibm-system started at 2020-10-26 17:10:29 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.735: INFO: 	Container ibm-cloud-provider-ip-149-81-128-30 ready: true, restart count 0
Oct 26 19:55:30.735: INFO: packageserver-d5dbff54b-xqj44 from openshift-operator-lifecycle-manager started at 2020-10-26 18:42:11 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.735: INFO: 	Container packageserver ready: true, restart count 0
Oct 26 19:55:30.735: INFO: downloads-56f66db77f-f9rtp from openshift-console started at 2020-10-26 16:58:23 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.735: INFO: 	Container download-server ready: true, restart count 0
Oct 26 19:55:30.735: INFO: sonobuoy-systemd-logs-daemon-set-a94f7e2648a3464d-9nhrv from sonobuoy started at 2020-10-26 18:20:50 +0000 UTC (2 container statuses recorded)
Oct 26 19:55:30.735: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Oct 26 19:55:30.735: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 26 19:55:30.735: INFO: telemeter-client-7c4c649567-vp6j2 from openshift-monitoring started at 2020-10-26 18:42:09 +0000 UTC (3 container statuses recorded)
Oct 26 19:55:30.735: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:55:30.735: INFO: 	Container reload ready: true, restart count 0
Oct 26 19:55:30.735: INFO: 	Container telemeter-client ready: true, restart count 0
Oct 26 19:55:30.735: INFO: calico-node-kchsj from calico-system started at 2020-10-26 16:58:05 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.735: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 19:55:30.735: INFO: olm-operator-cbd8cfd65-5hqds from openshift-operator-lifecycle-manager started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.735: INFO: 	Container olm-operator ready: true, restart count 0
Oct 26 19:55:30.735: INFO: service-ca-operator-54f4b4db4-wvhxp from openshift-service-ca-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.735: INFO: 	Container operator ready: true, restart count 0
Oct 26 19:55:30.735: INFO: cluster-image-registry-operator-854d785579-78wmp from openshift-image-registry started at 2020-10-26 16:58:22 +0000 UTC (2 container statuses recorded)
Oct 26 19:55:30.735: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Oct 26 19:55:30.735: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Oct 26 19:55:30.735: INFO: downloads-56f66db77f-qk97h from openshift-console started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.735: INFO: 	Container download-server ready: true, restart count 0
Oct 26 19:55:30.735: INFO: ibmcloud-block-storage-plugin-79495594d5-7m8wf from kube-system started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.735: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Oct 26 19:55:30.735: INFO: ibm-storage-watcher-7fc85c5589-t79km from kube-system started at 2020-10-26 16:58:23 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.735: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Oct 26 19:55:30.735: INFO: ingress-operator-549d4c77b5-8mj9f from openshift-ingress-operator started at 2020-10-26 16:58:24 +0000 UTC (2 container statuses recorded)
Oct 26 19:55:30.735: INFO: 	Container ingress-operator ready: true, restart count 0
Oct 26 19:55:30.735: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:55:30.735: INFO: network-operator-5647cdbff6-qn2hq from openshift-network-operator started at 2020-10-26 16:56:55 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.735: INFO: 	Container network-operator ready: true, restart count 0
Oct 26 19:55:30.735: INFO: tigera-operator-798cfbf7dd-x8hf6 from tigera-operator started at 2020-10-26 16:56:55 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.735: INFO: 	Container tigera-operator ready: true, restart count 2
Oct 26 19:55:30.735: INFO: dns-operator-5bd9bd8fcd-gj6cw from openshift-dns-operator started at 2020-10-26 16:58:22 +0000 UTC (2 container statuses recorded)
Oct 26 19:55:30.735: INFO: 	Container dns-operator ready: true, restart count 0
Oct 26 19:55:30.735: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:55:30.735: INFO: node-ca-rhts4 from openshift-image-registry started at 2020-10-26 16:59:40 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.735: INFO: 	Container node-ca ready: true, restart count 0
Oct 26 19:55:30.735: INFO: 
Logging pods the kubelet thinks is on node 10.123.240.174 before test
Oct 26 19:55:30.877: INFO: console-77979dd75-lnchz from openshift-console started at 2020-10-26 17:01:28 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.877: INFO: 	Container console ready: true, restart count 0
Oct 26 19:55:30.877: INFO: service-serving-cert-signer-6d656c4cf7-l9qz5 from openshift-service-ca started at 2020-10-26 16:59:01 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.877: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Oct 26 19:55:30.877: INFO: prometheus-operator-d5df96f57-bthbz from openshift-monitoring started at 2020-10-26 17:05:34 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container prometheus-operator ready: true, restart count 0
Oct 26 19:55:30.878: INFO: thanos-querier-85d68cbb66-5hkh8 from openshift-monitoring started at 2020-10-26 17:06:37 +0000 UTC (4 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:55:30.878: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 26 19:55:30.878: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 26 19:55:30.878: INFO: 	Container thanos-querier ready: true, restart count 0
Oct 26 19:55:30.878: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-10-26 17:06:52 +0000 UTC (7 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:55:30.878: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 26 19:55:30.878: INFO: 	Container prometheus ready: true, restart count 1
Oct 26 19:55:30.878: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Oct 26 19:55:30.878: INFO: 	Container prometheus-proxy ready: true, restart count 0
Oct 26 19:55:30.878: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Oct 26 19:55:30.878: INFO: 	Container thanos-sidecar ready: true, restart count 0
Oct 26 19:55:30.878: INFO: certified-operators-59fc9cd9b5-84rhs from openshift-marketplace started at 2020-10-26 18:00:44 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container certified-operators ready: true, restart count 0
Oct 26 19:55:30.878: INFO: ibmcloud-block-storage-driver-s88xs from kube-system started at 2020-10-26 16:57:41 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Oct 26 19:55:30.878: INFO: calico-typha-6b7867b64d-j6hnv from calico-system started at 2020-10-26 16:58:05 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 19:55:30.878: INFO: calico-node-jfjg2 from calico-system started at 2020-10-26 16:58:05 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 19:55:30.878: INFO: apiservice-cabundle-injector-566d6d6dc7-lv6bf from openshift-service-ca started at 2020-10-26 16:59:02 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Oct 26 19:55:30.878: INFO: node-ca-lbgdr from openshift-image-registry started at 2020-10-26 16:59:40 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container node-ca ready: true, restart count 0
Oct 26 19:55:30.878: INFO: grafana-f6757cb99-n9dgx from openshift-monitoring started at 2020-10-26 17:05:54 +0000 UTC (2 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container grafana ready: true, restart count 0
Oct 26 19:55:30.878: INFO: 	Container grafana-proxy ready: true, restart count 0
Oct 26 19:55:30.878: INFO: multus-wz2pl from openshift-multus started at 2020-10-26 16:57:37 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container kube-multus ready: true, restart count 0
Oct 26 19:55:30.878: INFO: ibm-keepalived-watcher-nxrtl from kube-system started at 2020-10-26 16:57:37 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 19:55:30.878: INFO: kube-state-metrics-6bb5fc9995-w9q9p from openshift-monitoring started at 2020-10-26 16:59:03 +0000 UTC (3 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Oct 26 19:55:30.878: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Oct 26 19:55:30.878: INFO: 	Container kube-state-metrics ready: true, restart count 0
Oct 26 19:55:30.878: INFO: tuned-nlq8g from openshift-cluster-node-tuning-operator started at 2020-10-26 17:00:14 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container tuned ready: true, restart count 0
Oct 26 19:55:30.878: INFO: sonobuoy-systemd-logs-daemon-set-a94f7e2648a3464d-wf2jr from sonobuoy started at 2020-10-26 18:20:51 +0000 UTC (2 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Oct 26 19:55:30.878: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 26 19:55:30.878: INFO: vpn-68d7d4d68d-clwcb from kube-system started at 2020-10-26 17:05:39 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container vpn ready: true, restart count 0
Oct 26 19:55:30.878: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-10-26 17:06:05 +0000 UTC (3 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container alertmanager ready: true, restart count 0
Oct 26 19:55:30.878: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 26 19:55:30.878: INFO: 	Container config-reloader ready: true, restart count 0
Oct 26 19:55:30.878: INFO: multus-admission-controller-f46wp from openshift-multus started at 2020-10-26 16:58:27 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container multus-admission-controller ready: true, restart count 0
Oct 26 19:55:30.878: INFO: ibm-cloud-provider-ip-149-81-128-30-5bdf48d794-9qftf from ibm-system started at 2020-10-26 18:42:09 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container ibm-cloud-provider-ip-149-81-128-30 ready: true, restart count 0
Oct 26 19:55:30.878: INFO: router-default-56c7ff9d54-tz4cl from openshift-ingress started at 2020-10-26 17:00:10 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container router ready: true, restart count 0
Oct 26 19:55:30.878: INFO: redhat-operators-688fdf87f8-nfrk9 from openshift-marketplace started at 2020-10-26 17:00:16 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container redhat-operators ready: true, restart count 0
Oct 26 19:55:30.878: INFO: registry-pvc-permissions-9fhwg from openshift-image-registry started at 2020-10-26 17:01:54 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container pvc-permissions ready: false, restart count 0
Oct 26 19:55:30.878: INFO: prometheus-adapter-6cccbf8dbb-97f5w from openshift-monitoring started at 2020-10-26 17:05:48 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container prometheus-adapter ready: true, restart count 0
Oct 26 19:55:30.878: INFO: openshift-kube-proxy-rs72s from openshift-kube-proxy started at 2020-10-26 16:57:37 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 26 19:55:30.878: INFO: dns-default-2hlsj from openshift-dns started at 2020-10-26 16:59:44 +0000 UTC (2 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container dns ready: true, restart count 0
Oct 26 19:55:30.878: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 26 19:55:30.878: INFO: community-operators-bd644f5c5-wb2l9 from openshift-marketplace started at 2020-10-26 17:00:16 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container community-operators ready: true, restart count 0
Oct 26 19:55:30.878: INFO: ibm-master-proxy-static-10.123.240.174 from kube-system started at 2020-10-26 16:57:35 +0000 UTC (2 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 19:55:30.878: INFO: 	Container pause ready: true, restart count 0
Oct 26 19:55:30.878: INFO: node-exporter-hpt5d from openshift-monitoring started at 2020-10-26 16:59:04 +0000 UTC (2 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:55:30.878: INFO: 	Container node-exporter ready: true, restart count 0
Oct 26 19:55:30.878: INFO: configmap-cabundle-injector-6676dbc567-zdqgt from openshift-service-ca started at 2020-10-26 16:59:02 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Oct 26 19:55:30.878: INFO: image-registry-57486bbffb-v7qmg from openshift-image-registry started at 2020-10-26 17:01:54 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.878: INFO: 	Container registry ready: true, restart count 0
Oct 26 19:55:30.878: INFO: 
Logging pods the kubelet thinks is on node 10.123.240.178 before test
Oct 26 19:55:30.935: INFO: calico-node-d227t from calico-system started at 2020-10-26 16:58:05 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.935: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 19:55:30.935: INFO: multus-admission-controller-qxdxf from openshift-multus started at 2020-10-26 18:42:44 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.935: INFO: 	Container multus-admission-controller ready: true, restart count 0
Oct 26 19:55:30.935: INFO: ibm-master-proxy-static-10.123.240.178 from kube-system started at 2020-10-26 16:57:44 +0000 UTC (2 container statuses recorded)
Oct 26 19:55:30.935: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 19:55:30.935: INFO: 	Container pause ready: true, restart count 0
Oct 26 19:55:30.935: INFO: node-ca-hsxg7 from openshift-image-registry started at 2020-10-26 16:59:40 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.935: INFO: 	Container node-ca ready: true, restart count 0
Oct 26 19:55:30.935: INFO: packageserver-d5dbff54b-q2kg6 from openshift-operator-lifecycle-manager started at 2020-10-26 18:42:16 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.935: INFO: 	Container packageserver ready: true, restart count 0
Oct 26 19:55:30.935: INFO: ibmcloud-block-storage-driver-2pnvt from kube-system started at 2020-10-26 16:57:55 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.935: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Oct 26 19:55:30.935: INFO: console-77979dd75-sj7zj from openshift-console started at 2020-10-26 18:42:09 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.935: INFO: 	Container console ready: true, restart count 0
Oct 26 19:55:30.935: INFO: openshift-state-metrics-6888cfb99c-snkdc from openshift-monitoring started at 2020-10-26 18:42:09 +0000 UTC (3 container statuses recorded)
Oct 26 19:55:30.935: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Oct 26 19:55:30.935: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Oct 26 19:55:30.935: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Oct 26 19:55:30.935: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-10-26 18:42:24 +0000 UTC (3 container statuses recorded)
Oct 26 19:55:30.935: INFO: 	Container alertmanager ready: true, restart count 0
Oct 26 19:55:30.935: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 26 19:55:30.935: INFO: 	Container config-reloader ready: true, restart count 0
Oct 26 19:55:30.935: INFO: sonobuoy-e2e-job-6258e003556947b6 from sonobuoy started at 2020-10-26 18:20:50 +0000 UTC (2 container statuses recorded)
Oct 26 19:55:30.935: INFO: 	Container e2e ready: true, restart count 0
Oct 26 19:55:30.935: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 26 19:55:30.935: INFO: sonobuoy-systemd-logs-daemon-set-a94f7e2648a3464d-lj6zg from sonobuoy started at 2020-10-26 18:20:51 +0000 UTC (2 container statuses recorded)
Oct 26 19:55:30.935: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Oct 26 19:55:30.935: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 26 19:55:30.935: INFO: thanos-querier-85d68cbb66-lgj99 from openshift-monitoring started at 2020-10-26 18:42:09 +0000 UTC (4 container statuses recorded)
Oct 26 19:55:30.935: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:55:30.935: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 26 19:55:30.935: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 26 19:55:30.935: INFO: 	Container thanos-querier ready: true, restart count 0
Oct 26 19:55:30.935: INFO: multus-nht6l from openshift-multus started at 2020-10-26 16:57:47 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.935: INFO: 	Container kube-multus ready: true, restart count 0
Oct 26 19:55:30.935: INFO: openshift-kube-proxy-v9jgr from openshift-kube-proxy started at 2020-10-26 16:57:47 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.935: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 26 19:55:30.935: INFO: calico-typha-6b7867b64d-bjwpm from calico-system started at 2020-10-26 17:00:10 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.936: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 19:55:30.936: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-10-26 18:42:54 +0000 UTC (7 container statuses recorded)
Oct 26 19:55:30.936: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:55:30.936: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 26 19:55:30.936: INFO: 	Container prometheus ready: true, restart count 1
Oct 26 19:55:30.936: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Oct 26 19:55:30.936: INFO: 	Container prometheus-proxy ready: true, restart count 0
Oct 26 19:55:30.936: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Oct 26 19:55:30.936: INFO: 	Container thanos-sidecar ready: true, restart count 0
Oct 26 19:55:30.936: INFO: ibm-keepalived-watcher-zr4ms from kube-system started at 2020-10-26 16:57:47 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.936: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 19:55:30.936: INFO: dns-default-qn8jr from openshift-dns started at 2020-10-26 16:59:44 +0000 UTC (2 container statuses recorded)
Oct 26 19:55:30.936: INFO: 	Container dns ready: true, restart count 0
Oct 26 19:55:30.936: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 26 19:55:30.936: INFO: tuned-2ksq9 from openshift-cluster-node-tuning-operator started at 2020-10-26 17:00:14 +0000 UTC (1 container statuses recorded)
Oct 26 19:55:30.936: INFO: 	Container tuned ready: true, restart count 0
Oct 26 19:55:30.936: INFO: node-exporter-vw6ds from openshift-monitoring started at 2020-10-26 16:59:04 +0000 UTC (2 container statuses recorded)
Oct 26 19:55:30.936: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:55:30.936: INFO: 	Container node-exporter ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node 10.123.240.172
STEP: verifying the node has the label node 10.123.240.174
STEP: verifying the node has the label node 10.123.240.178
Oct 26 19:55:31.148: INFO: Pod calico-kube-controllers-599969f895-cnxrs requesting resource cpu=0m on Node 10.123.240.172
Oct 26 19:55:31.148: INFO: Pod calico-node-d227t requesting resource cpu=0m on Node 10.123.240.178
Oct 26 19:55:31.148: INFO: Pod calico-node-jfjg2 requesting resource cpu=0m on Node 10.123.240.174
Oct 26 19:55:31.148: INFO: Pod calico-node-kchsj requesting resource cpu=0m on Node 10.123.240.172
Oct 26 19:55:31.148: INFO: Pod calico-typha-6b7867b64d-bjwpm requesting resource cpu=0m on Node 10.123.240.178
Oct 26 19:55:31.148: INFO: Pod calico-typha-6b7867b64d-j6hnv requesting resource cpu=0m on Node 10.123.240.174
Oct 26 19:55:31.148: INFO: Pod calico-typha-6b7867b64d-wwhp2 requesting resource cpu=0m on Node 10.123.240.172
Oct 26 19:55:31.148: INFO: Pod ibm-cloud-provider-ip-149-81-128-30-5bdf48d794-9qftf requesting resource cpu=5m on Node 10.123.240.174
Oct 26 19:55:31.148: INFO: Pod ibm-cloud-provider-ip-149-81-128-30-5bdf48d794-vdds8 requesting resource cpu=5m on Node 10.123.240.172
Oct 26 19:55:31.148: INFO: Pod ibm-file-plugin-6b86cbfbc6-vw94d requesting resource cpu=50m on Node 10.123.240.172
Oct 26 19:55:31.148: INFO: Pod ibm-keepalived-watcher-nxrtl requesting resource cpu=5m on Node 10.123.240.174
Oct 26 19:55:31.148: INFO: Pod ibm-keepalived-watcher-rdht2 requesting resource cpu=5m on Node 10.123.240.172
Oct 26 19:55:31.148: INFO: Pod ibm-keepalived-watcher-zr4ms requesting resource cpu=5m on Node 10.123.240.178
Oct 26 19:55:31.148: INFO: Pod ibm-master-proxy-static-10.123.240.172 requesting resource cpu=25m on Node 10.123.240.172
Oct 26 19:55:31.148: INFO: Pod ibm-master-proxy-static-10.123.240.174 requesting resource cpu=25m on Node 10.123.240.174
Oct 26 19:55:31.148: INFO: Pod ibm-master-proxy-static-10.123.240.178 requesting resource cpu=25m on Node 10.123.240.178
Oct 26 19:55:31.148: INFO: Pod ibm-storage-watcher-7fc85c5589-t79km requesting resource cpu=50m on Node 10.123.240.172
Oct 26 19:55:31.148: INFO: Pod ibmcloud-block-storage-driver-2pnvt requesting resource cpu=50m on Node 10.123.240.178
Oct 26 19:55:31.148: INFO: Pod ibmcloud-block-storage-driver-fm8d2 requesting resource cpu=50m on Node 10.123.240.172
Oct 26 19:55:31.148: INFO: Pod ibmcloud-block-storage-driver-s88xs requesting resource cpu=50m on Node 10.123.240.174
Oct 26 19:55:31.148: INFO: Pod ibmcloud-block-storage-plugin-79495594d5-7m8wf requesting resource cpu=50m on Node 10.123.240.172
Oct 26 19:55:31.148: INFO: Pod vpn-68d7d4d68d-clwcb requesting resource cpu=5m on Node 10.123.240.174
Oct 26 19:55:31.148: INFO: Pod cluster-node-tuning-operator-86b7f98f7b-qjdxp requesting resource cpu=10m on Node 10.123.240.172
Oct 26 19:55:31.148: INFO: Pod tuned-2ksq9 requesting resource cpu=10m on Node 10.123.240.178
Oct 26 19:55:31.148: INFO: Pod tuned-f8xvt requesting resource cpu=10m on Node 10.123.240.172
Oct 26 19:55:31.148: INFO: Pod tuned-nlq8g requesting resource cpu=10m on Node 10.123.240.174
Oct 26 19:55:31.148: INFO: Pod cluster-samples-operator-644946789c-zw8th requesting resource cpu=20m on Node 10.123.240.172
Oct 26 19:55:31.148: INFO: Pod cluster-storage-operator-6c6dd4b587-52s64 requesting resource cpu=10m on Node 10.123.240.172
Oct 26 19:55:31.148: INFO: Pod console-operator-7f9f78ff66-kf42x requesting resource cpu=10m on Node 10.123.240.172
Oct 26 19:55:31.148: INFO: Pod console-77979dd75-lnchz requesting resource cpu=10m on Node 10.123.240.174
Oct 26 19:55:31.148: INFO: Pod console-77979dd75-sj7zj requesting resource cpu=10m on Node 10.123.240.178
Oct 26 19:55:31.148: INFO: Pod downloads-56f66db77f-f9rtp requesting resource cpu=10m on Node 10.123.240.172
Oct 26 19:55:31.148: INFO: Pod downloads-56f66db77f-qk97h requesting resource cpu=10m on Node 10.123.240.172
Oct 26 19:55:31.148: INFO: Pod dns-operator-5bd9bd8fcd-gj6cw requesting resource cpu=20m on Node 10.123.240.172
Oct 26 19:55:31.148: INFO: Pod dns-default-2hlsj requesting resource cpu=110m on Node 10.123.240.174
Oct 26 19:55:31.148: INFO: Pod dns-default-qn8jr requesting resource cpu=110m on Node 10.123.240.178
Oct 26 19:55:31.148: INFO: Pod dns-default-qnchn requesting resource cpu=110m on Node 10.123.240.172
Oct 26 19:55:31.148: INFO: Pod cluster-image-registry-operator-854d785579-78wmp requesting resource cpu=20m on Node 10.123.240.172
Oct 26 19:55:31.148: INFO: Pod image-registry-57486bbffb-v7qmg requesting resource cpu=100m on Node 10.123.240.174
Oct 26 19:55:31.148: INFO: Pod node-ca-hsxg7 requesting resource cpu=10m on Node 10.123.240.178
Oct 26 19:55:31.148: INFO: Pod node-ca-lbgdr requesting resource cpu=10m on Node 10.123.240.174
Oct 26 19:55:31.148: INFO: Pod node-ca-rhts4 requesting resource cpu=10m on Node 10.123.240.172
Oct 26 19:55:31.148: INFO: Pod ingress-operator-549d4c77b5-8mj9f requesting resource cpu=20m on Node 10.123.240.172
Oct 26 19:55:31.148: INFO: Pod router-default-56c7ff9d54-2kbrj requesting resource cpu=100m on Node 10.123.240.172
Oct 26 19:55:31.148: INFO: Pod router-default-56c7ff9d54-tz4cl requesting resource cpu=100m on Node 10.123.240.174
Oct 26 19:55:31.148: INFO: Pod openshift-kube-proxy-k67gq requesting resource cpu=100m on Node 10.123.240.172
Oct 26 19:55:31.148: INFO: Pod openshift-kube-proxy-rs72s requesting resource cpu=100m on Node 10.123.240.174
Oct 26 19:55:31.148: INFO: Pod openshift-kube-proxy-v9jgr requesting resource cpu=100m on Node 10.123.240.178
Oct 26 19:55:31.148: INFO: Pod certified-operators-59fc9cd9b5-84rhs requesting resource cpu=10m on Node 10.123.240.174
Oct 26 19:55:31.148: INFO: Pod community-operators-bd644f5c5-wb2l9 requesting resource cpu=10m on Node 10.123.240.174
Oct 26 19:55:31.148: INFO: Pod marketplace-operator-c74f66688-865bs requesting resource cpu=10m on Node 10.123.240.172
Oct 26 19:55:31.149: INFO: Pod redhat-operators-688fdf87f8-nfrk9 requesting resource cpu=10m on Node 10.123.240.174
Oct 26 19:55:31.149: INFO: Pod alertmanager-main-0 requesting resource cpu=6m on Node 10.123.240.178
Oct 26 19:55:31.149: INFO: Pod alertmanager-main-1 requesting resource cpu=6m on Node 10.123.240.174
Oct 26 19:55:31.149: INFO: Pod alertmanager-main-2 requesting resource cpu=6m on Node 10.123.240.172
Oct 26 19:55:31.149: INFO: Pod cluster-monitoring-operator-77bbbf9cb7-sd49n requesting resource cpu=10m on Node 10.123.240.172
Oct 26 19:55:31.149: INFO: Pod grafana-f6757cb99-n9dgx requesting resource cpu=5m on Node 10.123.240.174
Oct 26 19:55:31.149: INFO: Pod kube-state-metrics-6bb5fc9995-w9q9p requesting resource cpu=4m on Node 10.123.240.174
Oct 26 19:55:31.149: INFO: Pod node-exporter-hpt5d requesting resource cpu=9m on Node 10.123.240.174
Oct 26 19:55:31.149: INFO: Pod node-exporter-pbqtq requesting resource cpu=9m on Node 10.123.240.172
Oct 26 19:55:31.149: INFO: Pod node-exporter-vw6ds requesting resource cpu=9m on Node 10.123.240.178
Oct 26 19:55:31.149: INFO: Pod openshift-state-metrics-6888cfb99c-snkdc requesting resource cpu=3m on Node 10.123.240.178
Oct 26 19:55:31.149: INFO: Pod prometheus-adapter-6cccbf8dbb-4bpjw requesting resource cpu=1m on Node 10.123.240.172
Oct 26 19:55:31.149: INFO: Pod prometheus-adapter-6cccbf8dbb-97f5w requesting resource cpu=1m on Node 10.123.240.174
Oct 26 19:55:31.149: INFO: Pod prometheus-k8s-0 requesting resource cpu=76m on Node 10.123.240.174
Oct 26 19:55:31.149: INFO: Pod prometheus-k8s-1 requesting resource cpu=76m on Node 10.123.240.178
Oct 26 19:55:31.149: INFO: Pod prometheus-operator-d5df96f57-bthbz requesting resource cpu=5m on Node 10.123.240.174
Oct 26 19:55:31.149: INFO: Pod telemeter-client-7c4c649567-vp6j2 requesting resource cpu=3m on Node 10.123.240.172
Oct 26 19:55:31.149: INFO: Pod thanos-querier-85d68cbb66-5hkh8 requesting resource cpu=8m on Node 10.123.240.174
Oct 26 19:55:31.149: INFO: Pod thanos-querier-85d68cbb66-lgj99 requesting resource cpu=8m on Node 10.123.240.178
Oct 26 19:55:31.149: INFO: Pod multus-admission-controller-4dblm requesting resource cpu=10m on Node 10.123.240.172
Oct 26 19:55:31.149: INFO: Pod multus-admission-controller-f46wp requesting resource cpu=10m on Node 10.123.240.174
Oct 26 19:55:31.149: INFO: Pod multus-admission-controller-qxdxf requesting resource cpu=10m on Node 10.123.240.178
Oct 26 19:55:31.149: INFO: Pod multus-nht6l requesting resource cpu=10m on Node 10.123.240.178
Oct 26 19:55:31.149: INFO: Pod multus-wk7tv requesting resource cpu=10m on Node 10.123.240.172
Oct 26 19:55:31.149: INFO: Pod multus-wz2pl requesting resource cpu=10m on Node 10.123.240.174
Oct 26 19:55:31.149: INFO: Pod network-operator-5647cdbff6-qn2hq requesting resource cpu=10m on Node 10.123.240.172
Oct 26 19:55:31.149: INFO: Pod catalog-operator-7bf86b4f96-kpnmt requesting resource cpu=10m on Node 10.123.240.172
Oct 26 19:55:31.149: INFO: Pod olm-operator-cbd8cfd65-5hqds requesting resource cpu=10m on Node 10.123.240.172
Oct 26 19:55:31.149: INFO: Pod packageserver-d5dbff54b-q2kg6 requesting resource cpu=10m on Node 10.123.240.178
Oct 26 19:55:31.149: INFO: Pod packageserver-d5dbff54b-xqj44 requesting resource cpu=10m on Node 10.123.240.172
Oct 26 19:55:31.149: INFO: Pod service-ca-operator-54f4b4db4-wvhxp requesting resource cpu=10m on Node 10.123.240.172
Oct 26 19:55:31.149: INFO: Pod apiservice-cabundle-injector-566d6d6dc7-lv6bf requesting resource cpu=10m on Node 10.123.240.174
Oct 26 19:55:31.149: INFO: Pod configmap-cabundle-injector-6676dbc567-zdqgt requesting resource cpu=10m on Node 10.123.240.174
Oct 26 19:55:31.149: INFO: Pod service-serving-cert-signer-6d656c4cf7-l9qz5 requesting resource cpu=10m on Node 10.123.240.174
Oct 26 19:55:31.149: INFO: Pod openshift-service-catalog-apiserver-operator-d6cf765ff-k9snw requesting resource cpu=0m on Node 10.123.240.172
Oct 26 19:55:31.149: INFO: Pod openshift-service-catalog-controller-manager-operator-6fcbsks6s requesting resource cpu=10m on Node 10.123.240.172
Oct 26 19:55:31.149: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.123.240.172
Oct 26 19:55:31.149: INFO: Pod sonobuoy-e2e-job-6258e003556947b6 requesting resource cpu=0m on Node 10.123.240.178
Oct 26 19:55:31.149: INFO: Pod sonobuoy-systemd-logs-daemon-set-a94f7e2648a3464d-9nhrv requesting resource cpu=0m on Node 10.123.240.172
Oct 26 19:55:31.149: INFO: Pod sonobuoy-systemd-logs-daemon-set-a94f7e2648a3464d-lj6zg requesting resource cpu=0m on Node 10.123.240.178
Oct 26 19:55:31.149: INFO: Pod sonobuoy-systemd-logs-daemon-set-a94f7e2648a3464d-wf2jr requesting resource cpu=0m on Node 10.123.240.174
Oct 26 19:55:31.149: INFO: Pod tigera-operator-798cfbf7dd-x8hf6 requesting resource cpu=100m on Node 10.123.240.172
STEP: Starting Pods to consume most of the cluster CPU.
Oct 26 19:55:31.149: INFO: Creating a pod which consumes cpu=2097m on Node 10.123.240.172
Oct 26 19:55:31.195: INFO: Creating a pod which consumes cpu=2230m on Node 10.123.240.174
Oct 26 19:55:31.237: INFO: Creating a pod which consumes cpu=2420m on Node 10.123.240.178
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-01c1d4ca-aaad-4f6c-9150-98a2c00fb15e.1641a2f8af0d34ad], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7888/filler-pod-01c1d4ca-aaad-4f6c-9150-98a2c00fb15e to 10.123.240.178]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-01c1d4ca-aaad-4f6c-9150-98a2c00fb15e.1641a2f90c781327], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-01c1d4ca-aaad-4f6c-9150-98a2c00fb15e.1641a2f919b5e68b], Reason = [Created], Message = [Created container filler-pod-01c1d4ca-aaad-4f6c-9150-98a2c00fb15e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-01c1d4ca-aaad-4f6c-9150-98a2c00fb15e.1641a2f91cb65d53], Reason = [Started], Message = [Started container filler-pod-01c1d4ca-aaad-4f6c-9150-98a2c00fb15e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9fc67118-d65e-42e2-b20b-8f1793acc7c6.1641a2f8aa1e7a10], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7888/filler-pod-9fc67118-d65e-42e2-b20b-8f1793acc7c6 to 10.123.240.172]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9fc67118-d65e-42e2-b20b-8f1793acc7c6.1641a2f8ead96320], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9fc67118-d65e-42e2-b20b-8f1793acc7c6.1641a2f8f7935930], Reason = [Created], Message = [Created container filler-pod-9fc67118-d65e-42e2-b20b-8f1793acc7c6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9fc67118-d65e-42e2-b20b-8f1793acc7c6.1641a2f8fa6b4faa], Reason = [Started], Message = [Started container filler-pod-9fc67118-d65e-42e2-b20b-8f1793acc7c6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d0cd885e-0902-41cd-a6ad-8879c8422138.1641a2f8acd47447], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7888/filler-pod-d0cd885e-0902-41cd-a6ad-8879c8422138 to 10.123.240.174]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d0cd885e-0902-41cd-a6ad-8879c8422138.1641a2f8e86f3967], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d0cd885e-0902-41cd-a6ad-8879c8422138.1641a2f8f47b05cb], Reason = [Created], Message = [Created container filler-pod-d0cd885e-0902-41cd-a6ad-8879c8422138]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d0cd885e-0902-41cd-a6ad-8879c8422138.1641a2f8f715db62], Reason = [Started], Message = [Started container filler-pod-d0cd885e-0902-41cd-a6ad-8879c8422138]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1641a2f9a22d902b], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.123.240.172
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.123.240.174
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.123.240.178
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:55:36.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7888" for this suite.
Oct 26 19:55:44.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:55:46.656: INFO: namespace sched-pred-7888 deletion completed in 10.097771552s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:16.282 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:55:46.659: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-4658
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Oct 26 19:55:46.848: INFO: Found 0 stateful pods, waiting for 3
Oct 26 19:55:56.861: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 26 19:55:56.861: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 26 19:55:56.861: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Oct 26 19:55:56.931: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Oct 26 19:56:07.019: INFO: Updating stateful set ss2
Oct 26 19:56:07.043: INFO: Waiting for Pod statefulset-4658/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Oct 26 19:56:17.162: INFO: Found 1 stateful pods, waiting for 3
Oct 26 19:56:27.179: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 26 19:56:27.179: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 26 19:56:27.179: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Oct 26 19:56:27.239: INFO: Updating stateful set ss2
Oct 26 19:56:27.267: INFO: Waiting for Pod statefulset-4658/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Oct 26 19:56:37.291: INFO: Waiting for Pod statefulset-4658/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Oct 26 19:56:47.336: INFO: Updating stateful set ss2
Oct 26 19:56:47.358: INFO: Waiting for StatefulSet statefulset-4658/ss2 to complete update
Oct 26 19:56:47.358: INFO: Waiting for Pod statefulset-4658/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Oct 26 19:56:57.381: INFO: Waiting for StatefulSet statefulset-4658/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Oct 26 19:57:07.382: INFO: Deleting all statefulset in ns statefulset-4658
Oct 26 19:57:07.392: INFO: Scaling statefulset ss2 to 0
Oct 26 19:57:27.450: INFO: Waiting for statefulset status.replicas updated to 0
Oct 26 19:57:27.460: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:57:27.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4658" for this suite.
Oct 26 19:57:35.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:57:37.565: INFO: namespace statefulset-4658 deletion completed in 10.041494526s

• [SLOW TEST:110.906 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:57:37.565: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6023.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6023.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6023.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6023.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6023.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6023.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6023.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6023.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6023.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6023.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6023.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6023.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6023.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 150.109.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.109.150_udp@PTR;check="$$(dig +tcp +noall +answer +search 150.109.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.109.150_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6023.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6023.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6023.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6023.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6023.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6023.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6023.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6023.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6023.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6023.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6023.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6023.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6023.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 150.109.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.109.150_udp@PTR;check="$$(dig +tcp +noall +answer +search 150.109.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.109.150_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 26 19:57:41.897: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6023.svc.cluster.local from pod dns-6023/dns-test-9a529cea-0a0b-4c39-a0d1-c1b20744db2f: the server could not find the requested resource (get pods dns-test-9a529cea-0a0b-4c39-a0d1-c1b20744db2f)
Oct 26 19:57:41.913: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6023.svc.cluster.local from pod dns-6023/dns-test-9a529cea-0a0b-4c39-a0d1-c1b20744db2f: the server could not find the requested resource (get pods dns-test-9a529cea-0a0b-4c39-a0d1-c1b20744db2f)
Oct 26 19:57:41.929: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6023.svc.cluster.local from pod dns-6023/dns-test-9a529cea-0a0b-4c39-a0d1-c1b20744db2f: the server could not find the requested resource (get pods dns-test-9a529cea-0a0b-4c39-a0d1-c1b20744db2f)
Oct 26 19:57:42.114: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6023.svc.cluster.local from pod dns-6023/dns-test-9a529cea-0a0b-4c39-a0d1-c1b20744db2f: the server could not find the requested resource (get pods dns-test-9a529cea-0a0b-4c39-a0d1-c1b20744db2f)
Oct 26 19:57:42.129: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6023.svc.cluster.local from pod dns-6023/dns-test-9a529cea-0a0b-4c39-a0d1-c1b20744db2f: the server could not find the requested resource (get pods dns-test-9a529cea-0a0b-4c39-a0d1-c1b20744db2f)
Oct 26 19:57:42.224: INFO: Lookups using dns-6023/dns-test-9a529cea-0a0b-4c39-a0d1-c1b20744db2f failed for: [wheezy_tcp@dns-test-service.dns-6023.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6023.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6023.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6023.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6023.svc.cluster.local]

Oct 26 19:57:47.573: INFO: DNS probes using dns-6023/dns-test-9a529cea-0a0b-4c39-a0d1-c1b20744db2f succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:57:47.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6023" for this suite.
Oct 26 19:57:55.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:57:57.851: INFO: namespace dns-6023 deletion completed in 10.044952745s

• [SLOW TEST:20.286 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:57:57.854: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Oct 26 19:57:57.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 create -f - --namespace=kubectl-6258'
Oct 26 19:57:58.896: INFO: stderr: ""
Oct 26 19:57:58.897: INFO: stdout: "pod/pause created\n"
Oct 26 19:57:58.897: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Oct 26 19:57:58.897: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6258" to be "running and ready"
Oct 26 19:57:58.927: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 30.18219ms
Oct 26 19:58:00.937: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.040843731s
Oct 26 19:58:00.937: INFO: Pod "pause" satisfied condition "running and ready"
Oct 26 19:58:00.937: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Oct 26 19:58:00.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 label pods pause testing-label=testing-label-value --namespace=kubectl-6258'
Oct 26 19:58:01.133: INFO: stderr: ""
Oct 26 19:58:01.133: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Oct 26 19:58:01.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pod pause -L testing-label --namespace=kubectl-6258'
Oct 26 19:58:01.279: INFO: stderr: ""
Oct 26 19:58:01.279: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Oct 26 19:58:01.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 label pods pause testing-label- --namespace=kubectl-6258'
Oct 26 19:58:01.471: INFO: stderr: ""
Oct 26 19:58:01.471: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Oct 26 19:58:01.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pod pause -L testing-label --namespace=kubectl-6258'
Oct 26 19:58:01.627: INFO: stderr: ""
Oct 26 19:58:01.627: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Oct 26 19:58:01.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 delete --grace-period=0 --force -f - --namespace=kubectl-6258'
Oct 26 19:58:01.803: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 26 19:58:01.803: INFO: stdout: "pod \"pause\" force deleted\n"
Oct 26 19:58:01.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get rc,svc -l name=pause --no-headers --namespace=kubectl-6258'
Oct 26 19:58:02.044: INFO: stderr: "No resources found in kubectl-6258 namespace.\n"
Oct 26 19:58:02.044: INFO: stdout: ""
Oct 26 19:58:02.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods -l name=pause --namespace=kubectl-6258 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Oct 26 19:58:02.193: INFO: stderr: ""
Oct 26 19:58:02.193: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:58:02.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6258" for this suite.
Oct 26 19:58:10.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:58:12.350: INFO: namespace kubectl-6258 deletion completed in 10.12255641s

• [SLOW TEST:14.496 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:58:12.351: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Oct 26 19:58:12.561: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ec359a84-9e33-4ed0-b3b2-cd64ac4fe66f" in namespace "downward-api-975" to be "success or failure"
Oct 26 19:58:12.572: INFO: Pod "downwardapi-volume-ec359a84-9e33-4ed0-b3b2-cd64ac4fe66f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.023785ms
Oct 26 19:58:14.583: INFO: Pod "downwardapi-volume-ec359a84-9e33-4ed0-b3b2-cd64ac4fe66f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021865358s
Oct 26 19:58:16.596: INFO: Pod "downwardapi-volume-ec359a84-9e33-4ed0-b3b2-cd64ac4fe66f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034654414s
STEP: Saw pod success
Oct 26 19:58:16.596: INFO: Pod "downwardapi-volume-ec359a84-9e33-4ed0-b3b2-cd64ac4fe66f" satisfied condition "success or failure"
Oct 26 19:58:16.607: INFO: Trying to get logs from node 10.123.240.178 pod downwardapi-volume-ec359a84-9e33-4ed0-b3b2-cd64ac4fe66f container client-container: <nil>
STEP: delete the pod
Oct 26 19:58:16.717: INFO: Waiting for pod downwardapi-volume-ec359a84-9e33-4ed0-b3b2-cd64ac4fe66f to disappear
Oct 26 19:58:16.728: INFO: Pod downwardapi-volume-ec359a84-9e33-4ed0-b3b2-cd64ac4fe66f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:58:16.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-975" for this suite.
Oct 26 19:58:24.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:58:26.780: INFO: namespace downward-api-975 deletion completed in 10.018044228s

• [SLOW TEST:14.429 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:58:26.781: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:58:31.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2941" for this suite.
Oct 26 19:58:39.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:58:41.540: INFO: namespace watch-2941 deletion completed in 10.092289918s

• [SLOW TEST:14.759 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:58:41.540: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Oct 26 19:58:41.762: INFO: Waiting up to 5m0s for pod "pod-206cab31-ad43-4d9c-a751-1cf27d7ae362" in namespace "emptydir-4631" to be "success or failure"
Oct 26 19:58:41.774: INFO: Pod "pod-206cab31-ad43-4d9c-a751-1cf27d7ae362": Phase="Pending", Reason="", readiness=false. Elapsed: 11.235009ms
Oct 26 19:58:43.784: INFO: Pod "pod-206cab31-ad43-4d9c-a751-1cf27d7ae362": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022039769s
Oct 26 19:58:45.797: INFO: Pod "pod-206cab31-ad43-4d9c-a751-1cf27d7ae362": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034380567s
STEP: Saw pod success
Oct 26 19:58:45.797: INFO: Pod "pod-206cab31-ad43-4d9c-a751-1cf27d7ae362" satisfied condition "success or failure"
Oct 26 19:58:45.808: INFO: Trying to get logs from node 10.123.240.178 pod pod-206cab31-ad43-4d9c-a751-1cf27d7ae362 container test-container: <nil>
STEP: delete the pod
Oct 26 19:58:45.865: INFO: Waiting for pod pod-206cab31-ad43-4d9c-a751-1cf27d7ae362 to disappear
Oct 26 19:58:45.876: INFO: Pod pod-206cab31-ad43-4d9c-a751-1cf27d7ae362 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:58:45.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4631" for this suite.
Oct 26 19:58:53.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:58:55.931: INFO: namespace emptydir-4631 deletion completed in 10.024044363s

• [SLOW TEST:14.390 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:58:55.931: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:58:58.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7553" for this suite.
Oct 26 19:59:18.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:59:20.266: INFO: namespace containers-7553 deletion completed in 22.060726726s

• [SLOW TEST:24.336 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:59:20.270: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Oct 26 19:59:20.428: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Oct 26 19:59:20.498: INFO: Waiting for terminating namespaces to be deleted...
Oct 26 19:59:20.522: INFO: 
Logging pods the kubelet thinks is on node 10.123.240.172 before test
Oct 26 19:59:20.625: INFO: calico-node-kchsj from calico-system started at 2020-10-26 16:58:05 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 19:59:20.626: INFO: olm-operator-cbd8cfd65-5hqds from openshift-operator-lifecycle-manager started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container olm-operator ready: true, restart count 0
Oct 26 19:59:20.626: INFO: service-ca-operator-54f4b4db4-wvhxp from openshift-service-ca-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container operator ready: true, restart count 0
Oct 26 19:59:20.626: INFO: downloads-56f66db77f-f9rtp from openshift-console started at 2020-10-26 16:58:23 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container download-server ready: true, restart count 0
Oct 26 19:59:20.626: INFO: sonobuoy-systemd-logs-daemon-set-a94f7e2648a3464d-9nhrv from sonobuoy started at 2020-10-26 18:20:50 +0000 UTC (2 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Oct 26 19:59:20.626: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 26 19:59:20.626: INFO: telemeter-client-7c4c649567-vp6j2 from openshift-monitoring started at 2020-10-26 18:42:09 +0000 UTC (3 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:59:20.626: INFO: 	Container reload ready: true, restart count 0
Oct 26 19:59:20.626: INFO: 	Container telemeter-client ready: true, restart count 0
Oct 26 19:59:20.626: INFO: ingress-operator-549d4c77b5-8mj9f from openshift-ingress-operator started at 2020-10-26 16:58:24 +0000 UTC (2 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container ingress-operator ready: true, restart count 0
Oct 26 19:59:20.626: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:59:20.626: INFO: network-operator-5647cdbff6-qn2hq from openshift-network-operator started at 2020-10-26 16:56:55 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container network-operator ready: true, restart count 0
Oct 26 19:59:20.626: INFO: tigera-operator-798cfbf7dd-x8hf6 from tigera-operator started at 2020-10-26 16:56:55 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container tigera-operator ready: true, restart count 2
Oct 26 19:59:20.626: INFO: dns-operator-5bd9bd8fcd-gj6cw from openshift-dns-operator started at 2020-10-26 16:58:22 +0000 UTC (2 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container dns-operator ready: true, restart count 0
Oct 26 19:59:20.626: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:59:20.626: INFO: cluster-image-registry-operator-854d785579-78wmp from openshift-image-registry started at 2020-10-26 16:58:22 +0000 UTC (2 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Oct 26 19:59:20.626: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Oct 26 19:59:20.626: INFO: downloads-56f66db77f-qk97h from openshift-console started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container download-server ready: true, restart count 0
Oct 26 19:59:20.626: INFO: ibmcloud-block-storage-plugin-79495594d5-7m8wf from kube-system started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Oct 26 19:59:20.626: INFO: ibm-storage-watcher-7fc85c5589-t79km from kube-system started at 2020-10-26 16:58:23 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Oct 26 19:59:20.626: INFO: node-ca-rhts4 from openshift-image-registry started at 2020-10-26 16:59:40 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container node-ca ready: true, restart count 0
Oct 26 19:59:20.626: INFO: catalog-operator-7bf86b4f96-kpnmt from openshift-operator-lifecycle-manager started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container catalog-operator ready: true, restart count 0
Oct 26 19:59:20.626: INFO: ibm-file-plugin-6b86cbfbc6-vw94d from kube-system started at 2020-10-26 16:58:23 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Oct 26 19:59:20.626: INFO: cluster-samples-operator-644946789c-zw8th from openshift-cluster-samples-operator started at 2020-10-26 17:01:55 +0000 UTC (2 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Oct 26 19:59:20.626: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Oct 26 19:59:20.626: INFO: dns-default-qnchn from openshift-dns started at 2020-10-26 16:59:44 +0000 UTC (2 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container dns ready: true, restart count 0
Oct 26 19:59:20.626: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 26 19:59:20.626: INFO: router-default-56c7ff9d54-2kbrj from openshift-ingress started at 2020-10-26 18:42:09 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container router ready: true, restart count 0
Oct 26 19:59:20.626: INFO: cluster-storage-operator-6c6dd4b587-52s64 from openshift-cluster-storage-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Oct 26 19:59:20.626: INFO: openshift-service-catalog-apiserver-operator-d6cf765ff-k9snw from openshift-service-catalog-apiserver-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container operator ready: true, restart count 1
Oct 26 19:59:20.626: INFO: sonobuoy from sonobuoy started at 2020-10-26 18:20:41 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Oct 26 19:59:20.626: INFO: ibmcloud-block-storage-driver-fm8d2 from kube-system started at 2020-10-26 16:56:55 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Oct 26 19:59:20.626: INFO: console-operator-7f9f78ff66-kf42x from openshift-console-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container console-operator ready: true, restart count 1
Oct 26 19:59:20.626: INFO: calico-kube-controllers-599969f895-cnxrs from calico-system started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Oct 26 19:59:20.626: INFO: multus-admission-controller-4dblm from openshift-multus started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container multus-admission-controller ready: true, restart count 0
Oct 26 19:59:20.626: INFO: prometheus-adapter-6cccbf8dbb-4bpjw from openshift-monitoring started at 2020-10-26 17:05:48 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container prometheus-adapter ready: true, restart count 0
Oct 26 19:59:20.626: INFO: ibm-master-proxy-static-10.123.240.172 from kube-system started at 2020-10-26 16:56:50 +0000 UTC (2 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 19:59:20.626: INFO: 	Container pause ready: true, restart count 0
Oct 26 19:59:20.626: INFO: multus-wk7tv from openshift-multus started at 2020-10-26 16:57:19 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container kube-multus ready: true, restart count 0
Oct 26 19:59:20.626: INFO: openshift-kube-proxy-k67gq from openshift-kube-proxy started at 2020-10-26 16:57:26 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 26 19:59:20.626: INFO: cluster-node-tuning-operator-86b7f98f7b-qjdxp from openshift-cluster-node-tuning-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Oct 26 19:59:20.626: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-10-26 17:05:50 +0000 UTC (3 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container alertmanager ready: true, restart count 0
Oct 26 19:59:20.626: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 26 19:59:20.626: INFO: 	Container config-reloader ready: true, restart count 0
Oct 26 19:59:20.626: INFO: ibm-keepalived-watcher-rdht2 from kube-system started at 2020-10-26 16:56:52 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 19:59:20.626: INFO: marketplace-operator-c74f66688-865bs from openshift-marketplace started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container marketplace-operator ready: true, restart count 0
Oct 26 19:59:20.626: INFO: openshift-service-catalog-controller-manager-operator-6fcbsks6s from openshift-service-catalog-controller-manager-operator started at 2020-10-26 16:58:22 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container operator ready: true, restart count 1
Oct 26 19:59:20.626: INFO: cluster-monitoring-operator-77bbbf9cb7-sd49n from openshift-monitoring started at 2020-10-26 16:58:24 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Oct 26 19:59:20.626: INFO: node-exporter-pbqtq from openshift-monitoring started at 2020-10-26 16:59:03 +0000 UTC (2 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:59:20.626: INFO: 	Container node-exporter ready: true, restart count 0
Oct 26 19:59:20.626: INFO: calico-typha-6b7867b64d-wwhp2 from calico-system started at 2020-10-26 17:00:10 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 19:59:20.626: INFO: tuned-f8xvt from openshift-cluster-node-tuning-operator started at 2020-10-26 17:00:14 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container tuned ready: true, restart count 0
Oct 26 19:59:20.626: INFO: ibm-cloud-provider-ip-149-81-128-30-5bdf48d794-vdds8 from ibm-system started at 2020-10-26 17:10:29 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container ibm-cloud-provider-ip-149-81-128-30 ready: true, restart count 0
Oct 26 19:59:20.626: INFO: packageserver-d5dbff54b-xqj44 from openshift-operator-lifecycle-manager started at 2020-10-26 18:42:11 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.626: INFO: 	Container packageserver ready: true, restart count 0
Oct 26 19:59:20.626: INFO: 
Logging pods the kubelet thinks is on node 10.123.240.174 before test
Oct 26 19:59:20.718: INFO: ibm-master-proxy-static-10.123.240.174 from kube-system started at 2020-10-26 16:57:35 +0000 UTC (2 container statuses recorded)
Oct 26 19:59:20.718: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 19:59:20.718: INFO: 	Container pause ready: true, restart count 0
Oct 26 19:59:20.718: INFO: node-exporter-hpt5d from openshift-monitoring started at 2020-10-26 16:59:04 +0000 UTC (2 container statuses recorded)
Oct 26 19:59:20.718: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:59:20.718: INFO: 	Container node-exporter ready: true, restart count 0
Oct 26 19:59:20.718: INFO: configmap-cabundle-injector-6676dbc567-zdqgt from openshift-service-ca started at 2020-10-26 16:59:02 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.718: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Oct 26 19:59:20.718: INFO: image-registry-57486bbffb-v7qmg from openshift-image-registry started at 2020-10-26 17:01:54 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.718: INFO: 	Container registry ready: true, restart count 0
Oct 26 19:59:20.718: INFO: console-77979dd75-lnchz from openshift-console started at 2020-10-26 17:01:28 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.718: INFO: 	Container console ready: true, restart count 0
Oct 26 19:59:20.718: INFO: service-serving-cert-signer-6d656c4cf7-l9qz5 from openshift-service-ca started at 2020-10-26 16:59:01 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Oct 26 19:59:20.719: INFO: prometheus-operator-d5df96f57-bthbz from openshift-monitoring started at 2020-10-26 17:05:34 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container prometheus-operator ready: true, restart count 0
Oct 26 19:59:20.719: INFO: thanos-querier-85d68cbb66-5hkh8 from openshift-monitoring started at 2020-10-26 17:06:37 +0000 UTC (4 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:59:20.719: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 26 19:59:20.719: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 26 19:59:20.719: INFO: 	Container thanos-querier ready: true, restart count 0
Oct 26 19:59:20.719: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-10-26 17:06:52 +0000 UTC (7 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:59:20.719: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 26 19:59:20.719: INFO: 	Container prometheus ready: true, restart count 1
Oct 26 19:59:20.719: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Oct 26 19:59:20.719: INFO: 	Container prometheus-proxy ready: true, restart count 0
Oct 26 19:59:20.719: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Oct 26 19:59:20.719: INFO: 	Container thanos-sidecar ready: true, restart count 0
Oct 26 19:59:20.719: INFO: certified-operators-59fc9cd9b5-84rhs from openshift-marketplace started at 2020-10-26 18:00:44 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container certified-operators ready: true, restart count 0
Oct 26 19:59:20.719: INFO: ibmcloud-block-storage-driver-s88xs from kube-system started at 2020-10-26 16:57:41 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Oct 26 19:59:20.719: INFO: calico-typha-6b7867b64d-j6hnv from calico-system started at 2020-10-26 16:58:05 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 19:59:20.719: INFO: calico-node-jfjg2 from calico-system started at 2020-10-26 16:58:05 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 19:59:20.719: INFO: apiservice-cabundle-injector-566d6d6dc7-lv6bf from openshift-service-ca started at 2020-10-26 16:59:02 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Oct 26 19:59:20.719: INFO: node-ca-lbgdr from openshift-image-registry started at 2020-10-26 16:59:40 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container node-ca ready: true, restart count 0
Oct 26 19:59:20.719: INFO: grafana-f6757cb99-n9dgx from openshift-monitoring started at 2020-10-26 17:05:54 +0000 UTC (2 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container grafana ready: true, restart count 0
Oct 26 19:59:20.719: INFO: 	Container grafana-proxy ready: true, restart count 0
Oct 26 19:59:20.719: INFO: multus-wz2pl from openshift-multus started at 2020-10-26 16:57:37 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container kube-multus ready: true, restart count 0
Oct 26 19:59:20.719: INFO: ibm-keepalived-watcher-nxrtl from kube-system started at 2020-10-26 16:57:37 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 19:59:20.719: INFO: kube-state-metrics-6bb5fc9995-w9q9p from openshift-monitoring started at 2020-10-26 16:59:03 +0000 UTC (3 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Oct 26 19:59:20.719: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Oct 26 19:59:20.719: INFO: 	Container kube-state-metrics ready: true, restart count 0
Oct 26 19:59:20.719: INFO: tuned-nlq8g from openshift-cluster-node-tuning-operator started at 2020-10-26 17:00:14 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container tuned ready: true, restart count 0
Oct 26 19:59:20.719: INFO: sonobuoy-systemd-logs-daemon-set-a94f7e2648a3464d-wf2jr from sonobuoy started at 2020-10-26 18:20:51 +0000 UTC (2 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Oct 26 19:59:20.719: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 26 19:59:20.719: INFO: vpn-68d7d4d68d-clwcb from kube-system started at 2020-10-26 17:05:39 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container vpn ready: true, restart count 0
Oct 26 19:59:20.719: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-10-26 17:06:05 +0000 UTC (3 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container alertmanager ready: true, restart count 0
Oct 26 19:59:20.719: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 26 19:59:20.719: INFO: 	Container config-reloader ready: true, restart count 0
Oct 26 19:59:20.719: INFO: multus-admission-controller-f46wp from openshift-multus started at 2020-10-26 16:58:27 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container multus-admission-controller ready: true, restart count 0
Oct 26 19:59:20.719: INFO: ibm-cloud-provider-ip-149-81-128-30-5bdf48d794-9qftf from ibm-system started at 2020-10-26 18:42:09 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container ibm-cloud-provider-ip-149-81-128-30 ready: true, restart count 0
Oct 26 19:59:20.719: INFO: router-default-56c7ff9d54-tz4cl from openshift-ingress started at 2020-10-26 17:00:10 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container router ready: true, restart count 0
Oct 26 19:59:20.719: INFO: redhat-operators-688fdf87f8-nfrk9 from openshift-marketplace started at 2020-10-26 17:00:16 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container redhat-operators ready: true, restart count 0
Oct 26 19:59:20.719: INFO: registry-pvc-permissions-9fhwg from openshift-image-registry started at 2020-10-26 17:01:54 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container pvc-permissions ready: false, restart count 0
Oct 26 19:59:20.719: INFO: prometheus-adapter-6cccbf8dbb-97f5w from openshift-monitoring started at 2020-10-26 17:05:48 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container prometheus-adapter ready: true, restart count 0
Oct 26 19:59:20.719: INFO: openshift-kube-proxy-rs72s from openshift-kube-proxy started at 2020-10-26 16:57:37 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 26 19:59:20.719: INFO: dns-default-2hlsj from openshift-dns started at 2020-10-26 16:59:44 +0000 UTC (2 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container dns ready: true, restart count 0
Oct 26 19:59:20.719: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 26 19:59:20.719: INFO: community-operators-bd644f5c5-wb2l9 from openshift-marketplace started at 2020-10-26 17:00:16 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.719: INFO: 	Container community-operators ready: true, restart count 0
Oct 26 19:59:20.719: INFO: 
Logging pods the kubelet thinks is on node 10.123.240.178 before test
Oct 26 19:59:20.781: INFO: sonobuoy-e2e-job-6258e003556947b6 from sonobuoy started at 2020-10-26 18:20:50 +0000 UTC (2 container statuses recorded)
Oct 26 19:59:20.781: INFO: 	Container e2e ready: true, restart count 0
Oct 26 19:59:20.781: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 26 19:59:20.781: INFO: console-77979dd75-sj7zj from openshift-console started at 2020-10-26 18:42:09 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.781: INFO: 	Container console ready: true, restart count 0
Oct 26 19:59:20.781: INFO: openshift-state-metrics-6888cfb99c-snkdc from openshift-monitoring started at 2020-10-26 18:42:09 +0000 UTC (3 container statuses recorded)
Oct 26 19:59:20.781: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Oct 26 19:59:20.781: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Oct 26 19:59:20.781: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Oct 26 19:59:20.781: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-10-26 18:42:24 +0000 UTC (3 container statuses recorded)
Oct 26 19:59:20.781: INFO: 	Container alertmanager ready: true, restart count 0
Oct 26 19:59:20.781: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 26 19:59:20.781: INFO: 	Container config-reloader ready: true, restart count 0
Oct 26 19:59:20.781: INFO: multus-nht6l from openshift-multus started at 2020-10-26 16:57:47 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.781: INFO: 	Container kube-multus ready: true, restart count 0
Oct 26 19:59:20.781: INFO: sonobuoy-systemd-logs-daemon-set-a94f7e2648a3464d-lj6zg from sonobuoy started at 2020-10-26 18:20:51 +0000 UTC (2 container statuses recorded)
Oct 26 19:59:20.781: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Oct 26 19:59:20.781: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 26 19:59:20.781: INFO: thanos-querier-85d68cbb66-lgj99 from openshift-monitoring started at 2020-10-26 18:42:09 +0000 UTC (4 container statuses recorded)
Oct 26 19:59:20.781: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:59:20.781: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 26 19:59:20.781: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 26 19:59:20.781: INFO: 	Container thanos-querier ready: true, restart count 0
Oct 26 19:59:20.781: INFO: ibm-keepalived-watcher-zr4ms from kube-system started at 2020-10-26 16:57:47 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.781: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 19:59:20.781: INFO: openshift-kube-proxy-v9jgr from openshift-kube-proxy started at 2020-10-26 16:57:47 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.781: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 26 19:59:20.781: INFO: calico-typha-6b7867b64d-bjwpm from calico-system started at 2020-10-26 17:00:10 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.781: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 19:59:20.781: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-10-26 18:42:54 +0000 UTC (7 container statuses recorded)
Oct 26 19:59:20.781: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:59:20.781: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 26 19:59:20.781: INFO: 	Container prometheus ready: true, restart count 1
Oct 26 19:59:20.781: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Oct 26 19:59:20.781: INFO: 	Container prometheus-proxy ready: true, restart count 0
Oct 26 19:59:20.781: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Oct 26 19:59:20.781: INFO: 	Container thanos-sidecar ready: true, restart count 0
Oct 26 19:59:20.781: INFO: node-exporter-vw6ds from openshift-monitoring started at 2020-10-26 16:59:04 +0000 UTC (2 container statuses recorded)
Oct 26 19:59:20.781: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 26 19:59:20.781: INFO: 	Container node-exporter ready: true, restart count 0
Oct 26 19:59:20.781: INFO: dns-default-qn8jr from openshift-dns started at 2020-10-26 16:59:44 +0000 UTC (2 container statuses recorded)
Oct 26 19:59:20.781: INFO: 	Container dns ready: true, restart count 0
Oct 26 19:59:20.781: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 26 19:59:20.781: INFO: tuned-2ksq9 from openshift-cluster-node-tuning-operator started at 2020-10-26 17:00:14 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.781: INFO: 	Container tuned ready: true, restart count 0
Oct 26 19:59:20.781: INFO: ibm-master-proxy-static-10.123.240.178 from kube-system started at 2020-10-26 16:57:44 +0000 UTC (2 container statuses recorded)
Oct 26 19:59:20.781: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 19:59:20.781: INFO: 	Container pause ready: true, restart count 0
Oct 26 19:59:20.781: INFO: calico-node-d227t from calico-system started at 2020-10-26 16:58:05 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.781: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 19:59:20.781: INFO: multus-admission-controller-qxdxf from openshift-multus started at 2020-10-26 18:42:44 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.781: INFO: 	Container multus-admission-controller ready: true, restart count 0
Oct 26 19:59:20.781: INFO: ibmcloud-block-storage-driver-2pnvt from kube-system started at 2020-10-26 16:57:55 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.781: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Oct 26 19:59:20.781: INFO: node-ca-hsxg7 from openshift-image-registry started at 2020-10-26 16:59:40 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.781: INFO: 	Container node-ca ready: true, restart count 0
Oct 26 19:59:20.781: INFO: packageserver-d5dbff54b-q2kg6 from openshift-operator-lifecycle-manager started at 2020-10-26 18:42:16 +0000 UTC (1 container statuses recorded)
Oct 26 19:59:20.781: INFO: 	Container packageserver ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-d6ececad-bb45-4f68-aa40-88a9e1918fee 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-d6ececad-bb45-4f68-aa40-88a9e1918fee off the node 10.123.240.178
STEP: verifying the node doesn't have the label kubernetes.io/e2e-d6ececad-bb45-4f68-aa40-88a9e1918fee
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 19:59:39.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9911" for this suite.
Oct 26 19:59:57.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 19:59:59.194: INFO: namespace sched-pred-9911 deletion completed in 19.998810401s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:38.924 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 19:59:59.196: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Oct 26 19:59:59.912: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Oct 26 20:00:01.947: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739339199, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739339199, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739339199, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739339199, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 20:00:04.997: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 20:00:05.018: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:00:06.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9374" for this suite.
Oct 26 20:00:14.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:00:16.560: INFO: namespace crd-webhook-9374 deletion completed in 10.002643004s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:17.423 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:00:16.619: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Oct 26 20:00:16.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-9273 -- logs-generator --log-lines-total 100 --run-duration 20s'
Oct 26 20:00:16.944: INFO: stderr: ""
Oct 26 20:00:16.944: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Oct 26 20:00:16.944: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Oct 26 20:00:16.944: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9273" to be "running and ready, or succeeded"
Oct 26 20:00:16.959: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 15.248354ms
Oct 26 20:00:18.972: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027589422s
Oct 26 20:00:20.985: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.040899633s
Oct 26 20:00:20.985: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Oct 26 20:00:20.985: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Oct 26 20:00:20.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 logs logs-generator logs-generator --namespace=kubectl-9273'
Oct 26 20:00:21.323: INFO: stderr: ""
Oct 26 20:00:21.323: INFO: stdout: "I1026 20:00:18.464005       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/pm6 522\nI1026 20:00:18.664182       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/8lq 554\nI1026 20:00:18.864307       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/dvl 354\nI1026 20:00:19.064256       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/qfs 369\nI1026 20:00:19.264211       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/h9kc 460\nI1026 20:00:19.464199       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/rsd 325\nI1026 20:00:19.664177       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/lclw 204\nI1026 20:00:19.864215       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/wg2v 442\nI1026 20:00:20.064183       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/8qg 360\nI1026 20:00:20.264177       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/22tv 505\nI1026 20:00:20.464190       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/b7h 310\nI1026 20:00:20.664190       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/5vk 332\nI1026 20:00:20.864242       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/8bx 449\nI1026 20:00:21.067302       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/662 392\nI1026 20:00:21.264192       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/bjp 386\n"
STEP: limiting log lines
Oct 26 20:00:21.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 logs logs-generator logs-generator --namespace=kubectl-9273 --tail=1'
Oct 26 20:00:21.501: INFO: stderr: ""
Oct 26 20:00:21.501: INFO: stdout: "I1026 20:00:21.464207       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/4vq7 429\n"
STEP: limiting log bytes
Oct 26 20:00:21.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 logs logs-generator logs-generator --namespace=kubectl-9273 --limit-bytes=1'
Oct 26 20:00:21.697: INFO: stderr: ""
Oct 26 20:00:21.697: INFO: stdout: "I"
STEP: exposing timestamps
Oct 26 20:00:21.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 logs logs-generator logs-generator --namespace=kubectl-9273 --tail=1 --timestamps'
Oct 26 20:00:21.994: INFO: stderr: ""
Oct 26 20:00:21.994: INFO: stdout: "2020-10-26T15:00:21.86434593-05:00 I1026 20:00:21.864253       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/qs6 444\n"
STEP: restricting to a time range
Oct 26 20:00:24.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 logs logs-generator logs-generator --namespace=kubectl-9273 --since=1s'
Oct 26 20:00:24.681: INFO: stderr: ""
Oct 26 20:00:24.681: INFO: stdout: "I1026 20:00:23.864192       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/kube-system/pods/b5r 471\nI1026 20:00:24.064185       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/default/pods/mqh 510\nI1026 20:00:24.264193       1 logs_generator.go:76] 29 POST /api/v1/namespaces/kube-system/pods/grzp 359\nI1026 20:00:24.464186       1 logs_generator.go:76] 30 GET /api/v1/namespaces/ns/pods/46sv 202\nI1026 20:00:24.664182       1 logs_generator.go:76] 31 POST /api/v1/namespaces/kube-system/pods/vt7j 439\n"
Oct 26 20:00:24.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 logs logs-generator logs-generator --namespace=kubectl-9273 --since=24h'
Oct 26 20:00:24.854: INFO: stderr: ""
Oct 26 20:00:24.854: INFO: stdout: "I1026 20:00:18.464005       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/pm6 522\nI1026 20:00:18.664182       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/8lq 554\nI1026 20:00:18.864307       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/dvl 354\nI1026 20:00:19.064256       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/qfs 369\nI1026 20:00:19.264211       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/h9kc 460\nI1026 20:00:19.464199       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/rsd 325\nI1026 20:00:19.664177       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/lclw 204\nI1026 20:00:19.864215       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/wg2v 442\nI1026 20:00:20.064183       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/8qg 360\nI1026 20:00:20.264177       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/22tv 505\nI1026 20:00:20.464190       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/b7h 310\nI1026 20:00:20.664190       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/5vk 332\nI1026 20:00:20.864242       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/8bx 449\nI1026 20:00:21.067302       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/662 392\nI1026 20:00:21.264192       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/bjp 386\nI1026 20:00:21.464207       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/4vq7 429\nI1026 20:00:21.664198       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/sbgp 443\nI1026 20:00:21.864253       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/qs6 444\nI1026 20:00:22.064172       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/8gg 503\nI1026 20:00:22.264238       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/dsr4 421\nI1026 20:00:22.464214       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/l9d 385\nI1026 20:00:22.664210       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/s2k 534\nI1026 20:00:22.864198       1 logs_generator.go:76] 22 GET /api/v1/namespaces/default/pods/7vsw 580\nI1026 20:00:23.064185       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/default/pods/5ndz 287\nI1026 20:00:23.264209       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/rq5x 406\nI1026 20:00:23.464166       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/kzz5 595\nI1026 20:00:23.664174       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/szg5 362\nI1026 20:00:23.864192       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/kube-system/pods/b5r 471\nI1026 20:00:24.064185       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/default/pods/mqh 510\nI1026 20:00:24.264193       1 logs_generator.go:76] 29 POST /api/v1/namespaces/kube-system/pods/grzp 359\nI1026 20:00:24.464186       1 logs_generator.go:76] 30 GET /api/v1/namespaces/ns/pods/46sv 202\nI1026 20:00:24.664182       1 logs_generator.go:76] 31 POST /api/v1/namespaces/kube-system/pods/vt7j 439\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Oct 26 20:00:24.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 delete pod logs-generator --namespace=kubectl-9273'
Oct 26 20:00:34.125: INFO: stderr: ""
Oct 26 20:00:34.125: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:00:34.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9273" for this suite.
Oct 26 20:00:42.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:00:44.279: INFO: namespace kubectl-9273 deletion completed in 10.13137586s

• [SLOW TEST:27.659 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:00:44.279: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Oct 26 20:00:44.599: INFO: Number of nodes with available pods: 0
Oct 26 20:00:44.600: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:00:45.656: INFO: Number of nodes with available pods: 0
Oct 26 20:00:45.656: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:00:46.628: INFO: Number of nodes with available pods: 2
Oct 26 20:00:46.629: INFO: Node 10.123.240.174 is running more than one daemon pod
Oct 26 20:00:47.633: INFO: Number of nodes with available pods: 3
Oct 26 20:00:47.633: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Oct 26 20:00:47.703: INFO: Number of nodes with available pods: 2
Oct 26 20:00:47.703: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:00:48.731: INFO: Number of nodes with available pods: 2
Oct 26 20:00:48.731: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:00:49.745: INFO: Number of nodes with available pods: 3
Oct 26 20:00:49.745: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1551, will wait for the garbage collector to delete the pods
Oct 26 20:00:49.863: INFO: Deleting DaemonSet.extensions daemon-set took: 24.568277ms
Oct 26 20:00:50.363: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.316777ms
Oct 26 20:01:04.175: INFO: Number of nodes with available pods: 0
Oct 26 20:01:04.175: INFO: Number of running nodes: 0, number of available pods: 0
Oct 26 20:01:04.186: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1551/daemonsets","resourceVersion":"80352"},"items":null}

Oct 26 20:01:04.197: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1551/pods","resourceVersion":"80352"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:01:04.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1551" for this suite.
Oct 26 20:01:12.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:01:14.421: INFO: namespace daemonsets-1551 deletion completed in 10.146822569s

• [SLOW TEST:30.142 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:01:14.421: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Oct 26 20:01:14.612: INFO: Waiting up to 5m0s for pod "pod-85e1dc8b-37ac-4d3f-a88a-d90ab10bc87f" in namespace "emptydir-5218" to be "success or failure"
Oct 26 20:01:14.620: INFO: Pod "pod-85e1dc8b-37ac-4d3f-a88a-d90ab10bc87f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.523254ms
Oct 26 20:01:16.632: INFO: Pod "pod-85e1dc8b-37ac-4d3f-a88a-d90ab10bc87f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020143591s
Oct 26 20:01:18.653: INFO: Pod "pod-85e1dc8b-37ac-4d3f-a88a-d90ab10bc87f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040996068s
Oct 26 20:01:20.666: INFO: Pod "pod-85e1dc8b-37ac-4d3f-a88a-d90ab10bc87f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054392692s
STEP: Saw pod success
Oct 26 20:01:20.666: INFO: Pod "pod-85e1dc8b-37ac-4d3f-a88a-d90ab10bc87f" satisfied condition "success or failure"
Oct 26 20:01:20.678: INFO: Trying to get logs from node 10.123.240.172 pod pod-85e1dc8b-37ac-4d3f-a88a-d90ab10bc87f container test-container: <nil>
STEP: delete the pod
Oct 26 20:01:20.764: INFO: Waiting for pod pod-85e1dc8b-37ac-4d3f-a88a-d90ab10bc87f to disappear
Oct 26 20:01:20.776: INFO: Pod pod-85e1dc8b-37ac-4d3f-a88a-d90ab10bc87f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:01:20.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5218" for this suite.
Oct 26 20:01:28.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:01:30.890: INFO: namespace emptydir-5218 deletion completed in 10.075859254s

• [SLOW TEST:16.469 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:01:30.890: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Oct 26 20:01:31.066: INFO: Waiting up to 5m0s for pod "pod-be98f6f0-9e4c-471f-8bb0-ca505df3f922" in namespace "emptydir-2139" to be "success or failure"
Oct 26 20:01:31.077: INFO: Pod "pod-be98f6f0-9e4c-471f-8bb0-ca505df3f922": Phase="Pending", Reason="", readiness=false. Elapsed: 10.831766ms
Oct 26 20:01:33.089: INFO: Pod "pod-be98f6f0-9e4c-471f-8bb0-ca505df3f922": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0232966s
STEP: Saw pod success
Oct 26 20:01:33.089: INFO: Pod "pod-be98f6f0-9e4c-471f-8bb0-ca505df3f922" satisfied condition "success or failure"
Oct 26 20:01:33.100: INFO: Trying to get logs from node 10.123.240.172 pod pod-be98f6f0-9e4c-471f-8bb0-ca505df3f922 container test-container: <nil>
STEP: delete the pod
Oct 26 20:01:33.156: INFO: Waiting for pod pod-be98f6f0-9e4c-471f-8bb0-ca505df3f922 to disappear
Oct 26 20:01:33.167: INFO: Pod pod-be98f6f0-9e4c-471f-8bb0-ca505df3f922 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:01:33.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2139" for this suite.
Oct 26 20:01:41.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:01:43.252: INFO: namespace emptydir-2139 deletion completed in 10.057079707s

• [SLOW TEST:12.362 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:01:43.254: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Oct 26 20:01:44.493: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1685535-1070-44e4-b48a-895d710615d2" in namespace "downward-api-4392" to be "success or failure"
Oct 26 20:01:44.506: INFO: Pod "downwardapi-volume-d1685535-1070-44e4-b48a-895d710615d2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.748647ms
Oct 26 20:01:46.519: INFO: Pod "downwardapi-volume-d1685535-1070-44e4-b48a-895d710615d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025633454s
STEP: Saw pod success
Oct 26 20:01:46.519: INFO: Pod "downwardapi-volume-d1685535-1070-44e4-b48a-895d710615d2" satisfied condition "success or failure"
Oct 26 20:01:46.531: INFO: Trying to get logs from node 10.123.240.172 pod downwardapi-volume-d1685535-1070-44e4-b48a-895d710615d2 container client-container: <nil>
STEP: delete the pod
Oct 26 20:01:46.585: INFO: Waiting for pod downwardapi-volume-d1685535-1070-44e4-b48a-895d710615d2 to disappear
Oct 26 20:01:46.596: INFO: Pod downwardapi-volume-d1685535-1070-44e4-b48a-895d710615d2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:01:46.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4392" for this suite.
Oct 26 20:01:54.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:01:56.763: INFO: namespace downward-api-4392 deletion completed in 10.145563639s

• [SLOW TEST:13.509 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:01:56.763: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-71328247-2bf9-42e5-bf70-053e97e23e44
STEP: Creating a pod to test consume configMaps
Oct 26 20:01:56.973: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9561e132-0cb0-4330-b455-d5c6e49724ff" in namespace "projected-3305" to be "success or failure"
Oct 26 20:01:56.999: INFO: Pod "pod-projected-configmaps-9561e132-0cb0-4330-b455-d5c6e49724ff": Phase="Pending", Reason="", readiness=false. Elapsed: 25.489322ms
Oct 26 20:01:59.015: INFO: Pod "pod-projected-configmaps-9561e132-0cb0-4330-b455-d5c6e49724ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040848233s
STEP: Saw pod success
Oct 26 20:01:59.015: INFO: Pod "pod-projected-configmaps-9561e132-0cb0-4330-b455-d5c6e49724ff" satisfied condition "success or failure"
Oct 26 20:01:59.028: INFO: Trying to get logs from node 10.123.240.172 pod pod-projected-configmaps-9561e132-0cb0-4330-b455-d5c6e49724ff container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 20:01:59.104: INFO: Waiting for pod pod-projected-configmaps-9561e132-0cb0-4330-b455-d5c6e49724ff to disappear
Oct 26 20:01:59.124: INFO: Pod pod-projected-configmaps-9561e132-0cb0-4330-b455-d5c6e49724ff no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:01:59.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3305" for this suite.
Oct 26 20:02:07.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:02:09.170: INFO: namespace projected-3305 deletion completed in 10.015767227s

• [SLOW TEST:12.407 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:02:09.171: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-538399db-e846-409f-a5b6-66b52530d506
STEP: Creating a pod to test consume configMaps
Oct 26 20:02:09.373: INFO: Waiting up to 5m0s for pod "pod-configmaps-a879c583-dec6-4235-bd6e-4313f23a8674" in namespace "configmap-5621" to be "success or failure"
Oct 26 20:02:09.385: INFO: Pod "pod-configmaps-a879c583-dec6-4235-bd6e-4313f23a8674": Phase="Pending", Reason="", readiness=false. Elapsed: 12.235166ms
Oct 26 20:02:11.397: INFO: Pod "pod-configmaps-a879c583-dec6-4235-bd6e-4313f23a8674": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024076874s
STEP: Saw pod success
Oct 26 20:02:11.397: INFO: Pod "pod-configmaps-a879c583-dec6-4235-bd6e-4313f23a8674" satisfied condition "success or failure"
Oct 26 20:02:11.407: INFO: Trying to get logs from node 10.123.240.178 pod pod-configmaps-a879c583-dec6-4235-bd6e-4313f23a8674 container configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 20:02:11.489: INFO: Waiting for pod pod-configmaps-a879c583-dec6-4235-bd6e-4313f23a8674 to disappear
Oct 26 20:02:11.505: INFO: Pod pod-configmaps-a879c583-dec6-4235-bd6e-4313f23a8674 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:02:11.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5621" for this suite.
Oct 26 20:02:19.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:02:21.600: INFO: namespace configmap-5621 deletion completed in 10.061676348s

• [SLOW TEST:12.428 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:02:21.600: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Oct 26 20:02:21.794: INFO: Waiting up to 5m0s for pod "pod-3fe86848-f7b2-4026-91ec-d325ee7b85ea" in namespace "emptydir-5392" to be "success or failure"
Oct 26 20:02:21.803: INFO: Pod "pod-3fe86848-f7b2-4026-91ec-d325ee7b85ea": Phase="Pending", Reason="", readiness=false. Elapsed: 9.893663ms
Oct 26 20:02:23.815: INFO: Pod "pod-3fe86848-f7b2-4026-91ec-d325ee7b85ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021903703s
Oct 26 20:02:25.835: INFO: Pod "pod-3fe86848-f7b2-4026-91ec-d325ee7b85ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041817066s
STEP: Saw pod success
Oct 26 20:02:25.835: INFO: Pod "pod-3fe86848-f7b2-4026-91ec-d325ee7b85ea" satisfied condition "success or failure"
Oct 26 20:02:25.860: INFO: Trying to get logs from node 10.123.240.178 pod pod-3fe86848-f7b2-4026-91ec-d325ee7b85ea container test-container: <nil>
STEP: delete the pod
Oct 26 20:02:25.937: INFO: Waiting for pod pod-3fe86848-f7b2-4026-91ec-d325ee7b85ea to disappear
Oct 26 20:02:25.951: INFO: Pod pod-3fe86848-f7b2-4026-91ec-d325ee7b85ea no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:02:25.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5392" for this suite.
Oct 26 20:02:34.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:02:36.060: INFO: namespace emptydir-5392 deletion completed in 10.083208686s

• [SLOW TEST:14.460 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:02:36.060: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Oct 26 20:02:36.280: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d3282067-f95f-4cdb-9e66-cd48011e65b1" in namespace "downward-api-8115" to be "success or failure"
Oct 26 20:02:36.291: INFO: Pod "downwardapi-volume-d3282067-f95f-4cdb-9e66-cd48011e65b1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.306333ms
Oct 26 20:02:38.305: INFO: Pod "downwardapi-volume-d3282067-f95f-4cdb-9e66-cd48011e65b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024595236s
STEP: Saw pod success
Oct 26 20:02:38.305: INFO: Pod "downwardapi-volume-d3282067-f95f-4cdb-9e66-cd48011e65b1" satisfied condition "success or failure"
Oct 26 20:02:38.315: INFO: Trying to get logs from node 10.123.240.178 pod downwardapi-volume-d3282067-f95f-4cdb-9e66-cd48011e65b1 container client-container: <nil>
STEP: delete the pod
Oct 26 20:02:38.390: INFO: Waiting for pod downwardapi-volume-d3282067-f95f-4cdb-9e66-cd48011e65b1 to disappear
Oct 26 20:02:38.428: INFO: Pod downwardapi-volume-d3282067-f95f-4cdb-9e66-cd48011e65b1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:02:38.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8115" for this suite.
Oct 26 20:02:46.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:02:48.531: INFO: namespace downward-api-8115 deletion completed in 10.067935427s

• [SLOW TEST:12.471 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:02:48.531: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Oct 26 20:02:48.706: INFO: Waiting up to 5m0s for pod "client-containers-6dc21a2e-3b84-4e6e-8083-57404c91b525" in namespace "containers-9233" to be "success or failure"
Oct 26 20:02:48.717: INFO: Pod "client-containers-6dc21a2e-3b84-4e6e-8083-57404c91b525": Phase="Pending", Reason="", readiness=false. Elapsed: 10.64312ms
Oct 26 20:02:50.728: INFO: Pod "client-containers-6dc21a2e-3b84-4e6e-8083-57404c91b525": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022298221s
STEP: Saw pod success
Oct 26 20:02:50.729: INFO: Pod "client-containers-6dc21a2e-3b84-4e6e-8083-57404c91b525" satisfied condition "success or failure"
Oct 26 20:02:50.740: INFO: Trying to get logs from node 10.123.240.172 pod client-containers-6dc21a2e-3b84-4e6e-8083-57404c91b525 container test-container: <nil>
STEP: delete the pod
Oct 26 20:02:50.804: INFO: Waiting for pod client-containers-6dc21a2e-3b84-4e6e-8083-57404c91b525 to disappear
Oct 26 20:02:50.814: INFO: Pod client-containers-6dc21a2e-3b84-4e6e-8083-57404c91b525 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:02:50.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9233" for this suite.
Oct 26 20:02:58.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:03:00.911: INFO: namespace containers-9233 deletion completed in 10.053362738s

• [SLOW TEST:12.380 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:03:00.912: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-8672
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct 26 20:03:01.061: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Oct 26 20:03:23.537: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.37.208:8080/dial?request=hostName&protocol=http&host=172.30.84.10&port=8080&tries=1'] Namespace:pod-network-test-8672 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 20:03:23.537: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 20:03:23.734: INFO: Waiting for endpoints: map[]
Oct 26 20:03:23.756: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.37.208:8080/dial?request=hostName&protocol=http&host=172.30.101.234&port=8080&tries=1'] Namespace:pod-network-test-8672 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 20:03:23.756: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 20:03:23.962: INFO: Waiting for endpoints: map[]
Oct 26 20:03:23.973: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.37.208:8080/dial?request=hostName&protocol=http&host=172.30.37.200&port=8080&tries=1'] Namespace:pod-network-test-8672 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 20:03:23.973: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 20:03:24.500: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:03:24.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8672" for this suite.
Oct 26 20:03:32.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:03:34.641: INFO: namespace pod-network-test-8672 deletion completed in 10.113437534s

• [SLOW TEST:33.729 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:03:34.641: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Oct 26 20:03:43.962: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-517 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 20:03:43.962: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 20:03:44.151: INFO: Exec stderr: ""
Oct 26 20:03:44.151: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-517 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 20:03:44.151: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 20:03:44.340: INFO: Exec stderr: ""
Oct 26 20:03:44.340: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-517 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 20:03:44.340: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 20:03:45.074: INFO: Exec stderr: ""
Oct 26 20:03:45.074: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-517 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 20:03:45.074: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 20:03:45.294: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Oct 26 20:03:45.294: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-517 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 20:03:45.294: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 20:03:45.510: INFO: Exec stderr: ""
Oct 26 20:03:45.510: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-517 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 20:03:45.510: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 20:03:45.812: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Oct 26 20:03:45.812: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-517 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 20:03:45.812: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 20:03:46.064: INFO: Exec stderr: ""
Oct 26 20:03:46.064: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-517 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 20:03:46.064: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 20:03:46.297: INFO: Exec stderr: ""
Oct 26 20:03:46.297: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-517 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 20:03:46.297: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 20:03:46.583: INFO: Exec stderr: ""
Oct 26 20:03:46.583: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-517 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 20:03:46.583: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 20:03:46.792: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:03:46.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-517" for this suite.
Oct 26 20:04:40.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:04:42.834: INFO: namespace e2e-kubelet-etc-hosts-517 deletion completed in 56.009136084s

• [SLOW TEST:68.193 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:04:42.836: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Oct 26 20:04:43.022: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3fd4068d-50df-4360-8e57-aee2a6d532f3" in namespace "projected-857" to be "success or failure"
Oct 26 20:04:43.031: INFO: Pod "downwardapi-volume-3fd4068d-50df-4360-8e57-aee2a6d532f3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.496051ms
Oct 26 20:04:45.043: INFO: Pod "downwardapi-volume-3fd4068d-50df-4360-8e57-aee2a6d532f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020668178s
STEP: Saw pod success
Oct 26 20:04:45.043: INFO: Pod "downwardapi-volume-3fd4068d-50df-4360-8e57-aee2a6d532f3" satisfied condition "success or failure"
Oct 26 20:04:45.053: INFO: Trying to get logs from node 10.123.240.172 pod downwardapi-volume-3fd4068d-50df-4360-8e57-aee2a6d532f3 container client-container: <nil>
STEP: delete the pod
Oct 26 20:04:45.135: INFO: Waiting for pod downwardapi-volume-3fd4068d-50df-4360-8e57-aee2a6d532f3 to disappear
Oct 26 20:04:45.145: INFO: Pod downwardapi-volume-3fd4068d-50df-4360-8e57-aee2a6d532f3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:04:45.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-857" for this suite.
Oct 26 20:04:53.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:04:55.244: INFO: namespace projected-857 deletion completed in 10.070973811s

• [SLOW TEST:12.408 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:04:55.244: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:05:12.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4799" for this suite.
Oct 26 20:05:20.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:05:22.266: INFO: namespace namespaces-4799 deletion completed in 10.154103435s
STEP: Destroying namespace "nsdeletetest-728" for this suite.
Oct 26 20:05:22.278: INFO: Namespace nsdeletetest-728 was already deleted
STEP: Destroying namespace "nsdeletetest-2355" for this suite.
Oct 26 20:05:30.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:05:32.336: INFO: namespace nsdeletetest-2355 deletion completed in 10.057903552s

• [SLOW TEST:37.092 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:05:32.338: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Oct 26 20:05:37.135: INFO: Successfully updated pod "labelsupdatebc178d41-0244-4ed7-a967-55fec2f6e3c3"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:05:39.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4182" for this suite.
Oct 26 20:05:53.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:05:55.395: INFO: namespace downward-api-4182 deletion completed in 16.154363521s

• [SLOW TEST:23.057 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:05:55.396: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 20:05:55.552: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:06:02.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8376" for this suite.
Oct 26 20:06:10.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:06:13.055: INFO: namespace custom-resource-definition-8376 deletion completed in 10.13356151s

• [SLOW TEST:17.659 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:06:13.055: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:06:26.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7906" for this suite.
Oct 26 20:06:34.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:06:36.622: INFO: namespace resourcequota-7906 deletion completed in 10.120825813s

• [SLOW TEST:23.567 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:06:36.622: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Oct 26 20:06:36.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 create -f - --namespace=kubectl-6045'
Oct 26 20:06:37.375: INFO: stderr: ""
Oct 26 20:06:37.375: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Oct 26 20:06:38.387: INFO: Selector matched 1 pods for map[app:redis]
Oct 26 20:06:38.387: INFO: Found 0 / 1
Oct 26 20:06:39.388: INFO: Selector matched 1 pods for map[app:redis]
Oct 26 20:06:39.388: INFO: Found 0 / 1
Oct 26 20:06:40.389: INFO: Selector matched 1 pods for map[app:redis]
Oct 26 20:06:40.389: INFO: Found 1 / 1
Oct 26 20:06:40.389: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Oct 26 20:06:40.402: INFO: Selector matched 1 pods for map[app:redis]
Oct 26 20:06:40.402: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Oct 26 20:06:40.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 patch pod redis-master-dp7wg --namespace=kubectl-6045 -p {"metadata":{"annotations":{"x":"y"}}}'
Oct 26 20:06:40.572: INFO: stderr: ""
Oct 26 20:06:40.572: INFO: stdout: "pod/redis-master-dp7wg patched\n"
STEP: checking annotations
Oct 26 20:06:40.583: INFO: Selector matched 1 pods for map[app:redis]
Oct 26 20:06:40.583: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:06:40.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6045" for this suite.
Oct 26 20:06:54.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:06:56.856: INFO: namespace kubectl-6045 deletion completed in 16.234210916s

• [SLOW TEST:20.234 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:06:56.858: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Oct 26 20:06:57.064: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-5505" to be "success or failure"
Oct 26 20:06:57.076: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 12.084762ms
Oct 26 20:06:59.089: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025157823s
Oct 26 20:07:01.111: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046510264s
STEP: Saw pod success
Oct 26 20:07:01.111: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Oct 26 20:07:01.126: INFO: Trying to get logs from node 10.123.240.174 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Oct 26 20:07:01.221: INFO: Waiting for pod pod-host-path-test to disappear
Oct 26 20:07:01.230: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:07:01.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-5505" for this suite.
Oct 26 20:07:09.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:07:11.352: INFO: namespace hostpath-5505 deletion completed in 10.086400991s

• [SLOW TEST:14.494 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:07:11.353: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 20:07:11.538: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-9214f7df-a7c0-4001-a905-b2173a12e2fe
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-9214f7df-a7c0-4001-a905-b2173a12e2fe
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:08:36.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3811" for this suite.
Oct 26 20:09:09.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:09:11.059: INFO: namespace projected-3811 deletion completed in 34.091154546s

• [SLOW TEST:119.706 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:09:11.060: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 20:09:11.220: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Oct 26 20:09:13.335: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:09:14.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5678" for this suite.
Oct 26 20:09:22.425: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:09:24.495: INFO: namespace replication-controller-5678 deletion completed in 10.111721427s

• [SLOW TEST:13.435 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:09:24.495: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Oct 26 20:09:29.795: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:09:30.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4333" for this suite.
Oct 26 20:09:44.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:09:46.975: INFO: namespace replicaset-4333 deletion completed in 16.053958975s

• [SLOW TEST:22.480 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:09:46.976: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Oct 26 20:09:47.123: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 20:09:56.400: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:10:32.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8506" for this suite.
Oct 26 20:10:40.123: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:10:42.214: INFO: namespace crd-publish-openapi-8506 deletion completed in 10.138311469s

• [SLOW TEST:55.238 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:10:42.214: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:10:42.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-1783" for this suite.
Oct 26 20:10:50.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:10:52.425: INFO: namespace tables-1783 deletion completed in 10.052321816s

• [SLOW TEST:10.211 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:10:52.425: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-4e5d0b5f-8b53-4d9a-bd45-cd34836dfcfe
STEP: Creating a pod to test consume configMaps
Oct 26 20:10:52.631: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b6f1eb94-b4d2-4581-a8f0-b44d3589e694" in namespace "projected-6927" to be "success or failure"
Oct 26 20:10:52.642: INFO: Pod "pod-projected-configmaps-b6f1eb94-b4d2-4581-a8f0-b44d3589e694": Phase="Pending", Reason="", readiness=false. Elapsed: 10.484184ms
Oct 26 20:10:54.653: INFO: Pod "pod-projected-configmaps-b6f1eb94-b4d2-4581-a8f0-b44d3589e694": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021651975s
STEP: Saw pod success
Oct 26 20:10:54.653: INFO: Pod "pod-projected-configmaps-b6f1eb94-b4d2-4581-a8f0-b44d3589e694" satisfied condition "success or failure"
Oct 26 20:10:54.663: INFO: Trying to get logs from node 10.123.240.178 pod pod-projected-configmaps-b6f1eb94-b4d2-4581-a8f0-b44d3589e694 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 20:10:54.743: INFO: Waiting for pod pod-projected-configmaps-b6f1eb94-b4d2-4581-a8f0-b44d3589e694 to disappear
Oct 26 20:10:54.753: INFO: Pod pod-projected-configmaps-b6f1eb94-b4d2-4581-a8f0-b44d3589e694 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:10:54.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6927" for this suite.
Oct 26 20:11:02.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:11:04.831: INFO: namespace projected-6927 deletion completed in 10.058971411s

• [SLOW TEST:12.406 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:11:04.831: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-487
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-487
STEP: creating replication controller externalsvc in namespace services-487
I1026 20:11:05.141171      23 runners.go:184] Created replication controller with name: externalsvc, namespace: services-487, replica count: 2
I1026 20:11:08.192690      23 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Oct 26 20:11:08.256: INFO: Creating new exec pod
Oct 26 20:11:12.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=services-487 execpodmf52x -- /bin/sh -x -c nslookup clusterip-service'
Oct 26 20:11:13.074: INFO: stderr: "+ nslookup clusterip-service\n"
Oct 26 20:11:13.074: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-487.svc.cluster.local\tcanonical name = externalsvc.services-487.svc.cluster.local.\nName:\texternalsvc.services-487.svc.cluster.local\nAddress: 172.21.16.178\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-487, will wait for the garbage collector to delete the pods
Oct 26 20:11:13.164: INFO: Deleting ReplicationController externalsvc took: 27.6114ms
Oct 26 20:11:13.664: INFO: Terminating ReplicationController externalsvc pods took: 500.315635ms
Oct 26 20:11:20.630: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:11:20.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-487" for this suite.
Oct 26 20:11:28.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:11:30.778: INFO: namespace services-487 deletion completed in 10.047872054s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:25.947 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:11:30.778: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
Oct 26 20:11:30.913: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 26 20:12:31.084: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 20:12:31.107: INFO: Starting informer...
STEP: Starting pods...
Oct 26 20:12:31.394: INFO: Pod1 is running on 10.123.240.178. Tainting Node
Oct 26 20:12:35.692: INFO: Pod2 is running on 10.123.240.178. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Oct 26 20:12:54.078: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Oct 26 20:13:14.105: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:13:14.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-1031" for this suite.
Oct 26 20:13:22.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:13:24.269: INFO: namespace taint-multiple-pods-1031 deletion completed in 10.103390951s

• [SLOW TEST:113.491 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:13:24.271: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Oct 26 20:13:24.474: INFO: Waiting up to 5m0s for pod "pod-4d91070a-ae04-4f8e-bf91-c483e2a2abcf" in namespace "emptydir-9954" to be "success or failure"
Oct 26 20:13:24.490: INFO: Pod "pod-4d91070a-ae04-4f8e-bf91-c483e2a2abcf": Phase="Pending", Reason="", readiness=false. Elapsed: 15.251321ms
Oct 26 20:13:26.501: INFO: Pod "pod-4d91070a-ae04-4f8e-bf91-c483e2a2abcf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027014639s
Oct 26 20:13:28.517: INFO: Pod "pod-4d91070a-ae04-4f8e-bf91-c483e2a2abcf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042276983s
STEP: Saw pod success
Oct 26 20:13:28.517: INFO: Pod "pod-4d91070a-ae04-4f8e-bf91-c483e2a2abcf" satisfied condition "success or failure"
Oct 26 20:13:28.527: INFO: Trying to get logs from node 10.123.240.178 pod pod-4d91070a-ae04-4f8e-bf91-c483e2a2abcf container test-container: <nil>
STEP: delete the pod
Oct 26 20:13:28.636: INFO: Waiting for pod pod-4d91070a-ae04-4f8e-bf91-c483e2a2abcf to disappear
Oct 26 20:13:28.648: INFO: Pod pod-4d91070a-ae04-4f8e-bf91-c483e2a2abcf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:13:28.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9954" for this suite.
Oct 26 20:13:36.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:13:38.774: INFO: namespace emptydir-9954 deletion completed in 10.1113769s

• [SLOW TEST:14.503 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:13:38.774: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Oct 26 20:13:38.966: INFO: Waiting up to 5m0s for pod "pod-237a58f8-b93d-448e-95d8-f6eaeab82497" in namespace "emptydir-7402" to be "success or failure"
Oct 26 20:13:38.981: INFO: Pod "pod-237a58f8-b93d-448e-95d8-f6eaeab82497": Phase="Pending", Reason="", readiness=false. Elapsed: 15.517007ms
Oct 26 20:13:41.008: INFO: Pod "pod-237a58f8-b93d-448e-95d8-f6eaeab82497": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042666088s
Oct 26 20:13:43.024: INFO: Pod "pod-237a58f8-b93d-448e-95d8-f6eaeab82497": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058133044s
STEP: Saw pod success
Oct 26 20:13:43.024: INFO: Pod "pod-237a58f8-b93d-448e-95d8-f6eaeab82497" satisfied condition "success or failure"
Oct 26 20:13:43.036: INFO: Trying to get logs from node 10.123.240.178 pod pod-237a58f8-b93d-448e-95d8-f6eaeab82497 container test-container: <nil>
STEP: delete the pod
Oct 26 20:13:43.099: INFO: Waiting for pod pod-237a58f8-b93d-448e-95d8-f6eaeab82497 to disappear
Oct 26 20:13:43.117: INFO: Pod pod-237a58f8-b93d-448e-95d8-f6eaeab82497 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:13:43.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7402" for this suite.
Oct 26 20:13:51.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:13:53.261: INFO: namespace emptydir-7402 deletion completed in 10.113959433s

• [SLOW TEST:14.487 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:13:53.265: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Oct 26 20:13:53.476: INFO: Waiting up to 5m0s for pod "downward-api-ba12257b-306b-44bc-87e1-779a1edace9b" in namespace "downward-api-2097" to be "success or failure"
Oct 26 20:13:53.496: INFO: Pod "downward-api-ba12257b-306b-44bc-87e1-779a1edace9b": Phase="Pending", Reason="", readiness=false. Elapsed: 19.914197ms
Oct 26 20:13:55.509: INFO: Pod "downward-api-ba12257b-306b-44bc-87e1-779a1edace9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032503188s
STEP: Saw pod success
Oct 26 20:13:55.509: INFO: Pod "downward-api-ba12257b-306b-44bc-87e1-779a1edace9b" satisfied condition "success or failure"
Oct 26 20:13:55.519: INFO: Trying to get logs from node 10.123.240.178 pod downward-api-ba12257b-306b-44bc-87e1-779a1edace9b container dapi-container: <nil>
STEP: delete the pod
Oct 26 20:13:55.585: INFO: Waiting for pod downward-api-ba12257b-306b-44bc-87e1-779a1edace9b to disappear
Oct 26 20:13:55.596: INFO: Pod downward-api-ba12257b-306b-44bc-87e1-779a1edace9b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:13:55.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2097" for this suite.
Oct 26 20:14:03.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:14:05.720: INFO: namespace downward-api-2097 deletion completed in 10.098116932s

• [SLOW TEST:12.455 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:14:05.720: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Oct 26 20:14:46.070: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W1026 20:14:46.070600      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Oct 26 20:14:46.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3624" for this suite.
Oct 26 20:14:56.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:14:58.129: INFO: namespace gc-3624 deletion completed in 12.034995443s

• [SLOW TEST:52.409 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:14:58.129: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:15:58.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5741" for this suite.
Oct 26 20:16:30.499: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:16:32.525: INFO: namespace container-probe-5741 deletion completed in 34.066190865s

• [SLOW TEST:94.395 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:16:32.525: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Oct 26 20:16:32.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 create -f - --namespace=kubectl-6345'
Oct 26 20:16:33.328: INFO: stderr: ""
Oct 26 20:16:33.328: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 26 20:16:33.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6345'
Oct 26 20:16:33.483: INFO: stderr: ""
Oct 26 20:16:33.483: INFO: stdout: "update-demo-nautilus-fb7rx update-demo-nautilus-k9297 "
Oct 26 20:16:33.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-nautilus-fb7rx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6345'
Oct 26 20:16:33.622: INFO: stderr: ""
Oct 26 20:16:33.622: INFO: stdout: ""
Oct 26 20:16:33.622: INFO: update-demo-nautilus-fb7rx is created but not running
Oct 26 20:16:38.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6345'
Oct 26 20:16:38.801: INFO: stderr: ""
Oct 26 20:16:38.801: INFO: stdout: "update-demo-nautilus-fb7rx update-demo-nautilus-k9297 "
Oct 26 20:16:38.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-nautilus-fb7rx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6345'
Oct 26 20:16:38.939: INFO: stderr: ""
Oct 26 20:16:38.939: INFO: stdout: "true"
Oct 26 20:16:38.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-nautilus-fb7rx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6345'
Oct 26 20:16:39.077: INFO: stderr: ""
Oct 26 20:16:39.077: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct 26 20:16:39.077: INFO: validating pod update-demo-nautilus-fb7rx
Oct 26 20:16:39.114: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 26 20:16:39.114: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 26 20:16:39.114: INFO: update-demo-nautilus-fb7rx is verified up and running
Oct 26 20:16:39.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-nautilus-k9297 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6345'
Oct 26 20:16:39.309: INFO: stderr: ""
Oct 26 20:16:39.309: INFO: stdout: "true"
Oct 26 20:16:39.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-nautilus-k9297 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6345'
Oct 26 20:16:39.450: INFO: stderr: ""
Oct 26 20:16:39.450: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct 26 20:16:39.450: INFO: validating pod update-demo-nautilus-k9297
Oct 26 20:16:39.476: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 26 20:16:39.476: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 26 20:16:39.476: INFO: update-demo-nautilus-k9297 is verified up and running
STEP: using delete to clean up resources
Oct 26 20:16:39.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 delete --grace-period=0 --force -f - --namespace=kubectl-6345'
Oct 26 20:16:39.639: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 26 20:16:39.639: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Oct 26 20:16:39.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6345'
Oct 26 20:16:39.814: INFO: stderr: "No resources found in kubectl-6345 namespace.\n"
Oct 26 20:16:39.814: INFO: stdout: ""
Oct 26 20:16:39.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods -l name=update-demo --namespace=kubectl-6345 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Oct 26 20:16:39.959: INFO: stderr: ""
Oct 26 20:16:39.959: INFO: stdout: "update-demo-nautilus-fb7rx\nupdate-demo-nautilus-k9297\n"
Oct 26 20:16:40.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6345'
Oct 26 20:16:40.641: INFO: stderr: "No resources found in kubectl-6345 namespace.\n"
Oct 26 20:16:40.641: INFO: stdout: ""
Oct 26 20:16:40.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods -l name=update-demo --namespace=kubectl-6345 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Oct 26 20:16:40.811: INFO: stderr: ""
Oct 26 20:16:40.811: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:16:40.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6345" for this suite.
Oct 26 20:16:48.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:16:50.970: INFO: namespace kubectl-6345 deletion completed in 10.125679811s

• [SLOW TEST:18.446 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:16:50.971: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 20:16:51.669: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 20:16:53.705: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739340211, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739340211, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739340211, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739340211, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 20:16:56.750: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 20:16:56.762: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:16:58.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-268" for this suite.
Oct 26 20:17:06.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:17:08.353: INFO: namespace webhook-268 deletion completed in 10.099980635s
STEP: Destroying namespace "webhook-268-markers" for this suite.
Oct 26 20:17:16.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:17:18.371: INFO: namespace webhook-268-markers deletion completed in 10.018176869s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:27.455 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:17:18.428: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-df9a27e5-d36b-4bc1-9a0a-87d81b9f6aae
STEP: Creating a pod to test consume secrets
Oct 26 20:17:18.634: INFO: Waiting up to 5m0s for pod "pod-secrets-866d696c-a117-459d-9ba0-6ff250a8f203" in namespace "secrets-4259" to be "success or failure"
Oct 26 20:17:18.645: INFO: Pod "pod-secrets-866d696c-a117-459d-9ba0-6ff250a8f203": Phase="Pending", Reason="", readiness=false. Elapsed: 10.718398ms
Oct 26 20:17:20.658: INFO: Pod "pod-secrets-866d696c-a117-459d-9ba0-6ff250a8f203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024044296s
Oct 26 20:17:22.672: INFO: Pod "pod-secrets-866d696c-a117-459d-9ba0-6ff250a8f203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037327343s
STEP: Saw pod success
Oct 26 20:17:22.672: INFO: Pod "pod-secrets-866d696c-a117-459d-9ba0-6ff250a8f203" satisfied condition "success or failure"
Oct 26 20:17:22.682: INFO: Trying to get logs from node 10.123.240.178 pod pod-secrets-866d696c-a117-459d-9ba0-6ff250a8f203 container secret-volume-test: <nil>
STEP: delete the pod
Oct 26 20:17:22.764: INFO: Waiting for pod pod-secrets-866d696c-a117-459d-9ba0-6ff250a8f203 to disappear
Oct 26 20:17:22.775: INFO: Pod pod-secrets-866d696c-a117-459d-9ba0-6ff250a8f203 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:17:22.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4259" for this suite.
Oct 26 20:17:30.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:17:32.865: INFO: namespace secrets-4259 deletion completed in 10.061640045s

• [SLOW TEST:14.438 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:17:32.867: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 20:17:33.082: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-1f7ceb9c-3121-45a4-be87-18358f75c5b2
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:17:35.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7308" for this suite.
Oct 26 20:17:49.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:17:51.271: INFO: namespace configmap-7308 deletion completed in 16.008838591s

• [SLOW TEST:18.404 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:17:51.271: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Oct 26 20:17:51.569: INFO: Waiting up to 5m0s for pod "downward-api-e60779e0-b422-4c9a-9eaa-ebdbcfe0c8a9" in namespace "downward-api-7712" to be "success or failure"
Oct 26 20:17:51.584: INFO: Pod "downward-api-e60779e0-b422-4c9a-9eaa-ebdbcfe0c8a9": Phase="Pending", Reason="", readiness=false. Elapsed: 15.004843ms
Oct 26 20:17:53.597: INFO: Pod "downward-api-e60779e0-b422-4c9a-9eaa-ebdbcfe0c8a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028027153s
Oct 26 20:17:55.609: INFO: Pod "downward-api-e60779e0-b422-4c9a-9eaa-ebdbcfe0c8a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039877238s
STEP: Saw pod success
Oct 26 20:17:55.609: INFO: Pod "downward-api-e60779e0-b422-4c9a-9eaa-ebdbcfe0c8a9" satisfied condition "success or failure"
Oct 26 20:17:55.619: INFO: Trying to get logs from node 10.123.240.178 pod downward-api-e60779e0-b422-4c9a-9eaa-ebdbcfe0c8a9 container dapi-container: <nil>
STEP: delete the pod
Oct 26 20:17:55.675: INFO: Waiting for pod downward-api-e60779e0-b422-4c9a-9eaa-ebdbcfe0c8a9 to disappear
Oct 26 20:17:55.688: INFO: Pod downward-api-e60779e0-b422-4c9a-9eaa-ebdbcfe0c8a9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:17:55.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7712" for this suite.
Oct 26 20:18:03.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:18:05.917: INFO: namespace downward-api-7712 deletion completed in 10.208020647s

• [SLOW TEST:14.645 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:18:05.918: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 20:18:06.979: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 20:18:09.014: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739340287, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739340287, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739340287, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739340286, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 20:18:12.057: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:18:12.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7750" for this suite.
Oct 26 20:18:20.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:18:22.304: INFO: namespace webhook-7750 deletion completed in 10.083901261s
STEP: Destroying namespace "webhook-7750-markers" for this suite.
Oct 26 20:18:30.357: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:18:32.322: INFO: namespace webhook-7750-markers deletion completed in 10.018095814s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:26.459 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:18:32.378: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 20:18:32.579: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:18:33.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7847" for this suite.
Oct 26 20:18:41.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:18:43.287: INFO: namespace custom-resource-definition-7847 deletion completed in 10.050579982s

• [SLOW TEST:10.910 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:18:43.287: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Oct 26 20:18:43.477: INFO: Waiting up to 5m0s for pod "var-expansion-a74c16e5-6bbd-44c2-bc78-84f2d032beb3" in namespace "var-expansion-8832" to be "success or failure"
Oct 26 20:18:43.487: INFO: Pod "var-expansion-a74c16e5-6bbd-44c2-bc78-84f2d032beb3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.283683ms
Oct 26 20:18:45.501: INFO: Pod "var-expansion-a74c16e5-6bbd-44c2-bc78-84f2d032beb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023933018s
STEP: Saw pod success
Oct 26 20:18:45.501: INFO: Pod "var-expansion-a74c16e5-6bbd-44c2-bc78-84f2d032beb3" satisfied condition "success or failure"
Oct 26 20:18:45.522: INFO: Trying to get logs from node 10.123.240.178 pod var-expansion-a74c16e5-6bbd-44c2-bc78-84f2d032beb3 container dapi-container: <nil>
STEP: delete the pod
Oct 26 20:18:45.610: INFO: Waiting for pod var-expansion-a74c16e5-6bbd-44c2-bc78-84f2d032beb3 to disappear
Oct 26 20:18:45.621: INFO: Pod var-expansion-a74c16e5-6bbd-44c2-bc78-84f2d032beb3 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:18:45.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8832" for this suite.
Oct 26 20:18:53.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:18:55.740: INFO: namespace var-expansion-8832 deletion completed in 10.088604972s

• [SLOW TEST:12.452 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:18:55.740: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 20:18:56.060: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Oct 26 20:18:56.087: INFO: Number of nodes with available pods: 0
Oct 26 20:18:56.087: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Oct 26 20:18:56.169: INFO: Number of nodes with available pods: 0
Oct 26 20:18:56.169: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:18:57.182: INFO: Number of nodes with available pods: 0
Oct 26 20:18:57.182: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:18:58.181: INFO: Number of nodes with available pods: 0
Oct 26 20:18:58.181: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:18:59.183: INFO: Number of nodes with available pods: 1
Oct 26 20:18:59.183: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Oct 26 20:18:59.241: INFO: Number of nodes with available pods: 1
Oct 26 20:18:59.241: INFO: Number of running nodes: 0, number of available pods: 1
Oct 26 20:19:00.256: INFO: Number of nodes with available pods: 0
Oct 26 20:19:00.256: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Oct 26 20:19:00.285: INFO: Number of nodes with available pods: 0
Oct 26 20:19:00.285: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:19:01.296: INFO: Number of nodes with available pods: 0
Oct 26 20:19:01.296: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:19:02.298: INFO: Number of nodes with available pods: 0
Oct 26 20:19:02.298: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:19:03.299: INFO: Number of nodes with available pods: 0
Oct 26 20:19:03.299: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:19:04.301: INFO: Number of nodes with available pods: 0
Oct 26 20:19:04.301: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:19:05.299: INFO: Number of nodes with available pods: 0
Oct 26 20:19:05.299: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:19:06.298: INFO: Number of nodes with available pods: 0
Oct 26 20:19:06.298: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:19:07.297: INFO: Number of nodes with available pods: 0
Oct 26 20:19:07.297: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:19:08.298: INFO: Number of nodes with available pods: 0
Oct 26 20:19:08.298: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:19:09.307: INFO: Number of nodes with available pods: 0
Oct 26 20:19:09.308: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:19:10.298: INFO: Number of nodes with available pods: 0
Oct 26 20:19:10.298: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:19:11.298: INFO: Number of nodes with available pods: 0
Oct 26 20:19:11.298: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:19:12.296: INFO: Number of nodes with available pods: 1
Oct 26 20:19:12.296: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7743, will wait for the garbage collector to delete the pods
Oct 26 20:19:12.415: INFO: Deleting DaemonSet.extensions daemon-set took: 28.286029ms
Oct 26 20:19:12.915: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.415737ms
Oct 26 20:19:20.629: INFO: Number of nodes with available pods: 0
Oct 26 20:19:20.629: INFO: Number of running nodes: 0, number of available pods: 0
Oct 26 20:19:20.641: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7743/daemonsets","resourceVersion":"89090"},"items":null}

Oct 26 20:19:20.653: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7743/pods","resourceVersion":"89090"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:19:20.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7743" for this suite.
Oct 26 20:19:28.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:19:30.888: INFO: namespace daemonsets-7743 deletion completed in 10.128239109s

• [SLOW TEST:35.148 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:19:30.888: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 20:19:31.115: INFO: Waiting up to 5m0s for pod "busybox-user-65534-a478efac-c576-442c-8197-e328ebac280a" in namespace "security-context-test-3981" to be "success or failure"
Oct 26 20:19:31.124: INFO: Pod "busybox-user-65534-a478efac-c576-442c-8197-e328ebac280a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.557661ms
Oct 26 20:19:33.136: INFO: Pod "busybox-user-65534-a478efac-c576-442c-8197-e328ebac280a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021565459s
Oct 26 20:19:35.149: INFO: Pod "busybox-user-65534-a478efac-c576-442c-8197-e328ebac280a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033853638s
Oct 26 20:19:35.149: INFO: Pod "busybox-user-65534-a478efac-c576-442c-8197-e328ebac280a" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:19:35.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3981" for this suite.
Oct 26 20:19:43.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:19:45.246: INFO: namespace security-context-test-3981 deletion completed in 10.077301185s

• [SLOW TEST:14.358 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:19:45.248: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-629
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-629
STEP: Deleting pre-stop pod
Oct 26 20:19:54.618: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:19:54.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-629" for this suite.
Oct 26 20:20:34.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:20:36.733: INFO: namespace prestop-629 deletion completed in 42.066029184s

• [SLOW TEST:51.485 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:20:36.733: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 20:20:37.019: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"adf1c9a4-5f43-4fab-8859-5cf0ad8dbbe5", Controller:(*bool)(0xc002e23666), BlockOwnerDeletion:(*bool)(0xc002e23667)}}
Oct 26 20:20:37.038: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"cbc1f0e5-e7eb-4c0c-b97d-a657aa53cc17", Controller:(*bool)(0xc000454c96), BlockOwnerDeletion:(*bool)(0xc000454c97)}}
Oct 26 20:20:37.080: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"c1c9c48b-8818-4d38-b934-8e6d014f6fbf", Controller:(*bool)(0xc002e237e6), BlockOwnerDeletion:(*bool)(0xc002e237e7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:20:42.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8515" for this suite.
Oct 26 20:20:50.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:20:52.266: INFO: namespace gc-8515 deletion completed in 10.083784355s

• [SLOW TEST:15.533 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:20:52.267: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Oct 26 20:20:52.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-3982'
Oct 26 20:20:52.618: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Oct 26 20:20:52.618: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Oct 26 20:20:54.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 delete deployment e2e-test-httpd-deployment --namespace=kubectl-3982'
Oct 26 20:20:54.820: INFO: stderr: ""
Oct 26 20:20:54.820: INFO: stdout: "deployment.extensions \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:20:54.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3982" for this suite.
Oct 26 20:21:02.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:21:04.903: INFO: namespace kubectl-3982 deletion completed in 10.053562401s

• [SLOW TEST:12.637 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:21:04.905: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Oct 26 20:21:11.238: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct 26 20:21:11.250: INFO: Pod pod-with-prestop-exec-hook still exists
Oct 26 20:21:13.250: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct 26 20:21:13.266: INFO: Pod pod-with-prestop-exec-hook still exists
Oct 26 20:21:15.250: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct 26 20:21:15.273: INFO: Pod pod-with-prestop-exec-hook still exists
Oct 26 20:21:17.250: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct 26 20:21:17.262: INFO: Pod pod-with-prestop-exec-hook still exists
Oct 26 20:21:19.250: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct 26 20:21:19.273: INFO: Pod pod-with-prestop-exec-hook still exists
Oct 26 20:21:21.250: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct 26 20:21:21.263: INFO: Pod pod-with-prestop-exec-hook still exists
Oct 26 20:21:23.250: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct 26 20:21:23.265: INFO: Pod pod-with-prestop-exec-hook still exists
Oct 26 20:21:25.250: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct 26 20:21:25.262: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:21:25.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7914" for this suite.
Oct 26 20:21:39.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:21:41.391: INFO: namespace container-lifecycle-hook-7914 deletion completed in 16.039015352s

• [SLOW TEST:36.486 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:21:41.391: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Oct 26 20:21:47.767: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 26 20:21:47.782: INFO: Pod pod-with-poststart-http-hook still exists
Oct 26 20:21:49.782: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 26 20:21:49.796: INFO: Pod pod-with-poststart-http-hook still exists
Oct 26 20:21:51.782: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 26 20:21:51.793: INFO: Pod pod-with-poststart-http-hook still exists
Oct 26 20:21:53.782: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 26 20:21:53.794: INFO: Pod pod-with-poststart-http-hook still exists
Oct 26 20:21:55.782: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 26 20:21:55.793: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:21:55.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9048" for this suite.
Oct 26 20:22:27.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:22:29.861: INFO: namespace container-lifecycle-hook-9048 deletion completed in 34.037863427s

• [SLOW TEST:48.470 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:22:29.861: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Oct 26 20:22:40.131: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W1026 20:22:40.131763      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Oct 26 20:22:40.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7697" for this suite.
Oct 26 20:22:48.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:22:50.227: INFO: namespace gc-7697 deletion completed in 10.047979394s

• [SLOW TEST:20.365 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:22:50.227: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-3777
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-3777
STEP: Creating statefulset with conflicting port in namespace statefulset-3777
STEP: Waiting until pod test-pod will start running in namespace statefulset-3777
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3777
Oct 26 20:22:54.554: INFO: Observed stateful pod in namespace: statefulset-3777, name: ss-0, uid: eb91102a-5144-47f0-9ce7-106697aed214, status phase: Pending. Waiting for statefulset controller to delete.
Oct 26 20:22:54.588: INFO: Observed stateful pod in namespace: statefulset-3777, name: ss-0, uid: eb91102a-5144-47f0-9ce7-106697aed214, status phase: Failed. Waiting for statefulset controller to delete.
Oct 26 20:22:54.603: INFO: Observed stateful pod in namespace: statefulset-3777, name: ss-0, uid: eb91102a-5144-47f0-9ce7-106697aed214, status phase: Failed. Waiting for statefulset controller to delete.
Oct 26 20:22:54.612: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3777
STEP: Removing pod with conflicting port in namespace statefulset-3777
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3777 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Oct 26 20:22:58.692: INFO: Deleting all statefulset in ns statefulset-3777
Oct 26 20:22:58.703: INFO: Scaling statefulset ss to 0
Oct 26 20:23:18.753: INFO: Waiting for statefulset status.replicas updated to 0
Oct 26 20:23:18.763: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:23:18.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3777" for this suite.
Oct 26 20:23:26.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:23:28.909: INFO: namespace statefulset-3777 deletion completed in 10.070089998s

• [SLOW TEST:38.682 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:23:28.909: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2264.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2264.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2264.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2264.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2264.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2264.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 26 20:23:33.324: INFO: DNS probes using dns-2264/dns-test-ab17dfce-6820-4967-9112-5975ff69f58f succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:23:33.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2264" for this suite.
Oct 26 20:23:41.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:23:43.477: INFO: namespace dns-2264 deletion completed in 10.050699793s

• [SLOW TEST:14.568 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:23:43.479: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Oct 26 20:23:43.687: INFO: Waiting up to 5m0s for pod "pod-222fbb23-f832-4790-b929-4515afa1c9ce" in namespace "emptydir-7453" to be "success or failure"
Oct 26 20:23:43.698: INFO: Pod "pod-222fbb23-f832-4790-b929-4515afa1c9ce": Phase="Pending", Reason="", readiness=false. Elapsed: 10.534014ms
Oct 26 20:23:45.709: INFO: Pod "pod-222fbb23-f832-4790-b929-4515afa1c9ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021630912s
Oct 26 20:23:47.725: INFO: Pod "pod-222fbb23-f832-4790-b929-4515afa1c9ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037606513s
STEP: Saw pod success
Oct 26 20:23:47.725: INFO: Pod "pod-222fbb23-f832-4790-b929-4515afa1c9ce" satisfied condition "success or failure"
Oct 26 20:23:47.738: INFO: Trying to get logs from node 10.123.240.178 pod pod-222fbb23-f832-4790-b929-4515afa1c9ce container test-container: <nil>
STEP: delete the pod
Oct 26 20:23:47.910: INFO: Waiting for pod pod-222fbb23-f832-4790-b929-4515afa1c9ce to disappear
Oct 26 20:23:47.933: INFO: Pod pod-222fbb23-f832-4790-b929-4515afa1c9ce no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:23:47.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7453" for this suite.
Oct 26 20:23:56.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:23:58.102: INFO: namespace emptydir-7453 deletion completed in 10.146126834s

• [SLOW TEST:14.623 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:23:58.102: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-da5bf8d8-9bc7-44d0-9e0d-ac321ac940f9
STEP: Creating a pod to test consume secrets
Oct 26 20:23:58.508: INFO: Waiting up to 5m0s for pod "pod-secrets-c10b0d9e-9d70-4d43-926e-5a079506470b" in namespace "secrets-2465" to be "success or failure"
Oct 26 20:23:58.519: INFO: Pod "pod-secrets-c10b0d9e-9d70-4d43-926e-5a079506470b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.078528ms
Oct 26 20:24:00.530: INFO: Pod "pod-secrets-c10b0d9e-9d70-4d43-926e-5a079506470b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022347791s
STEP: Saw pod success
Oct 26 20:24:00.530: INFO: Pod "pod-secrets-c10b0d9e-9d70-4d43-926e-5a079506470b" satisfied condition "success or failure"
Oct 26 20:24:00.540: INFO: Trying to get logs from node 10.123.240.178 pod pod-secrets-c10b0d9e-9d70-4d43-926e-5a079506470b container secret-volume-test: <nil>
STEP: delete the pod
Oct 26 20:24:00.599: INFO: Waiting for pod pod-secrets-c10b0d9e-9d70-4d43-926e-5a079506470b to disappear
Oct 26 20:24:00.612: INFO: Pod pod-secrets-c10b0d9e-9d70-4d43-926e-5a079506470b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:24:00.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2465" for this suite.
Oct 26 20:24:08.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:24:10.696: INFO: namespace secrets-2465 deletion completed in 10.058997526s
STEP: Destroying namespace "secret-namespace-5911" for this suite.
Oct 26 20:24:18.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:24:20.818: INFO: namespace secret-namespace-5911 deletion completed in 10.122334342s

• [SLOW TEST:22.716 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:24:20.819: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 20:24:21.953: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 20:24:23.999: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739340661, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739340661, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739340662, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739340661, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 20:24:27.048: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 20:24:27.059: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9361-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:24:28.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4343" for this suite.
Oct 26 20:24:36.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:24:38.528: INFO: namespace webhook-4343 deletion completed in 10.079759453s
STEP: Destroying namespace "webhook-4343-markers" for this suite.
Oct 26 20:24:46.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:24:48.532: INFO: namespace webhook-4343-markers deletion completed in 10.004315074s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:27.772 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:24:48.591: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1026 20:24:58.994722      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Oct 26 20:24:58.995: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:24:58.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6274" for this suite.
Oct 26 20:25:09.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:25:11.078: INFO: namespace gc-6274 deletion completed in 12.063923671s

• [SLOW TEST:22.487 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:25:11.079: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 20:25:11.933: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 20:25:13.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739340711, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739340711, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739340711, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739340711, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 20:25:17.015: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:25:17.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9825" for this suite.
Oct 26 20:25:25.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:25:27.762: INFO: namespace webhook-9825 deletion completed in 10.092941181s
STEP: Destroying namespace "webhook-9825-markers" for this suite.
Oct 26 20:25:35.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:25:37.906: INFO: namespace webhook-9825-markers deletion completed in 10.143676492s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:26.924 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:25:38.003: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7614
I1026 20:25:38.194196      23 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7614, replica count: 1
I1026 20:25:39.244964      23 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1026 20:25:40.245262      23 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1026 20:25:41.245528      23 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 26 20:25:41.380: INFO: Created: latency-svc-ft7fp
Oct 26 20:25:41.399: INFO: Got endpoints: latency-svc-ft7fp [53.232624ms]
Oct 26 20:25:41.429: INFO: Created: latency-svc-8cssg
Oct 26 20:25:41.446: INFO: Created: latency-svc-95txr
Oct 26 20:25:41.450: INFO: Got endpoints: latency-svc-8cssg [51.031565ms]
Oct 26 20:25:41.457: INFO: Created: latency-svc-gwm29
Oct 26 20:25:41.460: INFO: Got endpoints: latency-svc-95txr [60.835642ms]
Oct 26 20:25:41.471: INFO: Created: latency-svc-sktgl
Oct 26 20:25:41.475: INFO: Got endpoints: latency-svc-gwm29 [76.090786ms]
Oct 26 20:25:41.485: INFO: Created: latency-svc-9wvd6
Oct 26 20:25:41.494: INFO: Got endpoints: latency-svc-sktgl [94.60004ms]
Oct 26 20:25:41.498: INFO: Created: latency-svc-wn75f
Oct 26 20:25:41.498: INFO: Got endpoints: latency-svc-9wvd6 [98.13822ms]
Oct 26 20:25:41.511: INFO: Created: latency-svc-bxjkw
Oct 26 20:25:41.514: INFO: Got endpoints: latency-svc-wn75f [114.38849ms]
Oct 26 20:25:41.521: INFO: Got endpoints: latency-svc-bxjkw [120.790159ms]
Oct 26 20:25:41.525: INFO: Created: latency-svc-qbldg
Oct 26 20:25:41.542: INFO: Created: latency-svc-n5jhf
Oct 26 20:25:41.544: INFO: Got endpoints: latency-svc-qbldg [144.358182ms]
Oct 26 20:25:41.559: INFO: Created: latency-svc-h586w
Oct 26 20:25:41.562: INFO: Got endpoints: latency-svc-n5jhf [162.440925ms]
Oct 26 20:25:41.570: INFO: Created: latency-svc-zg9c8
Oct 26 20:25:41.574: INFO: Got endpoints: latency-svc-h586w [173.991496ms]
Oct 26 20:25:41.580: INFO: Created: latency-svc-s6w9j
Oct 26 20:25:41.586: INFO: Got endpoints: latency-svc-zg9c8 [186.217944ms]
Oct 26 20:25:41.599: INFO: Created: latency-svc-kks2d
Oct 26 20:25:41.601: INFO: Got endpoints: latency-svc-s6w9j [200.912379ms]
Oct 26 20:25:41.611: INFO: Created: latency-svc-nfn9g
Oct 26 20:25:41.617: INFO: Got endpoints: latency-svc-kks2d [217.041397ms]
Oct 26 20:25:41.624: INFO: Created: latency-svc-wqflw
Oct 26 20:25:41.627: INFO: Got endpoints: latency-svc-nfn9g [226.870595ms]
Oct 26 20:25:41.636: INFO: Created: latency-svc-wnr9t
Oct 26 20:25:41.641: INFO: Got endpoints: latency-svc-wqflw [240.759461ms]
Oct 26 20:25:41.655: INFO: Created: latency-svc-5gqhn
Oct 26 20:25:41.664: INFO: Created: latency-svc-d9745
Oct 26 20:25:41.671: INFO: Got endpoints: latency-svc-wnr9t [220.733114ms]
Oct 26 20:25:41.679: INFO: Got endpoints: latency-svc-5gqhn [218.477643ms]
Oct 26 20:25:41.687: INFO: Got endpoints: latency-svc-d9745 [211.382123ms]
Oct 26 20:25:41.688: INFO: Created: latency-svc-9f9ts
Oct 26 20:25:41.691: INFO: Created: latency-svc-cdbg4
Oct 26 20:25:41.702: INFO: Got endpoints: latency-svc-9f9ts [61.114197ms]
Oct 26 20:25:41.708: INFO: Created: latency-svc-mwhpn
Oct 26 20:25:41.712: INFO: Got endpoints: latency-svc-cdbg4 [217.996397ms]
Oct 26 20:25:41.723: INFO: Created: latency-svc-4brwd
Oct 26 20:25:41.725: INFO: Got endpoints: latency-svc-mwhpn [226.922782ms]
Oct 26 20:25:41.740: INFO: Created: latency-svc-rpx85
Oct 26 20:25:41.743: INFO: Got endpoints: latency-svc-4brwd [229.353788ms]
Oct 26 20:25:41.754: INFO: Created: latency-svc-r47m4
Oct 26 20:25:41.760: INFO: Got endpoints: latency-svc-rpx85 [238.75865ms]
Oct 26 20:25:41.768: INFO: Created: latency-svc-wl7gr
Oct 26 20:25:41.775: INFO: Got endpoints: latency-svc-r47m4 [230.491737ms]
Oct 26 20:25:41.787: INFO: Created: latency-svc-sg4xf
Oct 26 20:25:41.798: INFO: Created: latency-svc-rq6q5
Oct 26 20:25:41.798: INFO: Got endpoints: latency-svc-wl7gr [235.548443ms]
Oct 26 20:25:41.807: INFO: Got endpoints: latency-svc-sg4xf [233.409291ms]
Oct 26 20:25:41.811: INFO: Created: latency-svc-5p87c
Oct 26 20:25:41.826: INFO: Got endpoints: latency-svc-rq6q5 [240.087432ms]
Oct 26 20:25:41.846: INFO: Created: latency-svc-2ghhl
Oct 26 20:25:41.846: INFO: Created: latency-svc-xgfzl
Oct 26 20:25:41.846: INFO: Got endpoints: latency-svc-5p87c [244.902961ms]
Oct 26 20:25:41.851: INFO: Got endpoints: latency-svc-xgfzl [233.984735ms]
Oct 26 20:25:41.859: INFO: Got endpoints: latency-svc-2ghhl [231.972202ms]
Oct 26 20:25:41.860: INFO: Created: latency-svc-495t7
Oct 26 20:25:41.873: INFO: Created: latency-svc-9r2zl
Oct 26 20:25:41.875: INFO: Got endpoints: latency-svc-495t7 [204.231136ms]
Oct 26 20:25:41.889: INFO: Created: latency-svc-xqnj6
Oct 26 20:25:41.893: INFO: Got endpoints: latency-svc-9r2zl [214.770798ms]
Oct 26 20:25:41.897: INFO: Created: latency-svc-t5kgj
Oct 26 20:25:41.903: INFO: Got endpoints: latency-svc-xqnj6 [215.726355ms]
Oct 26 20:25:41.914: INFO: Got endpoints: latency-svc-t5kgj [211.838184ms]
Oct 26 20:25:41.936: INFO: Created: latency-svc-wfxl7
Oct 26 20:25:41.950: INFO: Created: latency-svc-vjlr9
Oct 26 20:25:41.950: INFO: Got endpoints: latency-svc-wfxl7 [237.555251ms]
Oct 26 20:25:41.975: INFO: Got endpoints: latency-svc-vjlr9 [249.650045ms]
Oct 26 20:25:41.987: INFO: Created: latency-svc-wgjnf
Oct 26 20:25:41.988: INFO: Got endpoints: latency-svc-wgjnf [244.62793ms]
Oct 26 20:25:41.993: INFO: Created: latency-svc-qwnzx
Oct 26 20:25:42.016: INFO: Got endpoints: latency-svc-qwnzx [256.067717ms]
Oct 26 20:25:42.022: INFO: Created: latency-svc-hxtqt
Oct 26 20:25:42.039: INFO: Got endpoints: latency-svc-hxtqt [263.997439ms]
Oct 26 20:25:42.040: INFO: Created: latency-svc-hnvfq
Oct 26 20:25:42.042: INFO: Created: latency-svc-sf99c
Oct 26 20:25:42.050: INFO: Got endpoints: latency-svc-hnvfq [251.93404ms]
Oct 26 20:25:42.059: INFO: Created: latency-svc-c4msg
Oct 26 20:25:42.063: INFO: Got endpoints: latency-svc-sf99c [255.501167ms]
Oct 26 20:25:42.074: INFO: Created: latency-svc-dlp6m
Oct 26 20:25:42.075: INFO: Got endpoints: latency-svc-c4msg [249.00847ms]
Oct 26 20:25:42.087: INFO: Created: latency-svc-4cbwv
Oct 26 20:25:42.090: INFO: Got endpoints: latency-svc-dlp6m [244.657167ms]
Oct 26 20:25:42.100: INFO: Created: latency-svc-gj6cn
Oct 26 20:25:42.106: INFO: Got endpoints: latency-svc-4cbwv [254.617185ms]
Oct 26 20:25:42.114: INFO: Created: latency-svc-gtqp9
Oct 26 20:25:42.121: INFO: Got endpoints: latency-svc-gj6cn [261.764148ms]
Oct 26 20:25:42.128: INFO: Created: latency-svc-jvcms
Oct 26 20:25:42.134: INFO: Got endpoints: latency-svc-gtqp9 [258.445962ms]
Oct 26 20:25:42.141: INFO: Created: latency-svc-hcdxp
Oct 26 20:25:42.145: INFO: Got endpoints: latency-svc-jvcms [251.723417ms]
Oct 26 20:25:42.155: INFO: Created: latency-svc-6bxqn
Oct 26 20:25:42.161: INFO: Got endpoints: latency-svc-hcdxp [258.653312ms]
Oct 26 20:25:42.169: INFO: Created: latency-svc-r7bzp
Oct 26 20:25:42.185: INFO: Got endpoints: latency-svc-6bxqn [271.031944ms]
Oct 26 20:25:42.187: INFO: Got endpoints: latency-svc-r7bzp [236.148079ms]
Oct 26 20:25:42.188: INFO: Created: latency-svc-jqp4r
Oct 26 20:25:42.201: INFO: Got endpoints: latency-svc-jqp4r [226.086072ms]
Oct 26 20:25:42.206: INFO: Created: latency-svc-znpc2
Oct 26 20:25:42.222: INFO: Got endpoints: latency-svc-znpc2 [233.870011ms]
Oct 26 20:25:42.225: INFO: Created: latency-svc-mngnw
Oct 26 20:25:42.238: INFO: Created: latency-svc-rv8zx
Oct 26 20:25:42.246: INFO: Got endpoints: latency-svc-mngnw [230.060142ms]
Oct 26 20:25:42.256: INFO: Got endpoints: latency-svc-rv8zx [216.876899ms]
Oct 26 20:25:42.262: INFO: Created: latency-svc-zsnlh
Oct 26 20:25:42.272: INFO: Created: latency-svc-g6fjn
Oct 26 20:25:42.282: INFO: Got endpoints: latency-svc-zsnlh [231.659972ms]
Oct 26 20:25:42.286: INFO: Created: latency-svc-89ddd
Oct 26 20:25:42.294: INFO: Got endpoints: latency-svc-g6fjn [231.334911ms]
Oct 26 20:25:42.302: INFO: Created: latency-svc-brj59
Oct 26 20:25:42.305: INFO: Got endpoints: latency-svc-89ddd [229.401637ms]
Oct 26 20:25:42.314: INFO: Got endpoints: latency-svc-brj59 [221.723093ms]
Oct 26 20:25:42.326: INFO: Created: latency-svc-ptb67
Oct 26 20:25:42.330: INFO: Created: latency-svc-9w8nv
Oct 26 20:25:42.338: INFO: Got endpoints: latency-svc-ptb67 [231.716577ms]
Oct 26 20:25:42.350: INFO: Created: latency-svc-zrvdv
Oct 26 20:25:42.352: INFO: Got endpoints: latency-svc-9w8nv [231.209772ms]
Oct 26 20:25:42.361: INFO: Got endpoints: latency-svc-zrvdv [227.276191ms]
Oct 26 20:25:42.364: INFO: Created: latency-svc-r54z9
Oct 26 20:25:42.376: INFO: Got endpoints: latency-svc-r54z9 [230.898009ms]
Oct 26 20:25:42.378: INFO: Created: latency-svc-mvgbv
Oct 26 20:25:42.396: INFO: Got endpoints: latency-svc-mvgbv [234.637917ms]
Oct 26 20:25:42.398: INFO: Created: latency-svc-9sz6s
Oct 26 20:25:42.410: INFO: Created: latency-svc-544qx
Oct 26 20:25:42.421: INFO: Created: latency-svc-qmbgv
Oct 26 20:25:42.437: INFO: Created: latency-svc-wvn7h
Oct 26 20:25:42.451: INFO: Created: latency-svc-dj5f8
Oct 26 20:25:42.459: INFO: Got endpoints: latency-svc-wvn7h [236.402444ms]
Oct 26 20:25:42.459: INFO: Got endpoints: latency-svc-9sz6s [274.342484ms]
Oct 26 20:25:42.460: INFO: Got endpoints: latency-svc-qmbgv [258.935697ms]
Oct 26 20:25:42.460: INFO: Got endpoints: latency-svc-544qx [273.458ms]
Oct 26 20:25:42.462: INFO: Created: latency-svc-cj4mm
Oct 26 20:25:42.479: INFO: Created: latency-svc-97tkh
Oct 26 20:25:42.480: INFO: Got endpoints: latency-svc-dj5f8 [233.479158ms]
Oct 26 20:25:42.480: INFO: Got endpoints: latency-svc-cj4mm [223.820117ms]
Oct 26 20:25:42.496: INFO: Got endpoints: latency-svc-97tkh [213.771429ms]
Oct 26 20:25:42.499: INFO: Created: latency-svc-9xlts
Oct 26 20:25:42.512: INFO: Created: latency-svc-6xz4v
Oct 26 20:25:42.518: INFO: Got endpoints: latency-svc-9xlts [224.036086ms]
Oct 26 20:25:42.523: INFO: Created: latency-svc-4st2l
Oct 26 20:25:42.529: INFO: Got endpoints: latency-svc-6xz4v [223.961946ms]
Oct 26 20:25:42.542: INFO: Got endpoints: latency-svc-4st2l [227.436838ms]
Oct 26 20:25:42.544: INFO: Created: latency-svc-wm8bl
Oct 26 20:25:42.559: INFO: Got endpoints: latency-svc-wm8bl [220.512955ms]
Oct 26 20:25:42.563: INFO: Created: latency-svc-4bl4v
Oct 26 20:25:42.575: INFO: Created: latency-svc-krfbv
Oct 26 20:25:42.577: INFO: Got endpoints: latency-svc-4bl4v [225.38102ms]
Oct 26 20:25:42.593: INFO: Got endpoints: latency-svc-krfbv [232.009941ms]
Oct 26 20:25:42.595: INFO: Created: latency-svc-n4khk
Oct 26 20:25:42.616: INFO: Created: latency-svc-cp8b5
Oct 26 20:25:42.632: INFO: Created: latency-svc-6ltkh
Oct 26 20:25:42.634: INFO: Got endpoints: latency-svc-cp8b5 [237.767142ms]
Oct 26 20:25:42.635: INFO: Got endpoints: latency-svc-n4khk [257.886953ms]
Oct 26 20:25:42.653: INFO: Created: latency-svc-w9jbs
Oct 26 20:25:42.655: INFO: Got endpoints: latency-svc-6ltkh [194.982911ms]
Oct 26 20:25:42.683: INFO: Created: latency-svc-nm8jj
Oct 26 20:25:42.691: INFO: Got endpoints: latency-svc-w9jbs [231.872232ms]
Oct 26 20:25:42.702: INFO: Got endpoints: latency-svc-nm8jj [242.412887ms]
Oct 26 20:25:42.705: INFO: Created: latency-svc-rdzr8
Oct 26 20:25:42.724: INFO: Created: latency-svc-b4cz4
Oct 26 20:25:42.727: INFO: Got endpoints: latency-svc-rdzr8 [266.927622ms]
Oct 26 20:25:42.742: INFO: Got endpoints: latency-svc-b4cz4 [262.048688ms]
Oct 26 20:25:42.748: INFO: Created: latency-svc-69nmk
Oct 26 20:25:42.766: INFO: Got endpoints: latency-svc-69nmk [285.985605ms]
Oct 26 20:25:42.768: INFO: Created: latency-svc-dhjcz
Oct 26 20:25:42.779: INFO: Created: latency-svc-7f7d9
Oct 26 20:25:42.787: INFO: Got endpoints: latency-svc-dhjcz [290.442725ms]
Oct 26 20:25:42.797: INFO: Got endpoints: latency-svc-7f7d9 [278.032597ms]
Oct 26 20:25:42.802: INFO: Created: latency-svc-5tscx
Oct 26 20:25:42.814: INFO: Created: latency-svc-t44ss
Oct 26 20:25:42.826: INFO: Got endpoints: latency-svc-5tscx [297.164919ms]
Oct 26 20:25:42.831: INFO: Created: latency-svc-4246l
Oct 26 20:25:42.830: INFO: Got endpoints: latency-svc-t44ss [288.284069ms]
Oct 26 20:25:42.845: INFO: Created: latency-svc-ns6vg
Oct 26 20:25:42.845: INFO: Got endpoints: latency-svc-4246l [285.670164ms]
Oct 26 20:25:42.860: INFO: Got endpoints: latency-svc-ns6vg [282.67604ms]
Oct 26 20:25:42.863: INFO: Created: latency-svc-ck9r6
Oct 26 20:25:42.879: INFO: Got endpoints: latency-svc-ck9r6 [285.18348ms]
Oct 26 20:25:42.881: INFO: Created: latency-svc-4lbgf
Oct 26 20:25:42.895: INFO: Got endpoints: latency-svc-4lbgf [260.598718ms]
Oct 26 20:25:42.898: INFO: Created: latency-svc-kjgn7
Oct 26 20:25:42.916: INFO: Got endpoints: latency-svc-kjgn7 [280.94152ms]
Oct 26 20:25:42.925: INFO: Created: latency-svc-z57vk
Oct 26 20:25:42.945: INFO: Created: latency-svc-p88xp
Oct 26 20:25:42.957: INFO: Got endpoints: latency-svc-z57vk [301.535323ms]
Oct 26 20:25:42.963: INFO: Got endpoints: latency-svc-p88xp [271.867585ms]
Oct 26 20:25:42.967: INFO: Created: latency-svc-rqx5g
Oct 26 20:25:42.981: INFO: Created: latency-svc-ptg7w
Oct 26 20:25:42.998: INFO: Got endpoints: latency-svc-ptg7w [271.21044ms]
Oct 26 20:25:42.999: INFO: Got endpoints: latency-svc-rqx5g [296.486032ms]
Oct 26 20:25:43.009: INFO: Created: latency-svc-l2jdg
Oct 26 20:25:43.028: INFO: Created: latency-svc-nhpdx
Oct 26 20:25:43.032: INFO: Got endpoints: latency-svc-l2jdg [289.895315ms]
Oct 26 20:25:43.041: INFO: Created: latency-svc-lk28q
Oct 26 20:25:43.048: INFO: Got endpoints: latency-svc-nhpdx [282.091051ms]
Oct 26 20:25:43.050: INFO: Got endpoints: latency-svc-lk28q [263.4358ms]
Oct 26 20:25:43.072: INFO: Created: latency-svc-s6qkt
Oct 26 20:25:43.087: INFO: Got endpoints: latency-svc-s6qkt [290.383876ms]
Oct 26 20:25:43.089: INFO: Created: latency-svc-r6w6z
Oct 26 20:25:43.104: INFO: Created: latency-svc-97kfz
Oct 26 20:25:43.122: INFO: Got endpoints: latency-svc-r6w6z [294.771283ms]
Oct 26 20:25:43.127: INFO: Created: latency-svc-nt5kk
Oct 26 20:25:43.137: INFO: Got endpoints: latency-svc-97kfz [305.815209ms]
Oct 26 20:25:43.140: INFO: Got endpoints: latency-svc-nt5kk [294.587758ms]
Oct 26 20:25:43.154: INFO: Created: latency-svc-4s7wn
Oct 26 20:25:43.164: INFO: Got endpoints: latency-svc-4s7wn [303.959493ms]
Oct 26 20:25:43.169: INFO: Created: latency-svc-7l9lz
Oct 26 20:25:43.185: INFO: Created: latency-svc-255jl
Oct 26 20:25:43.189: INFO: Got endpoints: latency-svc-7l9lz [310.220002ms]
Oct 26 20:25:43.198: INFO: Created: latency-svc-fczxm
Oct 26 20:25:43.205: INFO: Got endpoints: latency-svc-255jl [310.372615ms]
Oct 26 20:25:43.208: INFO: Created: latency-svc-lznct
Oct 26 20:25:43.222: INFO: Created: latency-svc-gd497
Oct 26 20:25:43.225: INFO: Got endpoints: latency-svc-fczxm [309.365262ms]
Oct 26 20:25:43.229: INFO: Got endpoints: latency-svc-lznct [270.407619ms]
Oct 26 20:25:43.236: INFO: Created: latency-svc-d5z5k
Oct 26 20:25:43.242: INFO: Got endpoints: latency-svc-gd497 [278.909403ms]
Oct 26 20:25:43.253: INFO: Got endpoints: latency-svc-d5z5k [254.089989ms]
Oct 26 20:25:43.255: INFO: Created: latency-svc-mvf2r
Oct 26 20:25:43.271: INFO: Created: latency-svc-9284w
Oct 26 20:25:43.271: INFO: Got endpoints: latency-svc-mvf2r [272.275672ms]
Oct 26 20:25:43.277: INFO: Created: latency-svc-l9jsf
Oct 26 20:25:43.283: INFO: Got endpoints: latency-svc-9284w [251.108315ms]
Oct 26 20:25:43.294: INFO: Got endpoints: latency-svc-l9jsf [245.222314ms]
Oct 26 20:25:43.294: INFO: Created: latency-svc-qxh4r
Oct 26 20:25:43.305: INFO: Created: latency-svc-kg65m
Oct 26 20:25:43.309: INFO: Got endpoints: latency-svc-qxh4r [258.364081ms]
Oct 26 20:25:43.321: INFO: Got endpoints: latency-svc-kg65m [233.863995ms]
Oct 26 20:25:43.322: INFO: Created: latency-svc-92xjq
Oct 26 20:25:43.334: INFO: Got endpoints: latency-svc-92xjq [211.950157ms]
Oct 26 20:25:43.335: INFO: Created: latency-svc-j9bhs
Oct 26 20:25:43.347: INFO: Created: latency-svc-5lgcb
Oct 26 20:25:43.352: INFO: Got endpoints: latency-svc-j9bhs [215.484469ms]
Oct 26 20:25:43.367: INFO: Got endpoints: latency-svc-5lgcb [227.207159ms]
Oct 26 20:25:43.367: INFO: Created: latency-svc-b62cw
Oct 26 20:25:43.377: INFO: Created: latency-svc-xs9hg
Oct 26 20:25:43.381: INFO: Got endpoints: latency-svc-b62cw [216.993598ms]
Oct 26 20:25:43.388: INFO: Created: latency-svc-8562b
Oct 26 20:25:43.391: INFO: Got endpoints: latency-svc-xs9hg [202.041857ms]
Oct 26 20:25:43.399: INFO: Created: latency-svc-znsm7
Oct 26 20:25:43.404: INFO: Got endpoints: latency-svc-8562b [198.701452ms]
Oct 26 20:25:43.410: INFO: Created: latency-svc-t5f6r
Oct 26 20:25:43.416: INFO: Got endpoints: latency-svc-znsm7 [190.488766ms]
Oct 26 20:25:43.425: INFO: Created: latency-svc-n7gd5
Oct 26 20:25:43.431: INFO: Got endpoints: latency-svc-t5f6r [202.604769ms]
Oct 26 20:25:43.436: INFO: Got endpoints: latency-svc-n7gd5 [194.237847ms]
Oct 26 20:25:43.440: INFO: Created: latency-svc-q26dm
Oct 26 20:25:43.453: INFO: Got endpoints: latency-svc-q26dm [199.797989ms]
Oct 26 20:25:43.457: INFO: Created: latency-svc-bqcgk
Oct 26 20:25:43.467: INFO: Created: latency-svc-2j2qt
Oct 26 20:25:43.473: INFO: Got endpoints: latency-svc-bqcgk [201.488057ms]
Oct 26 20:25:43.488: INFO: Created: latency-svc-5drjm
Oct 26 20:25:43.497: INFO: Got endpoints: latency-svc-2j2qt [213.822565ms]
Oct 26 20:25:43.506: INFO: Got endpoints: latency-svc-5drjm [212.400725ms]
Oct 26 20:25:43.522: INFO: Created: latency-svc-d4rns
Oct 26 20:25:43.538: INFO: Got endpoints: latency-svc-d4rns [229.443302ms]
Oct 26 20:25:43.553: INFO: Created: latency-svc-vf42p
Oct 26 20:25:43.578: INFO: Got endpoints: latency-svc-vf42p [257.260752ms]
Oct 26 20:25:43.584: INFO: Created: latency-svc-x5lft
Oct 26 20:25:43.602: INFO: Got endpoints: latency-svc-x5lft [267.203913ms]
Oct 26 20:25:43.609: INFO: Created: latency-svc-v6v7l
Oct 26 20:25:43.618: INFO: Created: latency-svc-5nkmd
Oct 26 20:25:43.625: INFO: Got endpoints: latency-svc-v6v7l [272.883883ms]
Oct 26 20:25:43.630: INFO: Created: latency-svc-85ccw
Oct 26 20:25:43.642: INFO: Got endpoints: latency-svc-5nkmd [274.699579ms]
Oct 26 20:25:43.651: INFO: Got endpoints: latency-svc-85ccw [269.094795ms]
Oct 26 20:25:43.660: INFO: Created: latency-svc-xsqb7
Oct 26 20:25:43.671: INFO: Created: latency-svc-vnsx8
Oct 26 20:25:43.677: INFO: Got endpoints: latency-svc-xsqb7 [285.436466ms]
Oct 26 20:25:43.686: INFO: Got endpoints: latency-svc-vnsx8 [282.149319ms]
Oct 26 20:25:43.690: INFO: Created: latency-svc-rv6ph
Oct 26 20:25:43.710: INFO: Got endpoints: latency-svc-rv6ph [293.974797ms]
Oct 26 20:25:43.716: INFO: Created: latency-svc-h8c9s
Oct 26 20:25:43.731: INFO: Got endpoints: latency-svc-h8c9s [299.619537ms]
Oct 26 20:25:43.739: INFO: Created: latency-svc-jdnhz
Oct 26 20:25:43.756: INFO: Got endpoints: latency-svc-jdnhz [319.606008ms]
Oct 26 20:25:43.758: INFO: Created: latency-svc-5w7zm
Oct 26 20:25:43.778: INFO: Got endpoints: latency-svc-5w7zm [325.286405ms]
Oct 26 20:25:43.788: INFO: Created: latency-svc-hzkzv
Oct 26 20:25:43.802: INFO: Created: latency-svc-sxr85
Oct 26 20:25:43.810: INFO: Got endpoints: latency-svc-hzkzv [336.517698ms]
Oct 26 20:25:43.819: INFO: Created: latency-svc-nw64t
Oct 26 20:25:43.824: INFO: Got endpoints: latency-svc-sxr85 [326.825719ms]
Oct 26 20:25:43.835: INFO: Got endpoints: latency-svc-nw64t [328.271039ms]
Oct 26 20:25:43.837: INFO: Created: latency-svc-gppnw
Oct 26 20:25:43.848: INFO: Created: latency-svc-q8qpz
Oct 26 20:25:43.863: INFO: Got endpoints: latency-svc-gppnw [323.888075ms]
Oct 26 20:25:43.864: INFO: Created: latency-svc-cms9l
Oct 26 20:25:43.867: INFO: Got endpoints: latency-svc-q8qpz [288.697302ms]
Oct 26 20:25:43.877: INFO: Created: latency-svc-5smql
Oct 26 20:25:43.881: INFO: Got endpoints: latency-svc-cms9l [279.675738ms]
Oct 26 20:25:43.896: INFO: Got endpoints: latency-svc-5smql [270.399195ms]
Oct 26 20:25:43.899: INFO: Created: latency-svc-cjz65
Oct 26 20:25:43.907: INFO: Created: latency-svc-rtk55
Oct 26 20:25:43.917: INFO: Got endpoints: latency-svc-cjz65 [274.329134ms]
Oct 26 20:25:43.920: INFO: Created: latency-svc-r4v4s
Oct 26 20:25:43.926: INFO: Got endpoints: latency-svc-rtk55 [275.670712ms]
Oct 26 20:25:43.934: INFO: Created: latency-svc-825qk
Oct 26 20:25:43.943: INFO: Got endpoints: latency-svc-r4v4s [266.71947ms]
Oct 26 20:25:43.950: INFO: Created: latency-svc-z2n9p
Oct 26 20:25:43.952: INFO: Got endpoints: latency-svc-825qk [265.638989ms]
Oct 26 20:25:43.965: INFO: Created: latency-svc-hbqtl
Oct 26 20:25:43.967: INFO: Got endpoints: latency-svc-z2n9p [257.450213ms]
Oct 26 20:25:43.984: INFO: Got endpoints: latency-svc-hbqtl [252.555986ms]
Oct 26 20:25:43.986: INFO: Created: latency-svc-9nmcf
Oct 26 20:25:44.004: INFO: Created: latency-svc-6vbwn
Oct 26 20:25:44.005: INFO: Got endpoints: latency-svc-9nmcf [248.940452ms]
Oct 26 20:25:44.010: INFO: Created: latency-svc-8lh7x
Oct 26 20:25:44.026: INFO: Created: latency-svc-v7rws
Oct 26 20:25:44.026: INFO: Got endpoints: latency-svc-6vbwn [247.580225ms]
Oct 26 20:25:44.029: INFO: Got endpoints: latency-svc-8lh7x [219.13603ms]
Oct 26 20:25:44.042: INFO: Created: latency-svc-dpr5x
Oct 26 20:25:44.042: INFO: Got endpoints: latency-svc-v7rws [218.192596ms]
Oct 26 20:25:44.058: INFO: Got endpoints: latency-svc-dpr5x [222.631549ms]
Oct 26 20:25:44.061: INFO: Created: latency-svc-n96cl
Oct 26 20:25:44.070: INFO: Created: latency-svc-sb9fr
Oct 26 20:25:44.075: INFO: Got endpoints: latency-svc-n96cl [212.628571ms]
Oct 26 20:25:44.084: INFO: Created: latency-svc-hbjsr
Oct 26 20:25:44.095: INFO: Created: latency-svc-jffft
Oct 26 20:25:44.100: INFO: Got endpoints: latency-svc-sb9fr [232.154579ms]
Oct 26 20:25:44.101: INFO: Got endpoints: latency-svc-hbjsr [218.968656ms]
Oct 26 20:25:44.120: INFO: Got endpoints: latency-svc-jffft [224.293408ms]
Oct 26 20:25:44.121: INFO: Created: latency-svc-hfgth
Oct 26 20:25:44.132: INFO: Created: latency-svc-2fhgk
Oct 26 20:25:44.136: INFO: Got endpoints: latency-svc-hfgth [219.090721ms]
Oct 26 20:25:44.148: INFO: Got endpoints: latency-svc-2fhgk [221.614465ms]
Oct 26 20:25:44.149: INFO: Created: latency-svc-qswkf
Oct 26 20:25:44.161: INFO: Created: latency-svc-hqjh6
Oct 26 20:25:44.167: INFO: Got endpoints: latency-svc-qswkf [223.275362ms]
Oct 26 20:25:44.178: INFO: Created: latency-svc-d7rl8
Oct 26 20:25:44.179: INFO: Got endpoints: latency-svc-hqjh6 [227.016402ms]
Oct 26 20:25:44.191: INFO: Created: latency-svc-lh5nq
Oct 26 20:25:44.198: INFO: Got endpoints: latency-svc-d7rl8 [230.310849ms]
Oct 26 20:25:44.204: INFO: Created: latency-svc-v4pw2
Oct 26 20:25:44.209: INFO: Got endpoints: latency-svc-lh5nq [225.358468ms]
Oct 26 20:25:44.218: INFO: Created: latency-svc-jxgfb
Oct 26 20:25:44.229: INFO: Got endpoints: latency-svc-v4pw2 [223.38922ms]
Oct 26 20:25:44.236: INFO: Got endpoints: latency-svc-jxgfb [209.475148ms]
Oct 26 20:25:44.238: INFO: Created: latency-svc-t5hbg
Oct 26 20:25:44.250: INFO: Created: latency-svc-2wrtr
Oct 26 20:25:44.254: INFO: Got endpoints: latency-svc-t5hbg [224.756704ms]
Oct 26 20:25:44.258: INFO: Created: latency-svc-7d88m
Oct 26 20:25:44.264: INFO: Got endpoints: latency-svc-2wrtr [221.722825ms]
Oct 26 20:25:44.273: INFO: Created: latency-svc-g4jwp
Oct 26 20:25:44.275: INFO: Got endpoints: latency-svc-7d88m [217.025709ms]
Oct 26 20:25:44.282: INFO: Created: latency-svc-b4kjg
Oct 26 20:25:44.289: INFO: Got endpoints: latency-svc-g4jwp [213.713292ms]
Oct 26 20:25:44.298: INFO: Created: latency-svc-dct8x
Oct 26 20:25:44.300: INFO: Got endpoints: latency-svc-b4kjg [199.532329ms]
Oct 26 20:25:44.310: INFO: Got endpoints: latency-svc-dct8x [209.601174ms]
Oct 26 20:25:44.314: INFO: Created: latency-svc-kwvcd
Oct 26 20:25:44.328: INFO: Created: latency-svc-qzr9v
Oct 26 20:25:44.343: INFO: Got endpoints: latency-svc-kwvcd [222.459085ms]
Oct 26 20:25:44.344: INFO: Created: latency-svc-5sk7t
Oct 26 20:25:44.354: INFO: Got endpoints: latency-svc-qzr9v [218.164928ms]
Oct 26 20:25:44.355: INFO: Created: latency-svc-2bdvg
Oct 26 20:25:44.356: INFO: Got endpoints: latency-svc-5sk7t [208.102874ms]
Oct 26 20:25:44.363: INFO: Created: latency-svc-rclk5
Oct 26 20:25:44.367: INFO: Got endpoints: latency-svc-2bdvg [200.660921ms]
Oct 26 20:25:44.376: INFO: Got endpoints: latency-svc-rclk5 [196.40273ms]
Oct 26 20:25:44.379: INFO: Created: latency-svc-s98rr
Oct 26 20:25:44.391: INFO: Created: latency-svc-bbz22
Oct 26 20:25:44.395: INFO: Got endpoints: latency-svc-s98rr [197.005154ms]
Oct 26 20:25:44.405: INFO: Created: latency-svc-pj6sj
Oct 26 20:25:44.413: INFO: Got endpoints: latency-svc-bbz22 [203.324525ms]
Oct 26 20:25:44.423: INFO: Got endpoints: latency-svc-pj6sj [193.967716ms]
Oct 26 20:25:44.423: INFO: Created: latency-svc-p45n2
Oct 26 20:25:44.436: INFO: Got endpoints: latency-svc-p45n2 [200.272269ms]
Oct 26 20:25:44.440: INFO: Created: latency-svc-4tf8m
Oct 26 20:25:44.450: INFO: Created: latency-svc-fblvz
Oct 26 20:25:44.453: INFO: Got endpoints: latency-svc-4tf8m [199.082556ms]
Oct 26 20:25:44.458: INFO: Got endpoints: latency-svc-fblvz [194.345229ms]
Oct 26 20:25:44.459: INFO: Created: latency-svc-xw9d2
Oct 26 20:25:44.479: INFO: Got endpoints: latency-svc-xw9d2 [203.60457ms]
Oct 26 20:25:44.480: INFO: Created: latency-svc-5chxv
Oct 26 20:25:44.491: INFO: Created: latency-svc-r6246
Oct 26 20:25:44.497: INFO: Got endpoints: latency-svc-5chxv [207.992451ms]
Oct 26 20:25:44.503: INFO: Created: latency-svc-96qpk
Oct 26 20:25:44.507: INFO: Got endpoints: latency-svc-r6246 [207.613916ms]
Oct 26 20:25:44.519: INFO: Got endpoints: latency-svc-96qpk [208.621914ms]
Oct 26 20:25:44.519: INFO: Created: latency-svc-nc69d
Oct 26 20:25:44.538: INFO: Got endpoints: latency-svc-nc69d [195.271588ms]
Oct 26 20:25:44.540: INFO: Created: latency-svc-clvcb
Oct 26 20:25:44.558: INFO: Got endpoints: latency-svc-clvcb [203.737508ms]
Oct 26 20:25:44.561: INFO: Created: latency-svc-jlkll
Oct 26 20:25:44.573: INFO: Created: latency-svc-c2db6
Oct 26 20:25:44.575: INFO: Got endpoints: latency-svc-jlkll [218.786134ms]
Oct 26 20:25:44.613: INFO: Got endpoints: latency-svc-c2db6 [245.28571ms]
Oct 26 20:25:44.628: INFO: Created: latency-svc-flgsw
Oct 26 20:25:44.637: INFO: Created: latency-svc-qnn22
Oct 26 20:25:44.650: INFO: Got endpoints: latency-svc-flgsw [274.153963ms]
Oct 26 20:25:44.656: INFO: Created: latency-svc-zrb99
Oct 26 20:25:44.656: INFO: Got endpoints: latency-svc-zrb99 [242.628053ms]
Oct 26 20:25:44.656: INFO: Got endpoints: latency-svc-qnn22 [260.593258ms]
Oct 26 20:25:44.661: INFO: Created: latency-svc-m4hb7
Oct 26 20:25:44.661: INFO: Got endpoints: latency-svc-m4hb7 [238.14913ms]
Oct 26 20:25:44.661: INFO: Latencies: [51.031565ms 60.835642ms 61.114197ms 76.090786ms 94.60004ms 98.13822ms 114.38849ms 120.790159ms 144.358182ms 162.440925ms 173.991496ms 186.217944ms 190.488766ms 193.967716ms 194.237847ms 194.345229ms 194.982911ms 195.271588ms 196.40273ms 197.005154ms 198.701452ms 199.082556ms 199.532329ms 199.797989ms 200.272269ms 200.660921ms 200.912379ms 201.488057ms 202.041857ms 202.604769ms 203.324525ms 203.60457ms 203.737508ms 204.231136ms 207.613916ms 207.992451ms 208.102874ms 208.621914ms 209.475148ms 209.601174ms 211.382123ms 211.838184ms 211.950157ms 212.400725ms 212.628571ms 213.713292ms 213.771429ms 213.822565ms 214.770798ms 215.484469ms 215.726355ms 216.876899ms 216.993598ms 217.025709ms 217.041397ms 217.996397ms 218.164928ms 218.192596ms 218.477643ms 218.786134ms 218.968656ms 219.090721ms 219.13603ms 220.512955ms 220.733114ms 221.614465ms 221.722825ms 221.723093ms 222.459085ms 222.631549ms 223.275362ms 223.38922ms 223.820117ms 223.961946ms 224.036086ms 224.293408ms 224.756704ms 225.358468ms 225.38102ms 226.086072ms 226.870595ms 226.922782ms 227.016402ms 227.207159ms 227.276191ms 227.436838ms 229.353788ms 229.401637ms 229.443302ms 230.060142ms 230.310849ms 230.491737ms 230.898009ms 231.209772ms 231.334911ms 231.659972ms 231.716577ms 231.872232ms 231.972202ms 232.009941ms 232.154579ms 233.409291ms 233.479158ms 233.863995ms 233.870011ms 233.984735ms 234.637917ms 235.548443ms 236.148079ms 236.402444ms 237.555251ms 237.767142ms 238.14913ms 238.75865ms 240.087432ms 240.759461ms 242.412887ms 242.628053ms 244.62793ms 244.657167ms 244.902961ms 245.222314ms 245.28571ms 247.580225ms 248.940452ms 249.00847ms 249.650045ms 251.108315ms 251.723417ms 251.93404ms 252.555986ms 254.089989ms 254.617185ms 255.501167ms 256.067717ms 257.260752ms 257.450213ms 257.886953ms 258.364081ms 258.445962ms 258.653312ms 258.935697ms 260.593258ms 260.598718ms 261.764148ms 262.048688ms 263.4358ms 263.997439ms 265.638989ms 266.71947ms 266.927622ms 267.203913ms 269.094795ms 270.399195ms 270.407619ms 271.031944ms 271.21044ms 271.867585ms 272.275672ms 272.883883ms 273.458ms 274.153963ms 274.329134ms 274.342484ms 274.699579ms 275.670712ms 278.032597ms 278.909403ms 279.675738ms 280.94152ms 282.091051ms 282.149319ms 282.67604ms 285.18348ms 285.436466ms 285.670164ms 285.985605ms 288.284069ms 288.697302ms 289.895315ms 290.383876ms 290.442725ms 293.974797ms 294.587758ms 294.771283ms 296.486032ms 297.164919ms 299.619537ms 301.535323ms 303.959493ms 305.815209ms 309.365262ms 310.220002ms 310.372615ms 319.606008ms 323.888075ms 325.286405ms 326.825719ms 328.271039ms 336.517698ms]
Oct 26 20:25:44.661: INFO: 50 %ile: 232.154579ms
Oct 26 20:25:44.661: INFO: 90 %ile: 290.383876ms
Oct 26 20:25:44.661: INFO: 99 %ile: 328.271039ms
Oct 26 20:25:44.661: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:25:44.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7614" for this suite.
Oct 26 20:26:16.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:26:18.727: INFO: namespace svc-latency-7614 deletion completed in 34.030473869s

• [SLOW TEST:40.724 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:26:18.728: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-79cf8967-a4f5-468c-b038-30ddc2e9afc3 in namespace container-probe-9504
Oct 26 20:26:22.973: INFO: Started pod busybox-79cf8967-a4f5-468c-b038-30ddc2e9afc3 in namespace container-probe-9504
STEP: checking the pod's current state and verifying that restartCount is present
Oct 26 20:26:22.993: INFO: Initial restart count of pod busybox-79cf8967-a4f5-468c-b038-30ddc2e9afc3 is 0
Oct 26 20:27:13.335: INFO: Restart count of pod container-probe-9504/busybox-79cf8967-a4f5-468c-b038-30ddc2e9afc3 is now 1 (50.341256101s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:27:13.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9504" for this suite.
Oct 26 20:27:21.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:27:23.485: INFO: namespace container-probe-9504 deletion completed in 10.068495286s

• [SLOW TEST:64.757 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:27:23.486: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 20:27:23.655: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Oct 26 20:27:28.668: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Oct 26 20:27:28.668: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Oct 26 20:27:28.733: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6803 /apis/apps/v1/namespaces/deployment-6803/deployments/test-cleanup-deployment 86f3aa51-c372-4e75-abe0-1dfcefccd153 94508 1 2020-10-26 20:27:28 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003eb0a18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Oct 26 20:27:28.743: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Oct 26 20:27:28.743: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Oct 26 20:27:28.743: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-6803 /apis/apps/v1/namespaces/deployment-6803/replicasets/test-cleanup-controller d9ecb372-5bc4-4b0c-8d03-9a1b01d9a0fa 94509 1 2020-10-26 20:27:23 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 86f3aa51-c372-4e75-abe0-1dfcefccd153 0xc003eb0da7 0xc003eb0da8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003eb0e08 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Oct 26 20:27:28.753: INFO: Pod "test-cleanup-controller-8psn8" is available:
&Pod{ObjectMeta:{test-cleanup-controller-8psn8 test-cleanup-controller- deployment-6803 /api/v1/namespaces/deployment-6803/pods/test-cleanup-controller-8psn8 f6201a20-fcc1-43d9-b378-629174ce92d7 94497 0 2020-10-26 20:27:23 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:172.30.37.211/32 cni.projectcalico.org/podIPs:172.30.37.211/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.37.211"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-cleanup-controller d9ecb372-5bc4-4b0c-8d03-9a1b01d9a0fa 0xc00c540897 0xc00c540898}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tn9b9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tn9b9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tn9b9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.240.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 20:27:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 20:27:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 20:27:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 20:27:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.240.178,PodIP:172.30.37.211,StartTime:2020-10-26 20:27:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-26 20:27:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://e2e34e9345cfcf0b01eae75d14bdbfb28cee0ad184a5c19eb57ce7c2fd0fd5f9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.37.211,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:27:28.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6803" for this suite.
Oct 26 20:27:36.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:27:38.768: INFO: namespace deployment-6803 deletion completed in 9.985484273s

• [SLOW TEST:15.282 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:27:38.768: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:27:38.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1360" for this suite.
Oct 26 20:28:11.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:28:13.078: INFO: namespace pods-1360 deletion completed in 34.092676652s

• [SLOW TEST:34.311 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:28:13.080: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:28:40.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5214" for this suite.
Oct 26 20:28:48.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:28:50.071: INFO: namespace container-runtime-5214 deletion completed in 10.034008954s

• [SLOW TEST:36.992 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:28:50.072: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-f7d2c0d3-7f09-4542-be9e-abb4ff67bcdb
STEP: Creating a pod to test consume configMaps
Oct 26 20:28:50.302: INFO: Waiting up to 5m0s for pod "pod-configmaps-3657be98-5e18-4571-8937-9db1d1e1771c" in namespace "configmap-9828" to be "success or failure"
Oct 26 20:28:50.320: INFO: Pod "pod-configmaps-3657be98-5e18-4571-8937-9db1d1e1771c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.402064ms
Oct 26 20:28:52.334: INFO: Pod "pod-configmaps-3657be98-5e18-4571-8937-9db1d1e1771c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031574812s
STEP: Saw pod success
Oct 26 20:28:52.334: INFO: Pod "pod-configmaps-3657be98-5e18-4571-8937-9db1d1e1771c" satisfied condition "success or failure"
Oct 26 20:28:52.347: INFO: Trying to get logs from node 10.123.240.178 pod pod-configmaps-3657be98-5e18-4571-8937-9db1d1e1771c container configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 20:28:52.449: INFO: Waiting for pod pod-configmaps-3657be98-5e18-4571-8937-9db1d1e1771c to disappear
Oct 26 20:28:52.461: INFO: Pod pod-configmaps-3657be98-5e18-4571-8937-9db1d1e1771c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:28:52.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9828" for this suite.
Oct 26 20:29:00.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:29:02.712: INFO: namespace configmap-9828 deletion completed in 10.225147738s

• [SLOW TEST:12.640 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:29:02.713: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-b5da5913-0761-4098-b927-8251fed16335
STEP: Creating a pod to test consume configMaps
Oct 26 20:29:02.916: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fc6bb1ac-ab20-48e9-94b0-207965bdf7fa" in namespace "projected-6002" to be "success or failure"
Oct 26 20:29:02.926: INFO: Pod "pod-projected-configmaps-fc6bb1ac-ab20-48e9-94b0-207965bdf7fa": Phase="Pending", Reason="", readiness=false. Elapsed: 9.581764ms
Oct 26 20:29:04.939: INFO: Pod "pod-projected-configmaps-fc6bb1ac-ab20-48e9-94b0-207965bdf7fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022904331s
Oct 26 20:29:06.951: INFO: Pod "pod-projected-configmaps-fc6bb1ac-ab20-48e9-94b0-207965bdf7fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035208982s
STEP: Saw pod success
Oct 26 20:29:06.951: INFO: Pod "pod-projected-configmaps-fc6bb1ac-ab20-48e9-94b0-207965bdf7fa" satisfied condition "success or failure"
Oct 26 20:29:06.962: INFO: Trying to get logs from node 10.123.240.178 pod pod-projected-configmaps-fc6bb1ac-ab20-48e9-94b0-207965bdf7fa container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 20:29:07.029: INFO: Waiting for pod pod-projected-configmaps-fc6bb1ac-ab20-48e9-94b0-207965bdf7fa to disappear
Oct 26 20:29:07.040: INFO: Pod pod-projected-configmaps-fc6bb1ac-ab20-48e9-94b0-207965bdf7fa no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:29:07.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6002" for this suite.
Oct 26 20:29:15.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:29:17.135: INFO: namespace projected-6002 deletion completed in 10.062786834s

• [SLOW TEST:14.423 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:29:17.140: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Oct 26 20:29:17.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-3174'
Oct 26 20:29:17.737: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Oct 26 20:29:17.737: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Oct 26 20:29:19.774: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-2bjzb]
Oct 26 20:29:19.774: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-2bjzb" in namespace "kubectl-3174" to be "running and ready"
Oct 26 20:29:19.787: INFO: Pod "e2e-test-httpd-rc-2bjzb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.137753ms
Oct 26 20:29:21.799: INFO: Pod "e2e-test-httpd-rc-2bjzb": Phase="Running", Reason="", readiness=true. Elapsed: 2.024592921s
Oct 26 20:29:21.799: INFO: Pod "e2e-test-httpd-rc-2bjzb" satisfied condition "running and ready"
Oct 26 20:29:21.799: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-2bjzb]
Oct 26 20:29:21.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 logs rc/e2e-test-httpd-rc --namespace=kubectl-3174'
Oct 26 20:29:22.412: INFO: stderr: ""
Oct 26 20:29:22.412: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.30.37.206. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.30.37.206. Set the 'ServerName' directive globally to suppress this message\n[Mon Oct 26 20:29:19.225213 2020] [mpm_event:notice] [pid 1:tid 140706215299944] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Mon Oct 26 20:29:19.225299 2020] [core:notice] [pid 1:tid 140706215299944] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Oct 26 20:29:22.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 delete rc e2e-test-httpd-rc --namespace=kubectl-3174'
Oct 26 20:29:22.583: INFO: stderr: ""
Oct 26 20:29:22.583: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:29:22.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3174" for this suite.
Oct 26 20:29:36.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:29:38.619: INFO: namespace kubectl-3174 deletion completed in 16.009086637s

• [SLOW TEST:21.479 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:29:38.620: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 20:29:39.205: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 20:29:41.241: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739340979, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739340979, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739340979, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739340979, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 20:29:44.284: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 20:29:44.295: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-460-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:29:45.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5782" for this suite.
Oct 26 20:29:53.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:29:55.696: INFO: namespace webhook-5782 deletion completed in 10.058608164s
STEP: Destroying namespace "webhook-5782-markers" for this suite.
Oct 26 20:30:03.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:30:05.744: INFO: namespace webhook-5782-markers deletion completed in 10.04758621s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:27.187 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:30:05.807: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-7336
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct 26 20:30:05.964: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Oct 26 20:30:32.433: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.101.250:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7336 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 20:30:32.433: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 20:30:32.803: INFO: Found all expected endpoints: [netserver-0]
Oct 26 20:30:32.814: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.37.197:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7336 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 20:30:32.814: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 20:30:33.039: INFO: Found all expected endpoints: [netserver-1]
Oct 26 20:30:33.081: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.84.18:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7336 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 20:30:33.082: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
Oct 26 20:30:33.296: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:30:33.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7336" for this suite.
Oct 26 20:30:41.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:30:43.372: INFO: namespace pod-network-test-7336 deletion completed in 10.052462218s

• [SLOW TEST:37.565 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:30:43.372: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Oct 26 20:30:43.586: INFO: Waiting up to 5m0s for pod "downward-api-5a69cb2a-fea7-4171-844e-1270b0295786" in namespace "downward-api-8655" to be "success or failure"
Oct 26 20:30:43.596: INFO: Pod "downward-api-5a69cb2a-fea7-4171-844e-1270b0295786": Phase="Pending", Reason="", readiness=false. Elapsed: 10.014609ms
Oct 26 20:30:45.608: INFO: Pod "downward-api-5a69cb2a-fea7-4171-844e-1270b0295786": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022436518s
Oct 26 20:30:47.620: INFO: Pod "downward-api-5a69cb2a-fea7-4171-844e-1270b0295786": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034575846s
Oct 26 20:30:49.631: INFO: Pod "downward-api-5a69cb2a-fea7-4171-844e-1270b0295786": Phase="Pending", Reason="", readiness=false. Elapsed: 6.04553939s
Oct 26 20:30:51.643: INFO: Pod "downward-api-5a69cb2a-fea7-4171-844e-1270b0295786": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.05689176s
STEP: Saw pod success
Oct 26 20:30:51.643: INFO: Pod "downward-api-5a69cb2a-fea7-4171-844e-1270b0295786" satisfied condition "success or failure"
Oct 26 20:30:51.653: INFO: Trying to get logs from node 10.123.240.174 pod downward-api-5a69cb2a-fea7-4171-844e-1270b0295786 container dapi-container: <nil>
STEP: delete the pod
Oct 26 20:30:51.760: INFO: Waiting for pod downward-api-5a69cb2a-fea7-4171-844e-1270b0295786 to disappear
Oct 26 20:30:51.771: INFO: Pod downward-api-5a69cb2a-fea7-4171-844e-1270b0295786 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:30:51.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8655" for this suite.
Oct 26 20:30:59.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:31:01.878: INFO: namespace downward-api-8655 deletion completed in 10.086295637s

• [SLOW TEST:18.506 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:31:01.878: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Oct 26 20:31:02.014: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:31:06.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-54" for this suite.
Oct 26 20:31:20.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:31:22.499: INFO: namespace init-container-54 deletion completed in 16.04461133s

• [SLOW TEST:20.621 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:31:22.500: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 20:31:24.803: INFO: Waiting up to 5m0s for pod "client-envvars-1748e49c-6c8b-4312-9d7a-847a32e5d92d" in namespace "pods-5463" to be "success or failure"
Oct 26 20:31:24.815: INFO: Pod "client-envvars-1748e49c-6c8b-4312-9d7a-847a32e5d92d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.173027ms
Oct 26 20:31:26.826: INFO: Pod "client-envvars-1748e49c-6c8b-4312-9d7a-847a32e5d92d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023148362s
STEP: Saw pod success
Oct 26 20:31:26.826: INFO: Pod "client-envvars-1748e49c-6c8b-4312-9d7a-847a32e5d92d" satisfied condition "success or failure"
Oct 26 20:31:26.837: INFO: Trying to get logs from node 10.123.240.178 pod client-envvars-1748e49c-6c8b-4312-9d7a-847a32e5d92d container env3cont: <nil>
STEP: delete the pod
Oct 26 20:31:26.923: INFO: Waiting for pod client-envvars-1748e49c-6c8b-4312-9d7a-847a32e5d92d to disappear
Oct 26 20:31:26.935: INFO: Pod client-envvars-1748e49c-6c8b-4312-9d7a-847a32e5d92d no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:31:26.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5463" for this suite.
Oct 26 20:31:41.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:31:43.040: INFO: namespace pods-5463 deletion completed in 16.081816321s

• [SLOW TEST:20.541 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:31:43.040: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Oct 26 20:31:43.197: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:31:47.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4187" for this suite.
Oct 26 20:31:55.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:31:57.600: INFO: namespace init-container-4187 deletion completed in 9.994052049s

• [SLOW TEST:14.559 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:31:57.600: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Oct 26 20:31:57.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 cluster-info'
Oct 26 20:31:57.889: INFO: stderr: ""
Oct 26 20:31:57.889: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:31:57.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7011" for this suite.
Oct 26 20:32:05.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:32:08.040: INFO: namespace kubectl-7011 deletion completed in 10.121286139s

• [SLOW TEST:10.440 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:32:08.042: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6009
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6009
STEP: creating replication controller externalsvc in namespace services-6009
I1026 20:32:08.308239      23 runners.go:184] Created replication controller with name: externalsvc, namespace: services-6009, replica count: 2
I1026 20:32:11.358790      23 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Oct 26 20:32:11.424: INFO: Creating new exec pod
Oct 26 20:32:15.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=services-6009 execpodbknf8 -- /bin/sh -x -c nslookup nodeport-service'
Oct 26 20:32:15.867: INFO: stderr: "+ nslookup nodeport-service\n"
Oct 26 20:32:15.867: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-6009.svc.cluster.local\tcanonical name = externalsvc.services-6009.svc.cluster.local.\nName:\texternalsvc.services-6009.svc.cluster.local\nAddress: 172.21.79.99\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6009, will wait for the garbage collector to delete the pods
Oct 26 20:32:15.955: INFO: Deleting ReplicationController externalsvc took: 26.916797ms
Oct 26 20:32:16.555: INFO: Terminating ReplicationController externalsvc pods took: 600.331187ms
Oct 26 20:32:25.614: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:32:25.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6009" for this suite.
Oct 26 20:32:33.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:32:35.819: INFO: namespace services-6009 deletion completed in 10.129931465s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:27.777 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:32:35.820: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Oct 26 20:32:36.002: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:32:44.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5462" for this suite.
Oct 26 20:32:52.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:32:54.225: INFO: namespace pods-5462 deletion completed in 10.114643529s

• [SLOW TEST:18.405 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:32:54.225: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7968.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7968.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 26 20:32:58.638: INFO: DNS probes using dns-7968/dns-test-4f15b2fb-ee51-4207-8f96-0ef581e06d09 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:32:58.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7968" for this suite.
Oct 26 20:33:06.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:33:08.814: INFO: namespace dns-7968 deletion completed in 10.094860047s

• [SLOW TEST:14.589 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:33:08.816: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-rhqs
STEP: Creating a pod to test atomic-volume-subpath
Oct 26 20:33:09.068: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-rhqs" in namespace "subpath-2829" to be "success or failure"
Oct 26 20:33:09.094: INFO: Pod "pod-subpath-test-configmap-rhqs": Phase="Pending", Reason="", readiness=false. Elapsed: 26.072681ms
Oct 26 20:33:11.106: INFO: Pod "pod-subpath-test-configmap-rhqs": Phase="Running", Reason="", readiness=true. Elapsed: 2.038109024s
Oct 26 20:33:13.120: INFO: Pod "pod-subpath-test-configmap-rhqs": Phase="Running", Reason="", readiness=true. Elapsed: 4.052531586s
Oct 26 20:33:15.134: INFO: Pod "pod-subpath-test-configmap-rhqs": Phase="Running", Reason="", readiness=true. Elapsed: 6.066031566s
Oct 26 20:33:17.146: INFO: Pod "pod-subpath-test-configmap-rhqs": Phase="Running", Reason="", readiness=true. Elapsed: 8.07843458s
Oct 26 20:33:19.159: INFO: Pod "pod-subpath-test-configmap-rhqs": Phase="Running", Reason="", readiness=true. Elapsed: 10.091665802s
Oct 26 20:33:21.174: INFO: Pod "pod-subpath-test-configmap-rhqs": Phase="Running", Reason="", readiness=true. Elapsed: 12.106496827s
Oct 26 20:33:23.185: INFO: Pod "pod-subpath-test-configmap-rhqs": Phase="Running", Reason="", readiness=true. Elapsed: 14.117234479s
Oct 26 20:33:25.198: INFO: Pod "pod-subpath-test-configmap-rhqs": Phase="Running", Reason="", readiness=true. Elapsed: 16.130731327s
Oct 26 20:33:27.209: INFO: Pod "pod-subpath-test-configmap-rhqs": Phase="Running", Reason="", readiness=true. Elapsed: 18.141333909s
Oct 26 20:33:29.220: INFO: Pod "pod-subpath-test-configmap-rhqs": Phase="Running", Reason="", readiness=true. Elapsed: 20.152658183s
Oct 26 20:33:31.236: INFO: Pod "pod-subpath-test-configmap-rhqs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.168172043s
STEP: Saw pod success
Oct 26 20:33:31.236: INFO: Pod "pod-subpath-test-configmap-rhqs" satisfied condition "success or failure"
Oct 26 20:33:31.256: INFO: Trying to get logs from node 10.123.240.178 pod pod-subpath-test-configmap-rhqs container test-container-subpath-configmap-rhqs: <nil>
STEP: delete the pod
Oct 26 20:33:31.314: INFO: Waiting for pod pod-subpath-test-configmap-rhqs to disappear
Oct 26 20:33:31.325: INFO: Pod pod-subpath-test-configmap-rhqs no longer exists
STEP: Deleting pod pod-subpath-test-configmap-rhqs
Oct 26 20:33:31.326: INFO: Deleting pod "pod-subpath-test-configmap-rhqs" in namespace "subpath-2829"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:33:31.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2829" for this suite.
Oct 26 20:33:39.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:33:41.485: INFO: namespace subpath-2829 deletion completed in 10.123161528s

• [SLOW TEST:32.669 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:33:41.485: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 20:33:41.643: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Oct 26 20:33:51.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 --namespace=crd-publish-openapi-4291 create -f -'
Oct 26 20:33:52.738: INFO: stderr: ""
Oct 26 20:33:52.738: INFO: stdout: "e2e-test-crd-publish-openapi-1701-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Oct 26 20:33:52.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 --namespace=crd-publish-openapi-4291 delete e2e-test-crd-publish-openapi-1701-crds test-cr'
Oct 26 20:33:52.916: INFO: stderr: ""
Oct 26 20:33:52.916: INFO: stdout: "e2e-test-crd-publish-openapi-1701-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Oct 26 20:33:52.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 --namespace=crd-publish-openapi-4291 apply -f -'
Oct 26 20:33:53.699: INFO: stderr: ""
Oct 26 20:33:53.699: INFO: stdout: "e2e-test-crd-publish-openapi-1701-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Oct 26 20:33:53.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 --namespace=crd-publish-openapi-4291 delete e2e-test-crd-publish-openapi-1701-crds test-cr'
Oct 26 20:33:53.904: INFO: stderr: ""
Oct 26 20:33:53.904: INFO: stdout: "e2e-test-crd-publish-openapi-1701-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Oct 26 20:33:53.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 explain e2e-test-crd-publish-openapi-1701-crds'
Oct 26 20:33:54.500: INFO: stderr: ""
Oct 26 20:33:54.500: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1701-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:34:04.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4291" for this suite.
Oct 26 20:34:12.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:34:14.315: INFO: namespace crd-publish-openapi-4291 deletion completed in 10.090337696s

• [SLOW TEST:32.831 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:34:14.316: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:34:18.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5322" for this suite.
Oct 26 20:35:08.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:35:10.707: INFO: namespace kubelet-test-5322 deletion completed in 52.086947572s

• [SLOW TEST:56.391 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:35:10.707: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Oct 26 20:35:10.897: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b59f156-b8a4-4eaa-af09-ffbd846bec42" in namespace "projected-9239" to be "success or failure"
Oct 26 20:35:10.908: INFO: Pod "downwardapi-volume-2b59f156-b8a4-4eaa-af09-ffbd846bec42": Phase="Pending", Reason="", readiness=false. Elapsed: 11.752601ms
Oct 26 20:35:12.922: INFO: Pod "downwardapi-volume-2b59f156-b8a4-4eaa-af09-ffbd846bec42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025224476s
Oct 26 20:35:14.935: INFO: Pod "downwardapi-volume-2b59f156-b8a4-4eaa-af09-ffbd846bec42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038416449s
STEP: Saw pod success
Oct 26 20:35:14.935: INFO: Pod "downwardapi-volume-2b59f156-b8a4-4eaa-af09-ffbd846bec42" satisfied condition "success or failure"
Oct 26 20:35:14.947: INFO: Trying to get logs from node 10.123.240.174 pod downwardapi-volume-2b59f156-b8a4-4eaa-af09-ffbd846bec42 container client-container: <nil>
STEP: delete the pod
Oct 26 20:35:15.028: INFO: Waiting for pod downwardapi-volume-2b59f156-b8a4-4eaa-af09-ffbd846bec42 to disappear
Oct 26 20:35:15.039: INFO: Pod downwardapi-volume-2b59f156-b8a4-4eaa-af09-ffbd846bec42 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:35:15.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9239" for this suite.
Oct 26 20:35:23.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:35:25.102: INFO: namespace projected-9239 deletion completed in 10.03229135s

• [SLOW TEST:14.395 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:35:25.105: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 20:35:25.705: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 20:35:27.747: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341325, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341325, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341325, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341325, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 20:35:30.804: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:35:30.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6013" for this suite.
Oct 26 20:35:39.038: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:35:41.052: INFO: namespace webhook-6013 deletion completed in 10.056998096s
STEP: Destroying namespace "webhook-6013-markers" for this suite.
Oct 26 20:35:49.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:35:51.176: INFO: namespace webhook-6013-markers deletion completed in 10.123784083s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:26.132 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:35:51.237: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Oct 26 20:35:51.458: INFO: Waiting up to 5m0s for pod "pod-4fbdd17e-8bfd-4389-837f-e2f4f5399df2" in namespace "emptydir-969" to be "success or failure"
Oct 26 20:35:51.471: INFO: Pod "pod-4fbdd17e-8bfd-4389-837f-e2f4f5399df2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.617505ms
Oct 26 20:35:53.481: INFO: Pod "pod-4fbdd17e-8bfd-4389-837f-e2f4f5399df2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023105707s
STEP: Saw pod success
Oct 26 20:35:53.481: INFO: Pod "pod-4fbdd17e-8bfd-4389-837f-e2f4f5399df2" satisfied condition "success or failure"
Oct 26 20:35:53.491: INFO: Trying to get logs from node 10.123.240.178 pod pod-4fbdd17e-8bfd-4389-837f-e2f4f5399df2 container test-container: <nil>
STEP: delete the pod
Oct 26 20:35:53.579: INFO: Waiting for pod pod-4fbdd17e-8bfd-4389-837f-e2f4f5399df2 to disappear
Oct 26 20:35:53.588: INFO: Pod pod-4fbdd17e-8bfd-4389-837f-e2f4f5399df2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:35:53.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-969" for this suite.
Oct 26 20:36:01.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:36:03.700: INFO: namespace emptydir-969 deletion completed in 10.077669921s

• [SLOW TEST:12.463 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:36:03.701: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 20:36:03.926: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-1432e7f0-6c97-4b6c-887b-e042b5ee5ff3" in namespace "security-context-test-8789" to be "success or failure"
Oct 26 20:36:03.939: INFO: Pod "busybox-readonly-false-1432e7f0-6c97-4b6c-887b-e042b5ee5ff3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.27612ms
Oct 26 20:36:05.952: INFO: Pod "busybox-readonly-false-1432e7f0-6c97-4b6c-887b-e042b5ee5ff3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026352798s
Oct 26 20:36:07.967: INFO: Pod "busybox-readonly-false-1432e7f0-6c97-4b6c-887b-e042b5ee5ff3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04114578s
Oct 26 20:36:07.967: INFO: Pod "busybox-readonly-false-1432e7f0-6c97-4b6c-887b-e042b5ee5ff3" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:36:07.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8789" for this suite.
Oct 26 20:36:16.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:36:18.053: INFO: namespace security-context-test-8789 deletion completed in 10.052922412s

• [SLOW TEST:14.353 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:36:18.054: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 20:36:18.242: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-59e78904-3b40-4caf-9853-e853a777a2bf
STEP: Creating secret with name s-test-opt-upd-c9d5b806-8bf3-4fa0-bd84-ae0054bd6a49
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-59e78904-3b40-4caf-9853-e853a777a2bf
STEP: Updating secret s-test-opt-upd-c9d5b806-8bf3-4fa0-bd84-ae0054bd6a49
STEP: Creating secret with name s-test-opt-create-cc1d348b-6569-425c-a1b8-7af353c63ee2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:37:51.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1254" for this suite.
Oct 26 20:38:06.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:38:08.071: INFO: namespace secrets-1254 deletion completed in 16.114727636s

• [SLOW TEST:110.018 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:38:08.072: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-795fd9d6-6da9-499e-8d2b-73e7af6823d2 in namespace container-probe-2103
Oct 26 20:38:12.345: INFO: Started pod busybox-795fd9d6-6da9-499e-8d2b-73e7af6823d2 in namespace container-probe-2103
STEP: checking the pod's current state and verifying that restartCount is present
Oct 26 20:38:12.357: INFO: Initial restart count of pod busybox-795fd9d6-6da9-499e-8d2b-73e7af6823d2 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:42:13.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2103" for this suite.
Oct 26 20:42:22.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:42:24.035: INFO: namespace container-probe-2103 deletion completed in 10.06556601s

• [SLOW TEST:255.963 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:42:24.035: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Oct 26 20:42:24.840: INFO: created pod pod-service-account-defaultsa
Oct 26 20:42:24.840: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Oct 26 20:42:24.876: INFO: created pod pod-service-account-mountsa
Oct 26 20:42:24.876: INFO: pod pod-service-account-mountsa service account token volume mount: true
Oct 26 20:42:24.913: INFO: created pod pod-service-account-nomountsa
Oct 26 20:42:24.914: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Oct 26 20:42:24.955: INFO: created pod pod-service-account-defaultsa-mountspec
Oct 26 20:42:24.955: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Oct 26 20:42:25.004: INFO: created pod pod-service-account-mountsa-mountspec
Oct 26 20:42:25.005: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Oct 26 20:42:25.053: INFO: created pod pod-service-account-nomountsa-mountspec
Oct 26 20:42:25.054: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Oct 26 20:42:25.116: INFO: created pod pod-service-account-defaultsa-nomountspec
Oct 26 20:42:25.116: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Oct 26 20:42:25.156: INFO: created pod pod-service-account-mountsa-nomountspec
Oct 26 20:42:25.156: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Oct 26 20:42:25.197: INFO: created pod pod-service-account-nomountsa-nomountspec
Oct 26 20:42:25.197: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:42:25.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2426" for this suite.
Oct 26 20:42:33.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:42:35.366: INFO: namespace svcaccounts-2426 deletion completed in 10.13196727s

• [SLOW TEST:11.331 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:42:35.367: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 20:42:36.555: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 20:42:38.592: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341756, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341756, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341756, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341756, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 20:42:41.635: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Oct 26 20:42:41.711: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:42:41.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1066" for this suite.
Oct 26 20:42:49.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:42:51.822: INFO: namespace webhook-1066 deletion completed in 10.048430882s
STEP: Destroying namespace "webhook-1066-markers" for this suite.
Oct 26 20:42:59.877: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:43:01.880: INFO: namespace webhook-1066-markers deletion completed in 10.057859289s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:26.580 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:43:01.949: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-hvczh in namespace proxy-6209
I1026 20:43:02.214544      23 runners.go:184] Created replication controller with name: proxy-service-hvczh, namespace: proxy-6209, replica count: 1
I1026 20:43:03.266334      23 runners.go:184] proxy-service-hvczh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1026 20:43:04.266579      23 runners.go:184] proxy-service-hvczh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1026 20:43:05.266908      23 runners.go:184] proxy-service-hvczh Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 26 20:43:05.280: INFO: setup took 3.174120346s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Oct 26 20:43:05.303: INFO: (0) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 22.951396ms)
Oct 26 20:43:05.311: INFO: (0) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 30.309204ms)
Oct 26 20:43:05.315: INFO: (0) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/rewriteme">test</a> (200; 33.822877ms)
Oct 26 20:43:05.315: INFO: (0) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">... (200; 34.189648ms)
Oct 26 20:43:05.315: INFO: (0) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">test<... (200; 34.002531ms)
Oct 26 20:43:05.318: INFO: (0) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 37.262997ms)
Oct 26 20:43:05.318: INFO: (0) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 37.297196ms)
Oct 26 20:43:05.318: INFO: (0) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname2/proxy/: bar (200; 38.170905ms)
Oct 26 20:43:05.318: INFO: (0) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname1/proxy/: foo (200; 38.0388ms)
Oct 26 20:43:05.321: INFO: (0) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname1/proxy/: foo (200; 39.970771ms)
Oct 26 20:43:05.321: INFO: (0) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname2/proxy/: bar (200; 39.914247ms)
Oct 26 20:43:05.330: INFO: (0) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:460/proxy/: tls baz (200; 49.860495ms)
Oct 26 20:43:05.333: INFO: (0) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname1/proxy/: tls baz (200; 52.506914ms)
Oct 26 20:43:05.333: INFO: (0) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:462/proxy/: tls qux (200; 52.034088ms)
Oct 26 20:43:05.336: INFO: (0) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/tlsrewritem... (200; 55.432311ms)
Oct 26 20:43:05.339: INFO: (0) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname2/proxy/: tls qux (200; 58.46304ms)
Oct 26 20:43:05.365: INFO: (1) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:462/proxy/: tls qux (200; 24.850354ms)
Oct 26 20:43:05.368: INFO: (1) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/rewriteme">test</a> (200; 27.687309ms)
Oct 26 20:43:05.369: INFO: (1) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 29.633405ms)
Oct 26 20:43:05.369: INFO: (1) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/tlsrewritem... (200; 28.675805ms)
Oct 26 20:43:05.369: INFO: (1) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 29.370622ms)
Oct 26 20:43:05.369: INFO: (1) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">... (200; 29.147013ms)
Oct 26 20:43:05.369: INFO: (1) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:460/proxy/: tls baz (200; 29.563091ms)
Oct 26 20:43:05.370: INFO: (1) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 29.765927ms)
Oct 26 20:43:05.370: INFO: (1) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 29.370247ms)
Oct 26 20:43:05.370: INFO: (1) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">test<... (200; 29.831686ms)
Oct 26 20:43:05.370: INFO: (1) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname2/proxy/: tls qux (200; 30.616409ms)
Oct 26 20:43:05.370: INFO: (1) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname1/proxy/: tls baz (200; 30.871802ms)
Oct 26 20:43:05.373: INFO: (1) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname1/proxy/: foo (200; 33.147061ms)
Oct 26 20:43:05.373: INFO: (1) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname2/proxy/: bar (200; 32.85246ms)
Oct 26 20:43:05.373: INFO: (1) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname1/proxy/: foo (200; 33.739018ms)
Oct 26 20:43:05.373: INFO: (1) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname2/proxy/: bar (200; 32.488987ms)
Oct 26 20:43:05.391: INFO: (2) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">... (200; 17.045848ms)
Oct 26 20:43:05.395: INFO: (2) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/rewriteme">test</a> (200; 21.262846ms)
Oct 26 20:43:05.395: INFO: (2) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 21.578033ms)
Oct 26 20:43:05.398: INFO: (2) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:460/proxy/: tls baz (200; 24.216361ms)
Oct 26 20:43:05.398: INFO: (2) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 24.007684ms)
Oct 26 20:43:05.398: INFO: (2) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">test<... (200; 24.362334ms)
Oct 26 20:43:05.398: INFO: (2) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 24.377892ms)
Oct 26 20:43:05.398: INFO: (2) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/tlsrewritem... (200; 24.362428ms)
Oct 26 20:43:05.399: INFO: (2) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname1/proxy/: tls baz (200; 26.151358ms)
Oct 26 20:43:05.399: INFO: (2) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:462/proxy/: tls qux (200; 25.780367ms)
Oct 26 20:43:05.399: INFO: (2) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 25.851048ms)
Oct 26 20:43:05.402: INFO: (2) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname1/proxy/: foo (200; 28.345804ms)
Oct 26 20:43:05.404: INFO: (2) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname1/proxy/: foo (200; 30.478829ms)
Oct 26 20:43:05.404: INFO: (2) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname2/proxy/: tls qux (200; 30.740136ms)
Oct 26 20:43:05.404: INFO: (2) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname2/proxy/: bar (200; 30.587284ms)
Oct 26 20:43:05.404: INFO: (2) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname2/proxy/: bar (200; 31.216469ms)
Oct 26 20:43:05.420: INFO: (3) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 15.767408ms)
Oct 26 20:43:05.424: INFO: (3) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/rewriteme">test</a> (200; 18.887608ms)
Oct 26 20:43:05.429: INFO: (3) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:460/proxy/: tls baz (200; 22.950838ms)
Oct 26 20:43:05.429: INFO: (3) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 23.436468ms)
Oct 26 20:43:05.429: INFO: (3) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 23.26549ms)
Oct 26 20:43:05.429: INFO: (3) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/tlsrewritem... (200; 23.993088ms)
Oct 26 20:43:05.429: INFO: (3) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 23.957908ms)
Oct 26 20:43:05.429: INFO: (3) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">... (200; 24.255288ms)
Oct 26 20:43:05.429: INFO: (3) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:462/proxy/: tls qux (200; 24.076958ms)
Oct 26 20:43:05.429: INFO: (3) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">test<... (200; 24.558197ms)
Oct 26 20:43:05.432: INFO: (3) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname2/proxy/: bar (200; 26.854772ms)
Oct 26 20:43:05.435: INFO: (3) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname2/proxy/: tls qux (200; 29.707431ms)
Oct 26 20:43:05.438: INFO: (3) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname1/proxy/: tls baz (200; 32.399531ms)
Oct 26 20:43:05.438: INFO: (3) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname1/proxy/: foo (200; 32.552108ms)
Oct 26 20:43:05.438: INFO: (3) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname2/proxy/: bar (200; 32.844367ms)
Oct 26 20:43:05.438: INFO: (3) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname1/proxy/: foo (200; 32.487352ms)
Oct 26 20:43:05.454: INFO: (4) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 16.004632ms)
Oct 26 20:43:05.462: INFO: (4) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/rewriteme">test</a> (200; 22.682187ms)
Oct 26 20:43:05.462: INFO: (4) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/tlsrewritem... (200; 23.18351ms)
Oct 26 20:43:05.462: INFO: (4) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:462/proxy/: tls qux (200; 23.016924ms)
Oct 26 20:43:05.463: INFO: (4) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 23.7587ms)
Oct 26 20:43:05.463: INFO: (4) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 23.921253ms)
Oct 26 20:43:05.463: INFO: (4) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:460/proxy/: tls baz (200; 23.887475ms)
Oct 26 20:43:05.463: INFO: (4) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">... (200; 23.774773ms)
Oct 26 20:43:05.465: INFO: (4) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname2/proxy/: tls qux (200; 25.882755ms)
Oct 26 20:43:05.465: INFO: (4) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">test<... (200; 25.459403ms)
Oct 26 20:43:05.465: INFO: (4) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 26.466825ms)
Oct 26 20:43:05.468: INFO: (4) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname2/proxy/: bar (200; 28.772853ms)
Oct 26 20:43:05.471: INFO: (4) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname1/proxy/: foo (200; 31.900689ms)
Oct 26 20:43:05.471: INFO: (4) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname1/proxy/: tls baz (200; 32.442178ms)
Oct 26 20:43:05.471: INFO: (4) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname2/proxy/: bar (200; 32.716138ms)
Oct 26 20:43:05.471: INFO: (4) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname1/proxy/: foo (200; 32.224442ms)
Oct 26 20:43:05.493: INFO: (5) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/tlsrewritem... (200; 19.65625ms)
Oct 26 20:43:05.496: INFO: (5) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname1/proxy/: tls baz (200; 23.542187ms)
Oct 26 20:43:05.496: INFO: (5) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/rewriteme">test</a> (200; 24.026591ms)
Oct 26 20:43:05.496: INFO: (5) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 22.862193ms)
Oct 26 20:43:05.496: INFO: (5) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 23.793568ms)
Oct 26 20:43:05.496: INFO: (5) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:460/proxy/: tls baz (200; 22.928806ms)
Oct 26 20:43:05.496: INFO: (5) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">... (200; 23.432774ms)
Oct 26 20:43:05.496: INFO: (5) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:462/proxy/: tls qux (200; 24.631505ms)
Oct 26 20:43:05.496: INFO: (5) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 24.570736ms)
Oct 26 20:43:05.496: INFO: (5) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 24.916749ms)
Oct 26 20:43:05.497: INFO: (5) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">test<... (200; 23.833698ms)
Oct 26 20:43:05.503: INFO: (5) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname1/proxy/: foo (200; 30.76965ms)
Oct 26 20:43:05.504: INFO: (5) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname2/proxy/: bar (200; 32.173374ms)
Oct 26 20:43:05.506: INFO: (5) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname2/proxy/: tls qux (200; 33.62415ms)
Oct 26 20:43:05.506: INFO: (5) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname1/proxy/: foo (200; 33.861026ms)
Oct 26 20:43:05.506: INFO: (5) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname2/proxy/: bar (200; 34.051674ms)
Oct 26 20:43:05.524: INFO: (6) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/tlsrewritem... (200; 17.546252ms)
Oct 26 20:43:05.531: INFO: (6) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 24.561238ms)
Oct 26 20:43:05.534: INFO: (6) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/rewriteme">test</a> (200; 26.880782ms)
Oct 26 20:43:05.534: INFO: (6) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:462/proxy/: tls qux (200; 27.079549ms)
Oct 26 20:43:05.534: INFO: (6) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 26.462404ms)
Oct 26 20:43:05.534: INFO: (6) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">test<... (200; 27.6847ms)
Oct 26 20:43:05.534: INFO: (6) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 27.270197ms)
Oct 26 20:43:05.534: INFO: (6) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 26.972211ms)
Oct 26 20:43:05.535: INFO: (6) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname1/proxy/: foo (200; 29.393504ms)
Oct 26 20:43:05.539: INFO: (6) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname2/proxy/: bar (200; 32.281833ms)
Oct 26 20:43:05.539: INFO: (6) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname1/proxy/: foo (200; 32.351868ms)
Oct 26 20:43:05.539: INFO: (6) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">... (200; 33.058896ms)
Oct 26 20:43:05.540: INFO: (6) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname1/proxy/: tls baz (200; 32.485815ms)
Oct 26 20:43:05.540: INFO: (6) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:460/proxy/: tls baz (200; 32.109066ms)
Oct 26 20:43:05.540: INFO: (6) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname2/proxy/: bar (200; 33.168752ms)
Oct 26 20:43:05.540: INFO: (6) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname2/proxy/: tls qux (200; 32.685211ms)
Oct 26 20:43:05.564: INFO: (7) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">... (200; 23.753297ms)
Oct 26 20:43:05.568: INFO: (7) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:462/proxy/: tls qux (200; 27.068139ms)
Oct 26 20:43:05.570: INFO: (7) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">test<... (200; 29.601746ms)
Oct 26 20:43:05.570: INFO: (7) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 30.047815ms)
Oct 26 20:43:05.571: INFO: (7) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 29.992327ms)
Oct 26 20:43:05.571: INFO: (7) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 30.337696ms)
Oct 26 20:43:05.571: INFO: (7) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:460/proxy/: tls baz (200; 30.428056ms)
Oct 26 20:43:05.571: INFO: (7) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 30.331045ms)
Oct 26 20:43:05.571: INFO: (7) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/tlsrewritem... (200; 30.353515ms)
Oct 26 20:43:05.571: INFO: (7) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/rewriteme">test</a> (200; 31.057605ms)
Oct 26 20:43:05.571: INFO: (7) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname2/proxy/: tls qux (200; 30.796282ms)
Oct 26 20:43:05.576: INFO: (7) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname1/proxy/: tls baz (200; 35.488342ms)
Oct 26 20:43:05.578: INFO: (7) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname2/proxy/: bar (200; 37.775339ms)
Oct 26 20:43:05.579: INFO: (7) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname1/proxy/: foo (200; 37.941877ms)
Oct 26 20:43:05.579: INFO: (7) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname2/proxy/: bar (200; 38.139753ms)
Oct 26 20:43:05.579: INFO: (7) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname1/proxy/: foo (200; 38.178922ms)
Oct 26 20:43:05.595: INFO: (8) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:460/proxy/: tls baz (200; 15.893205ms)
Oct 26 20:43:05.601: INFO: (8) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:462/proxy/: tls qux (200; 21.319004ms)
Oct 26 20:43:05.601: INFO: (8) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">... (200; 21.918737ms)
Oct 26 20:43:05.604: INFO: (8) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/rewriteme">test</a> (200; 24.041603ms)
Oct 26 20:43:05.604: INFO: (8) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 23.351442ms)
Oct 26 20:43:05.604: INFO: (8) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 24.197196ms)
Oct 26 20:43:05.604: INFO: (8) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">test<... (200; 24.780314ms)
Oct 26 20:43:05.604: INFO: (8) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 24.659764ms)
Oct 26 20:43:05.604: INFO: (8) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 24.045911ms)
Oct 26 20:43:05.604: INFO: (8) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/tlsrewritem... (200; 24.781305ms)
Oct 26 20:43:05.606: INFO: (8) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname2/proxy/: bar (200; 26.605276ms)
Oct 26 20:43:05.609: INFO: (8) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname2/proxy/: bar (200; 29.143159ms)
Oct 26 20:43:05.611: INFO: (8) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname2/proxy/: tls qux (200; 31.123725ms)
Oct 26 20:43:05.611: INFO: (8) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname1/proxy/: foo (200; 31.395954ms)
Oct 26 20:43:05.612: INFO: (8) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname1/proxy/: foo (200; 31.584547ms)
Oct 26 20:43:05.616: INFO: (8) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname1/proxy/: tls baz (200; 36.244731ms)
Oct 26 20:43:05.635: INFO: (9) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/rewriteme">test</a> (200; 18.364114ms)
Oct 26 20:43:05.637: INFO: (9) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">... (200; 20.603248ms)
Oct 26 20:43:05.637: INFO: (9) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:462/proxy/: tls qux (200; 20.668991ms)
Oct 26 20:43:05.638: INFO: (9) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 20.821104ms)
Oct 26 20:43:05.638: INFO: (9) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 21.400544ms)
Oct 26 20:43:05.642: INFO: (9) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:460/proxy/: tls baz (200; 24.65843ms)
Oct 26 20:43:05.642: INFO: (9) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 24.984702ms)
Oct 26 20:43:05.642: INFO: (9) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 25.000647ms)
Oct 26 20:43:05.642: INFO: (9) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">test<... (200; 25.453846ms)
Oct 26 20:43:05.642: INFO: (9) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/tlsrewritem... (200; 25.444112ms)
Oct 26 20:43:05.644: INFO: (9) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname1/proxy/: foo (200; 27.033855ms)
Oct 26 20:43:05.644: INFO: (9) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname2/proxy/: bar (200; 27.627465ms)
Oct 26 20:43:05.648: INFO: (9) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname1/proxy/: foo (200; 30.975178ms)
Oct 26 20:43:05.650: INFO: (9) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname2/proxy/: tls qux (200; 33.696233ms)
Oct 26 20:43:05.650: INFO: (9) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname2/proxy/: bar (200; 33.339435ms)
Oct 26 20:43:05.650: INFO: (9) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname1/proxy/: tls baz (200; 33.22119ms)
Oct 26 20:43:05.667: INFO: (10) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">... (200; 16.110743ms)
Oct 26 20:43:05.670: INFO: (10) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 17.942243ms)
Oct 26 20:43:05.672: INFO: (10) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/rewriteme">test</a> (200; 21.24265ms)
Oct 26 20:43:05.672: INFO: (10) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 21.503336ms)
Oct 26 20:43:05.673: INFO: (10) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/tlsrewritem... (200; 21.763671ms)
Oct 26 20:43:05.673: INFO: (10) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">test<... (200; 22.033543ms)
Oct 26 20:43:05.673: INFO: (10) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 20.985529ms)
Oct 26 20:43:05.673: INFO: (10) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:460/proxy/: tls baz (200; 21.075731ms)
Oct 26 20:43:05.673: INFO: (10) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 21.870517ms)
Oct 26 20:43:05.673: INFO: (10) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:462/proxy/: tls qux (200; 22.131456ms)
Oct 26 20:43:05.677: INFO: (10) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname2/proxy/: bar (200; 25.356125ms)
Oct 26 20:43:05.677: INFO: (10) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname1/proxy/: foo (200; 26.305244ms)
Oct 26 20:43:05.683: INFO: (10) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname1/proxy/: tls baz (200; 31.465064ms)
Oct 26 20:43:05.683: INFO: (10) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname1/proxy/: foo (200; 31.72413ms)
Oct 26 20:43:05.687: INFO: (10) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname2/proxy/: bar (200; 36.309768ms)
Oct 26 20:43:05.687: INFO: (10) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname2/proxy/: tls qux (200; 35.903035ms)
Oct 26 20:43:05.705: INFO: (11) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/tlsrewritem... (200; 17.912189ms)
Oct 26 20:43:05.708: INFO: (11) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 18.966203ms)
Oct 26 20:43:05.708: INFO: (11) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/rewriteme">test</a> (200; 20.140682ms)
Oct 26 20:43:05.710: INFO: (11) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">test<... (200; 21.25771ms)
Oct 26 20:43:05.710: INFO: (11) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 22.586992ms)
Oct 26 20:43:05.710: INFO: (11) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">... (200; 21.500608ms)
Oct 26 20:43:05.711: INFO: (11) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 22.583253ms)
Oct 26 20:43:05.711: INFO: (11) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:460/proxy/: tls baz (200; 22.621516ms)
Oct 26 20:43:05.711: INFO: (11) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 22.082036ms)
Oct 26 20:43:05.712: INFO: (11) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:462/proxy/: tls qux (200; 24.073221ms)
Oct 26 20:43:05.714: INFO: (11) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname2/proxy/: bar (200; 26.353095ms)
Oct 26 20:43:05.715: INFO: (11) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname2/proxy/: tls qux (200; 26.507916ms)
Oct 26 20:43:05.718: INFO: (11) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname1/proxy/: foo (200; 29.553451ms)
Oct 26 20:43:05.718: INFO: (11) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname2/proxy/: bar (200; 28.987398ms)
Oct 26 20:43:05.718: INFO: (11) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname1/proxy/: foo (200; 29.686041ms)
Oct 26 20:43:05.718: INFO: (11) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname1/proxy/: tls baz (200; 29.494678ms)
Oct 26 20:43:05.733: INFO: (12) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/rewriteme">test</a> (200; 15.354649ms)
Oct 26 20:43:05.738: INFO: (12) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 18.788246ms)
Oct 26 20:43:05.742: INFO: (12) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">test<... (200; 23.624958ms)
Oct 26 20:43:05.743: INFO: (12) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:462/proxy/: tls qux (200; 23.762599ms)
Oct 26 20:43:05.743: INFO: (12) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/tlsrewritem... (200; 23.990895ms)
Oct 26 20:43:05.743: INFO: (12) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 23.996882ms)
Oct 26 20:43:05.743: INFO: (12) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 24.296821ms)
Oct 26 20:43:05.743: INFO: (12) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:460/proxy/: tls baz (200; 24.228771ms)
Oct 26 20:43:05.743: INFO: (12) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 24.977794ms)
Oct 26 20:43:05.743: INFO: (12) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname2/proxy/: bar (200; 25.110383ms)
Oct 26 20:43:05.746: INFO: (12) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">... (200; 26.768797ms)
Oct 26 20:43:05.748: INFO: (12) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname1/proxy/: foo (200; 29.523542ms)
Oct 26 20:43:05.751: INFO: (12) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname1/proxy/: tls baz (200; 32.51606ms)
Oct 26 20:43:05.752: INFO: (12) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname1/proxy/: foo (200; 32.987995ms)
Oct 26 20:43:05.752: INFO: (12) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname2/proxy/: tls qux (200; 32.836751ms)
Oct 26 20:43:05.752: INFO: (12) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname2/proxy/: bar (200; 33.078155ms)
Oct 26 20:43:05.769: INFO: (13) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 16.999635ms)
Oct 26 20:43:05.773: INFO: (13) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 20.783284ms)
Oct 26 20:43:05.776: INFO: (13) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/tlsrewritem... (200; 23.342657ms)
Oct 26 20:43:05.777: INFO: (13) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/rewriteme">test</a> (200; 22.974327ms)
Oct 26 20:43:05.777: INFO: (13) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 24.028832ms)
Oct 26 20:43:05.777: INFO: (13) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:460/proxy/: tls baz (200; 23.965514ms)
Oct 26 20:43:05.777: INFO: (13) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 24.131548ms)
Oct 26 20:43:05.777: INFO: (13) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">... (200; 23.926217ms)
Oct 26 20:43:05.777: INFO: (13) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:462/proxy/: tls qux (200; 23.577574ms)
Oct 26 20:43:05.777: INFO: (13) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">test<... (200; 23.890155ms)
Oct 26 20:43:05.779: INFO: (13) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname1/proxy/: foo (200; 25.658834ms)
Oct 26 20:43:05.779: INFO: (13) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname1/proxy/: foo (200; 26.452324ms)
Oct 26 20:43:05.782: INFO: (13) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname2/proxy/: bar (200; 28.683965ms)
Oct 26 20:43:05.782: INFO: (13) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname1/proxy/: tls baz (200; 29.516778ms)
Oct 26 20:43:05.782: INFO: (13) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname2/proxy/: tls qux (200; 29.530141ms)
Oct 26 20:43:05.782: INFO: (13) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname2/proxy/: bar (200; 30.025212ms)
Oct 26 20:43:05.811: INFO: (14) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:460/proxy/: tls baz (200; 28.057899ms)
Oct 26 20:43:05.814: INFO: (14) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/tlsrewritem... (200; 31.443611ms)
Oct 26 20:43:05.814: INFO: (14) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 31.049427ms)
Oct 26 20:43:05.816: INFO: (14) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/rewriteme">test</a> (200; 33.847514ms)
Oct 26 20:43:05.816: INFO: (14) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 32.994801ms)
Oct 26 20:43:05.817: INFO: (14) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 34.016689ms)
Oct 26 20:43:05.817: INFO: (14) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:462/proxy/: tls qux (200; 34.042778ms)
Oct 26 20:43:05.817: INFO: (14) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">... (200; 33.254528ms)
Oct 26 20:43:05.822: INFO: (14) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname2/proxy/: bar (200; 39.00521ms)
Oct 26 20:43:05.822: INFO: (14) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname1/proxy/: foo (200; 38.923808ms)
Oct 26 20:43:05.824: INFO: (14) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 41.351111ms)
Oct 26 20:43:05.824: INFO: (14) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname2/proxy/: tls qux (200; 41.109837ms)
Oct 26 20:43:05.827: INFO: (14) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname1/proxy/: tls baz (200; 44.331983ms)
Oct 26 20:43:05.828: INFO: (14) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">test<... (200; 44.183146ms)
Oct 26 20:43:05.828: INFO: (14) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname1/proxy/: foo (200; 44.77241ms)
Oct 26 20:43:05.831: INFO: (14) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname2/proxy/: bar (200; 47.829022ms)
Oct 26 20:43:05.854: INFO: (15) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:462/proxy/: tls qux (200; 22.939914ms)
Oct 26 20:43:05.857: INFO: (15) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">test<... (200; 23.973534ms)
Oct 26 20:43:05.860: INFO: (15) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 28.565346ms)
Oct 26 20:43:05.860: INFO: (15) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 27.533537ms)
Oct 26 20:43:05.861: INFO: (15) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 28.704536ms)
Oct 26 20:43:05.861: INFO: (15) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">... (200; 27.998794ms)
Oct 26 20:43:05.861: INFO: (15) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/tlsrewritem... (200; 28.036075ms)
Oct 26 20:43:05.865: INFO: (15) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname1/proxy/: tls baz (200; 32.448232ms)
Oct 26 20:43:05.865: INFO: (15) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/rewriteme">test</a> (200; 31.855136ms)
Oct 26 20:43:05.867: INFO: (15) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:460/proxy/: tls baz (200; 35.14591ms)
Oct 26 20:43:05.870: INFO: (15) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname2/proxy/: tls qux (200; 38.01193ms)
Oct 26 20:43:05.871: INFO: (15) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname2/proxy/: bar (200; 38.07428ms)
Oct 26 20:43:05.871: INFO: (15) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname1/proxy/: foo (200; 38.531928ms)
Oct 26 20:43:05.871: INFO: (15) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname2/proxy/: bar (200; 39.054865ms)
Oct 26 20:43:05.873: INFO: (15) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 41.789787ms)
Oct 26 20:43:05.892: INFO: (15) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname1/proxy/: foo (200; 60.04615ms)
Oct 26 20:43:05.920: INFO: (16) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 27.539457ms)
Oct 26 20:43:05.927: INFO: (16) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:460/proxy/: tls baz (200; 33.613782ms)
Oct 26 20:43:05.927: INFO: (16) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 33.739191ms)
Oct 26 20:43:05.927: INFO: (16) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 33.836264ms)
Oct 26 20:43:05.930: INFO: (16) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">test<... (200; 36.917214ms)
Oct 26 20:43:05.934: INFO: (16) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">... (200; 40.470905ms)
Oct 26 20:43:05.934: INFO: (16) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/tlsrewritem... (200; 40.222933ms)
Oct 26 20:43:05.934: INFO: (16) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 40.386942ms)
Oct 26 20:43:05.934: INFO: (16) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname2/proxy/: bar (200; 40.749019ms)
Oct 26 20:43:05.934: INFO: (16) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname1/proxy/: foo (200; 41.318395ms)
Oct 26 20:43:05.934: INFO: (16) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname1/proxy/: tls baz (200; 41.170744ms)
Oct 26 20:43:05.934: INFO: (16) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:462/proxy/: tls qux (200; 40.4185ms)
Oct 26 20:43:05.934: INFO: (16) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname2/proxy/: tls qux (200; 41.154939ms)
Oct 26 20:43:05.934: INFO: (16) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/rewriteme">test</a> (200; 40.326641ms)
Oct 26 20:43:05.936: INFO: (16) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname2/proxy/: bar (200; 42.391747ms)
Oct 26 20:43:05.938: INFO: (16) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname1/proxy/: foo (200; 45.602867ms)
Oct 26 20:43:05.955: INFO: (17) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:460/proxy/: tls baz (200; 16.300209ms)
Oct 26 20:43:05.963: INFO: (17) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 25.095708ms)
Oct 26 20:43:05.964: INFO: (17) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/tlsrewritem... (200; 24.574466ms)
Oct 26 20:43:05.964: INFO: (17) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">test<... (200; 24.559411ms)
Oct 26 20:43:05.964: INFO: (17) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:462/proxy/: tls qux (200; 23.719563ms)
Oct 26 20:43:05.964: INFO: (17) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 25.103569ms)
Oct 26 20:43:05.964: INFO: (17) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 23.879814ms)
Oct 26 20:43:05.964: INFO: (17) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/rewriteme">test</a> (200; 24.005875ms)
Oct 26 20:43:05.966: INFO: (17) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname2/proxy/: bar (200; 26.795824ms)
Oct 26 20:43:05.969: INFO: (17) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname1/proxy/: foo (200; 30.427022ms)
Oct 26 20:43:05.972: INFO: (17) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname2/proxy/: tls qux (200; 33.942625ms)
Oct 26 20:43:05.973: INFO: (17) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname2/proxy/: bar (200; 32.595831ms)
Oct 26 20:43:05.973: INFO: (17) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname1/proxy/: foo (200; 33.965385ms)
Oct 26 20:43:05.980: INFO: (17) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">... (200; 40.54344ms)
Oct 26 20:43:05.980: INFO: (17) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 41.687652ms)
Oct 26 20:43:05.980: INFO: (17) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname1/proxy/: tls baz (200; 41.422944ms)
Oct 26 20:43:06.007: INFO: (18) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">test<... (200; 25.704456ms)
Oct 26 20:43:06.010: INFO: (18) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/tlsrewritem... (200; 28.940818ms)
Oct 26 20:43:06.014: INFO: (18) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 33.069568ms)
Oct 26 20:43:06.014: INFO: (18) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:462/proxy/: tls qux (200; 33.096731ms)
Oct 26 20:43:06.014: INFO: (18) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 33.291671ms)
Oct 26 20:43:06.014: INFO: (18) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 33.559903ms)
Oct 26 20:43:06.015: INFO: (18) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">... (200; 33.596954ms)
Oct 26 20:43:06.015: INFO: (18) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 34.040966ms)
Oct 26 20:43:06.015: INFO: (18) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:460/proxy/: tls baz (200; 34.05249ms)
Oct 26 20:43:06.015: INFO: (18) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/rewriteme">test</a> (200; 33.881883ms)
Oct 26 20:43:06.019: INFO: (18) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname2/proxy/: bar (200; 37.525846ms)
Oct 26 20:43:06.019: INFO: (18) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname2/proxy/: bar (200; 38.069477ms)
Oct 26 20:43:06.019: INFO: (18) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname2/proxy/: tls qux (200; 38.186704ms)
Oct 26 20:43:06.019: INFO: (18) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname1/proxy/: foo (200; 38.209038ms)
Oct 26 20:43:06.021: INFO: (18) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname1/proxy/: foo (200; 39.846384ms)
Oct 26 20:43:06.027: INFO: (18) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname1/proxy/: tls baz (200; 46.131901ms)
Oct 26 20:43:06.045: INFO: (19) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 17.671925ms)
Oct 26 20:43:06.048: INFO: (19) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:460/proxy/: tls baz (200; 20.048005ms)
Oct 26 20:43:06.062: INFO: (19) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">test<... (200; 33.113409ms)
Oct 26 20:43:06.062: INFO: (19) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:462/proxy/: tls qux (200; 34.955153ms)
Oct 26 20:43:06.062: INFO: (19) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s/proxy/rewriteme">test</a> (200; 33.377781ms)
Oct 26 20:43:06.062: INFO: (19) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname2/proxy/: tls qux (200; 34.61616ms)
Oct 26 20:43:06.062: INFO: (19) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname2/proxy/: bar (200; 34.823928ms)
Oct 26 20:43:06.062: INFO: (19) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname2/proxy/: bar (200; 34.098983ms)
Oct 26 20:43:06.063: INFO: (19) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:160/proxy/: foo (200; 33.487773ms)
Oct 26 20:43:06.063: INFO: (19) /api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/https:proxy-service-hvczh-d8r5s:443/proxy/tlsrewritem... (200; 33.841987ms)
Oct 26 20:43:06.063: INFO: (19) /api/v1/namespaces/proxy-6209/pods/proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 34.688667ms)
Oct 26 20:43:06.063: INFO: (19) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/: <a href="/api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:1080/proxy/rewriteme">... (200; 33.80595ms)
Oct 26 20:43:06.063: INFO: (19) /api/v1/namespaces/proxy-6209/pods/http:proxy-service-hvczh-d8r5s:162/proxy/: bar (200; 34.682327ms)
Oct 26 20:43:06.064: INFO: (19) /api/v1/namespaces/proxy-6209/services/proxy-service-hvczh:portname1/proxy/: foo (200; 36.221907ms)
Oct 26 20:43:06.074: INFO: (19) /api/v1/namespaces/proxy-6209/services/http:proxy-service-hvczh:portname1/proxy/: foo (200; 45.420113ms)
Oct 26 20:43:06.076: INFO: (19) /api/v1/namespaces/proxy-6209/services/https:proxy-service-hvczh:tlsportname1/proxy/: tls baz (200; 47.239826ms)
STEP: deleting ReplicationController proxy-service-hvczh in namespace proxy-6209, will wait for the garbage collector to delete the pods
Oct 26 20:43:06.163: INFO: Deleting ReplicationController proxy-service-hvczh took: 24.531549ms
Oct 26 20:43:06.263: INFO: Terminating ReplicationController proxy-service-hvczh pods took: 100.551528ms
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:43:08.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6209" for this suite.
Oct 26 20:43:16.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:43:17.613: INFO: namespace proxy-6209 deletion completed in 9.014498523s

• [SLOW TEST:15.665 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:43:17.614: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Oct 26 20:43:17.811: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Oct 26 20:43:18.320: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Oct 26 20:43:20.471: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 20:43:22.484: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 20:43:24.483: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 20:43:26.485: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 20:43:28.489: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 20:43:30.482: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 20:43:32.485: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739341798, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 20:43:35.089: INFO: Waited 575.340849ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:43:37.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9796" for this suite.
Oct 26 20:43:45.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:43:47.480: INFO: namespace aggregator-9796 deletion completed in 10.116932547s

• [SLOW TEST:29.866 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:43:47.482: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Oct 26 20:43:47.681: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5301 /api/v1/namespaces/watch-5301/configmaps/e2e-watch-test-watch-closed 1f6e1f5f-587a-4257-beda-8979f6b39311 101326 0 2020-10-26 20:43:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Oct 26 20:43:47.681: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5301 /api/v1/namespaces/watch-5301/configmaps/e2e-watch-test-watch-closed 1f6e1f5f-587a-4257-beda-8979f6b39311 101332 0 2020-10-26 20:43:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Oct 26 20:43:47.734: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5301 /api/v1/namespaces/watch-5301/configmaps/e2e-watch-test-watch-closed 1f6e1f5f-587a-4257-beda-8979f6b39311 101336 0 2020-10-26 20:43:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Oct 26 20:43:47.734: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5301 /api/v1/namespaces/watch-5301/configmaps/e2e-watch-test-watch-closed 1f6e1f5f-587a-4257-beda-8979f6b39311 101338 0 2020-10-26 20:43:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:43:47.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5301" for this suite.
Oct 26 20:43:55.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:43:57.778: INFO: namespace watch-5301 deletion completed in 10.017710154s

• [SLOW TEST:10.297 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:43:57.779: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Oct 26 20:43:58.995: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc038648-f116-4dc5-91ef-00ccc2d34c03" in namespace "projected-4281" to be "success or failure"
Oct 26 20:43:59.007: INFO: Pod "downwardapi-volume-fc038648-f116-4dc5-91ef-00ccc2d34c03": Phase="Pending", Reason="", readiness=false. Elapsed: 11.104372ms
Oct 26 20:44:01.023: INFO: Pod "downwardapi-volume-fc038648-f116-4dc5-91ef-00ccc2d34c03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027886351s
STEP: Saw pod success
Oct 26 20:44:01.023: INFO: Pod "downwardapi-volume-fc038648-f116-4dc5-91ef-00ccc2d34c03" satisfied condition "success or failure"
Oct 26 20:44:01.036: INFO: Trying to get logs from node 10.123.240.178 pod downwardapi-volume-fc038648-f116-4dc5-91ef-00ccc2d34c03 container client-container: <nil>
STEP: delete the pod
Oct 26 20:44:01.156: INFO: Waiting for pod downwardapi-volume-fc038648-f116-4dc5-91ef-00ccc2d34c03 to disappear
Oct 26 20:44:01.170: INFO: Pod downwardapi-volume-fc038648-f116-4dc5-91ef-00ccc2d34c03 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:44:01.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4281" for this suite.
Oct 26 20:44:09.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:44:11.330: INFO: namespace projected-4281 deletion completed in 10.13467315s

• [SLOW TEST:13.552 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:44:11.331: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Oct 26 20:44:11.555: INFO: Waiting up to 5m0s for pod "pod-63012f7b-1c15-4dad-8be7-bede43a2cd79" in namespace "emptydir-112" to be "success or failure"
Oct 26 20:44:11.567: INFO: Pod "pod-63012f7b-1c15-4dad-8be7-bede43a2cd79": Phase="Pending", Reason="", readiness=false. Elapsed: 11.748459ms
Oct 26 20:44:13.586: INFO: Pod "pod-63012f7b-1c15-4dad-8be7-bede43a2cd79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030580296s
Oct 26 20:44:15.601: INFO: Pod "pod-63012f7b-1c15-4dad-8be7-bede43a2cd79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046138939s
STEP: Saw pod success
Oct 26 20:44:15.601: INFO: Pod "pod-63012f7b-1c15-4dad-8be7-bede43a2cd79" satisfied condition "success or failure"
Oct 26 20:44:15.613: INFO: Trying to get logs from node 10.123.240.178 pod pod-63012f7b-1c15-4dad-8be7-bede43a2cd79 container test-container: <nil>
STEP: delete the pod
Oct 26 20:44:15.675: INFO: Waiting for pod pod-63012f7b-1c15-4dad-8be7-bede43a2cd79 to disappear
Oct 26 20:44:15.691: INFO: Pod pod-63012f7b-1c15-4dad-8be7-bede43a2cd79 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:44:15.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-112" for this suite.
Oct 26 20:44:23.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:44:25.793: INFO: namespace emptydir-112 deletion completed in 10.07604728s

• [SLOW TEST:14.462 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:44:25.793: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Oct 26 20:44:26.003: INFO: Waiting up to 5m0s for pod "pod-85299190-678c-4071-8907-faf925dec4ce" in namespace "emptydir-9001" to be "success or failure"
Oct 26 20:44:26.012: INFO: Pod "pod-85299190-678c-4071-8907-faf925dec4ce": Phase="Pending", Reason="", readiness=false. Elapsed: 8.779759ms
Oct 26 20:44:28.025: INFO: Pod "pod-85299190-678c-4071-8907-faf925dec4ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022098002s
Oct 26 20:44:30.039: INFO: Pod "pod-85299190-678c-4071-8907-faf925dec4ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035717475s
STEP: Saw pod success
Oct 26 20:44:30.039: INFO: Pod "pod-85299190-678c-4071-8907-faf925dec4ce" satisfied condition "success or failure"
Oct 26 20:44:30.053: INFO: Trying to get logs from node 10.123.240.178 pod pod-85299190-678c-4071-8907-faf925dec4ce container test-container: <nil>
STEP: delete the pod
Oct 26 20:44:30.111: INFO: Waiting for pod pod-85299190-678c-4071-8907-faf925dec4ce to disappear
Oct 26 20:44:30.120: INFO: Pod pod-85299190-678c-4071-8907-faf925dec4ce no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:44:30.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9001" for this suite.
Oct 26 20:44:38.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:44:40.176: INFO: namespace emptydir-9001 deletion completed in 10.029158404s

• [SLOW TEST:14.383 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:44:40.176: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 20:44:40.326: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:44:41.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5959" for this suite.
Oct 26 20:44:49.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:44:51.596: INFO: namespace custom-resource-definition-5959 deletion completed in 10.122033299s

• [SLOW TEST:11.420 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:44:51.597: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Oct 26 20:44:51.753: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:45:41.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-758" for this suite.
Oct 26 20:45:49.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:45:51.714: INFO: namespace crd-publish-openapi-758 deletion completed in 10.119147446s

• [SLOW TEST:60.117 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:45:51.714: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-037f1bb1-8568-490e-8de7-9d095d87222e in namespace container-probe-5534
Oct 26 20:45:54.058: INFO: Started pod liveness-037f1bb1-8568-490e-8de7-9d095d87222e in namespace container-probe-5534
STEP: checking the pod's current state and verifying that restartCount is present
Oct 26 20:45:54.067: INFO: Initial restart count of pod liveness-037f1bb1-8568-490e-8de7-9d095d87222e is 0
Oct 26 20:46:14.215: INFO: Restart count of pod container-probe-5534/liveness-037f1bb1-8568-490e-8de7-9d095d87222e is now 1 (20.147329449s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:46:14.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5534" for this suite.
Oct 26 20:46:22.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:46:24.351: INFO: namespace container-probe-5534 deletion completed in 10.079121625s

• [SLOW TEST:32.637 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:46:24.351: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Oct 26 20:46:24.597: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8cd500d9-0467-4f93-a5bc-1bf3bc5b72f8" in namespace "projected-6053" to be "success or failure"
Oct 26 20:46:24.609: INFO: Pod "downwardapi-volume-8cd500d9-0467-4f93-a5bc-1bf3bc5b72f8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.636839ms
Oct 26 20:46:26.621: INFO: Pod "downwardapi-volume-8cd500d9-0467-4f93-a5bc-1bf3bc5b72f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02307752s
Oct 26 20:46:28.635: INFO: Pod "downwardapi-volume-8cd500d9-0467-4f93-a5bc-1bf3bc5b72f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03717553s
STEP: Saw pod success
Oct 26 20:46:28.635: INFO: Pod "downwardapi-volume-8cd500d9-0467-4f93-a5bc-1bf3bc5b72f8" satisfied condition "success or failure"
Oct 26 20:46:28.646: INFO: Trying to get logs from node 10.123.240.178 pod downwardapi-volume-8cd500d9-0467-4f93-a5bc-1bf3bc5b72f8 container client-container: <nil>
STEP: delete the pod
Oct 26 20:46:28.746: INFO: Waiting for pod downwardapi-volume-8cd500d9-0467-4f93-a5bc-1bf3bc5b72f8 to disappear
Oct 26 20:46:28.758: INFO: Pod downwardapi-volume-8cd500d9-0467-4f93-a5bc-1bf3bc5b72f8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:46:28.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6053" for this suite.
Oct 26 20:46:36.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:46:38.845: INFO: namespace projected-6053 deletion completed in 10.066570687s

• [SLOW TEST:14.494 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:46:38.847: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-7954/configmap-test-3bc6173c-3fdf-454a-9573-9bc2f494a1eb
STEP: Creating a pod to test consume configMaps
Oct 26 20:46:39.044: INFO: Waiting up to 5m0s for pod "pod-configmaps-2196abdd-71b1-4caa-af31-4d230aba4965" in namespace "configmap-7954" to be "success or failure"
Oct 26 20:46:39.062: INFO: Pod "pod-configmaps-2196abdd-71b1-4caa-af31-4d230aba4965": Phase="Pending", Reason="", readiness=false. Elapsed: 18.164388ms
Oct 26 20:46:41.074: INFO: Pod "pod-configmaps-2196abdd-71b1-4caa-af31-4d230aba4965": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029985185s
STEP: Saw pod success
Oct 26 20:46:41.074: INFO: Pod "pod-configmaps-2196abdd-71b1-4caa-af31-4d230aba4965" satisfied condition "success or failure"
Oct 26 20:46:41.087: INFO: Trying to get logs from node 10.123.240.178 pod pod-configmaps-2196abdd-71b1-4caa-af31-4d230aba4965 container env-test: <nil>
STEP: delete the pod
Oct 26 20:46:41.161: INFO: Waiting for pod pod-configmaps-2196abdd-71b1-4caa-af31-4d230aba4965 to disappear
Oct 26 20:46:41.173: INFO: Pod pod-configmaps-2196abdd-71b1-4caa-af31-4d230aba4965 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:46:41.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7954" for this suite.
Oct 26 20:46:49.245: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:46:51.438: INFO: namespace configmap-7954 deletion completed in 10.243091271s

• [SLOW TEST:12.591 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:46:51.439: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Oct 26 20:46:51.802: INFO: Number of nodes with available pods: 0
Oct 26 20:46:51.802: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:46:52.843: INFO: Number of nodes with available pods: 0
Oct 26 20:46:52.843: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:46:53.833: INFO: Number of nodes with available pods: 1
Oct 26 20:46:53.833: INFO: Node 10.123.240.174 is running more than one daemon pod
Oct 26 20:46:54.839: INFO: Number of nodes with available pods: 2
Oct 26 20:46:54.839: INFO: Node 10.123.240.178 is running more than one daemon pod
Oct 26 20:46:55.839: INFO: Number of nodes with available pods: 3
Oct 26 20:46:55.839: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Oct 26 20:46:55.916: INFO: Number of nodes with available pods: 2
Oct 26 20:46:55.916: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:46:56.961: INFO: Number of nodes with available pods: 2
Oct 26 20:46:56.961: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:46:57.951: INFO: Number of nodes with available pods: 2
Oct 26 20:46:57.951: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:46:58.950: INFO: Number of nodes with available pods: 2
Oct 26 20:46:58.950: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:46:59.947: INFO: Number of nodes with available pods: 2
Oct 26 20:46:59.947: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:47:00.951: INFO: Number of nodes with available pods: 2
Oct 26 20:47:00.951: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:47:01.959: INFO: Number of nodes with available pods: 2
Oct 26 20:47:01.959: INFO: Node 10.123.240.172 is running more than one daemon pod
Oct 26 20:47:02.948: INFO: Number of nodes with available pods: 3
Oct 26 20:47:02.948: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7265, will wait for the garbage collector to delete the pods
Oct 26 20:47:03.050: INFO: Deleting DaemonSet.extensions daemon-set took: 27.294126ms
Oct 26 20:47:03.150: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.239535ms
Oct 26 20:47:15.571: INFO: Number of nodes with available pods: 0
Oct 26 20:47:15.571: INFO: Number of running nodes: 0, number of available pods: 0
Oct 26 20:47:15.592: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7265/daemonsets","resourceVersion":"102880"},"items":null}

Oct 26 20:47:15.605: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7265/pods","resourceVersion":"102880"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:47:15.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7265" for this suite.
Oct 26 20:47:23.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:47:25.833: INFO: namespace daemonsets-7265 deletion completed in 10.147048415s

• [SLOW TEST:34.394 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:47:25.833: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Oct 26 20:47:26.853: INFO: Pod name wrapped-volume-race-7954f5d0-8b45-4327-827e-f2f56958bc4c: Found 0 pods out of 5
Oct 26 20:47:31.903: INFO: Pod name wrapped-volume-race-7954f5d0-8b45-4327-827e-f2f56958bc4c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-7954f5d0-8b45-4327-827e-f2f56958bc4c in namespace emptydir-wrapper-7590, will wait for the garbage collector to delete the pods
Oct 26 20:47:32.063: INFO: Deleting ReplicationController wrapped-volume-race-7954f5d0-8b45-4327-827e-f2f56958bc4c took: 29.732258ms
Oct 26 20:47:32.663: INFO: Terminating ReplicationController wrapped-volume-race-7954f5d0-8b45-4327-827e-f2f56958bc4c pods took: 600.328612ms
STEP: Creating RC which spawns configmap-volume pods
Oct 26 20:48:10.634: INFO: Pod name wrapped-volume-race-f8704c1e-d0c7-4460-bbf9-1268edf8d93d: Found 0 pods out of 5
Oct 26 20:48:15.656: INFO: Pod name wrapped-volume-race-f8704c1e-d0c7-4460-bbf9-1268edf8d93d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f8704c1e-d0c7-4460-bbf9-1268edf8d93d in namespace emptydir-wrapper-7590, will wait for the garbage collector to delete the pods
Oct 26 20:48:15.817: INFO: Deleting ReplicationController wrapped-volume-race-f8704c1e-d0c7-4460-bbf9-1268edf8d93d took: 28.139386ms
Oct 26 20:48:16.417: INFO: Terminating ReplicationController wrapped-volume-race-f8704c1e-d0c7-4460-bbf9-1268edf8d93d pods took: 600.280448ms
STEP: Creating RC which spawns configmap-volume pods
Oct 26 20:49:00.795: INFO: Pod name wrapped-volume-race-8687f0e0-4fc6-4540-af6e-6338f653ffd1: Found 0 pods out of 5
Oct 26 20:49:05.853: INFO: Pod name wrapped-volume-race-8687f0e0-4fc6-4540-af6e-6338f653ffd1: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8687f0e0-4fc6-4540-af6e-6338f653ffd1 in namespace emptydir-wrapper-7590, will wait for the garbage collector to delete the pods
Oct 26 20:49:06.005: INFO: Deleting ReplicationController wrapped-volume-race-8687f0e0-4fc6-4540-af6e-6338f653ffd1 took: 31.187523ms
Oct 26 20:49:06.506: INFO: Terminating ReplicationController wrapped-volume-race-8687f0e0-4fc6-4540-af6e-6338f653ffd1 pods took: 500.375928ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:49:52.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7590" for this suite.
Oct 26 20:50:02.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:50:04.128: INFO: namespace emptydir-wrapper-7590 deletion completed in 12.038434524s

• [SLOW TEST:158.296 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:50:04.129: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2535
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-2535
I1026 20:50:04.379787      23 runners.go:184] Created replication controller with name: externalname-service, namespace: services-2535, replica count: 2
Oct 26 20:50:07.430: INFO: Creating new exec pod
I1026 20:50:07.430395      23 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 26 20:50:12.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=services-2535 execpod4sg6r -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Oct 26 20:50:13.150: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Oct 26 20:50:13.150: INFO: stdout: ""
Oct 26 20:50:13.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 exec --namespace=services-2535 execpod4sg6r -- /bin/sh -x -c nc -zv -t -w 2 172.21.101.137 80'
Oct 26 20:50:13.536: INFO: stderr: "+ nc -zv -t -w 2 172.21.101.137 80\nConnection to 172.21.101.137 80 port [tcp/http] succeeded!\n"
Oct 26 20:50:13.536: INFO: stdout: ""
Oct 26 20:50:13.536: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:50:13.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2535" for this suite.
Oct 26 20:50:21.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:50:23.792: INFO: namespace services-2535 deletion completed in 10.12965452s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:19.663 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:50:23.792: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Oct 26 20:50:23.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 create -f - --namespace=kubectl-4674'
Oct 26 20:50:24.607: INFO: stderr: ""
Oct 26 20:50:24.607: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 26 20:50:24.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4674'
Oct 26 20:50:24.763: INFO: stderr: ""
Oct 26 20:50:24.763: INFO: stdout: "update-demo-nautilus-lbjv4 update-demo-nautilus-pllhg "
Oct 26 20:50:24.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-nautilus-lbjv4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4674'
Oct 26 20:50:24.996: INFO: stderr: ""
Oct 26 20:50:24.997: INFO: stdout: ""
Oct 26 20:50:24.997: INFO: update-demo-nautilus-lbjv4 is created but not running
Oct 26 20:50:29.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4674'
Oct 26 20:50:30.158: INFO: stderr: ""
Oct 26 20:50:30.158: INFO: stdout: "update-demo-nautilus-lbjv4 update-demo-nautilus-pllhg "
Oct 26 20:50:30.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-nautilus-lbjv4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4674'
Oct 26 20:50:30.295: INFO: stderr: ""
Oct 26 20:50:30.295: INFO: stdout: "true"
Oct 26 20:50:30.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-nautilus-lbjv4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4674'
Oct 26 20:50:30.618: INFO: stderr: ""
Oct 26 20:50:30.618: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct 26 20:50:30.618: INFO: validating pod update-demo-nautilus-lbjv4
Oct 26 20:50:30.648: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 26 20:50:30.649: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 26 20:50:30.649: INFO: update-demo-nautilus-lbjv4 is verified up and running
Oct 26 20:50:30.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-nautilus-pllhg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4674'
Oct 26 20:50:30.801: INFO: stderr: ""
Oct 26 20:50:30.801: INFO: stdout: "true"
Oct 26 20:50:30.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-nautilus-pllhg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4674'
Oct 26 20:50:30.941: INFO: stderr: ""
Oct 26 20:50:30.941: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct 26 20:50:30.941: INFO: validating pod update-demo-nautilus-pllhg
Oct 26 20:50:30.964: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 26 20:50:30.964: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 26 20:50:30.964: INFO: update-demo-nautilus-pllhg is verified up and running
STEP: rolling-update to new replication controller
Oct 26 20:50:30.969: INFO: scanned /root for discovery docs: <nil>
Oct 26 20:50:30.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-4674'
Oct 26 20:50:56.698: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Oct 26 20:50:56.698: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 26 20:50:56.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4674'
Oct 26 20:50:56.843: INFO: stderr: ""
Oct 26 20:50:56.843: INFO: stdout: "update-demo-kitten-b22xb update-demo-kitten-cj8s6 "
Oct 26 20:50:56.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-kitten-b22xb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4674'
Oct 26 20:50:56.972: INFO: stderr: ""
Oct 26 20:50:56.972: INFO: stdout: "true"
Oct 26 20:50:56.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-kitten-b22xb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4674'
Oct 26 20:50:57.136: INFO: stderr: ""
Oct 26 20:50:57.136: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Oct 26 20:50:57.136: INFO: validating pod update-demo-kitten-b22xb
Oct 26 20:50:57.157: INFO: got data: {
  "image": "kitten.jpg"
}

Oct 26 20:50:57.158: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Oct 26 20:50:57.158: INFO: update-demo-kitten-b22xb is verified up and running
Oct 26 20:50:57.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-kitten-cj8s6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4674'
Oct 26 20:50:57.288: INFO: stderr: ""
Oct 26 20:50:57.288: INFO: stdout: "true"
Oct 26 20:50:57.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 get pods update-demo-kitten-cj8s6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4674'
Oct 26 20:50:57.418: INFO: stderr: ""
Oct 26 20:50:57.418: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Oct 26 20:50:57.418: INFO: validating pod update-demo-kitten-cj8s6
Oct 26 20:50:57.444: INFO: got data: {
  "image": "kitten.jpg"
}

Oct 26 20:50:57.444: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Oct 26 20:50:57.444: INFO: update-demo-kitten-cj8s6 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:50:57.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4674" for this suite.
Oct 26 20:51:19.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:51:21.538: INFO: namespace kubectl-4674 deletion completed in 24.068046018s

• [SLOW TEST:57.746 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:51:21.539: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 20:51:21.674: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Oct 26 20:51:31.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 --namespace=crd-publish-openapi-1203 create -f -'
Oct 26 20:51:32.319: INFO: stderr: ""
Oct 26 20:51:32.319: INFO: stdout: "e2e-test-crd-publish-openapi-4655-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Oct 26 20:51:32.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 --namespace=crd-publish-openapi-1203 delete e2e-test-crd-publish-openapi-4655-crds test-foo'
Oct 26 20:51:32.479: INFO: stderr: ""
Oct 26 20:51:32.479: INFO: stdout: "e2e-test-crd-publish-openapi-4655-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Oct 26 20:51:32.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 --namespace=crd-publish-openapi-1203 apply -f -'
Oct 26 20:51:33.189: INFO: stderr: ""
Oct 26 20:51:33.189: INFO: stdout: "e2e-test-crd-publish-openapi-4655-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Oct 26 20:51:33.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 --namespace=crd-publish-openapi-1203 delete e2e-test-crd-publish-openapi-4655-crds test-foo'
Oct 26 20:51:33.449: INFO: stderr: ""
Oct 26 20:51:33.449: INFO: stdout: "e2e-test-crd-publish-openapi-4655-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Oct 26 20:51:33.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 --namespace=crd-publish-openapi-1203 create -f -'
Oct 26 20:51:34.040: INFO: rc: 1
Oct 26 20:51:34.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 --namespace=crd-publish-openapi-1203 apply -f -'
Oct 26 20:51:34.625: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Oct 26 20:51:34.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 --namespace=crd-publish-openapi-1203 create -f -'
Oct 26 20:51:35.200: INFO: rc: 1
Oct 26 20:51:35.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 --namespace=crd-publish-openapi-1203 apply -f -'
Oct 26 20:51:35.806: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Oct 26 20:51:35.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 explain e2e-test-crd-publish-openapi-4655-crds'
Oct 26 20:51:36.482: INFO: stderr: ""
Oct 26 20:51:36.482: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4655-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Oct 26 20:51:36.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 explain e2e-test-crd-publish-openapi-4655-crds.metadata'
Oct 26 20:51:36.832: INFO: stderr: ""
Oct 26 20:51:36.832: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4655-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Oct 26 20:51:36.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 explain e2e-test-crd-publish-openapi-4655-crds.spec'
Oct 26 20:51:37.450: INFO: stderr: ""
Oct 26 20:51:37.450: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4655-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Oct 26 20:51:37.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 explain e2e-test-crd-publish-openapi-4655-crds.spec.bars'
Oct 26 20:51:38.096: INFO: stderr: ""
Oct 26 20:51:38.096: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4655-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Oct 26 20:51:38.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-668520336 explain e2e-test-crd-publish-openapi-4655-crds.spec.bars2'
Oct 26 20:51:38.433: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:51:47.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1203" for this suite.
Oct 26 20:51:55.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:51:57.616: INFO: namespace crd-publish-openapi-1203 deletion completed in 10.012914873s

• [SLOW TEST:36.077 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:51:57.621: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-24c5d6ee-a371-4b24-af5c-4f5320b76a45
STEP: Creating a pod to test consume secrets
Oct 26 20:51:57.828: INFO: Waiting up to 5m0s for pod "pod-secrets-ac567eba-f18d-408a-9df9-afa49a4a9955" in namespace "secrets-341" to be "success or failure"
Oct 26 20:51:57.839: INFO: Pod "pod-secrets-ac567eba-f18d-408a-9df9-afa49a4a9955": Phase="Pending", Reason="", readiness=false. Elapsed: 10.627661ms
Oct 26 20:51:59.852: INFO: Pod "pod-secrets-ac567eba-f18d-408a-9df9-afa49a4a9955": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023522676s
Oct 26 20:52:01.864: INFO: Pod "pod-secrets-ac567eba-f18d-408a-9df9-afa49a4a9955": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035072125s
STEP: Saw pod success
Oct 26 20:52:01.864: INFO: Pod "pod-secrets-ac567eba-f18d-408a-9df9-afa49a4a9955" satisfied condition "success or failure"
Oct 26 20:52:01.874: INFO: Trying to get logs from node 10.123.240.178 pod pod-secrets-ac567eba-f18d-408a-9df9-afa49a4a9955 container secret-volume-test: <nil>
STEP: delete the pod
Oct 26 20:52:01.978: INFO: Waiting for pod pod-secrets-ac567eba-f18d-408a-9df9-afa49a4a9955 to disappear
Oct 26 20:52:01.990: INFO: Pod pod-secrets-ac567eba-f18d-408a-9df9-afa49a4a9955 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:52:01.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-341" for this suite.
Oct 26 20:52:10.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:52:12.170: INFO: namespace secrets-341 deletion completed in 10.159322309s

• [SLOW TEST:14.550 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:52:12.171: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-4963
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-4963
Oct 26 20:52:12.430: INFO: Found 0 stateful pods, waiting for 1
Oct 26 20:52:22.447: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Oct 26 20:52:22.515: INFO: Deleting all statefulset in ns statefulset-4963
Oct 26 20:52:22.528: INFO: Scaling statefulset ss to 0
Oct 26 20:52:42.618: INFO: Waiting for statefulset status.replicas updated to 0
Oct 26 20:52:42.629: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:52:42.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4963" for this suite.
Oct 26 20:52:50.733: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:52:52.767: INFO: namespace statefulset-4963 deletion completed in 10.07562021s

• [SLOW TEST:40.596 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:52:52.767: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Oct 26 20:53:15.026: INFO: Container started at 2020-10-26 20:52:54 +0000 UTC, pod became ready at 2020-10-26 20:53:13 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:53:15.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-500" for this suite.
Oct 26 20:53:47.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:53:49.182: INFO: namespace container-probe-500 deletion completed in 34.140022622s

• [SLOW TEST:56.415 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:53:49.182: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-93dbfd9a-4ad0-4108-be65-9b8f4f71cc47
STEP: Creating a pod to test consume configMaps
Oct 26 20:53:49.426: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d7c2a9ac-6154-4369-a84d-59efec4577a1" in namespace "projected-7641" to be "success or failure"
Oct 26 20:53:49.437: INFO: Pod "pod-projected-configmaps-d7c2a9ac-6154-4369-a84d-59efec4577a1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.230791ms
Oct 26 20:53:51.451: INFO: Pod "pod-projected-configmaps-d7c2a9ac-6154-4369-a84d-59efec4577a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025451586s
STEP: Saw pod success
Oct 26 20:53:51.451: INFO: Pod "pod-projected-configmaps-d7c2a9ac-6154-4369-a84d-59efec4577a1" satisfied condition "success or failure"
Oct 26 20:53:51.462: INFO: Trying to get logs from node 10.123.240.178 pod pod-projected-configmaps-d7c2a9ac-6154-4369-a84d-59efec4577a1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 20:53:51.553: INFO: Waiting for pod pod-projected-configmaps-d7c2a9ac-6154-4369-a84d-59efec4577a1 to disappear
Oct 26 20:53:51.563: INFO: Pod pod-projected-configmaps-d7c2a9ac-6154-4369-a84d-59efec4577a1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:53:51.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7641" for this suite.
Oct 26 20:53:59.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:54:01.709: INFO: namespace projected-7641 deletion completed in 10.112958467s

• [SLOW TEST:12.527 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Oct 26 20:54:01.709: INFO: >>> kubeConfig: /tmp/kubeconfig-668520336
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Oct 26 20:54:01.935: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1023 /api/v1/namespaces/watch-1023/configmaps/e2e-watch-test-label-changed ca8477b4-ca39-4e81-a2d7-6a099ec8f91a 105943 0 2020-10-26 20:54:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Oct 26 20:54:01.935: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1023 /api/v1/namespaces/watch-1023/configmaps/e2e-watch-test-label-changed ca8477b4-ca39-4e81-a2d7-6a099ec8f91a 105946 0 2020-10-26 20:54:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Oct 26 20:54:01.935: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1023 /api/v1/namespaces/watch-1023/configmaps/e2e-watch-test-label-changed ca8477b4-ca39-4e81-a2d7-6a099ec8f91a 105949 0 2020-10-26 20:54:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Oct 26 20:54:12.052: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1023 /api/v1/namespaces/watch-1023/configmaps/e2e-watch-test-label-changed ca8477b4-ca39-4e81-a2d7-6a099ec8f91a 105997 0 2020-10-26 20:54:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Oct 26 20:54:12.052: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1023 /api/v1/namespaces/watch-1023/configmaps/e2e-watch-test-label-changed ca8477b4-ca39-4e81-a2d7-6a099ec8f91a 105998 0 2020-10-26 20:54:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Oct 26 20:54:12.052: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1023 /api/v1/namespaces/watch-1023/configmaps/e2e-watch-test-label-changed ca8477b4-ca39-4e81-a2d7-6a099ec8f91a 105999 0 2020-10-26 20:54:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Oct 26 20:54:12.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1023" for this suite.
Oct 26 20:54:20.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct 26 20:54:22.210: INFO: namespace watch-1023 deletion completed in 10.117841441s

• [SLOW TEST:20.501 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
Oct 26 20:54:22.211: INFO: Running AfterSuite actions on all nodes
Oct 26 20:54:22.211: INFO: Running AfterSuite actions on node 1
Oct 26 20:54:22.211: INFO: Skipping dumping logs from cluster

Ran 276 of 4897 Specs in 9176.094 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Pending | 4621 Skipped
PASS

Ginkgo ran 1 suite in 2h32m58.071435713s
Test Suite Passed
