I0817 04:13:10.793260      22 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-667650646
I0817 04:13:10.793479      22 e2e.go:92] Starting e2e run "0e964423-3911-477f-a0e1-33fa8866ee7f" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1597637588 - Will randomize all specs
Will run 276 of 4897 specs

Aug 17 04:13:10.808: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 04:13:10.811: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 17 04:13:10.850: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 17 04:13:10.929: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 17 04:13:10.930: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Aug 17 04:13:10.930: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 17 04:13:10.951: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Aug 17 04:13:10.951: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
Aug 17 04:13:10.951: INFO: e2e test version: v1.16.2
Aug 17 04:13:10.954: INFO: kube-apiserver version: v1.16.2+554af56
Aug 17 04:13:10.954: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 04:13:10.975: INFO: Cluster IP family: ipv4
SS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:13:10.976: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename pods
Aug 17 04:13:11.214: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 17 04:13:17.913: INFO: Successfully updated pod "pod-update-3316794c-46da-4f08-9d11-40e9decf9718"
STEP: verifying the updated pod is in kubernetes
Aug 17 04:13:17.947: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:13:17.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6919" for this suite.
Aug 17 04:13:54.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:13:56.102: INFO: namespace pods-6919 deletion completed in 38.140294972s

• [SLOW TEST:45.127 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:13:56.107: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4665.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4665.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4665.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4665.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 17 04:14:10.506: INFO: DNS probes using dns-test-fd1438e3-3082-4ec3-8020-d566944c8224 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4665.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4665.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4665.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4665.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 17 04:14:16.762: INFO: DNS probes using dns-test-d5e4781b-64d0-41fe-9c33-30320ecd15d1 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4665.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4665.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4665.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4665.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 17 04:14:21.072: INFO: DNS probes using dns-test-06853463-8b87-40c1-89b7-91e3fd154c7f succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:14:21.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4665" for this suite.
Aug 17 04:14:31.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:14:33.372: INFO: namespace dns-4665 deletion completed in 12.141067299s

• [SLOW TEST:37.266 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:14:33.372: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Aug 17 04:14:33.637: INFO: Waiting up to 5m0s for pod "client-containers-bd16e00a-5433-4530-b0e7-a58268533507" in namespace "containers-3341" to be "success or failure"
Aug 17 04:14:33.649: INFO: Pod "client-containers-bd16e00a-5433-4530-b0e7-a58268533507": Phase="Pending", Reason="", readiness=false. Elapsed: 11.773467ms
Aug 17 04:14:35.666: INFO: Pod "client-containers-bd16e00a-5433-4530-b0e7-a58268533507": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028175686s
Aug 17 04:14:37.681: INFO: Pod "client-containers-bd16e00a-5433-4530-b0e7-a58268533507": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04362407s
Aug 17 04:14:39.697: INFO: Pod "client-containers-bd16e00a-5433-4530-b0e7-a58268533507": Phase="Pending", Reason="", readiness=false. Elapsed: 6.059529743s
Aug 17 04:14:41.711: INFO: Pod "client-containers-bd16e00a-5433-4530-b0e7-a58268533507": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.073232414s
STEP: Saw pod success
Aug 17 04:14:41.711: INFO: Pod "client-containers-bd16e00a-5433-4530-b0e7-a58268533507" satisfied condition "success or failure"
Aug 17 04:14:41.723: INFO: Trying to get logs from node 10.241.148.42 pod client-containers-bd16e00a-5433-4530-b0e7-a58268533507 container test-container: <nil>
STEP: delete the pod
Aug 17 04:14:41.873: INFO: Waiting for pod client-containers-bd16e00a-5433-4530-b0e7-a58268533507 to disappear
Aug 17 04:14:41.887: INFO: Pod client-containers-bd16e00a-5433-4530-b0e7-a58268533507 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:14:41.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3341" for this suite.
Aug 17 04:14:51.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:14:54.071: INFO: namespace containers-3341 deletion completed in 12.173096606s

• [SLOW TEST:20.699 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:14:54.074: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Aug 17 04:14:54.302: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 04:15:03.570: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:15:38.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3319" for this suite.
Aug 17 04:15:46.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:15:50.921: INFO: namespace crd-publish-openapi-3319 deletion completed in 12.330873522s

• [SLOW TEST:56.848 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:15:50.923: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 17 04:15:51.189: INFO: Waiting up to 5m0s for pod "downwardapi-volume-47861c01-fbe1-4b8a-8a94-1954ddb99248" in namespace "downward-api-4556" to be "success or failure"
Aug 17 04:15:51.216: INFO: Pod "downwardapi-volume-47861c01-fbe1-4b8a-8a94-1954ddb99248": Phase="Pending", Reason="", readiness=false. Elapsed: 26.372921ms
Aug 17 04:15:53.332: INFO: Pod "downwardapi-volume-47861c01-fbe1-4b8a-8a94-1954ddb99248": Phase="Pending", Reason="", readiness=false. Elapsed: 2.142337691s
Aug 17 04:15:55.424: INFO: Pod "downwardapi-volume-47861c01-fbe1-4b8a-8a94-1954ddb99248": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.234602324s
STEP: Saw pod success
Aug 17 04:15:55.424: INFO: Pod "downwardapi-volume-47861c01-fbe1-4b8a-8a94-1954ddb99248" satisfied condition "success or failure"
Aug 17 04:15:55.534: INFO: Trying to get logs from node 10.241.148.42 pod downwardapi-volume-47861c01-fbe1-4b8a-8a94-1954ddb99248 container client-container: <nil>
STEP: delete the pod
Aug 17 04:15:55.820: INFO: Waiting for pod downwardapi-volume-47861c01-fbe1-4b8a-8a94-1954ddb99248 to disappear
Aug 17 04:15:55.922: INFO: Pod downwardapi-volume-47861c01-fbe1-4b8a-8a94-1954ddb99248 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:15:55.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4556" for this suite.
Aug 17 04:16:06.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:16:16.358: INFO: namespace downward-api-4556 deletion completed in 20.150882907s

• [SLOW TEST:25.435 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:16:16.358: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-824
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-824
I0817 04:16:17.438572      22 runners.go:184] Created replication controller with name: externalname-service, namespace: services-824, replica count: 2
I0817 04:16:20.589237      22 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0817 04:16:23.589858      22 runners.go:184] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 17 04:16:26.590: INFO: Creating new exec pod
I0817 04:16:26.590496      22 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 17 04:16:30.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=services-824 execpodjtbk7 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Aug 17 04:16:30.631: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 17 04:16:30.631: INFO: stdout: ""
Aug 17 04:16:30.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=services-824 execpodjtbk7 -- /bin/sh -x -c nc -zv -t -w 2 172.21.49.73 80'
Aug 17 04:16:31.368: INFO: stderr: "+ nc -zv -t -w 2 172.21.49.73 80\nConnection to 172.21.49.73 80 port [tcp/http] succeeded!\n"
Aug 17 04:16:31.368: INFO: stdout: ""
Aug 17 04:16:31.368: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:16:31.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-824" for this suite.
Aug 17 04:16:42.197: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:16:50.710: INFO: namespace services-824 deletion completed in 18.913442054s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:34.351 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:16:50.711: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 17 04:16:51.317: INFO: Waiting up to 5m0s for pod "downwardapi-volume-01d9fdfe-0312-4a49-ae9d-6a656724fb24" in namespace "downward-api-3437" to be "success or failure"
Aug 17 04:16:51.436: INFO: Pod "downwardapi-volume-01d9fdfe-0312-4a49-ae9d-6a656724fb24": Phase="Pending", Reason="", readiness=false. Elapsed: 118.071309ms
Aug 17 04:16:53.531: INFO: Pod "downwardapi-volume-01d9fdfe-0312-4a49-ae9d-6a656724fb24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.213322886s
Aug 17 04:16:55.649: INFO: Pod "downwardapi-volume-01d9fdfe-0312-4a49-ae9d-6a656724fb24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.331966639s
STEP: Saw pod success
Aug 17 04:16:55.650: INFO: Pod "downwardapi-volume-01d9fdfe-0312-4a49-ae9d-6a656724fb24" satisfied condition "success or failure"
Aug 17 04:16:55.780: INFO: Trying to get logs from node 10.241.148.42 pod downwardapi-volume-01d9fdfe-0312-4a49-ae9d-6a656724fb24 container client-container: <nil>
STEP: delete the pod
Aug 17 04:16:56.074: INFO: Waiting for pod downwardapi-volume-01d9fdfe-0312-4a49-ae9d-6a656724fb24 to disappear
Aug 17 04:16:56.211: INFO: Pod downwardapi-volume-01d9fdfe-0312-4a49-ae9d-6a656724fb24 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:16:56.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3437" for this suite.
Aug 17 04:17:04.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:17:15.111: INFO: namespace downward-api-3437 deletion completed in 18.63420307s

• [SLOW TEST:24.400 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:17:15.112: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 04:17:15.580: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:17:20.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1267" for this suite.
Aug 17 04:18:07.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:18:17.321: INFO: namespace pods-1267 deletion completed in 56.651484909s

• [SLOW TEST:62.209 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:18:17.321: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Aug 17 04:18:17.839: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 17 04:18:18.538: INFO: Waiting for terminating namespaces to be deleted...
Aug 17 04:18:18.762: INFO: 
Logging pods the kubelet thinks is on node 10.241.148.31 before test
Aug 17 04:18:19.997: INFO: thanos-querier-6b5fbd9754-ctb66 from openshift-monitoring started at 2020-08-17 03:05:37 +0000 UTC (4 container statuses recorded)
Aug 17 04:18:19.997: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:18:19.997: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 17 04:18:19.997: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 17 04:18:19.997: INFO: 	Container thanos-querier ready: true, restart count 0
Aug 17 04:18:19.997: INFO: multus-tvx65 from openshift-multus started at 2020-08-17 02:56:38 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:19.997: INFO: 	Container kube-multus ready: true, restart count 0
Aug 17 04:18:19.997: INFO: tuned-nmn6j from openshift-cluster-node-tuning-operator started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:19.997: INFO: 	Container tuned ready: true, restart count 0
Aug 17 04:18:19.997: INFO: ibm-cloud-provider-ip-169-48-239-26-5595944bf9-n8nts from ibm-system started at 2020-08-17 03:02:23 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:19.997: INFO: 	Container ibm-cloud-provider-ip-169-48-239-26 ready: true, restart count 0
Aug 17 04:18:19.997: INFO: console-76d8dc5894-2w62x from openshift-console started at 2020-08-17 02:59:39 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:19.997: INFO: 	Container console ready: true, restart count 0
Aug 17 04:18:19.997: INFO: ibmcloud-block-storage-driver-hfn98 from kube-system started at 2020-08-17 02:56:23 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:19.997: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 17 04:18:19.997: INFO: calico-node-lm8qf from calico-system started at 2020-08-17 02:57:31 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:19.997: INFO: 	Container calico-node ready: true, restart count 0
Aug 17 04:18:19.997: INFO: cluster-samples-operator-55944b8f44-fxlq8 from openshift-cluster-samples-operator started at 2020-08-17 02:59:01 +0000 UTC (2 container statuses recorded)
Aug 17 04:18:19.997: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Aug 17 04:18:19.997: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Aug 17 04:18:19.997: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-08-17 03:05:54 +0000 UTC (7 container statuses recorded)
Aug 17 04:18:19.997: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:18:19.997: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 17 04:18:19.997: INFO: 	Container prometheus ready: true, restart count 1
Aug 17 04:18:19.997: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 17 04:18:19.997: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 17 04:18:19.997: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 17 04:18:19.997: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 17 04:18:19.997: INFO: multus-admission-controller-ncsvb from openshift-multus started at 2020-08-17 02:57:44 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:19.997: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 17 04:18:19.997: INFO: router-default-57d78dfb48-mzjlc from openshift-ingress started at 2020-08-17 02:59:05 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:19.997: INFO: 	Container router ready: true, restart count 0
Aug 17 04:18:19.997: INFO: calico-typha-579947cf56-672t9 from calico-system started at 2020-08-17 02:59:26 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:19.997: INFO: 	Container calico-typha ready: true, restart count 0
Aug 17 04:18:19.997: INFO: sonobuoy-systemd-logs-daemon-set-c8f140ca93fa466f-pjc77 from sonobuoy started at 2020-08-17 04:12:38 +0000 UTC (2 container statuses recorded)
Aug 17 04:18:19.997: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 04:18:19.997: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 04:18:19.997: INFO: service-serving-cert-signer-7879bf8d9f-xtqt8 from openshift-service-ca started at 2020-08-17 02:58:07 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:19.997: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Aug 17 04:18:19.997: INFO: configmap-cabundle-injector-8446d4b88f-9k4sq from openshift-service-ca started at 2020-08-17 02:58:07 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:19.997: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Aug 17 04:18:19.997: INFO: kube-state-metrics-c5f65645-855s8 from openshift-monitoring started at 2020-08-17 02:58:30 +0000 UTC (3 container statuses recorded)
Aug 17 04:18:19.997: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 17 04:18:19.998: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 17 04:18:19.998: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 17 04:18:19.998: INFO: image-registry-5669fd49dd-xphdf from openshift-image-registry started at 2020-08-17 03:00:49 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:19.998: INFO: 	Container registry ready: true, restart count 0
Aug 17 04:18:19.998: INFO: ibm-keepalived-watcher-ncgwx from kube-system started at 2020-08-17 02:56:14 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:19.998: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 17 04:18:19.998: INFO: node-ca-2lszv from openshift-image-registry started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:19.998: INFO: 	Container node-ca ready: true, restart count 0
Aug 17 04:18:19.998: INFO: packageserver-66768b6f89-b74q9 from openshift-operator-lifecycle-manager started at 2020-08-17 03:05:22 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:19.998: INFO: 	Container packageserver ready: true, restart count 0
Aug 17 04:18:19.998: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-08-17 03:05:11 +0000 UTC (3 container statuses recorded)
Aug 17 04:18:19.998: INFO: 	Container alertmanager ready: true, restart count 0
Aug 17 04:18:19.998: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 17 04:18:19.998: INFO: 	Container config-reloader ready: true, restart count 0
Aug 17 04:18:19.998: INFO: ibm-master-proxy-static-10.241.148.31 from kube-system started at 2020-08-17 02:56:11 +0000 UTC (2 container statuses recorded)
Aug 17 04:18:19.998: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 17 04:18:19.998: INFO: 	Container pause ready: true, restart count 0
Aug 17 04:18:19.998: INFO: node-exporter-kkwj6 from openshift-monitoring started at 2020-08-17 02:58:34 +0000 UTC (2 container statuses recorded)
Aug 17 04:18:19.998: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:18:19.998: INFO: 	Container node-exporter ready: true, restart count 0
Aug 17 04:18:19.998: INFO: dns-default-sxjmd from openshift-dns started at 2020-08-17 02:59:01 +0000 UTC (2 container statuses recorded)
Aug 17 04:18:19.998: INFO: 	Container dns ready: true, restart count 0
Aug 17 04:18:19.998: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 17 04:18:19.998: INFO: prometheus-operator-56d9d699cb-m5pmc from openshift-monitoring started at 2020-08-17 03:04:39 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:19.998: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 17 04:18:19.998: INFO: openshift-kube-proxy-5nh74 from openshift-kube-proxy started at 2020-08-17 02:56:45 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:19.998: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 17 04:18:19.998: INFO: prometheus-adapter-67dbcc5cfc-xk5l6 from openshift-monitoring started at 2020-08-17 02:58:37 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:19.998: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 17 04:18:19.998: INFO: community-operators-65d9d5c9d6-zh8cl from openshift-marketplace started at 2020-08-17 02:59:24 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:19.998: INFO: 	Container community-operators ready: true, restart count 0
Aug 17 04:18:19.998: INFO: 
Logging pods the kubelet thinks is on node 10.241.148.42 before test
Aug 17 04:18:20.862: INFO: ibmcloud-block-storage-driver-5dth8 from kube-system started at 2020-08-17 02:56:14 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:20.862: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 17 04:18:20.862: INFO: openshift-kube-proxy-hswpd from openshift-kube-proxy started at 2020-08-17 02:56:45 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:20.862: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 17 04:18:20.862: INFO: telemeter-client-76746b8bb7-hh5jv from openshift-monitoring started at 2020-08-17 03:04:49 +0000 UTC (3 container statuses recorded)
Aug 17 04:18:20.862: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:18:20.862: INFO: 	Container reload ready: true, restart count 0
Aug 17 04:18:20.862: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 17 04:18:20.862: INFO: console-76d8dc5894-xnhrt from openshift-console started at 2020-08-17 02:59:20 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:20.862: INFO: 	Container console ready: true, restart count 0
Aug 17 04:18:20.862: INFO: apiservice-cabundle-injector-594fd4555f-j2v5w from openshift-service-ca started at 2020-08-17 02:58:07 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:20.862: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Aug 17 04:18:20.862: INFO: node-exporter-r6g64 from openshift-monitoring started at 2020-08-17 02:58:34 +0000 UTC (2 container statuses recorded)
Aug 17 04:18:20.862: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:18:20.862: INFO: 	Container node-exporter ready: true, restart count 0
Aug 17 04:18:20.862: INFO: router-default-57d78dfb48-8c54l from openshift-ingress started at 2020-08-17 02:59:05 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:20.862: INFO: 	Container router ready: true, restart count 0
Aug 17 04:18:20.862: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-08-17 03:01:11 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:20.862: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Aug 17 04:18:20.862: INFO: ibm-master-proxy-static-10.241.148.42 from kube-system started at 2020-08-17 02:56:02 +0000 UTC (2 container statuses recorded)
Aug 17 04:18:20.862: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 17 04:18:20.862: INFO: 	Container pause ready: true, restart count 0
Aug 17 04:18:20.862: INFO: multus-p85fw from openshift-multus started at 2020-08-17 02:56:38 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:20.862: INFO: 	Container kube-multus ready: true, restart count 0
Aug 17 04:18:20.862: INFO: calico-typha-579947cf56-86wz4 from calico-system started at 2020-08-17 02:57:30 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:20.862: INFO: 	Container calico-typha ready: true, restart count 0
Aug 17 04:18:20.862: INFO: sonobuoy-e2e-job-144f6878906f41e3 from sonobuoy started at 2020-08-17 04:12:38 +0000 UTC (2 container statuses recorded)
Aug 17 04:18:20.862: INFO: 	Container e2e ready: true, restart count 0
Aug 17 04:18:20.862: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 04:18:20.862: INFO: sonobuoy-systemd-logs-daemon-set-c8f140ca93fa466f-qhfmq from sonobuoy started at 2020-08-17 04:12:38 +0000 UTC (2 container statuses recorded)
Aug 17 04:18:20.862: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 04:18:20.862: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 04:18:20.862: INFO: openshift-state-metrics-5849d797d8-h6gdr from openshift-monitoring started at 2020-08-17 02:58:31 +0000 UTC (3 container statuses recorded)
Aug 17 04:18:20.862: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 17 04:18:20.862: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 17 04:18:20.862: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 17 04:18:20.862: INFO: sonobuoy from sonobuoy started at 2020-08-17 04:12:31 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:20.862: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 17 04:18:20.862: INFO: grafana-c9c7455d7-phgf6 from openshift-monitoring started at 2020-08-17 03:04:57 +0000 UTC (2 container statuses recorded)
Aug 17 04:18:20.862: INFO: 	Container grafana ready: true, restart count 0
Aug 17 04:18:20.862: INFO: 	Container grafana-proxy ready: true, restart count 0
Aug 17 04:18:20.862: INFO: node-ca-j567q from openshift-image-registry started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:20.862: INFO: 	Container node-ca ready: true, restart count 0
Aug 17 04:18:20.862: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-08-17 03:05:18 +0000 UTC (3 container statuses recorded)
Aug 17 04:18:20.863: INFO: 	Container alertmanager ready: true, restart count 0
Aug 17 04:18:20.863: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 17 04:18:20.863: INFO: 	Container config-reloader ready: true, restart count 0
Aug 17 04:18:20.863: INFO: ibm-keepalived-watcher-xf2gw from kube-system started at 2020-08-17 02:56:05 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:20.863: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 17 04:18:20.863: INFO: multus-admission-controller-bskrp from openshift-multus started at 2020-08-17 02:57:45 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:20.863: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 17 04:18:20.863: INFO: prometheus-adapter-67dbcc5cfc-5kt76 from openshift-monitoring started at 2020-08-17 02:58:37 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:20.863: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 17 04:18:20.863: INFO: certified-operators-549fcd7d77-lx6ml from openshift-marketplace started at 2020-08-17 02:59:24 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:20.863: INFO: 	Container certified-operators ready: true, restart count 0
Aug 17 04:18:20.863: INFO: tigera-operator-679798d94d-qrlds from tigera-operator started at 2020-08-17 02:56:13 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:20.863: INFO: 	Container tigera-operator ready: true, restart count 2
Aug 17 04:18:20.863: INFO: registry-pvc-permissions-j8h5n from openshift-image-registry started at 2020-08-17 03:00:49 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:20.863: INFO: 	Container pvc-permissions ready: false, restart count 0
Aug 17 04:18:20.863: INFO: calico-node-cxl67 from calico-system started at 2020-08-17 02:57:31 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:20.863: INFO: 	Container calico-node ready: true, restart count 0
Aug 17 04:18:20.863: INFO: tuned-b665k from openshift-cluster-node-tuning-operator started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:20.863: INFO: 	Container tuned ready: true, restart count 0
Aug 17 04:18:20.863: INFO: dns-default-kqx2s from openshift-dns started at 2020-08-17 02:59:01 +0000 UTC (2 container statuses recorded)
Aug 17 04:18:20.863: INFO: 	Container dns ready: true, restart count 0
Aug 17 04:18:20.863: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 17 04:18:20.863: INFO: network-operator-7986644c85-9jpc4 from openshift-network-operator started at 2020-08-17 02:56:13 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:20.863: INFO: 	Container network-operator ready: true, restart count 0
Aug 17 04:18:20.863: INFO: 
Logging pods the kubelet thinks is on node 10.241.148.50 before test
Aug 17 04:18:21.905: INFO: ibm-keepalived-watcher-x5mxk from kube-system started at 2020-08-17 02:56:22 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 17 04:18:21.905: INFO: cluster-storage-operator-557b75f8d5-899qb from openshift-cluster-storage-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Aug 17 04:18:21.905: INFO: ibm-master-proxy-static-10.241.148.50 from kube-system started at 2020-08-17 02:56:19 +0000 UTC (2 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 17 04:18:21.905: INFO: 	Container pause ready: true, restart count 0
Aug 17 04:18:21.905: INFO: dns-default-kfbzc from openshift-dns started at 2020-08-17 02:59:02 +0000 UTC (2 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container dns ready: true, restart count 0
Aug 17 04:18:21.905: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 17 04:18:21.905: INFO: cluster-node-tuning-operator-b5f884945-nmndb from openshift-cluster-node-tuning-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Aug 17 04:18:21.905: INFO: ibm-file-plugin-fdb69b446-bzpn7 from kube-system started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 17 04:18:21.905: INFO: openshift-service-catalog-controller-manager-operator-5496wjhfr from openshift-service-catalog-controller-manager-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container operator ready: true, restart count 1
Aug 17 04:18:21.905: INFO: marketplace-operator-6957767d58-6rcnt from openshift-marketplace started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container marketplace-operator ready: true, restart count 0
Aug 17 04:18:21.905: INFO: node-ca-467qn from openshift-image-registry started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container node-ca ready: true, restart count 0
Aug 17 04:18:21.905: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-08-17 03:05:34 +0000 UTC (7 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:18:21.905: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 17 04:18:21.905: INFO: 	Container prometheus ready: true, restart count 1
Aug 17 04:18:21.905: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 17 04:18:21.905: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 17 04:18:21.905: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 17 04:18:21.905: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 17 04:18:21.905: INFO: multus-wkz57 from openshift-multus started at 2020-08-17 02:56:38 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container kube-multus ready: true, restart count 0
Aug 17 04:18:21.905: INFO: openshift-service-catalog-apiserver-operator-78b9dddb6f-bbw72 from openshift-service-catalog-apiserver-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container operator ready: true, restart count 1
Aug 17 04:18:21.905: INFO: ingress-operator-695bc545b9-d4trd from openshift-ingress-operator started at 2020-08-17 02:57:42 +0000 UTC (2 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container ingress-operator ready: true, restart count 0
Aug 17 04:18:21.905: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:18:21.905: INFO: downloads-678f5d6564-wxxs4 from openshift-console started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container download-server ready: true, restart count 0
Aug 17 04:18:21.905: INFO: dns-operator-6f9cf66db7-ksmws from openshift-dns-operator started at 2020-08-17 02:57:44 +0000 UTC (2 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container dns-operator ready: true, restart count 0
Aug 17 04:18:21.905: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:18:21.905: INFO: redhat-operators-85f96bd7c-69xgq from openshift-marketplace started at 2020-08-17 02:59:24 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container redhat-operators ready: true, restart count 0
Aug 17 04:18:21.905: INFO: packageserver-66768b6f89-hjpwd from openshift-operator-lifecycle-manager started at 2020-08-17 03:05:13 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container packageserver ready: true, restart count 0
Aug 17 04:18:21.905: INFO: thanos-querier-6b5fbd9754-wzzrz from openshift-monitoring started at 2020-08-17 03:05:26 +0000 UTC (4 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:18:21.905: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 17 04:18:21.905: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 17 04:18:21.905: INFO: 	Container thanos-querier ready: true, restart count 0
Aug 17 04:18:21.905: INFO: console-operator-9878d4766-7tcvg from openshift-console-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container console-operator ready: true, restart count 1
Aug 17 04:18:21.905: INFO: downloads-678f5d6564-t5r44 from openshift-console started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container download-server ready: true, restart count 0
Aug 17 04:18:21.905: INFO: node-exporter-ztsvb from openshift-monitoring started at 2020-08-17 02:58:33 +0000 UTC (2 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:18:21.905: INFO: 	Container node-exporter ready: true, restart count 0
Aug 17 04:18:21.905: INFO: calico-typha-579947cf56-87rgj from calico-system started at 2020-08-17 02:59:26 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container calico-typha ready: true, restart count 0
Aug 17 04:18:21.905: INFO: sonobuoy-systemd-logs-daemon-set-c8f140ca93fa466f-j84lz from sonobuoy started at 2020-08-17 04:12:38 +0000 UTC (2 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 04:18:21.905: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 04:18:21.905: INFO: vpn-7b59d9f64-2rnfd from kube-system started at 2020-08-17 03:05:11 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container vpn ready: true, restart count 0
Aug 17 04:18:21.905: INFO: ibm-storage-watcher-77bf8b889-856t2 from kube-system started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 17 04:18:21.905: INFO: multus-admission-controller-7xrg7 from openshift-multus started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 17 04:18:21.905: INFO: tuned-6qthk from openshift-cluster-node-tuning-operator started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container tuned ready: true, restart count 0
Aug 17 04:18:21.905: INFO: catalog-operator-85f6c659cc-6zpxp from openshift-operator-lifecycle-manager started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 17 04:18:21.905: INFO: service-ca-operator-694cfbf5d5-2dhf8 from openshift-service-ca-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container operator ready: true, restart count 0
Aug 17 04:18:21.905: INFO: olm-operator-b5f57cdbb-wcj4n from openshift-operator-lifecycle-manager started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container olm-operator ready: true, restart count 0
Aug 17 04:18:21.905: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-08-17 03:04:54 +0000 UTC (3 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container alertmanager ready: true, restart count 0
Aug 17 04:18:21.905: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 17 04:18:21.905: INFO: 	Container config-reloader ready: true, restart count 0
Aug 17 04:18:21.905: INFO: ibmcloud-block-storage-driver-6l6kb from kube-system started at 2020-08-17 02:56:30 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 17 04:18:21.905: INFO: calico-node-hmdtm from calico-system started at 2020-08-17 02:57:31 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container calico-node ready: true, restart count 0
Aug 17 04:18:21.905: INFO: cluster-monitoring-operator-5b5659466f-bd9vb from openshift-monitoring started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Aug 17 04:18:21.905: INFO: cluster-image-registry-operator-6cfd58b66c-xpvcp from openshift-image-registry started at 2020-08-17 02:57:42 +0000 UTC (2 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Aug 17 04:18:21.905: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Aug 17 04:18:21.905: INFO: calico-kube-controllers-79d75767dd-hhcw2 from calico-system started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 17 04:18:21.905: INFO: ibmcloud-block-storage-plugin-68d5c65db9-m8qcr from kube-system started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Aug 17 04:18:21.905: INFO: ibm-cloud-provider-ip-169-48-239-26-5595944bf9-2pxq9 from ibm-system started at 2020-08-17 03:02:20 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container ibm-cloud-provider-ip-169-48-239-26 ready: true, restart count 0
Aug 17 04:18:21.905: INFO: openshift-kube-proxy-grn9g from openshift-kube-proxy started at 2020-08-17 02:56:45 +0000 UTC (1 container statuses recorded)
Aug 17 04:18:21.905: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-02be646f-698e-455b-a53d-8fbe2b081ed3 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-02be646f-698e-455b-a53d-8fbe2b081ed3 off the node 10.241.148.42
STEP: verifying the node doesn't have the label kubernetes.io/e2e-02be646f-698e-455b-a53d-8fbe2b081ed3
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:23:30.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9795" for this suite.
Aug 17 04:23:47.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:23:56.156: INFO: namespace sched-pred-9795 deletion completed in 25.532134053s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:338.836 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:23:56.157: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 04:23:56.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 version'
Aug 17 04:23:56.775: INFO: stderr: ""
Aug 17 04:23:56.775: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.2\", GitCommit:\"c97fe5036ef3df2967d086711e6c0c405941e14b\", GitTreeState:\"clean\", BuildDate:\"2019-10-15T19:18:23Z\", GoVersion:\"go1.12.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16+\", GitVersion:\"v1.16.2+554af56\", GitCommit:\"554af56\", GitTreeState:\"clean\", BuildDate:\"2020-07-27T23:14:32Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:23:56.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5439" for this suite.
Aug 17 04:24:07.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:24:17.298: INFO: namespace kubectl-5439 deletion completed in 20.278273365s

• [SLOW TEST:21.141 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:24:17.299: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 17 04:24:17.874: INFO: Waiting up to 5m0s for pod "pod-0b838fc1-76e3-4b1f-8cf0-e9e1492e2f51" in namespace "emptydir-5161" to be "success or failure"
Aug 17 04:24:17.975: INFO: Pod "pod-0b838fc1-76e3-4b1f-8cf0-e9e1492e2f51": Phase="Pending", Reason="", readiness=false. Elapsed: 100.120777ms
Aug 17 04:24:20.094: INFO: Pod "pod-0b838fc1-76e3-4b1f-8cf0-e9e1492e2f51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.21972239s
Aug 17 04:24:22.269: INFO: Pod "pod-0b838fc1-76e3-4b1f-8cf0-e9e1492e2f51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.394309698s
STEP: Saw pod success
Aug 17 04:24:22.269: INFO: Pod "pod-0b838fc1-76e3-4b1f-8cf0-e9e1492e2f51" satisfied condition "success or failure"
Aug 17 04:24:22.444: INFO: Trying to get logs from node 10.241.148.42 pod pod-0b838fc1-76e3-4b1f-8cf0-e9e1492e2f51 container test-container: <nil>
STEP: delete the pod
Aug 17 04:24:22.691: INFO: Waiting for pod pod-0b838fc1-76e3-4b1f-8cf0-e9e1492e2f51 to disappear
Aug 17 04:24:22.756: INFO: Pod pod-0b838fc1-76e3-4b1f-8cf0-e9e1492e2f51 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:24:22.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5161" for this suite.
Aug 17 04:24:31.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:24:39.535: INFO: namespace emptydir-5161 deletion completed in 16.658648728s

• [SLOW TEST:22.236 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:24:39.535: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 04:24:40.230: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-ae314f9c-c20d-490d-8586-094460ab8049
STEP: Creating secret with name s-test-opt-upd-3f577400-9ff5-401c-9a5d-668dcaa02693
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ae314f9c-c20d-490d-8586-094460ab8049
STEP: Updating secret s-test-opt-upd-3f577400-9ff5-401c-9a5d-668dcaa02693
STEP: Creating secret with name s-test-opt-create-5073dae2-07be-442d-927a-cce74b554bcc
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:26:18.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7901" for this suite.
Aug 17 04:26:37.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:26:46.379: INFO: namespace secrets-7901 deletion completed in 27.725249562s

• [SLOW TEST:126.843 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:26:46.379: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 04:26:46.863: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 17 04:26:56.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 --namespace=crd-publish-openapi-8251 create -f -'
Aug 17 04:26:57.690: INFO: stderr: ""
Aug 17 04:26:57.690: INFO: stdout: "e2e-test-crd-publish-openapi-3164-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 17 04:26:57.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 --namespace=crd-publish-openapi-8251 delete e2e-test-crd-publish-openapi-3164-crds test-cr'
Aug 17 04:26:58.492: INFO: stderr: ""
Aug 17 04:26:58.492: INFO: stdout: "e2e-test-crd-publish-openapi-3164-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Aug 17 04:26:58.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 --namespace=crd-publish-openapi-8251 apply -f -'
Aug 17 04:26:59.078: INFO: stderr: ""
Aug 17 04:26:59.078: INFO: stdout: "e2e-test-crd-publish-openapi-3164-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 17 04:26:59.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 --namespace=crd-publish-openapi-8251 delete e2e-test-crd-publish-openapi-3164-crds test-cr'
Aug 17 04:26:59.244: INFO: stderr: ""
Aug 17 04:26:59.244: INFO: stdout: "e2e-test-crd-publish-openapi-3164-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Aug 17 04:26:59.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 explain e2e-test-crd-publish-openapi-3164-crds'
Aug 17 04:26:59.932: INFO: stderr: ""
Aug 17 04:26:59.932: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3164-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:27:09.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8251" for this suite.
Aug 17 04:27:17.357: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:27:21.919: INFO: namespace crd-publish-openapi-8251 deletion completed in 12.759251296s

• [SLOW TEST:35.540 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:27:21.919: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 04:27:22.327: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Aug 17 04:27:22.403: INFO: Number of nodes with available pods: 0
Aug 17 04:27:22.403: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 04:27:23.482: INFO: Number of nodes with available pods: 0
Aug 17 04:27:23.482: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 04:27:24.449: INFO: Number of nodes with available pods: 0
Aug 17 04:27:24.449: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 04:27:25.464: INFO: Number of nodes with available pods: 0
Aug 17 04:27:25.464: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 04:27:26.465: INFO: Number of nodes with available pods: 0
Aug 17 04:27:26.466: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 04:27:27.443: INFO: Number of nodes with available pods: 0
Aug 17 04:27:27.443: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 04:27:28.452: INFO: Number of nodes with available pods: 0
Aug 17 04:27:28.452: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 04:27:29.449: INFO: Number of nodes with available pods: 0
Aug 17 04:27:29.449: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 04:27:30.468: INFO: Number of nodes with available pods: 0
Aug 17 04:27:30.468: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 04:27:31.449: INFO: Number of nodes with available pods: 0
Aug 17 04:27:31.449: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 04:27:32.472: INFO: Number of nodes with available pods: 1
Aug 17 04:27:32.472: INFO: Node 10.241.148.42 is running more than one daemon pod
Aug 17 04:27:33.562: INFO: Number of nodes with available pods: 3
Aug 17 04:27:33.562: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Aug 17 04:27:33.716: INFO: Wrong image for pod: daemon-set-fpztl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:33.716: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:33.716: INFO: Wrong image for pod: daemon-set-xnfqt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:34.763: INFO: Wrong image for pod: daemon-set-fpztl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:34.763: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:34.763: INFO: Wrong image for pod: daemon-set-xnfqt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:35.804: INFO: Wrong image for pod: daemon-set-fpztl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:35.804: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:35.804: INFO: Wrong image for pod: daemon-set-xnfqt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:36.762: INFO: Wrong image for pod: daemon-set-fpztl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:36.762: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:36.762: INFO: Wrong image for pod: daemon-set-xnfqt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:37.778: INFO: Wrong image for pod: daemon-set-fpztl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:37.778: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:37.778: INFO: Wrong image for pod: daemon-set-xnfqt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:37.778: INFO: Pod daemon-set-xnfqt is not available
Aug 17 04:27:38.773: INFO: Wrong image for pod: daemon-set-fpztl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:38.773: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:39.767: INFO: Wrong image for pod: daemon-set-fpztl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:39.767: INFO: Pod daemon-set-sf5kc is not available
Aug 17 04:27:39.767: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:40.765: INFO: Wrong image for pod: daemon-set-fpztl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:40.766: INFO: Pod daemon-set-sf5kc is not available
Aug 17 04:27:40.766: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:41.768: INFO: Wrong image for pod: daemon-set-fpztl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:41.768: INFO: Pod daemon-set-sf5kc is not available
Aug 17 04:27:41.768: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:42.763: INFO: Wrong image for pod: daemon-set-fpztl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:42.763: INFO: Pod daemon-set-sf5kc is not available
Aug 17 04:27:42.763: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:43.761: INFO: Wrong image for pod: daemon-set-fpztl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:43.761: INFO: Pod daemon-set-sf5kc is not available
Aug 17 04:27:43.761: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:44.775: INFO: Wrong image for pod: daemon-set-fpztl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:44.775: INFO: Pod daemon-set-sf5kc is not available
Aug 17 04:27:44.775: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:45.763: INFO: Wrong image for pod: daemon-set-fpztl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:45.763: INFO: Pod daemon-set-sf5kc is not available
Aug 17 04:27:45.763: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:46.819: INFO: Wrong image for pod: daemon-set-fpztl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:46.819: INFO: Pod daemon-set-sf5kc is not available
Aug 17 04:27:46.819: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:47.812: INFO: Wrong image for pod: daemon-set-fpztl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:47.812: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:48.816: INFO: Wrong image for pod: daemon-set-fpztl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:48.816: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:49.812: INFO: Wrong image for pod: daemon-set-fpztl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:49.812: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:50.797: INFO: Wrong image for pod: daemon-set-fpztl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:50.797: INFO: Pod daemon-set-fpztl is not available
Aug 17 04:27:50.797: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:51.763: INFO: Wrong image for pod: daemon-set-fpztl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:51.763: INFO: Pod daemon-set-fpztl is not available
Aug 17 04:27:51.763: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:52.766: INFO: Wrong image for pod: daemon-set-fpztl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:52.766: INFO: Pod daemon-set-fpztl is not available
Aug 17 04:27:52.766: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:53.762: INFO: Wrong image for pod: daemon-set-fpztl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:53.762: INFO: Pod daemon-set-fpztl is not available
Aug 17 04:27:53.762: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:54.768: INFO: Wrong image for pod: daemon-set-fpztl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:54.768: INFO: Pod daemon-set-fpztl is not available
Aug 17 04:27:54.768: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:55.781: INFO: Wrong image for pod: daemon-set-fpztl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:55.781: INFO: Pod daemon-set-fpztl is not available
Aug 17 04:27:55.781: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:56.765: INFO: Wrong image for pod: daemon-set-fpztl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:56.765: INFO: Pod daemon-set-fpztl is not available
Aug 17 04:27:56.765: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:57.775: INFO: Wrong image for pod: daemon-set-fpztl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:57.775: INFO: Pod daemon-set-fpztl is not available
Aug 17 04:27:57.775: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:58.764: INFO: Wrong image for pod: daemon-set-fpztl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:58.764: INFO: Pod daemon-set-fpztl is not available
Aug 17 04:27:58.764: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:27:59.776: INFO: Pod daemon-set-h9mdb is not available
Aug 17 04:27:59.776: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:28:00.774: INFO: Pod daemon-set-h9mdb is not available
Aug 17 04:28:00.774: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:28:01.764: INFO: Pod daemon-set-h9mdb is not available
Aug 17 04:28:01.764: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:28:02.766: INFO: Pod daemon-set-h9mdb is not available
Aug 17 04:28:02.767: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:28:03.781: INFO: Pod daemon-set-h9mdb is not available
Aug 17 04:28:03.781: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:28:04.762: INFO: Pod daemon-set-h9mdb is not available
Aug 17 04:28:04.762: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:28:05.764: INFO: Pod daemon-set-h9mdb is not available
Aug 17 04:28:05.766: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:28:06.767: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:28:07.765: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:28:08.764: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:28:08.764: INFO: Pod daemon-set-vm8th is not available
Aug 17 04:28:09.765: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:28:09.765: INFO: Pod daemon-set-vm8th is not available
Aug 17 04:28:10.767: INFO: Wrong image for pod: daemon-set-vm8th. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Aug 17 04:28:10.767: INFO: Pod daemon-set-vm8th is not available
Aug 17 04:28:11.764: INFO: Pod daemon-set-6zvgh is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Aug 17 04:28:11.834: INFO: Number of nodes with available pods: 2
Aug 17 04:28:11.834: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 04:28:12.896: INFO: Number of nodes with available pods: 2
Aug 17 04:28:12.896: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 04:28:13.890: INFO: Number of nodes with available pods: 2
Aug 17 04:28:13.890: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 04:28:14.875: INFO: Number of nodes with available pods: 2
Aug 17 04:28:14.875: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 04:28:15.891: INFO: Number of nodes with available pods: 2
Aug 17 04:28:15.892: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 04:28:16.981: INFO: Number of nodes with available pods: 3
Aug 17 04:28:16.981: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8290, will wait for the garbage collector to delete the pods
Aug 17 04:28:17.455: INFO: Deleting DaemonSet.extensions daemon-set took: 95.154637ms
Aug 17 04:28:18.155: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.320948ms
Aug 17 04:28:32.375: INFO: Number of nodes with available pods: 0
Aug 17 04:28:32.375: INFO: Number of running nodes: 0, number of available pods: 0
Aug 17 04:28:32.396: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8290/daemonsets","resourceVersion":"41373"},"items":null}

Aug 17 04:28:32.412: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8290/pods","resourceVersion":"41373"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:28:32.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8290" for this suite.
Aug 17 04:28:42.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:28:45.086: INFO: namespace daemonsets-8290 deletion completed in 12.576306582s

• [SLOW TEST:83.167 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:28:45.087: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-kbrz
STEP: Creating a pod to test atomic-volume-subpath
Aug 17 04:28:45.472: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-kbrz" in namespace "subpath-665" to be "success or failure"
Aug 17 04:28:45.493: INFO: Pod "pod-subpath-test-configmap-kbrz": Phase="Pending", Reason="", readiness=false. Elapsed: 21.244019ms
Aug 17 04:28:47.590: INFO: Pod "pod-subpath-test-configmap-kbrz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.11820023s
Aug 17 04:28:49.669: INFO: Pod "pod-subpath-test-configmap-kbrz": Phase="Running", Reason="", readiness=true. Elapsed: 4.197809464s
Aug 17 04:28:51.723: INFO: Pod "pod-subpath-test-configmap-kbrz": Phase="Running", Reason="", readiness=true. Elapsed: 6.251762985s
Aug 17 04:28:53.740: INFO: Pod "pod-subpath-test-configmap-kbrz": Phase="Running", Reason="", readiness=true. Elapsed: 8.268849029s
Aug 17 04:28:55.762: INFO: Pod "pod-subpath-test-configmap-kbrz": Phase="Running", Reason="", readiness=true. Elapsed: 10.290223031s
Aug 17 04:28:57.783: INFO: Pod "pod-subpath-test-configmap-kbrz": Phase="Running", Reason="", readiness=true. Elapsed: 12.311197035s
Aug 17 04:28:59.809: INFO: Pod "pod-subpath-test-configmap-kbrz": Phase="Running", Reason="", readiness=true. Elapsed: 14.337132334s
Aug 17 04:29:01.833: INFO: Pod "pod-subpath-test-configmap-kbrz": Phase="Running", Reason="", readiness=true. Elapsed: 16.36125819s
Aug 17 04:29:03.852: INFO: Pod "pod-subpath-test-configmap-kbrz": Phase="Running", Reason="", readiness=true. Elapsed: 18.380762337s
Aug 17 04:29:05.870: INFO: Pod "pod-subpath-test-configmap-kbrz": Phase="Running", Reason="", readiness=true. Elapsed: 20.398316662s
Aug 17 04:29:07.892: INFO: Pod "pod-subpath-test-configmap-kbrz": Phase="Running", Reason="", readiness=true. Elapsed: 22.420336328s
Aug 17 04:29:09.910: INFO: Pod "pod-subpath-test-configmap-kbrz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.43846064s
STEP: Saw pod success
Aug 17 04:29:09.910: INFO: Pod "pod-subpath-test-configmap-kbrz" satisfied condition "success or failure"
Aug 17 04:29:09.928: INFO: Trying to get logs from node 10.241.148.42 pod pod-subpath-test-configmap-kbrz container test-container-subpath-configmap-kbrz: <nil>
STEP: delete the pod
Aug 17 04:29:10.047: INFO: Waiting for pod pod-subpath-test-configmap-kbrz to disappear
Aug 17 04:29:10.076: INFO: Pod pod-subpath-test-configmap-kbrz no longer exists
STEP: Deleting pod pod-subpath-test-configmap-kbrz
Aug 17 04:29:10.077: INFO: Deleting pod "pod-subpath-test-configmap-kbrz" in namespace "subpath-665"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:29:10.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-665" for this suite.
Aug 17 04:29:20.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:29:22.832: INFO: namespace subpath-665 deletion completed in 12.667573013s

• [SLOW TEST:37.745 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:29:22.832: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 17 04:29:23.120: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6f92dad5-6339-4c4a-bac5-572c5b50559e" in namespace "downward-api-9104" to be "success or failure"
Aug 17 04:29:23.165: INFO: Pod "downwardapi-volume-6f92dad5-6339-4c4a-bac5-572c5b50559e": Phase="Pending", Reason="", readiness=false. Elapsed: 44.376652ms
Aug 17 04:29:25.184: INFO: Pod "downwardapi-volume-6f92dad5-6339-4c4a-bac5-572c5b50559e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.064235216s
STEP: Saw pod success
Aug 17 04:29:25.185: INFO: Pod "downwardapi-volume-6f92dad5-6339-4c4a-bac5-572c5b50559e" satisfied condition "success or failure"
Aug 17 04:29:25.207: INFO: Trying to get logs from node 10.241.148.42 pod downwardapi-volume-6f92dad5-6339-4c4a-bac5-572c5b50559e container client-container: <nil>
STEP: delete the pod
Aug 17 04:29:25.319: INFO: Waiting for pod downwardapi-volume-6f92dad5-6339-4c4a-bac5-572c5b50559e to disappear
Aug 17 04:29:25.335: INFO: Pod downwardapi-volume-6f92dad5-6339-4c4a-bac5-572c5b50559e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:29:25.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9104" for this suite.
Aug 17 04:29:35.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:29:38.308: INFO: namespace downward-api-9104 deletion completed in 12.918188119s

• [SLOW TEST:15.476 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:29:38.309: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 17 04:29:41.757: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:29:41.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9941" for this suite.
Aug 17 04:29:51.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:29:54.657: INFO: namespace container-runtime-9941 deletion completed in 12.769205395s

• [SLOW TEST:16.348 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:29:54.658: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:30:06.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2922" for this suite.
Aug 17 04:30:14.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:30:18.404: INFO: namespace resourcequota-2922 deletion completed in 12.133586355s

• [SLOW TEST:23.746 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:30:18.404: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Aug 17 04:30:19.120: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7502 /api/v1/namespaces/watch-7502/configmaps/e2e-watch-test-watch-closed 5a351313-e028-4a92-b7a3-e42bebadf83c 42181 0 2020-08-17 04:30:18 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 17 04:30:19.222: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7502 /api/v1/namespaces/watch-7502/configmaps/e2e-watch-test-watch-closed 5a351313-e028-4a92-b7a3-e42bebadf83c 42183 0 2020-08-17 04:30:18 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Aug 17 04:30:20.011: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7502 /api/v1/namespaces/watch-7502/configmaps/e2e-watch-test-watch-closed 5a351313-e028-4a92-b7a3-e42bebadf83c 42186 0 2020-08-17 04:30:18 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 17 04:30:20.012: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7502 /api/v1/namespaces/watch-7502/configmaps/e2e-watch-test-watch-closed 5a351313-e028-4a92-b7a3-e42bebadf83c 42190 0 2020-08-17 04:30:18 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:30:20.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7502" for this suite.
Aug 17 04:30:30.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:30:33.164: INFO: namespace watch-7502 deletion completed in 12.703755906s

• [SLOW TEST:14.760 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:30:33.165: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 17 04:30:33.510: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e191d233-a401-4187-b5bd-ab093e2787d0" in namespace "projected-5609" to be "success or failure"
Aug 17 04:30:33.563: INFO: Pod "downwardapi-volume-e191d233-a401-4187-b5bd-ab093e2787d0": Phase="Pending", Reason="", readiness=false. Elapsed: 52.831518ms
Aug 17 04:30:35.581: INFO: Pod "downwardapi-volume-e191d233-a401-4187-b5bd-ab093e2787d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.07102174s
STEP: Saw pod success
Aug 17 04:30:35.581: INFO: Pod "downwardapi-volume-e191d233-a401-4187-b5bd-ab093e2787d0" satisfied condition "success or failure"
Aug 17 04:30:35.599: INFO: Trying to get logs from node 10.241.148.42 pod downwardapi-volume-e191d233-a401-4187-b5bd-ab093e2787d0 container client-container: <nil>
STEP: delete the pod
Aug 17 04:30:35.731: INFO: Waiting for pod downwardapi-volume-e191d233-a401-4187-b5bd-ab093e2787d0 to disappear
Aug 17 04:30:35.747: INFO: Pod downwardapi-volume-e191d233-a401-4187-b5bd-ab093e2787d0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:30:35.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5609" for this suite.
Aug 17 04:30:43.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:30:46.588: INFO: namespace projected-5609 deletion completed in 10.780559507s

• [SLOW TEST:13.423 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:30:46.589: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:30:49.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4490" for this suite.
Aug 17 04:31:32.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:31:35.056: INFO: namespace kubelet-test-4490 deletion completed in 44.983612823s

• [SLOW TEST:48.467 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:31:35.056: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:32:12.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-690" for this suite.
Aug 17 04:32:21.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:32:23.804: INFO: namespace namespaces-690 deletion completed in 10.748761005s
STEP: Destroying namespace "nsdeletetest-8265" for this suite.
Aug 17 04:32:23.820: INFO: Namespace nsdeletetest-8265 was already deleted
STEP: Destroying namespace "nsdeletetest-9143" for this suite.
Aug 17 04:32:31.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:32:34.489: INFO: namespace nsdeletetest-9143 deletion completed in 10.669192771s

• [SLOW TEST:59.433 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:32:34.489: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Aug 17 04:32:40.970: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5756 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 17 04:32:40.970: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 04:32:41.224: INFO: Exec stderr: ""
Aug 17 04:32:41.224: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5756 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 17 04:32:41.224: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 04:32:42.191: INFO: Exec stderr: ""
Aug 17 04:32:42.191: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5756 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 17 04:32:42.191: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 04:32:42.432: INFO: Exec stderr: ""
Aug 17 04:32:42.432: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5756 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 17 04:32:42.432: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 04:32:42.678: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Aug 17 04:32:42.678: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5756 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 17 04:32:42.678: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 04:32:42.942: INFO: Exec stderr: ""
Aug 17 04:32:42.942: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5756 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 17 04:32:42.942: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 04:32:43.168: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Aug 17 04:32:43.168: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5756 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 17 04:32:43.169: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 04:32:43.404: INFO: Exec stderr: ""
Aug 17 04:32:43.404: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5756 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 17 04:32:43.404: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 04:32:43.745: INFO: Exec stderr: ""
Aug 17 04:32:43.745: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5756 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 17 04:32:43.745: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 04:32:43.963: INFO: Exec stderr: ""
Aug 17 04:32:43.963: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5756 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 17 04:32:43.963: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 04:32:44.207: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:32:44.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5756" for this suite.
Aug 17 04:33:36.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:33:39.006: INFO: namespace e2e-kubelet-etc-hosts-5756 deletion completed in 54.729836295s

• [SLOW TEST:64.517 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:33:39.006: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Aug 17 04:33:39.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 create -f - --namespace=kubectl-7079'
Aug 17 04:33:39.798: INFO: stderr: ""
Aug 17 04:33:39.798: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 17 04:33:39.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7079'
Aug 17 04:33:39.985: INFO: stderr: ""
Aug 17 04:33:39.985: INFO: stdout: "update-demo-nautilus-42xpv update-demo-nautilus-7rcqt "
Aug 17 04:33:39.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods update-demo-nautilus-42xpv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7079'
Aug 17 04:33:40.150: INFO: stderr: ""
Aug 17 04:33:40.150: INFO: stdout: ""
Aug 17 04:33:40.150: INFO: update-demo-nautilus-42xpv is created but not running
Aug 17 04:33:45.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7079'
Aug 17 04:33:45.293: INFO: stderr: ""
Aug 17 04:33:45.293: INFO: stdout: "update-demo-nautilus-42xpv update-demo-nautilus-7rcqt "
Aug 17 04:33:45.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods update-demo-nautilus-42xpv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7079'
Aug 17 04:33:45.454: INFO: stderr: ""
Aug 17 04:33:45.454: INFO: stdout: "true"
Aug 17 04:33:45.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods update-demo-nautilus-42xpv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7079'
Aug 17 04:33:45.629: INFO: stderr: ""
Aug 17 04:33:45.629: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 17 04:33:45.629: INFO: validating pod update-demo-nautilus-42xpv
Aug 17 04:33:45.665: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 17 04:33:45.665: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 17 04:33:45.665: INFO: update-demo-nautilus-42xpv is verified up and running
Aug 17 04:33:45.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods update-demo-nautilus-7rcqt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7079'
Aug 17 04:33:45.806: INFO: stderr: ""
Aug 17 04:33:45.806: INFO: stdout: "true"
Aug 17 04:33:45.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods update-demo-nautilus-7rcqt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7079'
Aug 17 04:33:45.955: INFO: stderr: ""
Aug 17 04:33:45.955: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 17 04:33:45.955: INFO: validating pod update-demo-nautilus-7rcqt
Aug 17 04:33:45.996: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 17 04:33:45.996: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 17 04:33:45.996: INFO: update-demo-nautilus-7rcqt is verified up and running
STEP: scaling down the replication controller
Aug 17 04:33:46.001: INFO: scanned /root for discovery docs: <nil>
Aug 17 04:33:46.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-7079'
Aug 17 04:33:47.422: INFO: stderr: ""
Aug 17 04:33:47.422: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 17 04:33:47.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7079'
Aug 17 04:33:47.571: INFO: stderr: ""
Aug 17 04:33:47.571: INFO: stdout: "update-demo-nautilus-42xpv update-demo-nautilus-7rcqt "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug 17 04:33:52.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7079'
Aug 17 04:33:52.742: INFO: stderr: ""
Aug 17 04:33:52.742: INFO: stdout: "update-demo-nautilus-42xpv "
Aug 17 04:33:52.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods update-demo-nautilus-42xpv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7079'
Aug 17 04:33:52.899: INFO: stderr: ""
Aug 17 04:33:52.899: INFO: stdout: "true"
Aug 17 04:33:52.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods update-demo-nautilus-42xpv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7079'
Aug 17 04:33:53.072: INFO: stderr: ""
Aug 17 04:33:53.072: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 17 04:33:53.072: INFO: validating pod update-demo-nautilus-42xpv
Aug 17 04:33:53.111: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 17 04:33:53.111: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 17 04:33:53.111: INFO: update-demo-nautilus-42xpv is verified up and running
STEP: scaling up the replication controller
Aug 17 04:33:53.115: INFO: scanned /root for discovery docs: <nil>
Aug 17 04:33:53.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-7079'
Aug 17 04:33:54.353: INFO: stderr: ""
Aug 17 04:33:54.353: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 17 04:33:54.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7079'
Aug 17 04:33:54.508: INFO: stderr: ""
Aug 17 04:33:54.508: INFO: stdout: "update-demo-nautilus-42xpv update-demo-nautilus-6lbhs "
Aug 17 04:33:54.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods update-demo-nautilus-42xpv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7079'
Aug 17 04:33:54.662: INFO: stderr: ""
Aug 17 04:33:54.662: INFO: stdout: "true"
Aug 17 04:33:54.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods update-demo-nautilus-42xpv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7079'
Aug 17 04:33:54.803: INFO: stderr: ""
Aug 17 04:33:54.803: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 17 04:33:54.803: INFO: validating pod update-demo-nautilus-42xpv
Aug 17 04:33:54.842: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 17 04:33:54.842: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 17 04:33:54.842: INFO: update-demo-nautilus-42xpv is verified up and running
Aug 17 04:33:54.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods update-demo-nautilus-6lbhs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7079'
Aug 17 04:33:55.012: INFO: stderr: ""
Aug 17 04:33:55.012: INFO: stdout: "true"
Aug 17 04:33:55.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods update-demo-nautilus-6lbhs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7079'
Aug 17 04:33:55.154: INFO: stderr: ""
Aug 17 04:33:55.154: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 17 04:33:55.154: INFO: validating pod update-demo-nautilus-6lbhs
Aug 17 04:33:55.190: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 17 04:33:55.191: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 17 04:33:55.191: INFO: update-demo-nautilus-6lbhs is verified up and running
STEP: using delete to clean up resources
Aug 17 04:33:55.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 delete --grace-period=0 --force -f - --namespace=kubectl-7079'
Aug 17 04:33:55.372: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 04:33:55.372: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 17 04:33:55.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7079'
Aug 17 04:33:55.581: INFO: stderr: "No resources found in kubectl-7079 namespace.\n"
Aug 17 04:33:55.581: INFO: stdout: ""
Aug 17 04:33:55.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods -l name=update-demo --namespace=kubectl-7079 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 17 04:33:55.732: INFO: stderr: ""
Aug 17 04:33:55.732: INFO: stdout: "update-demo-nautilus-42xpv\nupdate-demo-nautilus-6lbhs\n"
Aug 17 04:33:56.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7079'
Aug 17 04:33:56.426: INFO: stderr: "No resources found in kubectl-7079 namespace.\n"
Aug 17 04:33:56.426: INFO: stdout: ""
Aug 17 04:33:56.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods -l name=update-demo --namespace=kubectl-7079 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 17 04:33:56.575: INFO: stderr: ""
Aug 17 04:33:56.575: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:33:56.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7079" for this suite.
Aug 17 04:34:06.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:34:09.543: INFO: namespace kubectl-7079 deletion completed in 12.907217435s

• [SLOW TEST:30.537 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:34:09.543: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-55wq
STEP: Creating a pod to test atomic-volume-subpath
Aug 17 04:34:09.882: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-55wq" in namespace "subpath-8312" to be "success or failure"
Aug 17 04:34:09.905: INFO: Pod "pod-subpath-test-secret-55wq": Phase="Pending", Reason="", readiness=false. Elapsed: 23.262611ms
Aug 17 04:34:11.929: INFO: Pod "pod-subpath-test-secret-55wq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047217463s
Aug 17 04:34:13.951: INFO: Pod "pod-subpath-test-secret-55wq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068600954s
Aug 17 04:34:15.969: INFO: Pod "pod-subpath-test-secret-55wq": Phase="Running", Reason="", readiness=true. Elapsed: 6.086601175s
Aug 17 04:34:18.069: INFO: Pod "pod-subpath-test-secret-55wq": Phase="Running", Reason="", readiness=true. Elapsed: 8.187223404s
Aug 17 04:34:20.136: INFO: Pod "pod-subpath-test-secret-55wq": Phase="Running", Reason="", readiness=true. Elapsed: 10.254154257s
Aug 17 04:34:22.157: INFO: Pod "pod-subpath-test-secret-55wq": Phase="Running", Reason="", readiness=true. Elapsed: 12.275250025s
Aug 17 04:34:24.179: INFO: Pod "pod-subpath-test-secret-55wq": Phase="Running", Reason="", readiness=true. Elapsed: 14.29735453s
Aug 17 04:34:26.204: INFO: Pod "pod-subpath-test-secret-55wq": Phase="Running", Reason="", readiness=true. Elapsed: 16.321568102s
Aug 17 04:34:28.222: INFO: Pod "pod-subpath-test-secret-55wq": Phase="Running", Reason="", readiness=true. Elapsed: 18.339525036s
Aug 17 04:34:30.239: INFO: Pod "pod-subpath-test-secret-55wq": Phase="Running", Reason="", readiness=true. Elapsed: 20.356796113s
Aug 17 04:34:32.257: INFO: Pod "pod-subpath-test-secret-55wq": Phase="Running", Reason="", readiness=true. Elapsed: 22.374992438s
Aug 17 04:34:34.278: INFO: Pod "pod-subpath-test-secret-55wq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.395685398s
STEP: Saw pod success
Aug 17 04:34:34.278: INFO: Pod "pod-subpath-test-secret-55wq" satisfied condition "success or failure"
Aug 17 04:34:34.297: INFO: Trying to get logs from node 10.241.148.42 pod pod-subpath-test-secret-55wq container test-container-subpath-secret-55wq: <nil>
STEP: delete the pod
Aug 17 04:34:34.464: INFO: Waiting for pod pod-subpath-test-secret-55wq to disappear
Aug 17 04:34:34.489: INFO: Pod pod-subpath-test-secret-55wq no longer exists
STEP: Deleting pod pod-subpath-test-secret-55wq
Aug 17 04:34:34.489: INFO: Deleting pod "pod-subpath-test-secret-55wq" in namespace "subpath-8312"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:34:34.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8312" for this suite.
Aug 17 04:34:42.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:34:45.118: INFO: namespace subpath-8312 deletion completed in 10.506937589s

• [SLOW TEST:35.575 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:34:45.118: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:34:49.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9227" for this suite.
Aug 17 04:35:06.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:35:08.849: INFO: namespace replication-controller-9227 deletion completed in 18.836127264s

• [SLOW TEST:23.731 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:35:08.852: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-570edb8c-6e6d-4205-954c-88e7be326858 in namespace container-probe-7859
Aug 17 04:35:13.225: INFO: Started pod busybox-570edb8c-6e6d-4205-954c-88e7be326858 in namespace container-probe-7859
STEP: checking the pod's current state and verifying that restartCount is present
Aug 17 04:35:13.247: INFO: Initial restart count of pod busybox-570edb8c-6e6d-4205-954c-88e7be326858 is 0
Aug 17 04:36:02.157: INFO: Restart count of pod container-probe-7859/busybox-570edb8c-6e6d-4205-954c-88e7be326858 is now 1 (48.909885173s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:36:02.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7859" for this suite.
Aug 17 04:36:12.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:36:14.832: INFO: namespace container-probe-7859 deletion completed in 12.519588153s

• [SLOW TEST:65.979 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:36:14.832: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-1975
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 17 04:36:15.032: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 17 04:36:41.758: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.151.181 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1975 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 17 04:36:41.758: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 04:36:43.002: INFO: Found all expected endpoints: [netserver-0]
Aug 17 04:36:43.024: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.201.242 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1975 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 17 04:36:43.024: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 04:36:44.266: INFO: Found all expected endpoints: [netserver-1]
Aug 17 04:36:44.297: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.218.69 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1975 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 17 04:36:44.297: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 04:36:45.539: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:36:45.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1975" for this suite.
Aug 17 04:36:55.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:36:58.586: INFO: namespace pod-network-test-1975 deletion completed in 12.869134507s

• [SLOW TEST:43.754 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:36:58.587: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 17 04:36:59.450: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 17 04:37:01.502: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733235819, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733235819, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733235819, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733235819, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 17 04:37:04.618: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:37:04.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3073" for this suite.
Aug 17 04:37:15.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:37:20.948: INFO: namespace webhook-3073 deletion completed in 15.867898506s
STEP: Destroying namespace "webhook-3073-markers" for this suite.
Aug 17 04:37:29.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:37:31.535: INFO: namespace webhook-3073-markers deletion completed in 10.587329623s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:33.019 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:37:31.606: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 17 04:37:33.095: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 17 04:37:35.137: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733235853, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733235853, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733235853, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733235853, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 17 04:37:38.219: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Aug 17 04:37:38.326: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:37:38.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2798" for this suite.
Aug 17 04:37:48.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:37:52.314: INFO: namespace webhook-2798 deletion completed in 13.841998602s
STEP: Destroying namespace "webhook-2798-markers" for this suite.
Aug 17 04:38:02.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:38:05.019: INFO: namespace webhook-2798-markers deletion completed in 12.704830969s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:33.499 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:38:05.105: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 17 04:38:05.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-159'
Aug 17 04:38:05.770: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 17 04:38:05.770: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Aug 17 04:38:07.838: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-lh95j]
Aug 17 04:38:07.838: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-lh95j" in namespace "kubectl-159" to be "running and ready"
Aug 17 04:38:07.865: INFO: Pod "e2e-test-httpd-rc-lh95j": Phase="Running", Reason="", readiness=true. Elapsed: 27.56389ms
Aug 17 04:38:07.865: INFO: Pod "e2e-test-httpd-rc-lh95j" satisfied condition "running and ready"
Aug 17 04:38:07.865: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-lh95j]
Aug 17 04:38:07.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 logs rc/e2e-test-httpd-rc --namespace=kubectl-159'
Aug 17 04:38:08.089: INFO: stderr: ""
Aug 17 04:38:08.089: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.30.218.80. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.30.218.80. Set the 'ServerName' directive globally to suppress this message\n[Mon Aug 17 04:38:07.320255 2020] [mpm_event:notice] [pid 1:tid 140215084174184] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Mon Aug 17 04:38:07.320325 2020] [core:notice] [pid 1:tid 140215084174184] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Aug 17 04:38:08.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 delete rc e2e-test-httpd-rc --namespace=kubectl-159'
Aug 17 04:38:08.283: INFO: stderr: ""
Aug 17 04:38:08.283: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:38:08.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-159" for this suite.
Aug 17 04:38:18.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:38:22.178: INFO: namespace kubectl-159 deletion completed in 13.752711536s

• [SLOW TEST:17.073 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:38:22.182: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-a6bc8382-43ab-403c-8b3b-e43e29d5df52
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:38:22.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-141" for this suite.
Aug 17 04:38:30.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:38:33.525: INFO: namespace configmap-141 deletion completed in 11.012224813s

• [SLOW TEST:11.343 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:38:33.526: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 04:38:33.829: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:38:34.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8440" for this suite.
Aug 17 04:38:44.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:38:47.830: INFO: namespace custom-resource-definition-8440 deletion completed in 13.759476285s

• [SLOW TEST:14.304 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:38:47.832: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 17 04:38:48.372: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1d6a8ccb-5775-40e4-8cfb-88bce01f0734" in namespace "projected-2727" to be "success or failure"
Aug 17 04:38:48.464: INFO: Pod "downwardapi-volume-1d6a8ccb-5775-40e4-8cfb-88bce01f0734": Phase="Pending", Reason="", readiness=false. Elapsed: 92.46989ms
Aug 17 04:38:50.542: INFO: Pod "downwardapi-volume-1d6a8ccb-5775-40e4-8cfb-88bce01f0734": Phase="Pending", Reason="", readiness=false. Elapsed: 2.170191356s
Aug 17 04:38:52.566: INFO: Pod "downwardapi-volume-1d6a8ccb-5775-40e4-8cfb-88bce01f0734": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.193711708s
STEP: Saw pod success
Aug 17 04:38:52.566: INFO: Pod "downwardapi-volume-1d6a8ccb-5775-40e4-8cfb-88bce01f0734" satisfied condition "success or failure"
Aug 17 04:38:52.591: INFO: Trying to get logs from node 10.241.148.42 pod downwardapi-volume-1d6a8ccb-5775-40e4-8cfb-88bce01f0734 container client-container: <nil>
STEP: delete the pod
Aug 17 04:38:52.693: INFO: Waiting for pod downwardapi-volume-1d6a8ccb-5775-40e4-8cfb-88bce01f0734 to disappear
Aug 17 04:38:52.716: INFO: Pod downwardapi-volume-1d6a8ccb-5775-40e4-8cfb-88bce01f0734 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:38:52.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2727" for this suite.
Aug 17 04:39:02.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:39:05.395: INFO: namespace projected-2727 deletion completed in 12.560491056s

• [SLOW TEST:17.564 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:39:05.399: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 04:39:05.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 create -f - --namespace=kubectl-2649'
Aug 17 04:39:06.199: INFO: stderr: ""
Aug 17 04:39:06.199: INFO: stdout: "replicationcontroller/redis-master created\n"
Aug 17 04:39:06.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 create -f - --namespace=kubectl-2649'
Aug 17 04:39:06.828: INFO: stderr: ""
Aug 17 04:39:06.828: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 17 04:39:07.848: INFO: Selector matched 1 pods for map[app:redis]
Aug 17 04:39:07.848: INFO: Found 0 / 1
Aug 17 04:39:08.851: INFO: Selector matched 1 pods for map[app:redis]
Aug 17 04:39:08.851: INFO: Found 1 / 1
Aug 17 04:39:08.851: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 17 04:39:08.871: INFO: Selector matched 1 pods for map[app:redis]
Aug 17 04:39:08.871: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 17 04:39:08.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 describe pod redis-master-ptjrh --namespace=kubectl-2649'
Aug 17 04:39:09.727: INFO: stderr: ""
Aug 17 04:39:09.727: INFO: stdout: "Name:         redis-master-ptjrh\nNamespace:    kubectl-2649\nPriority:     0\nNode:         10.241.148.42/10.241.148.42\nStart Time:   Mon, 17 Aug 2020 04:39:06 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 172.30.218.86/32\n              cni.projectcalico.org/podIPs: 172.30.218.86/32\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"k8s-pod-network\",\n                    \"ips\": [\n                        \"172.30.218.86\"\n                    ],\n                    \"dns\": {}\n                }]\n              openshift.io/scc: privileged\nStatus:       Running\nIP:           172.30.218.86\nIPs:\n  IP:           172.30.218.86\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   cri-o://53a54e6caf70ce38d6c06a83d3e86027bd7145525ce10debdc2404e8f056d0f0\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 17 Aug 2020 04:39:07 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-l8lwz (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-l8lwz:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-l8lwz\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                    Message\n  ----    ------     ----       ----                    -------\n  Normal  Scheduled  <unknown>  default-scheduler       Successfully assigned kubectl-2649/redis-master-ptjrh to 10.241.148.42\n  Normal  Pulled     2s         kubelet, 10.241.148.42  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    2s         kubelet, 10.241.148.42  Created container redis-master\n  Normal  Started    2s         kubelet, 10.241.148.42  Started container redis-master\n"
Aug 17 04:39:09.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 describe rc redis-master --namespace=kubectl-2649'
Aug 17 04:39:09.928: INFO: stderr: ""
Aug 17 04:39:09.928: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-2649\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-ptjrh\n"
Aug 17 04:39:09.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 describe service redis-master --namespace=kubectl-2649'
Aug 17 04:39:10.128: INFO: stderr: ""
Aug 17 04:39:10.128: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-2649\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.21.68.153\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.30.218.86:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 17 04:39:10.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 describe node 10.241.148.31'
Aug 17 04:39:10.704: INFO: stderr: ""
Aug 17 04:39:10.704: INFO: stdout: "Name:               10.241.148.31\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-south\n                    failure-domain.beta.kubernetes.io/zone=dal12\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=169.59.216.115\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.241.148.31\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_7_64\n                    ibm-cloud.kubernetes.io/region=us-south\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=test-bssus3120j837gehulmg-kubee2epvge-default-00000258\n                    ibm-cloud.kubernetes.io/worker-pool-id=bssus3120j837gehulmg-102b513\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.3.31_1534_openshift\n                    ibm-cloud.kubernetes.io/zone=dal12\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.241.148.31\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    node.openshift.io/os_id=rhel\n                    privateVLAN=2747436\n                    publicVLAN=2747434\n                    topology.kubernetes.io/region=us-south\n                    topology.kubernetes.io/zone=dal12\nAnnotations:        projectcalico.org/IPv4Address: 10.241.148.31/25\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.201.192\nCreationTimestamp:  Mon, 17 Aug 2020 02:56:14 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 17 Aug 2020 02:57:46 +0000   Mon, 17 Aug 2020 02:57:46 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 17 Aug 2020 04:38:44 +0000   Mon, 17 Aug 2020 02:56:14 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 17 Aug 2020 04:38:44 +0000   Mon, 17 Aug 2020 02:56:14 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 17 Aug 2020 04:38:44 +0000   Mon, 17 Aug 2020 02:56:14 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 17 Aug 2020 04:38:44 +0000   Mon, 17 Aug 2020 02:57:44 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.241.148.31\n  ExternalIP:  169.59.216.115\n  Hostname:    10.241.148.31\nCapacity:\n cpu:                4\n ephemeral-storage:  103078840Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16260880Ki\n pods:               110\nAllocatable:\n cpu:                3910m\n ephemeral-storage:  100275095474\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             13484816Ki\n pods:               110\nSystem Info:\n Machine ID:                              2bcd107e773e40cc902e67f15c38b71e\n System UUID:                             84A95DF1-3B81-B39E-6354-98186BB4F8F4\n Boot ID:                                 15a48363-e0a0-4b84-a5bf-091d3f88076d\n Kernel Version:                          3.10.0-1127.18.2.el7.x86_64\n OS Image:                                Red Hat\n Operating System:                        linux\n Architecture:                            amd64\n Container Runtime Version:               cri-o://1.16.6-18.rhaos4.3.git538d861.el7\n Kubelet Version:                         v1.16.2+554af56\n Kube-Proxy Version:                      v1.16.2+554af56\nProviderID:                               ibm://856f38977b8848e0a6a67f09be3e597c///bssus3120j837gehulmg/test-bssus3120j837gehulmg-kubee2epvge-default-00000258\nNon-terminated Pods:                      (28 in total)\n  Namespace                               Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                               ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                           calico-node-lm8qf                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         101m\n  calico-system                           calico-typha-579947cf56-672t9                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         99m\n  ibm-system                              ibm-cloud-provider-ip-169-48-239-26-5595944bf9-n8nts       5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         96m\n  kube-system                             ibm-keepalived-watcher-ncgwx                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         102m\n  kube-system                             ibm-master-proxy-static-10.241.148.31                      25m (0%)      300m (7%)   32M (0%)         512M (3%)      101m\n  kube-system                             ibmcloud-block-storage-driver-hfn98                        50m (1%)      300m (7%)   100Mi (0%)       300Mi (2%)     102m\n  openshift-cluster-node-tuning-operator  tuned-nmn6j                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         100m\n  openshift-cluster-samples-operator      cluster-samples-operator-55944b8f44-fxlq8                  20m (0%)      0 (0%)      0 (0%)           0 (0%)         100m\n  openshift-console                       console-76d8dc5894-2w62x                                   10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         99m\n  openshift-dns                           dns-default-sxjmd                                          110m (2%)     0 (0%)      70Mi (0%)        512Mi (3%)     100m\n  openshift-image-registry                image-registry-5669fd49dd-xphdf                            100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         100m\n  openshift-image-registry                node-ca-2lszv                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         100m\n  openshift-ingress                       router-default-57d78dfb48-mzjlc                            100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         100m\n  openshift-kube-proxy                    openshift-kube-proxy-5nh74                                 100m (2%)     0 (0%)      200Mi (1%)       0 (0%)         102m\n  openshift-marketplace                   community-operators-65d9d5c9d6-zh8cl                       10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         99m\n  openshift-monitoring                    alertmanager-main-1                                        6m (0%)       0 (0%)      220Mi (1%)       0 (0%)         93m\n  openshift-monitoring                    kube-state-metrics-c5f65645-855s8                          4m (0%)       0 (0%)      120Mi (0%)       0 (0%)         100m\n  openshift-monitoring                    node-exporter-kkwj6                                        9m (0%)       0 (0%)      210Mi (1%)       0 (0%)         100m\n  openshift-monitoring                    prometheus-adapter-67dbcc5cfc-xk5l6                        1m (0%)       0 (0%)      20Mi (0%)        0 (0%)         100m\n  openshift-monitoring                    prometheus-k8s-0                                           76m (1%)      0 (0%)      1184Mi (8%)      0 (0%)         93m\n  openshift-monitoring                    prometheus-operator-56d9d699cb-m5pmc                       5m (0%)       0 (0%)      60Mi (0%)        0 (0%)         94m\n  openshift-monitoring                    thanos-querier-6b5fbd9754-ctb66                            8m (0%)       0 (0%)      72Mi (0%)        0 (0%)         93m\n  openshift-multus                        multus-admission-controller-ncsvb                          10m (0%)      0 (0%)      0 (0%)           0 (0%)         101m\n  openshift-multus                        multus-tvx65                                               10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         102m\n  openshift-operator-lifecycle-manager    packageserver-66768b6f89-b74q9                             10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         93m\n  openshift-service-ca                    configmap-cabundle-injector-8446d4b88f-9k4sq               10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         101m\n  openshift-service-ca                    service-serving-cert-signer-7879bf8d9f-xtqt8               10m (0%)      0 (0%)      120Mi (0%)       0 (0%)         101m\n  sonobuoy                                sonobuoy-systemd-logs-daemon-set-c8f140ca93fa466f-pjc77    0 (0%)        0 (0%)      0 (0%)           0 (0%)         26m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                714m (18%)       600m (15%)\n  memory             3531282Ki (26%)  1363443712 (9%)\n  ephemeral-storage  0 (0%)           0 (0%)\nEvents:\n  Type    Reason                   Age                  From                       Message\n  ----    ------                   ----                 ----                       -------\n  Normal  Starting                 102m                 kubelet, 10.241.148.31     Starting kubelet.\n  Normal  NodeAllocatableEnforced  102m                 kubelet, 10.241.148.31     Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientPID     102m (x7 over 102m)  kubelet, 10.241.148.31     Node 10.241.148.31 status is now: NodeHasSufficientPID\n  Normal  NodeHasSufficientMemory  102m (x8 over 102m)  kubelet, 10.241.148.31     Node 10.241.148.31 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    102m (x8 over 102m)  kubelet, 10.241.148.31     Node 10.241.148.31 status is now: NodeHasNoDiskPressure\n  Normal  Starting                 102m                 kube-proxy, 10.241.148.31  Starting kube-proxy.\n"
Aug 17 04:39:10.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 describe namespace kubectl-2649'
Aug 17 04:39:10.905: INFO: stderr: ""
Aug 17 04:39:10.905: INFO: stdout: "Name:         kubectl-2649\nLabels:       e2e-framework=kubectl\n              e2e-run=0e964423-3911-477f-a0e1-33fa8866ee7f\nAnnotations:  openshift.io/sa.scc.mcs: s0:c44,c9\n              openshift.io/sa.scc.supplemental-groups: 1001910000/10000\n              openshift.io/sa.scc.uid-range: 1001910000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:39:10.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2649" for this suite.
Aug 17 04:39:27.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:39:29.561: INFO: namespace kubectl-2649 deletion completed in 18.620875223s

• [SLOW TEST:24.162 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:39:29.564: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-60410672-8e98-480c-b733-7793b55143a1
STEP: Creating secret with name secret-projected-all-test-volume-c320bc73-9c00-49c8-8af2-e624c6fbc417
STEP: Creating a pod to test Check all projections for projected volume plugin
Aug 17 04:39:29.844: INFO: Waiting up to 5m0s for pod "projected-volume-f0a2f671-6bfc-4736-a885-201552a87f96" in namespace "projected-2237" to be "success or failure"
Aug 17 04:39:29.866: INFO: Pod "projected-volume-f0a2f671-6bfc-4736-a885-201552a87f96": Phase="Pending", Reason="", readiness=false. Elapsed: 21.042844ms
Aug 17 04:39:31.885: INFO: Pod "projected-volume-f0a2f671-6bfc-4736-a885-201552a87f96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040178119s
Aug 17 04:39:33.906: INFO: Pod "projected-volume-f0a2f671-6bfc-4736-a885-201552a87f96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061113563s
STEP: Saw pod success
Aug 17 04:39:33.906: INFO: Pod "projected-volume-f0a2f671-6bfc-4736-a885-201552a87f96" satisfied condition "success or failure"
Aug 17 04:39:33.924: INFO: Trying to get logs from node 10.241.148.42 pod projected-volume-f0a2f671-6bfc-4736-a885-201552a87f96 container projected-all-volume-test: <nil>
STEP: delete the pod
Aug 17 04:39:34.028: INFO: Waiting for pod projected-volume-f0a2f671-6bfc-4736-a885-201552a87f96 to disappear
Aug 17 04:39:34.052: INFO: Pod projected-volume-f0a2f671-6bfc-4736-a885-201552a87f96 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:39:34.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2237" for this suite.
Aug 17 04:39:42.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:39:44.877: INFO: namespace projected-2237 deletion completed in 10.683402792s

• [SLOW TEST:15.314 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:39:44.878: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-643b0439-a378-4ee7-8f6b-d6e1c4c68ab1
STEP: Creating a pod to test consume configMaps
Aug 17 04:39:45.175: INFO: Waiting up to 5m0s for pod "pod-configmaps-7cdadb9d-0648-492f-abbd-8708d49539a0" in namespace "configmap-7054" to be "success or failure"
Aug 17 04:39:45.209: INFO: Pod "pod-configmaps-7cdadb9d-0648-492f-abbd-8708d49539a0": Phase="Pending", Reason="", readiness=false. Elapsed: 33.237778ms
Aug 17 04:39:47.294: INFO: Pod "pod-configmaps-7cdadb9d-0648-492f-abbd-8708d49539a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.118251599s
Aug 17 04:39:49.393: INFO: Pod "pod-configmaps-7cdadb9d-0648-492f-abbd-8708d49539a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.217600486s
STEP: Saw pod success
Aug 17 04:39:49.393: INFO: Pod "pod-configmaps-7cdadb9d-0648-492f-abbd-8708d49539a0" satisfied condition "success or failure"
Aug 17 04:39:49.465: INFO: Trying to get logs from node 10.241.148.42 pod pod-configmaps-7cdadb9d-0648-492f-abbd-8708d49539a0 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 17 04:39:49.784: INFO: Waiting for pod pod-configmaps-7cdadb9d-0648-492f-abbd-8708d49539a0 to disappear
Aug 17 04:39:49.838: INFO: Pod pod-configmaps-7cdadb9d-0648-492f-abbd-8708d49539a0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:39:49.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7054" for this suite.
Aug 17 04:40:00.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:40:03.831: INFO: namespace configmap-7054 deletion completed in 13.593980224s

• [SLOW TEST:18.953 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:40:03.831: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Aug 17 04:40:06.359: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-667650646 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Aug 17 04:40:11.687: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:40:11.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-617" for this suite.
Aug 17 04:40:21.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:40:24.212: INFO: namespace pods-617 deletion completed in 12.480077162s

• [SLOW TEST:20.382 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:40:24.214: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 17 04:40:25.217: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 17 04:40:27.260: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733236025, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733236025, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733236025, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733236025, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 17 04:40:30.321: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 04:40:30.338: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5899-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:40:31.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9178" for this suite.
Aug 17 04:40:41.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:40:44.265: INFO: namespace webhook-9178 deletion completed in 12.446643867s
STEP: Destroying namespace "webhook-9178-markers" for this suite.
Aug 17 04:40:54.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:40:56.768: INFO: namespace webhook-9178-markers deletion completed in 12.502307353s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:32.629 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:40:56.843: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 17 04:40:57.144: INFO: Waiting up to 5m0s for pod "pod-45fded4d-052b-4e6a-9333-e388e4e62419" in namespace "emptydir-4643" to be "success or failure"
Aug 17 04:40:57.165: INFO: Pod "pod-45fded4d-052b-4e6a-9333-e388e4e62419": Phase="Pending", Reason="", readiness=false. Elapsed: 20.297069ms
Aug 17 04:40:59.184: INFO: Pod "pod-45fded4d-052b-4e6a-9333-e388e4e62419": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039301414s
Aug 17 04:41:01.206: INFO: Pod "pod-45fded4d-052b-4e6a-9333-e388e4e62419": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061539129s
STEP: Saw pod success
Aug 17 04:41:01.206: INFO: Pod "pod-45fded4d-052b-4e6a-9333-e388e4e62419" satisfied condition "success or failure"
Aug 17 04:41:01.229: INFO: Trying to get logs from node 10.241.148.42 pod pod-45fded4d-052b-4e6a-9333-e388e4e62419 container test-container: <nil>
STEP: delete the pod
Aug 17 04:41:01.354: INFO: Waiting for pod pod-45fded4d-052b-4e6a-9333-e388e4e62419 to disappear
Aug 17 04:41:01.373: INFO: Pod pod-45fded4d-052b-4e6a-9333-e388e4e62419 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:41:01.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4643" for this suite.
Aug 17 04:41:11.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:41:14.067: INFO: namespace emptydir-4643 deletion completed in 12.627596362s

• [SLOW TEST:17.224 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:41:14.067: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3543.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3543.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3543.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3543.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3543.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3543.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3543.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3543.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3543.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3543.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3543.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3543.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3543.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3543.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3543.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3543.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3543.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3543.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 17 04:41:20.731: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3543.svc.cluster.local from pod dns-3543/dns-test-17ac354e-6df5-40b7-982a-651bcb6f4821: the server could not find the requested resource (get pods dns-test-17ac354e-6df5-40b7-982a-651bcb6f4821)
Aug 17 04:41:20.773: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3543.svc.cluster.local from pod dns-3543/dns-test-17ac354e-6df5-40b7-982a-651bcb6f4821: the server could not find the requested resource (get pods dns-test-17ac354e-6df5-40b7-982a-651bcb6f4821)
Aug 17 04:41:20.840: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3543.svc.cluster.local from pod dns-3543/dns-test-17ac354e-6df5-40b7-982a-651bcb6f4821: the server could not find the requested resource (get pods dns-test-17ac354e-6df5-40b7-982a-651bcb6f4821)
Aug 17 04:41:20.934: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3543.svc.cluster.local from pod dns-3543/dns-test-17ac354e-6df5-40b7-982a-651bcb6f4821: the server could not find the requested resource (get pods dns-test-17ac354e-6df5-40b7-982a-651bcb6f4821)
Aug 17 04:41:20.965: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3543.svc.cluster.local from pod dns-3543/dns-test-17ac354e-6df5-40b7-982a-651bcb6f4821: the server could not find the requested resource (get pods dns-test-17ac354e-6df5-40b7-982a-651bcb6f4821)
Aug 17 04:41:21.000: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3543.svc.cluster.local from pod dns-3543/dns-test-17ac354e-6df5-40b7-982a-651bcb6f4821: the server could not find the requested resource (get pods dns-test-17ac354e-6df5-40b7-982a-651bcb6f4821)
Aug 17 04:41:21.079: INFO: Lookups using dns-3543/dns-test-17ac354e-6df5-40b7-982a-651bcb6f4821 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3543.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3543.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3543.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3543.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3543.svc.cluster.local jessie_udp@dns-test-service-2.dns-3543.svc.cluster.local]

Aug 17 04:41:26.448: INFO: DNS probes using dns-3543/dns-test-17ac354e-6df5-40b7-982a-651bcb6f4821 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:41:26.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3543" for this suite.
Aug 17 04:41:36.733: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:41:39.456: INFO: namespace dns-3543 deletion completed in 12.800549073s

• [SLOW TEST:25.389 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:41:39.457: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 17 04:41:39.744: INFO: Waiting up to 5m0s for pod "pod-96f6bfe7-2168-43ac-aef5-37946b812ccb" in namespace "emptydir-2761" to be "success or failure"
Aug 17 04:41:39.769: INFO: Pod "pod-96f6bfe7-2168-43ac-aef5-37946b812ccb": Phase="Pending", Reason="", readiness=false. Elapsed: 24.753318ms
Aug 17 04:41:41.789: INFO: Pod "pod-96f6bfe7-2168-43ac-aef5-37946b812ccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.045468868s
STEP: Saw pod success
Aug 17 04:41:41.789: INFO: Pod "pod-96f6bfe7-2168-43ac-aef5-37946b812ccb" satisfied condition "success or failure"
Aug 17 04:41:41.814: INFO: Trying to get logs from node 10.241.148.42 pod pod-96f6bfe7-2168-43ac-aef5-37946b812ccb container test-container: <nil>
STEP: delete the pod
Aug 17 04:41:41.921: INFO: Waiting for pod pod-96f6bfe7-2168-43ac-aef5-37946b812ccb to disappear
Aug 17 04:41:41.939: INFO: Pod pod-96f6bfe7-2168-43ac-aef5-37946b812ccb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:41:41.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2761" for this suite.
Aug 17 04:41:50.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:41:55.312: INFO: namespace emptydir-2761 deletion completed in 13.311329733s

• [SLOW TEST:15.856 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:41:55.313: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:42:12.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9957" for this suite.
Aug 17 04:42:20.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:42:23.487: INFO: namespace resourcequota-9957 deletion completed in 10.639373272s

• [SLOW TEST:28.175 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:42:23.488: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8507.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8507.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8507.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8507.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8507.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8507.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 17 04:42:29.114: INFO: DNS probes using dns-8507/dns-test-a6cfac43-f774-4d3c-9cc4-801c78ff8ffd succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:42:29.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8507" for this suite.
Aug 17 04:42:39.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:42:42.005: INFO: namespace dns-8507 deletion completed in 12.776738819s

• [SLOW TEST:18.517 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:42:42.005: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:42:42.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8416" for this suite.
Aug 17 04:42:52.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:42:54.887: INFO: namespace custom-resource-definition-8416 deletion completed in 12.553063162s

• [SLOW TEST:12.882 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:42:54.888: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Aug 17 04:42:55.151: INFO: Waiting up to 5m0s for pod "downward-api-37fdcc1a-84a8-487c-84be-2169e19334d1" in namespace "downward-api-4206" to be "success or failure"
Aug 17 04:42:55.173: INFO: Pod "downward-api-37fdcc1a-84a8-487c-84be-2169e19334d1": Phase="Pending", Reason="", readiness=false. Elapsed: 22.155951ms
Aug 17 04:42:57.195: INFO: Pod "downward-api-37fdcc1a-84a8-487c-84be-2169e19334d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.044044314s
STEP: Saw pod success
Aug 17 04:42:57.195: INFO: Pod "downward-api-37fdcc1a-84a8-487c-84be-2169e19334d1" satisfied condition "success or failure"
Aug 17 04:42:57.214: INFO: Trying to get logs from node 10.241.148.42 pod downward-api-37fdcc1a-84a8-487c-84be-2169e19334d1 container dapi-container: <nil>
STEP: delete the pod
Aug 17 04:42:57.324: INFO: Waiting for pod downward-api-37fdcc1a-84a8-487c-84be-2169e19334d1 to disappear
Aug 17 04:42:57.340: INFO: Pod downward-api-37fdcc1a-84a8-487c-84be-2169e19334d1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:42:57.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4206" for this suite.
Aug 17 04:43:07.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:43:10.075: INFO: namespace downward-api-4206 deletion completed in 12.687453347s

• [SLOW TEST:15.187 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:43:10.075: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3722.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3722.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3722.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3722.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3722.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3722.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 17 04:43:14.802: INFO: DNS probes using dns-3722/dns-test-bfc2ac79-cb45-41a6-b22c-dce4d787a138 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:43:14.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3722" for this suite.
Aug 17 04:43:25.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:43:27.623: INFO: namespace dns-3722 deletion completed in 12.612447177s

• [SLOW TEST:17.548 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:43:27.624: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 04:43:27.892: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 17 04:43:37.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 --namespace=crd-publish-openapi-514 create -f -'
Aug 17 04:43:38.401: INFO: stderr: ""
Aug 17 04:43:38.401: INFO: stdout: "e2e-test-crd-publish-openapi-9340-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 17 04:43:38.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 --namespace=crd-publish-openapi-514 delete e2e-test-crd-publish-openapi-9340-crds test-cr'
Aug 17 04:43:38.649: INFO: stderr: ""
Aug 17 04:43:38.649: INFO: stdout: "e2e-test-crd-publish-openapi-9340-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Aug 17 04:43:38.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 --namespace=crd-publish-openapi-514 apply -f -'
Aug 17 04:43:39.309: INFO: stderr: ""
Aug 17 04:43:39.309: INFO: stdout: "e2e-test-crd-publish-openapi-9340-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 17 04:43:39.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 --namespace=crd-publish-openapi-514 delete e2e-test-crd-publish-openapi-9340-crds test-cr'
Aug 17 04:43:39.510: INFO: stderr: ""
Aug 17 04:43:39.510: INFO: stdout: "e2e-test-crd-publish-openapi-9340-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Aug 17 04:43:39.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 explain e2e-test-crd-publish-openapi-9340-crds'
Aug 17 04:43:40.152: INFO: stderr: ""
Aug 17 04:43:40.152: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9340-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:43:49.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-514" for this suite.
Aug 17 04:43:57.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:44:00.349: INFO: namespace crd-publish-openapi-514 deletion completed in 10.861985691s

• [SLOW TEST:32.725 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:44:00.349: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Aug 17 04:44:00.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 cluster-info'
Aug 17 04:44:00.724: INFO: stderr: ""
Aug 17 04:44:00.724: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:44:00.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-591" for this suite.
Aug 17 04:44:08.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:44:11.339: INFO: namespace kubectl-591 deletion completed in 10.550274325s

• [SLOW TEST:10.990 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:44:11.339: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-dxsq
STEP: Creating a pod to test atomic-volume-subpath
Aug 17 04:44:11.750: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-dxsq" in namespace "subpath-945" to be "success or failure"
Aug 17 04:44:11.778: INFO: Pod "pod-subpath-test-downwardapi-dxsq": Phase="Pending", Reason="", readiness=false. Elapsed: 27.096219ms
Aug 17 04:44:13.803: INFO: Pod "pod-subpath-test-downwardapi-dxsq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052894838s
Aug 17 04:44:15.831: INFO: Pod "pod-subpath-test-downwardapi-dxsq": Phase="Running", Reason="", readiness=true. Elapsed: 4.080312354s
Aug 17 04:44:17.893: INFO: Pod "pod-subpath-test-downwardapi-dxsq": Phase="Running", Reason="", readiness=true. Elapsed: 6.142408576s
Aug 17 04:44:19.986: INFO: Pod "pod-subpath-test-downwardapi-dxsq": Phase="Running", Reason="", readiness=true. Elapsed: 8.234999476s
Aug 17 04:44:22.006: INFO: Pod "pod-subpath-test-downwardapi-dxsq": Phase="Running", Reason="", readiness=true. Elapsed: 10.255628163s
Aug 17 04:44:24.028: INFO: Pod "pod-subpath-test-downwardapi-dxsq": Phase="Running", Reason="", readiness=true. Elapsed: 12.277184778s
Aug 17 04:44:26.048: INFO: Pod "pod-subpath-test-downwardapi-dxsq": Phase="Running", Reason="", readiness=true. Elapsed: 14.297945028s
Aug 17 04:44:28.067: INFO: Pod "pod-subpath-test-downwardapi-dxsq": Phase="Running", Reason="", readiness=true. Elapsed: 16.316631019s
Aug 17 04:44:30.088: INFO: Pod "pod-subpath-test-downwardapi-dxsq": Phase="Running", Reason="", readiness=true. Elapsed: 18.337323239s
Aug 17 04:44:32.112: INFO: Pod "pod-subpath-test-downwardapi-dxsq": Phase="Running", Reason="", readiness=true. Elapsed: 20.361294505s
Aug 17 04:44:34.131: INFO: Pod "pod-subpath-test-downwardapi-dxsq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.380440182s
STEP: Saw pod success
Aug 17 04:44:34.131: INFO: Pod "pod-subpath-test-downwardapi-dxsq" satisfied condition "success or failure"
Aug 17 04:44:34.149: INFO: Trying to get logs from node 10.241.148.42 pod pod-subpath-test-downwardapi-dxsq container test-container-subpath-downwardapi-dxsq: <nil>
STEP: delete the pod
Aug 17 04:44:34.278: INFO: Waiting for pod pod-subpath-test-downwardapi-dxsq to disappear
Aug 17 04:44:34.294: INFO: Pod pod-subpath-test-downwardapi-dxsq no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-dxsq
Aug 17 04:44:34.294: INFO: Deleting pod "pod-subpath-test-downwardapi-dxsq" in namespace "subpath-945"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:44:34.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-945" for this suite.
Aug 17 04:44:44.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:44:48.748: INFO: namespace subpath-945 deletion completed in 14.368888038s

• [SLOW TEST:37.409 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:44:48.748: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 17 04:44:50.570: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 17 04:44:52.619: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733236290, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733236290, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733236290, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733236290, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 17 04:44:55.690: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:44:56.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9338" for this suite.
Aug 17 04:45:06.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:45:08.739: INFO: namespace webhook-9338 deletion completed in 12.659801574s
STEP: Destroying namespace "webhook-9338-markers" for this suite.
Aug 17 04:45:18.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:45:21.852: INFO: namespace webhook-9338-markers deletion completed in 13.112555585s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:33.187 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:45:21.936: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-mvlsp in namespace proxy-6490
I0817 04:45:22.288401      22 runners.go:184] Created replication controller with name: proxy-service-mvlsp, namespace: proxy-6490, replica count: 1
I0817 04:45:23.339285      22 runners.go:184] proxy-service-mvlsp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0817 04:45:24.339733      22 runners.go:184] proxy-service-mvlsp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0817 04:45:25.340210      22 runners.go:184] proxy-service-mvlsp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0817 04:45:26.340517      22 runners.go:184] proxy-service-mvlsp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0817 04:45:27.341035      22 runners.go:184] proxy-service-mvlsp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0817 04:45:28.341504      22 runners.go:184] proxy-service-mvlsp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0817 04:45:29.342075      22 runners.go:184] proxy-service-mvlsp Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 17 04:45:29.367: INFO: setup took 7.154162005s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Aug 17 04:45:29.420: INFO: (0) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/rewriteme">test</a> (200; 51.684856ms)
Aug 17 04:45:29.421: INFO: (0) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">... (200; 51.582594ms)
Aug 17 04:45:29.423: INFO: (0) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 54.495608ms)
Aug 17 04:45:29.435: INFO: (0) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">test<... (200; 66.146234ms)
Aug 17 04:45:29.435: INFO: (0) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 65.470669ms)
Aug 17 04:45:29.435: INFO: (0) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 66.75856ms)
Aug 17 04:45:29.438: INFO: (0) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname2/proxy/: bar (200; 70.107915ms)
Aug 17 04:45:29.446: INFO: (0) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:462/proxy/: tls qux (200; 76.5255ms)
Aug 17 04:45:29.446: INFO: (0) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 76.940074ms)
Aug 17 04:45:29.446: INFO: (0) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:460/proxy/: tls baz (200; 78.399864ms)
Aug 17 04:45:29.447: INFO: (0) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname1/proxy/: tls baz (200; 78.07922ms)
Aug 17 04:45:29.447: INFO: (0) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname1/proxy/: foo (200; 76.836127ms)
Aug 17 04:45:29.447: INFO: (0) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/tlsrewritem... (200; 78.864177ms)
Aug 17 04:45:29.447: INFO: (0) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname1/proxy/: foo (200; 77.945939ms)
Aug 17 04:45:29.447: INFO: (0) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname2/proxy/: bar (200; 77.786689ms)
Aug 17 04:45:29.450: INFO: (0) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname2/proxy/: tls qux (200; 81.284179ms)
Aug 17 04:45:29.481: INFO: (1) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/rewriteme">test</a> (200; 30.392612ms)
Aug 17 04:45:29.491: INFO: (1) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname1/proxy/: tls baz (200; 40.665664ms)
Aug 17 04:45:29.491: INFO: (1) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:462/proxy/: tls qux (200; 39.395994ms)
Aug 17 04:45:29.491: INFO: (1) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/tlsrewritem... (200; 39.640411ms)
Aug 17 04:45:29.497: INFO: (1) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname2/proxy/: tls qux (200; 45.648579ms)
Aug 17 04:45:29.497: INFO: (1) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 44.979389ms)
Aug 17 04:45:29.497: INFO: (1) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 45.076719ms)
Aug 17 04:45:29.497: INFO: (1) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">test<... (200; 45.427095ms)
Aug 17 04:45:29.497: INFO: (1) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:460/proxy/: tls baz (200; 44.907786ms)
Aug 17 04:45:29.497: INFO: (1) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 44.972546ms)
Aug 17 04:45:29.497: INFO: (1) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">... (200; 45.03601ms)
Aug 17 04:45:29.497: INFO: (1) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 44.998464ms)
Aug 17 04:45:29.499: INFO: (1) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname1/proxy/: foo (200; 47.673113ms)
Aug 17 04:45:29.499: INFO: (1) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname2/proxy/: bar (200; 47.599636ms)
Aug 17 04:45:29.504: INFO: (1) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname2/proxy/: bar (200; 52.156678ms)
Aug 17 04:45:29.504: INFO: (1) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname1/proxy/: foo (200; 53.073647ms)
Aug 17 04:45:29.539: INFO: (2) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 34.613556ms)
Aug 17 04:45:29.540: INFO: (2) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 34.901591ms)
Aug 17 04:45:29.540: INFO: (2) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:460/proxy/: tls baz (200; 34.706944ms)
Aug 17 04:45:29.544: INFO: (2) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/tlsrewritem... (200; 39.326481ms)
Aug 17 04:45:29.544: INFO: (2) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:462/proxy/: tls qux (200; 39.576583ms)
Aug 17 04:45:29.544: INFO: (2) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/rewriteme">test</a> (200; 39.945889ms)
Aug 17 04:45:29.544: INFO: (2) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">test<... (200; 39.932475ms)
Aug 17 04:45:29.545: INFO: (2) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 39.624397ms)
Aug 17 04:45:29.545: INFO: (2) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname1/proxy/: tls baz (200; 40.799031ms)
Aug 17 04:45:29.545: INFO: (2) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 39.8662ms)
Aug 17 04:45:29.545: INFO: (2) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">... (200; 39.918401ms)
Aug 17 04:45:29.549: INFO: (2) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname2/proxy/: bar (200; 44.254505ms)
Aug 17 04:45:29.550: INFO: (2) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname1/proxy/: foo (200; 45.144909ms)
Aug 17 04:45:29.550: INFO: (2) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname2/proxy/: bar (200; 45.019024ms)
Aug 17 04:45:29.556: INFO: (2) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname1/proxy/: foo (200; 51.387015ms)
Aug 17 04:45:29.556: INFO: (2) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname2/proxy/: tls qux (200; 52.198779ms)
Aug 17 04:45:29.597: INFO: (3) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/rewriteme">test</a> (200; 40.100411ms)
Aug 17 04:45:29.610: INFO: (3) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:462/proxy/: tls qux (200; 52.56748ms)
Aug 17 04:45:29.610: INFO: (3) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">... (200; 52.879621ms)
Aug 17 04:45:29.612: INFO: (3) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/tlsrewritem... (200; 54.115569ms)
Aug 17 04:45:29.612: INFO: (3) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 54.591679ms)
Aug 17 04:45:29.612: INFO: (3) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 54.265193ms)
Aug 17 04:45:29.612: INFO: (3) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 54.387454ms)
Aug 17 04:45:29.612: INFO: (3) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">test<... (200; 55.072037ms)
Aug 17 04:45:29.612: INFO: (3) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname1/proxy/: tls baz (200; 55.58773ms)
Aug 17 04:45:29.612: INFO: (3) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 54.644464ms)
Aug 17 04:45:29.612: INFO: (3) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:460/proxy/: tls baz (200; 54.228807ms)
Aug 17 04:45:29.620: INFO: (3) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname2/proxy/: tls qux (200; 63.654782ms)
Aug 17 04:45:29.620: INFO: (3) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname1/proxy/: foo (200; 63.79093ms)
Aug 17 04:45:29.621: INFO: (3) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname1/proxy/: foo (200; 63.680724ms)
Aug 17 04:45:29.626: INFO: (3) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname2/proxy/: bar (200; 69.476321ms)
Aug 17 04:45:29.627: INFO: (3) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname2/proxy/: bar (200; 69.540651ms)
Aug 17 04:45:29.669: INFO: (4) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 42.357867ms)
Aug 17 04:45:29.669: INFO: (4) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:462/proxy/: tls qux (200; 42.290894ms)
Aug 17 04:45:29.675: INFO: (4) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname1/proxy/: foo (200; 47.516806ms)
Aug 17 04:45:29.685: INFO: (4) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">test<... (200; 57.754047ms)
Aug 17 04:45:29.685: INFO: (4) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:460/proxy/: tls baz (200; 58.123716ms)
Aug 17 04:45:29.685: INFO: (4) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 58.510825ms)
Aug 17 04:45:29.686: INFO: (4) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname2/proxy/: tls qux (200; 58.919982ms)
Aug 17 04:45:29.691: INFO: (4) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">... (200; 63.717607ms)
Aug 17 04:45:29.691: INFO: (4) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 64.202881ms)
Aug 17 04:45:29.698: INFO: (4) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname1/proxy/: foo (200; 71.139373ms)
Aug 17 04:45:29.703: INFO: (4) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname2/proxy/: bar (200; 75.35176ms)
Aug 17 04:45:29.707: INFO: (4) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 79.792414ms)
Aug 17 04:45:29.707: INFO: (4) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/rewriteme">test</a> (200; 80.124157ms)
Aug 17 04:45:29.707: INFO: (4) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname2/proxy/: bar (200; 80.075ms)
Aug 17 04:45:29.707: INFO: (4) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname1/proxy/: tls baz (200; 80.436912ms)
Aug 17 04:45:29.708: INFO: (4) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/tlsrewritem... (200; 80.632756ms)
Aug 17 04:45:29.762: INFO: (5) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">test<... (200; 54.13744ms)
Aug 17 04:45:29.762: INFO: (5) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 54.045293ms)
Aug 17 04:45:29.763: INFO: (5) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/rewriteme">test</a> (200; 54.450365ms)
Aug 17 04:45:29.763: INFO: (5) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:460/proxy/: tls baz (200; 54.113743ms)
Aug 17 04:45:29.763: INFO: (5) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 54.388172ms)
Aug 17 04:45:29.763: INFO: (5) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname2/proxy/: bar (200; 54.885638ms)
Aug 17 04:45:29.763: INFO: (5) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname2/proxy/: tls qux (200; 54.875374ms)
Aug 17 04:45:29.770: INFO: (5) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:462/proxy/: tls qux (200; 61.37593ms)
Aug 17 04:45:29.770: INFO: (5) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname1/proxy/: foo (200; 61.689308ms)
Aug 17 04:45:29.770: INFO: (5) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname1/proxy/: tls baz (200; 62.078732ms)
Aug 17 04:45:29.770: INFO: (5) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/tlsrewritem... (200; 61.261684ms)
Aug 17 04:45:29.770: INFO: (5) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">... (200; 61.521579ms)
Aug 17 04:45:29.770: INFO: (5) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 61.40784ms)
Aug 17 04:45:29.770: INFO: (5) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname1/proxy/: foo (200; 61.814273ms)
Aug 17 04:45:29.770: INFO: (5) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 61.722156ms)
Aug 17 04:45:29.770: INFO: (5) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname2/proxy/: bar (200; 61.867197ms)
Aug 17 04:45:29.799: INFO: (6) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:462/proxy/: tls qux (200; 28.56417ms)
Aug 17 04:45:29.804: INFO: (6) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 33.376196ms)
Aug 17 04:45:29.805: INFO: (6) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/rewriteme">test</a> (200; 33.862073ms)
Aug 17 04:45:29.807: INFO: (6) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">test<... (200; 36.742454ms)
Aug 17 04:45:29.820: INFO: (6) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 48.975835ms)
Aug 17 04:45:29.820: INFO: (6) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">... (200; 49.186326ms)
Aug 17 04:45:29.824: INFO: (6) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname1/proxy/: foo (200; 53.665257ms)
Aug 17 04:45:29.825: INFO: (6) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 53.760928ms)
Aug 17 04:45:29.825: INFO: (6) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:460/proxy/: tls baz (200; 53.793562ms)
Aug 17 04:45:29.825: INFO: (6) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/tlsrewritem... (200; 53.926274ms)
Aug 17 04:45:29.825: INFO: (6) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 53.975947ms)
Aug 17 04:45:29.829: INFO: (6) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname2/proxy/: tls qux (200; 58.156029ms)
Aug 17 04:45:29.836: INFO: (6) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname2/proxy/: bar (200; 64.900872ms)
Aug 17 04:45:29.836: INFO: (6) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname2/proxy/: bar (200; 64.979007ms)
Aug 17 04:45:29.836: INFO: (6) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname1/proxy/: foo (200; 65.01998ms)
Aug 17 04:45:29.843: INFO: (6) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname1/proxy/: tls baz (200; 72.539662ms)
Aug 17 04:45:29.874: INFO: (7) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">... (200; 30.780724ms)
Aug 17 04:45:29.876: INFO: (7) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 32.701311ms)
Aug 17 04:45:29.877: INFO: (7) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/rewriteme">test</a> (200; 31.925229ms)
Aug 17 04:45:29.879: INFO: (7) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 34.996354ms)
Aug 17 04:45:29.879: INFO: (7) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/tlsrewritem... (200; 34.95068ms)
Aug 17 04:45:29.879: INFO: (7) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:460/proxy/: tls baz (200; 34.872362ms)
Aug 17 04:45:29.879: INFO: (7) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">test<... (200; 34.300371ms)
Aug 17 04:45:29.879: INFO: (7) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 34.607628ms)
Aug 17 04:45:29.879: INFO: (7) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:462/proxy/: tls qux (200; 34.697119ms)
Aug 17 04:45:29.885: INFO: (7) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 39.992464ms)
Aug 17 04:45:29.891: INFO: (7) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname2/proxy/: bar (200; 46.018467ms)
Aug 17 04:45:29.891: INFO: (7) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname1/proxy/: foo (200; 46.776693ms)
Aug 17 04:45:29.891: INFO: (7) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname1/proxy/: tls baz (200; 47.003719ms)
Aug 17 04:45:29.896: INFO: (7) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname1/proxy/: foo (200; 51.788572ms)
Aug 17 04:45:29.897: INFO: (7) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname2/proxy/: bar (200; 51.890797ms)
Aug 17 04:45:29.897: INFO: (7) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname2/proxy/: tls qux (200; 51.954977ms)
Aug 17 04:45:29.927: INFO: (8) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/tlsrewritem... (200; 30.260124ms)
Aug 17 04:45:29.927: INFO: (8) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 30.301033ms)
Aug 17 04:45:29.927: INFO: (8) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/rewriteme">test</a> (200; 30.419241ms)
Aug 17 04:45:29.938: INFO: (8) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 40.801923ms)
Aug 17 04:45:29.938: INFO: (8) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 40.804361ms)
Aug 17 04:45:29.938: INFO: (8) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">... (200; 40.844322ms)
Aug 17 04:45:29.938: INFO: (8) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">test<... (200; 41.228137ms)
Aug 17 04:45:29.938: INFO: (8) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname1/proxy/: foo (200; 41.636321ms)
Aug 17 04:45:29.943: INFO: (8) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 45.257411ms)
Aug 17 04:45:29.943: INFO: (8) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:460/proxy/: tls baz (200; 45.52805ms)
Aug 17 04:45:29.943: INFO: (8) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:462/proxy/: tls qux (200; 45.3714ms)
Aug 17 04:45:29.946: INFO: (8) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname1/proxy/: tls baz (200; 48.516318ms)
Aug 17 04:45:29.946: INFO: (8) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname2/proxy/: bar (200; 48.560012ms)
Aug 17 04:45:29.950: INFO: (8) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname2/proxy/: tls qux (200; 52.936695ms)
Aug 17 04:45:29.950: INFO: (8) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname1/proxy/: foo (200; 53.071748ms)
Aug 17 04:45:29.950: INFO: (8) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname2/proxy/: bar (200; 53.041187ms)
Aug 17 04:45:29.977: INFO: (9) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">test<... (200; 26.332088ms)
Aug 17 04:45:29.987: INFO: (9) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 35.871393ms)
Aug 17 04:45:29.987: INFO: (9) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:460/proxy/: tls baz (200; 35.734137ms)
Aug 17 04:45:29.987: INFO: (9) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">... (200; 35.913158ms)
Aug 17 04:45:29.987: INFO: (9) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:462/proxy/: tls qux (200; 35.872909ms)
Aug 17 04:45:29.987: INFO: (9) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 35.968367ms)
Aug 17 04:45:29.987: INFO: (9) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/tlsrewritem... (200; 35.855512ms)
Aug 17 04:45:29.987: INFO: (9) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 36.142882ms)
Aug 17 04:45:29.998: INFO: (9) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/rewriteme">test</a> (200; 46.891525ms)
Aug 17 04:45:29.998: INFO: (9) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 47.166946ms)
Aug 17 04:45:30.001: INFO: (9) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname2/proxy/: bar (200; 49.656922ms)
Aug 17 04:45:30.003: INFO: (9) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname1/proxy/: foo (200; 51.663083ms)
Aug 17 04:45:30.003: INFO: (9) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname1/proxy/: foo (200; 51.813866ms)
Aug 17 04:45:30.003: INFO: (9) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname2/proxy/: tls qux (200; 51.803892ms)
Aug 17 04:45:30.010: INFO: (9) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname2/proxy/: bar (200; 58.901457ms)
Aug 17 04:45:30.013: INFO: (9) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname1/proxy/: tls baz (200; 61.89331ms)
Aug 17 04:45:30.038: INFO: (10) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/rewriteme">test</a> (200; 24.894873ms)
Aug 17 04:45:30.042: INFO: (10) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">test<... (200; 28.602465ms)
Aug 17 04:45:30.049: INFO: (10) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname1/proxy/: tls baz (200; 36.152046ms)
Aug 17 04:45:30.052: INFO: (10) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/tlsrewritem... (200; 36.601463ms)
Aug 17 04:45:30.052: INFO: (10) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 37.584595ms)
Aug 17 04:45:30.054: INFO: (10) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:460/proxy/: tls baz (200; 38.886592ms)
Aug 17 04:45:30.054: INFO: (10) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 39.845702ms)
Aug 17 04:45:30.062: INFO: (10) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">... (200; 47.409032ms)
Aug 17 04:45:30.062: INFO: (10) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 46.503165ms)
Aug 17 04:45:30.066: INFO: (10) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname2/proxy/: tls qux (200; 52.706619ms)
Aug 17 04:45:30.066: INFO: (10) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname2/proxy/: bar (200; 51.981409ms)
Aug 17 04:45:30.066: INFO: (10) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:462/proxy/: tls qux (200; 51.403153ms)
Aug 17 04:45:30.066: INFO: (10) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 51.175294ms)
Aug 17 04:45:30.073: INFO: (10) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname2/proxy/: bar (200; 58.697701ms)
Aug 17 04:45:30.073: INFO: (10) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname1/proxy/: foo (200; 57.957902ms)
Aug 17 04:45:30.077: INFO: (10) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname1/proxy/: foo (200; 63.306599ms)
Aug 17 04:45:30.103: INFO: (11) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:460/proxy/: tls baz (200; 25.867552ms)
Aug 17 04:45:30.106: INFO: (11) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/rewriteme">test</a> (200; 28.536883ms)
Aug 17 04:45:30.108: INFO: (11) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 30.123772ms)
Aug 17 04:45:30.108: INFO: (11) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">test<... (200; 30.497077ms)
Aug 17 04:45:30.120: INFO: (11) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 42.263078ms)
Aug 17 04:45:30.120: INFO: (11) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:462/proxy/: tls qux (200; 42.578275ms)
Aug 17 04:45:30.120: INFO: (11) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">... (200; 42.625594ms)
Aug 17 04:45:30.125: INFO: (11) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 47.777399ms)
Aug 17 04:45:30.126: INFO: (11) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/tlsrewritem... (200; 47.875482ms)
Aug 17 04:45:30.126: INFO: (11) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 47.988036ms)
Aug 17 04:45:30.129: INFO: (11) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname2/proxy/: bar (200; 51.348135ms)
Aug 17 04:45:30.136: INFO: (11) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname1/proxy/: foo (200; 58.627339ms)
Aug 17 04:45:30.137: INFO: (11) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname2/proxy/: bar (200; 59.065331ms)
Aug 17 04:45:30.137: INFO: (11) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname1/proxy/: foo (200; 59.303988ms)
Aug 17 04:45:30.137: INFO: (11) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname2/proxy/: tls qux (200; 59.380101ms)
Aug 17 04:45:30.137: INFO: (11) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname1/proxy/: tls baz (200; 59.528265ms)
Aug 17 04:45:30.166: INFO: (12) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/tlsrewritem... (200; 28.461045ms)
Aug 17 04:45:30.169: INFO: (12) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">... (200; 31.56081ms)
Aug 17 04:45:30.177: INFO: (12) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">test<... (200; 38.952072ms)
Aug 17 04:45:30.177: INFO: (12) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:460/proxy/: tls baz (200; 39.410317ms)
Aug 17 04:45:30.178: INFO: (12) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/rewriteme">test</a> (200; 39.783718ms)
Aug 17 04:45:30.182: INFO: (12) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname2/proxy/: tls qux (200; 45.259086ms)
Aug 17 04:45:30.183: INFO: (12) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 44.826023ms)
Aug 17 04:45:30.183: INFO: (12) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 45.023821ms)
Aug 17 04:45:30.183: INFO: (12) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:462/proxy/: tls qux (200; 45.06444ms)
Aug 17 04:45:30.183: INFO: (12) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 45.098642ms)
Aug 17 04:45:30.183: INFO: (12) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 45.315912ms)
Aug 17 04:45:30.185: INFO: (12) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname2/proxy/: bar (200; 47.746938ms)
Aug 17 04:45:30.188: INFO: (12) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname1/proxy/: foo (200; 50.131273ms)
Aug 17 04:45:30.193: INFO: (12) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname2/proxy/: bar (200; 55.180634ms)
Aug 17 04:45:30.193: INFO: (12) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname1/proxy/: foo (200; 55.184592ms)
Aug 17 04:45:30.193: INFO: (12) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname1/proxy/: tls baz (200; 55.276348ms)
Aug 17 04:45:30.227: INFO: (13) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:460/proxy/: tls baz (200; 33.327515ms)
Aug 17 04:45:30.227: INFO: (13) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 33.542952ms)
Aug 17 04:45:30.227: INFO: (13) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/rewriteme">test</a> (200; 33.761964ms)
Aug 17 04:45:30.227: INFO: (13) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/tlsrewritem... (200; 33.619519ms)
Aug 17 04:45:30.232: INFO: (13) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 38.508486ms)
Aug 17 04:45:30.232: INFO: (13) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 38.682927ms)
Aug 17 04:45:30.233: INFO: (13) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">test<... (200; 39.013361ms)
Aug 17 04:45:30.233: INFO: (13) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname1/proxy/: tls baz (200; 39.650501ms)
Aug 17 04:45:30.233: INFO: (13) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 39.09883ms)
Aug 17 04:45:30.233: INFO: (13) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:462/proxy/: tls qux (200; 39.230514ms)
Aug 17 04:45:30.241: INFO: (13) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">... (200; 47.369191ms)
Aug 17 04:45:30.243: INFO: (13) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname2/proxy/: bar (200; 49.307726ms)
Aug 17 04:45:30.243: INFO: (13) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname2/proxy/: tls qux (200; 49.251471ms)
Aug 17 04:45:30.251: INFO: (13) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname2/proxy/: bar (200; 56.879311ms)
Aug 17 04:45:30.251: INFO: (13) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname1/proxy/: foo (200; 57.003021ms)
Aug 17 04:45:30.251: INFO: (13) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname1/proxy/: foo (200; 57.290319ms)
Aug 17 04:45:30.276: INFO: (14) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">test<... (200; 25.163132ms)
Aug 17 04:45:30.288: INFO: (14) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 36.575626ms)
Aug 17 04:45:30.288: INFO: (14) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 36.313957ms)
Aug 17 04:45:30.294: INFO: (14) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:462/proxy/: tls qux (200; 41.975109ms)
Aug 17 04:45:30.294: INFO: (14) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">... (200; 42.475561ms)
Aug 17 04:45:30.294: INFO: (14) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/tlsrewritem... (200; 41.837675ms)
Aug 17 04:45:30.297: INFO: (14) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/rewriteme">test</a> (200; 44.761622ms)
Aug 17 04:45:30.298: INFO: (14) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:460/proxy/: tls baz (200; 45.202072ms)
Aug 17 04:45:30.303: INFO: (14) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 51.305521ms)
Aug 17 04:45:30.304: INFO: (14) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 51.796875ms)
Aug 17 04:45:30.304: INFO: (14) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname1/proxy/: foo (200; 52.590737ms)
Aug 17 04:45:30.308: INFO: (14) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname2/proxy/: bar (200; 56.778847ms)
Aug 17 04:45:30.308: INFO: (14) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname1/proxy/: tls baz (200; 55.864221ms)
Aug 17 04:45:30.309: INFO: (14) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname2/proxy/: tls qux (200; 55.856336ms)
Aug 17 04:45:30.313: INFO: (14) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname2/proxy/: bar (200; 61.800917ms)
Aug 17 04:45:30.313: INFO: (14) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname1/proxy/: foo (200; 60.380441ms)
Aug 17 04:45:30.354: INFO: (15) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 40.02551ms)
Aug 17 04:45:30.355: INFO: (15) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/rewriteme">test</a> (200; 41.027568ms)
Aug 17 04:45:30.355: INFO: (15) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">test<... (200; 40.945903ms)
Aug 17 04:45:30.362: INFO: (15) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 46.68554ms)
Aug 17 04:45:30.362: INFO: (15) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname2/proxy/: tls qux (200; 48.276454ms)
Aug 17 04:45:30.362: INFO: (15) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:462/proxy/: tls qux (200; 47.031628ms)
Aug 17 04:45:30.362: INFO: (15) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 47.180523ms)
Aug 17 04:45:30.362: INFO: (15) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 46.642189ms)
Aug 17 04:45:30.362: INFO: (15) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/tlsrewritem... (200; 46.849929ms)
Aug 17 04:45:30.362: INFO: (15) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">... (200; 47.65056ms)
Aug 17 04:45:30.362: INFO: (15) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:460/proxy/: tls baz (200; 46.8058ms)
Aug 17 04:45:30.378: INFO: (15) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname1/proxy/: tls baz (200; 65.146249ms)
Aug 17 04:45:30.378: INFO: (15) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname2/proxy/: bar (200; 64.291741ms)
Aug 17 04:45:30.379: INFO: (15) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname1/proxy/: foo (200; 63.680423ms)
Aug 17 04:45:30.389: INFO: (15) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname1/proxy/: foo (200; 75.077942ms)
Aug 17 04:45:30.402: INFO: (15) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname2/proxy/: bar (200; 88.302479ms)
Aug 17 04:45:30.443: INFO: (16) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/tlsrewritem... (200; 39.906332ms)
Aug 17 04:45:30.448: INFO: (16) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 45.058547ms)
Aug 17 04:45:30.448: INFO: (16) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname2/proxy/: bar (200; 45.857951ms)
Aug 17 04:45:30.448: INFO: (16) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 45.620741ms)
Aug 17 04:45:30.456: INFO: (16) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:460/proxy/: tls baz (200; 52.785511ms)
Aug 17 04:45:30.456: INFO: (16) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 52.839244ms)
Aug 17 04:45:30.456: INFO: (16) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:462/proxy/: tls qux (200; 53.08612ms)
Aug 17 04:45:30.456: INFO: (16) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/rewriteme">test</a> (200; 53.126338ms)
Aug 17 04:45:30.456: INFO: (16) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">... (200; 53.274691ms)
Aug 17 04:45:30.456: INFO: (16) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 53.376223ms)
Aug 17 04:45:30.456: INFO: (16) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">test<... (200; 53.287194ms)
Aug 17 04:45:30.461: INFO: (16) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname1/proxy/: foo (200; 57.482789ms)
Aug 17 04:45:30.467: INFO: (16) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname2/proxy/: bar (200; 63.74921ms)
Aug 17 04:45:30.468: INFO: (16) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname1/proxy/: tls baz (200; 65.061199ms)
Aug 17 04:45:30.468: INFO: (16) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname2/proxy/: tls qux (200; 65.342251ms)
Aug 17 04:45:30.474: INFO: (16) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname1/proxy/: foo (200; 71.310517ms)
Aug 17 04:45:30.504: INFO: (17) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 29.546059ms)
Aug 17 04:45:30.515: INFO: (17) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/rewriteme">test</a> (200; 40.981156ms)
Aug 17 04:45:30.517: INFO: (17) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 41.998632ms)
Aug 17 04:45:30.518: INFO: (17) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:460/proxy/: tls baz (200; 42.924529ms)
Aug 17 04:45:30.527: INFO: (17) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/tlsrewritem... (200; 51.937395ms)
Aug 17 04:45:30.527: INFO: (17) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">... (200; 52.583532ms)
Aug 17 04:45:30.527: INFO: (17) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 52.308846ms)
Aug 17 04:45:30.528: INFO: (17) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:462/proxy/: tls qux (200; 52.635943ms)
Aug 17 04:45:30.528: INFO: (17) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 52.776509ms)
Aug 17 04:45:30.532: INFO: (17) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">test<... (200; 57.151964ms)
Aug 17 04:45:30.540: INFO: (17) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname2/proxy/: bar (200; 64.995647ms)
Aug 17 04:45:30.541: INFO: (17) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname1/proxy/: foo (200; 65.507491ms)
Aug 17 04:45:30.541: INFO: (17) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname2/proxy/: bar (200; 65.791435ms)
Aug 17 04:45:30.541: INFO: (17) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname2/proxy/: tls qux (200; 65.635412ms)
Aug 17 04:45:30.541: INFO: (17) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname1/proxy/: tls baz (200; 65.777772ms)
Aug 17 04:45:30.541: INFO: (17) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname1/proxy/: foo (200; 66.092601ms)
Aug 17 04:45:30.575: INFO: (18) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">test<... (200; 33.493705ms)
Aug 17 04:45:30.579: INFO: (18) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 37.406436ms)
Aug 17 04:45:30.580: INFO: (18) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">... (200; 38.804382ms)
Aug 17 04:45:30.585: INFO: (18) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 43.267263ms)
Aug 17 04:45:30.593: INFO: (18) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 51.247855ms)
Aug 17 04:45:30.593: INFO: (18) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 51.502021ms)
Aug 17 04:45:30.593: INFO: (18) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/tlsrewritem... (200; 51.366534ms)
Aug 17 04:45:30.603: INFO: (18) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname2/proxy/: tls qux (200; 61.289851ms)
Aug 17 04:45:30.603: INFO: (18) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:462/proxy/: tls qux (200; 61.577826ms)
Aug 17 04:45:30.603: INFO: (18) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:460/proxy/: tls baz (200; 61.491251ms)
Aug 17 04:45:30.603: INFO: (18) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/rewriteme">test</a> (200; 61.484546ms)
Aug 17 04:45:30.603: INFO: (18) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname2/proxy/: bar (200; 61.70931ms)
Aug 17 04:45:30.603: INFO: (18) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname1/proxy/: foo (200; 61.820981ms)
Aug 17 04:45:30.603: INFO: (18) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname2/proxy/: bar (200; 61.803723ms)
Aug 17 04:45:30.611: INFO: (18) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname1/proxy/: tls baz (200; 69.245881ms)
Aug 17 04:45:30.611: INFO: (18) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname1/proxy/: foo (200; 69.40569ms)
Aug 17 04:45:30.647: INFO: (19) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:460/proxy/: tls baz (200; 35.377292ms)
Aug 17 04:45:30.657: INFO: (19) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 45.586686ms)
Aug 17 04:45:30.657: INFO: (19) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">... (200; 45.635483ms)
Aug 17 04:45:30.658: INFO: (19) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:1080/proxy/rewriteme">test<... (200; 46.281317ms)
Aug 17 04:45:30.666: INFO: (19) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname1/proxy/: foo (200; 55.161884ms)
Aug 17 04:45:30.667: INFO: (19) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:160/proxy/: foo (200; 54.880221ms)
Aug 17 04:45:30.667: INFO: (19) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 55.109148ms)
Aug 17 04:45:30.667: INFO: (19) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:443/proxy/tlsrewritem... (200; 55.124547ms)
Aug 17 04:45:30.667: INFO: (19) /api/v1/namespaces/proxy-6490/pods/http:proxy-service-mvlsp-vqf8m:162/proxy/: bar (200; 55.112828ms)
Aug 17 04:45:30.667: INFO: (19) /api/v1/namespaces/proxy-6490/pods/https:proxy-service-mvlsp-vqf8m:462/proxy/: tls qux (200; 55.20285ms)
Aug 17 04:45:30.667: INFO: (19) /api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/: <a href="/api/v1/namespaces/proxy-6490/pods/proxy-service-mvlsp-vqf8m/proxy/rewriteme">test</a> (200; 55.304926ms)
Aug 17 04:45:30.667: INFO: (19) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname1/proxy/: tls baz (200; 56.015577ms)
Aug 17 04:45:30.668: INFO: (19) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname1/proxy/: foo (200; 56.41413ms)
Aug 17 04:45:30.668: INFO: (19) /api/v1/namespaces/proxy-6490/services/proxy-service-mvlsp:portname2/proxy/: bar (200; 56.273008ms)
Aug 17 04:45:30.675: INFO: (19) /api/v1/namespaces/proxy-6490/services/https:proxy-service-mvlsp:tlsportname2/proxy/: tls qux (200; 63.200731ms)
Aug 17 04:45:30.675: INFO: (19) /api/v1/namespaces/proxy-6490/services/http:proxy-service-mvlsp:portname2/proxy/: bar (200; 63.410697ms)
STEP: deleting ReplicationController proxy-service-mvlsp in namespace proxy-6490, will wait for the garbage collector to delete the pods
Aug 17 04:45:30.808: INFO: Deleting ReplicationController proxy-service-mvlsp took: 64.446989ms
Aug 17 04:45:31.508: INFO: Terminating ReplicationController proxy-service-mvlsp pods took: 700.3665ms
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:45:42.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6490" for this suite.
Aug 17 04:45:52.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:45:53.921: INFO: namespace proxy-6490 deletion completed in 11.478214199s

• [SLOW TEST:31.985 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:45:53.921: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Aug 17 04:45:58.923: INFO: Successfully updated pod "annotationupdate5ddcb564-31d9-49f3-a15e-0c2cab644be1"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:46:00.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2199" for this suite.
Aug 17 04:46:17.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:46:21.078: INFO: namespace projected-2199 deletion completed in 20.010441211s

• [SLOW TEST:27.156 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:46:21.078: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Aug 17 04:46:21.286: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 17 04:46:21.411: INFO: Waiting for terminating namespaces to be deleted...
Aug 17 04:46:21.442: INFO: 
Logging pods the kubelet thinks is on node 10.241.148.31 before test
Aug 17 04:46:21.652: INFO: router-default-57d78dfb48-mzjlc from openshift-ingress started at 2020-08-17 02:59:05 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.652: INFO: 	Container router ready: true, restart count 0
Aug 17 04:46:21.653: INFO: calico-typha-579947cf56-672t9 from calico-system started at 2020-08-17 02:59:26 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container calico-typha ready: true, restart count 0
Aug 17 04:46:21.653: INFO: sonobuoy-systemd-logs-daemon-set-c8f140ca93fa466f-pjc77 from sonobuoy started at 2020-08-17 04:12:38 +0000 UTC (2 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 04:46:21.653: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 04:46:21.653: INFO: multus-admission-controller-ncsvb from openshift-multus started at 2020-08-17 02:57:44 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 17 04:46:21.653: INFO: configmap-cabundle-injector-8446d4b88f-9k4sq from openshift-service-ca started at 2020-08-17 02:58:07 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Aug 17 04:46:21.653: INFO: kube-state-metrics-c5f65645-855s8 from openshift-monitoring started at 2020-08-17 02:58:30 +0000 UTC (3 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 17 04:46:21.653: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 17 04:46:21.653: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 17 04:46:21.653: INFO: image-registry-5669fd49dd-xphdf from openshift-image-registry started at 2020-08-17 03:00:49 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container registry ready: true, restart count 0
Aug 17 04:46:21.653: INFO: service-serving-cert-signer-7879bf8d9f-xtqt8 from openshift-service-ca started at 2020-08-17 02:58:07 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Aug 17 04:46:21.653: INFO: node-ca-2lszv from openshift-image-registry started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container node-ca ready: true, restart count 0
Aug 17 04:46:21.653: INFO: packageserver-66768b6f89-b74q9 from openshift-operator-lifecycle-manager started at 2020-08-17 03:05:22 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container packageserver ready: true, restart count 0
Aug 17 04:46:21.653: INFO: ibm-keepalived-watcher-ncgwx from kube-system started at 2020-08-17 02:56:14 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 17 04:46:21.653: INFO: node-exporter-kkwj6 from openshift-monitoring started at 2020-08-17 02:58:34 +0000 UTC (2 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:46:21.653: INFO: 	Container node-exporter ready: true, restart count 0
Aug 17 04:46:21.653: INFO: dns-default-sxjmd from openshift-dns started at 2020-08-17 02:59:01 +0000 UTC (2 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container dns ready: true, restart count 0
Aug 17 04:46:21.653: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 17 04:46:21.653: INFO: prometheus-operator-56d9d699cb-m5pmc from openshift-monitoring started at 2020-08-17 03:04:39 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 17 04:46:21.653: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-08-17 03:05:11 +0000 UTC (3 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container alertmanager ready: true, restart count 0
Aug 17 04:46:21.653: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 17 04:46:21.653: INFO: 	Container config-reloader ready: true, restart count 0
Aug 17 04:46:21.653: INFO: ibm-master-proxy-static-10.241.148.31 from kube-system started at 2020-08-17 02:56:11 +0000 UTC (2 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 17 04:46:21.653: INFO: 	Container pause ready: true, restart count 0
Aug 17 04:46:21.653: INFO: prometheus-adapter-67dbcc5cfc-xk5l6 from openshift-monitoring started at 2020-08-17 02:58:37 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 17 04:46:21.653: INFO: community-operators-65d9d5c9d6-zh8cl from openshift-marketplace started at 2020-08-17 02:59:24 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container community-operators ready: true, restart count 0
Aug 17 04:46:21.653: INFO: openshift-kube-proxy-5nh74 from openshift-kube-proxy started at 2020-08-17 02:56:45 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 17 04:46:21.653: INFO: thanos-querier-6b5fbd9754-ctb66 from openshift-monitoring started at 2020-08-17 03:05:37 +0000 UTC (4 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:46:21.653: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 17 04:46:21.653: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 17 04:46:21.653: INFO: 	Container thanos-querier ready: true, restart count 0
Aug 17 04:46:21.653: INFO: tuned-nmn6j from openshift-cluster-node-tuning-operator started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container tuned ready: true, restart count 0
Aug 17 04:46:21.653: INFO: multus-tvx65 from openshift-multus started at 2020-08-17 02:56:38 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container kube-multus ready: true, restart count 0
Aug 17 04:46:21.653: INFO: calico-node-lm8qf from calico-system started at 2020-08-17 02:57:31 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container calico-node ready: true, restart count 0
Aug 17 04:46:21.653: INFO: cluster-samples-operator-55944b8f44-fxlq8 from openshift-cluster-samples-operator started at 2020-08-17 02:59:01 +0000 UTC (2 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Aug 17 04:46:21.653: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Aug 17 04:46:21.653: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-08-17 03:05:54 +0000 UTC (7 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:46:21.653: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 17 04:46:21.653: INFO: 	Container prometheus ready: true, restart count 1
Aug 17 04:46:21.653: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 17 04:46:21.653: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 17 04:46:21.653: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 17 04:46:21.653: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 17 04:46:21.653: INFO: ibm-cloud-provider-ip-169-48-239-26-5595944bf9-n8nts from ibm-system started at 2020-08-17 03:02:23 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container ibm-cloud-provider-ip-169-48-239-26 ready: true, restart count 0
Aug 17 04:46:21.653: INFO: console-76d8dc5894-2w62x from openshift-console started at 2020-08-17 02:59:39 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container console ready: true, restart count 0
Aug 17 04:46:21.653: INFO: ibmcloud-block-storage-driver-hfn98 from kube-system started at 2020-08-17 02:56:23 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.653: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 17 04:46:21.653: INFO: 
Logging pods the kubelet thinks is on node 10.241.148.42 before test
Aug 17 04:46:21.778: INFO: sonobuoy-e2e-job-144f6878906f41e3 from sonobuoy started at 2020-08-17 04:12:38 +0000 UTC (2 container statuses recorded)
Aug 17 04:46:21.778: INFO: 	Container e2e ready: true, restart count 0
Aug 17 04:46:21.778: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 04:46:21.778: INFO: sonobuoy-systemd-logs-daemon-set-c8f140ca93fa466f-qhfmq from sonobuoy started at 2020-08-17 04:12:38 +0000 UTC (2 container statuses recorded)
Aug 17 04:46:21.778: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 04:46:21.778: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 04:46:21.778: INFO: openshift-state-metrics-5849d797d8-h6gdr from openshift-monitoring started at 2020-08-17 02:58:31 +0000 UTC (3 container statuses recorded)
Aug 17 04:46:21.778: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 17 04:46:21.778: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 17 04:46:21.778: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 17 04:46:21.778: INFO: sonobuoy from sonobuoy started at 2020-08-17 04:12:31 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.778: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 17 04:46:21.778: INFO: grafana-c9c7455d7-phgf6 from openshift-monitoring started at 2020-08-17 03:04:57 +0000 UTC (2 container statuses recorded)
Aug 17 04:46:21.778: INFO: 	Container grafana ready: true, restart count 0
Aug 17 04:46:21.778: INFO: 	Container grafana-proxy ready: true, restart count 0
Aug 17 04:46:21.778: INFO: node-ca-j567q from openshift-image-registry started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.778: INFO: 	Container node-ca ready: true, restart count 0
Aug 17 04:46:21.778: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-08-17 03:05:18 +0000 UTC (3 container statuses recorded)
Aug 17 04:46:21.778: INFO: 	Container alertmanager ready: true, restart count 0
Aug 17 04:46:21.778: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 17 04:46:21.778: INFO: 	Container config-reloader ready: true, restart count 0
Aug 17 04:46:21.778: INFO: ibm-keepalived-watcher-xf2gw from kube-system started at 2020-08-17 02:56:05 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.778: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 17 04:46:21.778: INFO: multus-admission-controller-bskrp from openshift-multus started at 2020-08-17 02:57:45 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.778: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 17 04:46:21.778: INFO: prometheus-adapter-67dbcc5cfc-5kt76 from openshift-monitoring started at 2020-08-17 02:58:37 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.778: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 17 04:46:21.778: INFO: certified-operators-549fcd7d77-lx6ml from openshift-marketplace started at 2020-08-17 02:59:24 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.778: INFO: 	Container certified-operators ready: true, restart count 0
Aug 17 04:46:21.778: INFO: tigera-operator-679798d94d-qrlds from tigera-operator started at 2020-08-17 02:56:13 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.778: INFO: 	Container tigera-operator ready: true, restart count 2
Aug 17 04:46:21.778: INFO: registry-pvc-permissions-j8h5n from openshift-image-registry started at 2020-08-17 03:00:49 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.778: INFO: 	Container pvc-permissions ready: false, restart count 0
Aug 17 04:46:21.778: INFO: calico-node-cxl67 from calico-system started at 2020-08-17 02:57:31 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.778: INFO: 	Container calico-node ready: true, restart count 0
Aug 17 04:46:21.778: INFO: tuned-b665k from openshift-cluster-node-tuning-operator started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.778: INFO: 	Container tuned ready: true, restart count 0
Aug 17 04:46:21.779: INFO: dns-default-kqx2s from openshift-dns started at 2020-08-17 02:59:01 +0000 UTC (2 container statuses recorded)
Aug 17 04:46:21.779: INFO: 	Container dns ready: true, restart count 0
Aug 17 04:46:21.779: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 17 04:46:21.779: INFO: network-operator-7986644c85-9jpc4 from openshift-network-operator started at 2020-08-17 02:56:13 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.779: INFO: 	Container network-operator ready: true, restart count 0
Aug 17 04:46:21.779: INFO: ibmcloud-block-storage-driver-5dth8 from kube-system started at 2020-08-17 02:56:14 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.779: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 17 04:46:21.779: INFO: openshift-kube-proxy-hswpd from openshift-kube-proxy started at 2020-08-17 02:56:45 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.779: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 17 04:46:21.779: INFO: telemeter-client-76746b8bb7-hh5jv from openshift-monitoring started at 2020-08-17 03:04:49 +0000 UTC (3 container statuses recorded)
Aug 17 04:46:21.779: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:46:21.779: INFO: 	Container reload ready: true, restart count 0
Aug 17 04:46:21.779: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 17 04:46:21.779: INFO: console-76d8dc5894-xnhrt from openshift-console started at 2020-08-17 02:59:20 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.779: INFO: 	Container console ready: true, restart count 0
Aug 17 04:46:21.779: INFO: apiservice-cabundle-injector-594fd4555f-j2v5w from openshift-service-ca started at 2020-08-17 02:58:07 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.779: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Aug 17 04:46:21.779: INFO: node-exporter-r6g64 from openshift-monitoring started at 2020-08-17 02:58:34 +0000 UTC (2 container statuses recorded)
Aug 17 04:46:21.779: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:46:21.779: INFO: 	Container node-exporter ready: true, restart count 0
Aug 17 04:46:21.779: INFO: router-default-57d78dfb48-8c54l from openshift-ingress started at 2020-08-17 02:59:05 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.779: INFO: 	Container router ready: true, restart count 0
Aug 17 04:46:21.779: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-08-17 03:01:11 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.779: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Aug 17 04:46:21.779: INFO: ibm-master-proxy-static-10.241.148.42 from kube-system started at 2020-08-17 02:56:02 +0000 UTC (2 container statuses recorded)
Aug 17 04:46:21.779: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 17 04:46:21.779: INFO: 	Container pause ready: true, restart count 0
Aug 17 04:46:21.779: INFO: multus-p85fw from openshift-multus started at 2020-08-17 02:56:38 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.779: INFO: 	Container kube-multus ready: true, restart count 0
Aug 17 04:46:21.779: INFO: calico-typha-579947cf56-86wz4 from calico-system started at 2020-08-17 02:57:30 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.779: INFO: 	Container calico-typha ready: true, restart count 0
Aug 17 04:46:21.779: INFO: 
Logging pods the kubelet thinks is on node 10.241.148.50 before test
Aug 17 04:46:21.971: INFO: sonobuoy-systemd-logs-daemon-set-c8f140ca93fa466f-j84lz from sonobuoy started at 2020-08-17 04:12:38 +0000 UTC (2 container statuses recorded)
Aug 17 04:46:21.971: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 04:46:21.972: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 04:46:21.972: INFO: vpn-7b59d9f64-2rnfd from kube-system started at 2020-08-17 03:05:11 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container vpn ready: true, restart count 0
Aug 17 04:46:21.972: INFO: ibm-storage-watcher-77bf8b889-856t2 from kube-system started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 17 04:46:21.972: INFO: downloads-678f5d6564-t5r44 from openshift-console started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container download-server ready: true, restart count 0
Aug 17 04:46:21.972: INFO: node-exporter-ztsvb from openshift-monitoring started at 2020-08-17 02:58:33 +0000 UTC (2 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:46:21.972: INFO: 	Container node-exporter ready: true, restart count 0
Aug 17 04:46:21.972: INFO: calico-typha-579947cf56-87rgj from calico-system started at 2020-08-17 02:59:26 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container calico-typha ready: true, restart count 0
Aug 17 04:46:21.972: INFO: catalog-operator-85f6c659cc-6zpxp from openshift-operator-lifecycle-manager started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 17 04:46:21.972: INFO: multus-admission-controller-7xrg7 from openshift-multus started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 17 04:46:21.972: INFO: tuned-6qthk from openshift-cluster-node-tuning-operator started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container tuned ready: true, restart count 0
Aug 17 04:46:21.972: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-08-17 03:04:54 +0000 UTC (3 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container alertmanager ready: true, restart count 0
Aug 17 04:46:21.972: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 17 04:46:21.972: INFO: 	Container config-reloader ready: true, restart count 0
Aug 17 04:46:21.972: INFO: ibmcloud-block-storage-driver-6l6kb from kube-system started at 2020-08-17 02:56:30 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 17 04:46:21.972: INFO: service-ca-operator-694cfbf5d5-2dhf8 from openshift-service-ca-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container operator ready: true, restart count 0
Aug 17 04:46:21.972: INFO: olm-operator-b5f57cdbb-wcj4n from openshift-operator-lifecycle-manager started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container olm-operator ready: true, restart count 0
Aug 17 04:46:21.972: INFO: calico-kube-controllers-79d75767dd-hhcw2 from calico-system started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 17 04:46:21.972: INFO: ibmcloud-block-storage-plugin-68d5c65db9-m8qcr from kube-system started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Aug 17 04:46:21.972: INFO: ibm-cloud-provider-ip-169-48-239-26-5595944bf9-2pxq9 from ibm-system started at 2020-08-17 03:02:20 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container ibm-cloud-provider-ip-169-48-239-26 ready: true, restart count 0
Aug 17 04:46:21.972: INFO: openshift-kube-proxy-grn9g from openshift-kube-proxy started at 2020-08-17 02:56:45 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 17 04:46:21.972: INFO: calico-node-hmdtm from calico-system started at 2020-08-17 02:57:31 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container calico-node ready: true, restart count 0
Aug 17 04:46:21.972: INFO: cluster-monitoring-operator-5b5659466f-bd9vb from openshift-monitoring started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Aug 17 04:46:21.972: INFO: cluster-image-registry-operator-6cfd58b66c-xpvcp from openshift-image-registry started at 2020-08-17 02:57:42 +0000 UTC (2 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Aug 17 04:46:21.972: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Aug 17 04:46:21.972: INFO: ibm-master-proxy-static-10.241.148.50 from kube-system started at 2020-08-17 02:56:19 +0000 UTC (2 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 17 04:46:21.972: INFO: 	Container pause ready: true, restart count 0
Aug 17 04:46:21.972: INFO: ibm-keepalived-watcher-x5mxk from kube-system started at 2020-08-17 02:56:22 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 17 04:46:21.972: INFO: cluster-storage-operator-557b75f8d5-899qb from openshift-cluster-storage-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Aug 17 04:46:21.972: INFO: cluster-node-tuning-operator-b5f884945-nmndb from openshift-cluster-node-tuning-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Aug 17 04:46:21.972: INFO: dns-default-kfbzc from openshift-dns started at 2020-08-17 02:59:02 +0000 UTC (2 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container dns ready: true, restart count 0
Aug 17 04:46:21.972: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 17 04:46:21.972: INFO: node-ca-467qn from openshift-image-registry started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container node-ca ready: true, restart count 0
Aug 17 04:46:21.972: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-08-17 03:05:34 +0000 UTC (7 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:46:21.972: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 17 04:46:21.972: INFO: 	Container prometheus ready: true, restart count 1
Aug 17 04:46:21.972: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 17 04:46:21.972: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 17 04:46:21.972: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 17 04:46:21.972: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 17 04:46:21.972: INFO: multus-wkz57 from openshift-multus started at 2020-08-17 02:56:38 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container kube-multus ready: true, restart count 0
Aug 17 04:46:21.972: INFO: ibm-file-plugin-fdb69b446-bzpn7 from kube-system started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 17 04:46:21.972: INFO: openshift-service-catalog-controller-manager-operator-5496wjhfr from openshift-service-catalog-controller-manager-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container operator ready: true, restart count 1
Aug 17 04:46:21.972: INFO: marketplace-operator-6957767d58-6rcnt from openshift-marketplace started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container marketplace-operator ready: true, restart count 0
Aug 17 04:46:21.972: INFO: dns-operator-6f9cf66db7-ksmws from openshift-dns-operator started at 2020-08-17 02:57:44 +0000 UTC (2 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container dns-operator ready: true, restart count 0
Aug 17 04:46:21.972: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:46:21.972: INFO: redhat-operators-85f96bd7c-69xgq from openshift-marketplace started at 2020-08-17 02:59:24 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container redhat-operators ready: true, restart count 0
Aug 17 04:46:21.972: INFO: packageserver-66768b6f89-hjpwd from openshift-operator-lifecycle-manager started at 2020-08-17 03:05:13 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container packageserver ready: true, restart count 0
Aug 17 04:46:21.972: INFO: thanos-querier-6b5fbd9754-wzzrz from openshift-monitoring started at 2020-08-17 03:05:26 +0000 UTC (4 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:46:21.972: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 17 04:46:21.972: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 17 04:46:21.972: INFO: 	Container thanos-querier ready: true, restart count 0
Aug 17 04:46:21.972: INFO: console-operator-9878d4766-7tcvg from openshift-console-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container console-operator ready: true, restart count 1
Aug 17 04:46:21.972: INFO: openshift-service-catalog-apiserver-operator-78b9dddb6f-bbw72 from openshift-service-catalog-apiserver-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container operator ready: true, restart count 1
Aug 17 04:46:21.972: INFO: ingress-operator-695bc545b9-d4trd from openshift-ingress-operator started at 2020-08-17 02:57:42 +0000 UTC (2 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container ingress-operator ready: true, restart count 0
Aug 17 04:46:21.972: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:46:21.972: INFO: downloads-678f5d6564-wxxs4 from openshift-console started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 04:46:21.972: INFO: 	Container download-server ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-9fe172be-0638-4e7f-9bd8-4960a7b6a6a0 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-9fe172be-0638-4e7f-9bd8-4960a7b6a6a0 off the node 10.241.148.42
STEP: verifying the node doesn't have the label kubernetes.io/e2e-9fe172be-0638-4e7f-9bd8-4960a7b6a6a0
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:46:38.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1817" for this suite.
Aug 17 04:47:04.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:47:07.289: INFO: namespace sched-pred-1817 deletion completed in 28.639199116s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:46.211 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:47:07.290: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 17 04:47:07.560: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9430d373-d6ae-4c30-8175-95bfc8f02372" in namespace "projected-6892" to be "success or failure"
Aug 17 04:47:07.585: INFO: Pod "downwardapi-volume-9430d373-d6ae-4c30-8175-95bfc8f02372": Phase="Pending", Reason="", readiness=false. Elapsed: 25.144933ms
Aug 17 04:47:09.613: INFO: Pod "downwardapi-volume-9430d373-d6ae-4c30-8175-95bfc8f02372": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052701392s
Aug 17 04:47:11.633: INFO: Pod "downwardapi-volume-9430d373-d6ae-4c30-8175-95bfc8f02372": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.07320699s
STEP: Saw pod success
Aug 17 04:47:11.633: INFO: Pod "downwardapi-volume-9430d373-d6ae-4c30-8175-95bfc8f02372" satisfied condition "success or failure"
Aug 17 04:47:11.655: INFO: Trying to get logs from node 10.241.148.31 pod downwardapi-volume-9430d373-d6ae-4c30-8175-95bfc8f02372 container client-container: <nil>
STEP: delete the pod
Aug 17 04:47:11.753: INFO: Waiting for pod downwardapi-volume-9430d373-d6ae-4c30-8175-95bfc8f02372 to disappear
Aug 17 04:47:11.775: INFO: Pod downwardapi-volume-9430d373-d6ae-4c30-8175-95bfc8f02372 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:47:11.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6892" for this suite.
Aug 17 04:47:21.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:47:24.361: INFO: namespace projected-6892 deletion completed in 12.527940132s

• [SLOW TEST:17.071 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:47:24.361: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Aug 17 04:47:24.645: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-667650646 proxy --unix-socket=/tmp/kubectl-proxy-unix395825485/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:47:24.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9046" for this suite.
Aug 17 04:47:34.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:47:37.594: INFO: namespace kubectl-9046 deletion completed in 12.793786418s

• [SLOW TEST:13.233 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:47:37.595: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 04:47:37.849: INFO: Creating deployment "webserver-deployment"
Aug 17 04:47:37.874: INFO: Waiting for observed generation 1
Aug 17 04:47:39.927: INFO: Waiting for all required pods to come up
Aug 17 04:47:40.001: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Aug 17 04:47:42.105: INFO: Waiting for deployment "webserver-deployment" to complete
Aug 17 04:47:42.137: INFO: Updating deployment "webserver-deployment" with a non-existent image
Aug 17 04:47:42.248: INFO: Updating deployment webserver-deployment
Aug 17 04:47:42.248: INFO: Waiting for observed generation 2
Aug 17 04:47:44.275: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 17 04:47:44.293: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 17 04:47:44.309: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 17 04:47:44.360: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 17 04:47:44.360: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 17 04:47:44.383: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 17 04:47:44.418: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Aug 17 04:47:44.418: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Aug 17 04:47:44.452: INFO: Updating deployment webserver-deployment
Aug 17 04:47:44.452: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Aug 17 04:47:44.534: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 17 04:47:44.562: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Aug 17 04:47:46.794: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5384 /apis/apps/v1/namespaces/deployment-5384/deployments/webserver-deployment 4c585640-f1c4-4d77-b99e-e058d1def27d 50158 3 2020-08-17 04:47:37 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc009f78e48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-08-17 04:47:44 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-08-17 04:47:45 +0000 UTC,LastTransitionTime:2020-08-17 04:47:37 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Aug 17 04:47:46.894: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-5384 /apis/apps/v1/namespaces/deployment-5384/replicasets/webserver-deployment-c7997dcc8 c3ab0601-81fc-4bd7-9f21-bf14ac23e1d1 50156 3 2020-08-17 04:47:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 4c585640-f1c4-4d77-b99e-e058d1def27d 0xc009f79367 0xc009f79368}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc009f793d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 17 04:47:46.895: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Aug 17 04:47:46.895: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-5384 /apis/apps/v1/namespaces/deployment-5384/replicasets/webserver-deployment-595b5b9587 3695f51f-6e34-4819-ae3a-6dd905fc7887 50083 3 2020-08-17 04:47:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 4c585640-f1c4-4d77-b99e-e058d1def27d 0xc009f792a7 0xc009f792a8}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc009f79308 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Aug 17 04:47:47.476: INFO: Pod "webserver-deployment-595b5b9587-2hgrn" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2hgrn webserver-deployment-595b5b9587- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-595b5b9587-2hgrn 4c81c8b9-370b-43d2-89ab-16574e3a2030 50071 0 2020-08-17 04:47:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3695f51f-6e34-4819-ae3a-6dd905fc7887 0xc0039e9e57 0xc0039e9e58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.42,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.42,PodIP:,StartTime:2020-08-17 04:47:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.477: INFO: Pod "webserver-deployment-595b5b9587-2m42h" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2m42h webserver-deployment-595b5b9587- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-595b5b9587-2m42h bbaae69c-ff3e-4983-9124-200508ec27cb 50175 0 2020-08-17 04:47:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.218.115/32 cni.projectcalico.org/podIPs:172.30.218.115/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.218.115"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3695f51f-6e34-4819-ae3a-6dd905fc7887 0xc0039e9fd7 0xc0039e9fd8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.42,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.42,PodIP:,StartTime:2020-08-17 04:47:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.477: INFO: Pod "webserver-deployment-595b5b9587-4g6ws" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4g6ws webserver-deployment-595b5b9587- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-595b5b9587-4g6ws 643a22ab-4db8-43c4-ba70-a8fe97f9036c 49901 0 2020-08-17 04:47:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.201.244/32 cni.projectcalico.org/podIPs:172.30.201.244/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.201.244"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3695f51f-6e34-4819-ae3a-6dd905fc7887 0xc00387a157 0xc00387a158}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.31,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.31,PodIP:172.30.201.244,StartTime:2020-08-17 04:47:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-17 04:47:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://3003728e7fc5dca40664b349fce92804d3d31290cf81bc42ba3885b5d9567d2c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.201.244,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.478: INFO: Pod "webserver-deployment-595b5b9587-8hpnl" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-8hpnl webserver-deployment-595b5b9587- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-595b5b9587-8hpnl 27788058-1b7c-4b9d-aca9-ac807234ef98 50231 0 2020-08-17 04:47:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.201.250/32 cni.projectcalico.org/podIPs:172.30.201.250/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.201.250"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3695f51f-6e34-4819-ae3a-6dd905fc7887 0xc00387a2f7 0xc00387a2f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.31,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.31,PodIP:,StartTime:2020-08-17 04:47:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.478: INFO: Pod "webserver-deployment-595b5b9587-b5cnh" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-b5cnh webserver-deployment-595b5b9587- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-595b5b9587-b5cnh 323bbe2b-9e69-4acf-a2e4-af2f62cb5749 49885 0 2020-08-17 04:47:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.151.182/32 cni.projectcalico.org/podIPs:172.30.151.182/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.151.182"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3695f51f-6e34-4819-ae3a-6dd905fc7887 0xc00387a477 0xc00387a478}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.50,PodIP:172.30.151.182,StartTime:2020-08-17 04:47:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-17 04:47:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://27fa18632f540240b7bf29e81714320be7d0bdf6fa0dd99705f7394b4117fb61,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.151.182,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.481: INFO: Pod "webserver-deployment-595b5b9587-b6htc" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-b6htc webserver-deployment-595b5b9587- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-595b5b9587-b6htc 97706df2-bf28-40e7-8d53-e3a8aa422f66 50205 0 2020-08-17 04:47:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.218.117/32 cni.projectcalico.org/podIPs:172.30.218.117/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.218.117"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3695f51f-6e34-4819-ae3a-6dd905fc7887 0xc00387a617 0xc00387a618}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.42,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.42,PodIP:,StartTime:2020-08-17 04:47:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.481: INFO: Pod "webserver-deployment-595b5b9587-bjntn" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bjntn webserver-deployment-595b5b9587- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-595b5b9587-bjntn c1b6a61d-d839-4303-90ec-25f64261953e 49871 0 2020-08-17 04:47:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.218.110/32 cni.projectcalico.org/podIPs:172.30.218.110/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.218.110"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3695f51f-6e34-4819-ae3a-6dd905fc7887 0xc00387a797 0xc00387a798}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.42,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.42,PodIP:172.30.218.110,StartTime:2020-08-17 04:47:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-17 04:47:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://90005838b733f3afb5f19c9a0487855b95e68fd2fb747fe6b2b402d53155f4ee,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.218.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.482: INFO: Pod "webserver-deployment-595b5b9587-dnvqt" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-dnvqt webserver-deployment-595b5b9587- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-595b5b9587-dnvqt 0cbce453-64a3-437b-a0d4-f8467cf380ae 49898 0 2020-08-17 04:47:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.201.245/32 cni.projectcalico.org/podIPs:172.30.201.245/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.201.245"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3695f51f-6e34-4819-ae3a-6dd905fc7887 0xc00387a937 0xc00387a938}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.31,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.31,PodIP:172.30.201.245,StartTime:2020-08-17 04:47:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-17 04:47:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://f78ac4a7e3168de4da1ea8f8674434a7f8fd982ec9003a123743aaa3c686b275,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.201.245,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.482: INFO: Pod "webserver-deployment-595b5b9587-f7fnq" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-f7fnq webserver-deployment-595b5b9587- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-595b5b9587-f7fnq 11faf24e-2652-4601-b2fc-919560f833b3 49877 0 2020-08-17 04:47:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.151.183/32 cni.projectcalico.org/podIPs:172.30.151.183/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.151.183"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3695f51f-6e34-4819-ae3a-6dd905fc7887 0xc00387aad7 0xc00387aad8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.50,PodIP:172.30.151.183,StartTime:2020-08-17 04:47:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-17 04:47:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://69b5dac8b706c4daf37511ad01d399937a170cc449de64321703311e563e5874,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.151.183,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.483: INFO: Pod "webserver-deployment-595b5b9587-f8rh4" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-f8rh4 webserver-deployment-595b5b9587- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-595b5b9587-f8rh4 bf3a4cb9-8316-4ed6-b1b0-697cd18dda6d 50098 0 2020-08-17 04:47:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3695f51f-6e34-4819-ae3a-6dd905fc7887 0xc00387ac77 0xc00387ac78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.50,PodIP:,StartTime:2020-08-17 04:47:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.483: INFO: Pod "webserver-deployment-595b5b9587-fbjnm" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-fbjnm webserver-deployment-595b5b9587- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-595b5b9587-fbjnm d86c89d8-60a7-4fbd-bf53-9188f197af6c 49911 0 2020-08-17 04:47:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.151.184/32 cni.projectcalico.org/podIPs:172.30.151.184/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.151.184"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3695f51f-6e34-4819-ae3a-6dd905fc7887 0xc00387adf7 0xc00387adf8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.50,PodIP:172.30.151.184,StartTime:2020-08-17 04:47:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-17 04:47:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://1e27795772d62f4a9ab1d6f80467d151b545195099bb3b7869a867c3fc411605,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.151.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.484: INFO: Pod "webserver-deployment-595b5b9587-gqpgx" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-gqpgx webserver-deployment-595b5b9587- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-595b5b9587-gqpgx 312ee417-faf8-42ed-933b-0d738388cc7a 50107 0 2020-08-17 04:47:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3695f51f-6e34-4819-ae3a-6dd905fc7887 0xc00387af97 0xc00387af98}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.31,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.31,PodIP:,StartTime:2020-08-17 04:47:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.484: INFO: Pod "webserver-deployment-595b5b9587-grtsq" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-grtsq webserver-deployment-595b5b9587- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-595b5b9587-grtsq 23154503-e8da-4a62-af1b-e8fa99fab6cd 49919 0 2020-08-17 04:47:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.201.246/32 cni.projectcalico.org/podIPs:172.30.201.246/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.201.246"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3695f51f-6e34-4819-ae3a-6dd905fc7887 0xc00387b117 0xc00387b118}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.31,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.31,PodIP:172.30.201.246,StartTime:2020-08-17 04:47:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-17 04:47:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://ea8a405043f942854df18f234f3b0e3079dc50279798556841878f18d7326852,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.201.246,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.485: INFO: Pod "webserver-deployment-595b5b9587-h57xz" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-h57xz webserver-deployment-595b5b9587- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-595b5b9587-h57xz 941120a2-27a2-4818-ba5c-3feafc64aa88 50192 0 2020-08-17 04:47:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.151.187/32 cni.projectcalico.org/podIPs:172.30.151.187/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.151.187"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3695f51f-6e34-4819-ae3a-6dd905fc7887 0xc00387b2b7 0xc00387b2b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.50,PodIP:,StartTime:2020-08-17 04:47:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.485: INFO: Pod "webserver-deployment-595b5b9587-nthf2" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-nthf2 webserver-deployment-595b5b9587- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-595b5b9587-nthf2 5a7f2c0a-1a0d-4c15-bb45-129f3f5befa3 50195 0 2020-08-17 04:47:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.218.116/32 cni.projectcalico.org/podIPs:172.30.218.116/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.218.116"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3695f51f-6e34-4819-ae3a-6dd905fc7887 0xc00387b437 0xc00387b438}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.42,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.42,PodIP:,StartTime:2020-08-17 04:47:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.485: INFO: Pod "webserver-deployment-595b5b9587-rwpwd" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rwpwd webserver-deployment-595b5b9587- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-595b5b9587-rwpwd 555383b7-a617-418f-851c-87366d779443 50111 0 2020-08-17 04:47:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3695f51f-6e34-4819-ae3a-6dd905fc7887 0xc00387b5b7 0xc00387b5b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.31,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.31,PodIP:,StartTime:2020-08-17 04:47:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.486: INFO: Pod "webserver-deployment-595b5b9587-s2dcx" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-s2dcx webserver-deployment-595b5b9587- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-595b5b9587-s2dcx 73678d8e-af23-4049-8512-61522c5c340a 49880 0 2020-08-17 04:47:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.218.108/32 cni.projectcalico.org/podIPs:172.30.218.108/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.218.108"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3695f51f-6e34-4819-ae3a-6dd905fc7887 0xc00387b737 0xc00387b738}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.42,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.42,PodIP:172.30.218.108,StartTime:2020-08-17 04:47:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-17 04:47:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://3182b387c6911557d35964b2a8799ac9050e687d5360a53d0c393408033ebc99,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.218.108,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.486: INFO: Pod "webserver-deployment-595b5b9587-slfj8" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-slfj8 webserver-deployment-595b5b9587- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-595b5b9587-slfj8 e2f3ef59-845d-43e0-853a-6e43a98b0c94 50226 0 2020-08-17 04:47:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.201.251/32 cni.projectcalico.org/podIPs:172.30.201.251/32 openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3695f51f-6e34-4819-ae3a-6dd905fc7887 0xc00387b8d7 0xc00387b8d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.31,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.31,PodIP:,StartTime:2020-08-17 04:47:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.487: INFO: Pod "webserver-deployment-595b5b9587-t762v" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-t762v webserver-deployment-595b5b9587- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-595b5b9587-t762v e36faaef-7f26-439c-8258-3bf4c4ccd5a7 50105 0 2020-08-17 04:47:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3695f51f-6e34-4819-ae3a-6dd905fc7887 0xc00387ba57 0xc00387ba58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.50,PodIP:,StartTime:2020-08-17 04:47:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.487: INFO: Pod "webserver-deployment-595b5b9587-wcv26" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-wcv26 webserver-deployment-595b5b9587- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-595b5b9587-wcv26 52b3a3b6-23d7-4e9d-a337-5ce5ba7b7e30 50198 0 2020-08-17 04:47:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.201.248/32 cni.projectcalico.org/podIPs:172.30.201.248/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.201.248"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3695f51f-6e34-4819-ae3a-6dd905fc7887 0xc00387bbd7 0xc00387bbd8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.31,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.31,PodIP:,StartTime:2020-08-17 04:47:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.487: INFO: Pod "webserver-deployment-c7997dcc8-2z9mz" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-2z9mz webserver-deployment-c7997dcc8- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-c7997dcc8-2z9mz a5cf46ae-db70-48c8-8e40-5e32eda0abb9 50237 0 2020-08-17 04:47:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.201.252/32 cni.projectcalico.org/podIPs:172.30.201.252/32 openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c3ab0601-81fc-4bd7-9f21-bf14ac23e1d1 0xc00387bd57 0xc00387bd58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.31,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.31,PodIP:,StartTime:2020-08-17 04:47:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.488: INFO: Pod "webserver-deployment-c7997dcc8-9vxjk" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-9vxjk webserver-deployment-c7997dcc8- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-c7997dcc8-9vxjk d2fec8f4-fd98-4a9f-86a8-fd69c9f9cf39 50110 0 2020-08-17 04:47:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c3ab0601-81fc-4bd7-9f21-bf14ac23e1d1 0xc00387bef7 0xc00387bef8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.50,PodIP:,StartTime:2020-08-17 04:47:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.488: INFO: Pod "webserver-deployment-c7997dcc8-b8fc7" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-b8fc7 webserver-deployment-c7997dcc8- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-c7997dcc8-b8fc7 02f3e5e8-a084-4018-9244-d62ad45fbfd1 50141 0 2020-08-17 04:47:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c3ab0601-81fc-4bd7-9f21-bf14ac23e1d1 0xc005390097 0xc005390098}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.31,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.31,PodIP:,StartTime:2020-08-17 04:47:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.489: INFO: Pod "webserver-deployment-c7997dcc8-c97bm" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-c97bm webserver-deployment-c7997dcc8- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-c7997dcc8-c97bm f38c7a92-79b4-43ff-8095-c2e21ea626de 50136 0 2020-08-17 04:47:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c3ab0601-81fc-4bd7-9f21-bf14ac23e1d1 0xc005390237 0xc005390238}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.42,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.42,PodIP:,StartTime:2020-08-17 04:47:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.492: INFO: Pod "webserver-deployment-c7997dcc8-cb2nj" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-cb2nj webserver-deployment-c7997dcc8- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-c7997dcc8-cb2nj f2342a59-1846-412b-8eaa-532bc6d56aa2 50183 0 2020-08-17 04:47:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.151.186/32 cni.projectcalico.org/podIPs:172.30.151.186/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.151.186"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c3ab0601-81fc-4bd7-9f21-bf14ac23e1d1 0xc0053903d7 0xc0053903d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.50,PodIP:172.30.151.186,StartTime:2020-08-17 04:47:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.151.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.492: INFO: Pod "webserver-deployment-c7997dcc8-g56z7" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-g56z7 webserver-deployment-c7997dcc8- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-c7997dcc8-g56z7 d428706b-9bf5-40ab-b963-b60e59d069ff 50215 0 2020-08-17 04:47:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.151.188/32 cni.projectcalico.org/podIPs:172.30.151.188/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.151.188"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c3ab0601-81fc-4bd7-9f21-bf14ac23e1d1 0xc0053905a7 0xc0053905a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.50,PodIP:,StartTime:2020-08-17 04:47:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.492: INFO: Pod "webserver-deployment-c7997dcc8-nl8rh" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-nl8rh webserver-deployment-c7997dcc8- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-c7997dcc8-nl8rh b721fd24-aa4f-40c7-b8d8-63b0d227bfa3 50185 0 2020-08-17 04:47:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.218.114/32 cni.projectcalico.org/podIPs:172.30.218.114/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.218.114"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c3ab0601-81fc-4bd7-9f21-bf14ac23e1d1 0xc005390747 0xc005390748}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.42,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.42,PodIP:172.30.218.114,StartTime:2020-08-17 04:47:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.218.114,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.493: INFO: Pod "webserver-deployment-c7997dcc8-nrf8r" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-nrf8r webserver-deployment-c7997dcc8- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-c7997dcc8-nrf8r 5e8849e1-5eff-4210-aad2-b8047b5a14ae 50174 0 2020-08-17 04:47:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.218.113/32 cni.projectcalico.org/podIPs:172.30.218.113/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.218.113"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c3ab0601-81fc-4bd7-9f21-bf14ac23e1d1 0xc005390917 0xc005390918}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.42,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.42,PodIP:172.30.218.113,StartTime:2020-08-17 04:47:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.218.113,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.493: INFO: Pod "webserver-deployment-c7997dcc8-p2qhj" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-p2qhj webserver-deployment-c7997dcc8- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-c7997dcc8-p2qhj 8a0b5da3-d4be-4f7c-9834-c0f1bfeb1863 50129 0 2020-08-17 04:47:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c3ab0601-81fc-4bd7-9f21-bf14ac23e1d1 0xc005390ae7 0xc005390ae8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.31,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.31,PodIP:,StartTime:2020-08-17 04:47:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.493: INFO: Pod "webserver-deployment-c7997dcc8-pxz8b" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-pxz8b webserver-deployment-c7997dcc8- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-c7997dcc8-pxz8b c2b978ac-364c-4b10-979b-59b1c8e3bfe6 50213 0 2020-08-17 04:47:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.201.249/32 cni.projectcalico.org/podIPs:172.30.201.249/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.201.249"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c3ab0601-81fc-4bd7-9f21-bf14ac23e1d1 0xc005390c87 0xc005390c88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.31,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.31,PodIP:,StartTime:2020-08-17 04:47:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.494: INFO: Pod "webserver-deployment-c7997dcc8-qhlbp" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qhlbp webserver-deployment-c7997dcc8- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-c7997dcc8-qhlbp 6029e50f-3af4-4588-badf-09ebf0a53bff 50162 0 2020-08-17 04:47:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.201.247/32 cni.projectcalico.org/podIPs:172.30.201.247/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.201.247"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c3ab0601-81fc-4bd7-9f21-bf14ac23e1d1 0xc005390e27 0xc005390e28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.31,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.31,PodIP:172.30.201.247,StartTime:2020-08-17 04:47:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.201.247,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.494: INFO: Pod "webserver-deployment-c7997dcc8-wcx2d" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-wcx2d webserver-deployment-c7997dcc8- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-c7997dcc8-wcx2d b00a6f38-5761-498b-b36f-e57678b931ef 50239 0 2020-08-17 04:47:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.151.189/32 cni.projectcalico.org/podIPs:172.30.151.189/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.151.189"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c3ab0601-81fc-4bd7-9f21-bf14ac23e1d1 0xc005390ff7 0xc005390ff8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.50,PodIP:,StartTime:2020-08-17 04:47:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 04:47:47.494: INFO: Pod "webserver-deployment-c7997dcc8-xr26p" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-xr26p webserver-deployment-c7997dcc8- deployment-5384 /api/v1/namespaces/deployment-5384/pods/webserver-deployment-c7997dcc8-xr26p 1b5109c0-e5ac-4526-9dfb-bde25783e0f0 50154 0 2020-08-17 04:47:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.151.185/32 cni.projectcalico.org/podIPs:172.30.151.185/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.151.185"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c3ab0601-81fc-4bd7-9f21-bf14ac23e1d1 0xc005391197 0xc005391198}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qtzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qtzwr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qtzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xtlkv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:47:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.50,PodIP:172.30.151.185,StartTime:2020-08-17 04:47:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.151.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:47:47.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5384" for this suite.
Aug 17 04:48:03.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:48:06.380: INFO: namespace deployment-5384 deletion completed in 18.655550106s

• [SLOW TEST:28.785 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:48:06.380: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 04:48:06.672: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 17 04:48:15.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 --namespace=crd-publish-openapi-6309 create -f -'
Aug 17 04:48:16.772: INFO: stderr: ""
Aug 17 04:48:16.772: INFO: stdout: "e2e-test-crd-publish-openapi-2147-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 17 04:48:16.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 --namespace=crd-publish-openapi-6309 delete e2e-test-crd-publish-openapi-2147-crds test-cr'
Aug 17 04:48:17.028: INFO: stderr: ""
Aug 17 04:48:17.028: INFO: stdout: "e2e-test-crd-publish-openapi-2147-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Aug 17 04:48:17.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 --namespace=crd-publish-openapi-6309 apply -f -'
Aug 17 04:48:18.205: INFO: stderr: ""
Aug 17 04:48:18.206: INFO: stdout: "e2e-test-crd-publish-openapi-2147-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 17 04:48:18.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 --namespace=crd-publish-openapi-6309 delete e2e-test-crd-publish-openapi-2147-crds test-cr'
Aug 17 04:48:18.391: INFO: stderr: ""
Aug 17 04:48:18.391: INFO: stdout: "e2e-test-crd-publish-openapi-2147-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Aug 17 04:48:18.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 explain e2e-test-crd-publish-openapi-2147-crds'
Aug 17 04:48:18.769: INFO: stderr: ""
Aug 17 04:48:18.769: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2147-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:48:27.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6309" for this suite.
Aug 17 04:48:37.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:48:40.136: INFO: namespace crd-publish-openapi-6309 deletion completed in 12.520523089s

• [SLOW TEST:33.756 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:48:40.136: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-f427332b-9bd1-4fdb-8844-1a3586e31769
STEP: Creating a pod to test consume secrets
Aug 17 04:48:40.429: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c141896a-0568-4d70-adc2-4c2cb1949353" in namespace "projected-489" to be "success or failure"
Aug 17 04:48:40.468: INFO: Pod "pod-projected-secrets-c141896a-0568-4d70-adc2-4c2cb1949353": Phase="Pending", Reason="", readiness=false. Elapsed: 39.317902ms
Aug 17 04:48:42.487: INFO: Pod "pod-projected-secrets-c141896a-0568-4d70-adc2-4c2cb1949353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.05825926s
STEP: Saw pod success
Aug 17 04:48:42.487: INFO: Pod "pod-projected-secrets-c141896a-0568-4d70-adc2-4c2cb1949353" satisfied condition "success or failure"
Aug 17 04:48:42.506: INFO: Trying to get logs from node 10.241.148.42 pod pod-projected-secrets-c141896a-0568-4d70-adc2-4c2cb1949353 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 17 04:48:42.628: INFO: Waiting for pod pod-projected-secrets-c141896a-0568-4d70-adc2-4c2cb1949353 to disappear
Aug 17 04:48:42.644: INFO: Pod pod-projected-secrets-c141896a-0568-4d70-adc2-4c2cb1949353 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:48:42.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-489" for this suite.
Aug 17 04:48:52.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:48:55.436: INFO: namespace projected-489 deletion completed in 12.724631182s

• [SLOW TEST:15.299 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:48:55.436: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 04:48:55.619: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:48:56.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1019" for this suite.
Aug 17 04:49:06.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:49:09.581: INFO: namespace custom-resource-definition-1019 deletion completed in 12.745044023s

• [SLOW TEST:14.146 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:49:09.582: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 17 04:49:09.869: INFO: Waiting up to 5m0s for pod "pod-d683a463-5d4a-46bb-921d-4b8cb0ce9231" in namespace "emptydir-7110" to be "success or failure"
Aug 17 04:49:09.889: INFO: Pod "pod-d683a463-5d4a-46bb-921d-4b8cb0ce9231": Phase="Pending", Reason="", readiness=false. Elapsed: 19.98496ms
Aug 17 04:49:11.910: INFO: Pod "pod-d683a463-5d4a-46bb-921d-4b8cb0ce9231": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040630567s
Aug 17 04:49:13.928: INFO: Pod "pod-d683a463-5d4a-46bb-921d-4b8cb0ce9231": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058520771s
STEP: Saw pod success
Aug 17 04:49:13.928: INFO: Pod "pod-d683a463-5d4a-46bb-921d-4b8cb0ce9231" satisfied condition "success or failure"
Aug 17 04:49:13.946: INFO: Trying to get logs from node 10.241.148.42 pod pod-d683a463-5d4a-46bb-921d-4b8cb0ce9231 container test-container: <nil>
STEP: delete the pod
Aug 17 04:49:14.044: INFO: Waiting for pod pod-d683a463-5d4a-46bb-921d-4b8cb0ce9231 to disappear
Aug 17 04:49:14.065: INFO: Pod pod-d683a463-5d4a-46bb-921d-4b8cb0ce9231 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:49:14.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7110" for this suite.
Aug 17 04:49:24.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:49:26.768: INFO: namespace emptydir-7110 deletion completed in 12.66107315s

• [SLOW TEST:17.186 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:49:26.768: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Aug 17 04:49:28.092: INFO: Waiting up to 5m0s for pod "downward-api-0a28d8d3-ce7c-4fca-9392-d8a8dc3d8157" in namespace "downward-api-8449" to be "success or failure"
Aug 17 04:49:28.109: INFO: Pod "downward-api-0a28d8d3-ce7c-4fca-9392-d8a8dc3d8157": Phase="Pending", Reason="", readiness=false. Elapsed: 17.582643ms
Aug 17 04:49:30.157: INFO: Pod "downward-api-0a28d8d3-ce7c-4fca-9392-d8a8dc3d8157": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065579425s
Aug 17 04:49:32.183: INFO: Pod "downward-api-0a28d8d3-ce7c-4fca-9392-d8a8dc3d8157": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.090794774s
STEP: Saw pod success
Aug 17 04:49:32.183: INFO: Pod "downward-api-0a28d8d3-ce7c-4fca-9392-d8a8dc3d8157" satisfied condition "success or failure"
Aug 17 04:49:32.209: INFO: Trying to get logs from node 10.241.148.42 pod downward-api-0a28d8d3-ce7c-4fca-9392-d8a8dc3d8157 container dapi-container: <nil>
STEP: delete the pod
Aug 17 04:49:32.339: INFO: Waiting for pod downward-api-0a28d8d3-ce7c-4fca-9392-d8a8dc3d8157 to disappear
Aug 17 04:49:32.390: INFO: Pod downward-api-0a28d8d3-ce7c-4fca-9392-d8a8dc3d8157 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:49:32.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8449" for this suite.
Aug 17 04:49:42.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:49:45.000: INFO: namespace downward-api-8449 deletion completed in 12.536248586s

• [SLOW TEST:18.231 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:49:45.000: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:50:15.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3128" for this suite.
Aug 17 04:50:26.171: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:50:28.839: INFO: namespace container-runtime-3128 deletion completed in 12.73778813s

• [SLOW TEST:43.839 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:50:28.840: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Aug 17 04:50:29.088: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 17 04:50:29.269: INFO: Waiting for terminating namespaces to be deleted...
Aug 17 04:50:29.300: INFO: 
Logging pods the kubelet thinks is on node 10.241.148.31 before test
Aug 17 04:50:29.463: INFO: ibm-master-proxy-static-10.241.148.31 from kube-system started at 2020-08-17 02:56:11 +0000 UTC (2 container statuses recorded)
Aug 17 04:50:29.463: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 17 04:50:29.463: INFO: 	Container pause ready: true, restart count 0
Aug 17 04:50:29.463: INFO: node-exporter-kkwj6 from openshift-monitoring started at 2020-08-17 02:58:34 +0000 UTC (2 container statuses recorded)
Aug 17 04:50:29.463: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:50:29.463: INFO: 	Container node-exporter ready: true, restart count 0
Aug 17 04:50:29.463: INFO: dns-default-sxjmd from openshift-dns started at 2020-08-17 02:59:01 +0000 UTC (2 container statuses recorded)
Aug 17 04:50:29.463: INFO: 	Container dns ready: true, restart count 0
Aug 17 04:50:29.463: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 17 04:50:29.463: INFO: prometheus-operator-56d9d699cb-m5pmc from openshift-monitoring started at 2020-08-17 03:04:39 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.463: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 17 04:50:29.463: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-08-17 03:05:11 +0000 UTC (3 container statuses recorded)
Aug 17 04:50:29.463: INFO: 	Container alertmanager ready: true, restart count 0
Aug 17 04:50:29.463: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 17 04:50:29.463: INFO: 	Container config-reloader ready: true, restart count 0
Aug 17 04:50:29.463: INFO: openshift-kube-proxy-5nh74 from openshift-kube-proxy started at 2020-08-17 02:56:45 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.463: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 17 04:50:29.463: INFO: prometheus-adapter-67dbcc5cfc-xk5l6 from openshift-monitoring started at 2020-08-17 02:58:37 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.463: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 17 04:50:29.463: INFO: community-operators-65d9d5c9d6-zh8cl from openshift-marketplace started at 2020-08-17 02:59:24 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.463: INFO: 	Container community-operators ready: true, restart count 0
Aug 17 04:50:29.463: INFO: thanos-querier-6b5fbd9754-ctb66 from openshift-monitoring started at 2020-08-17 03:05:37 +0000 UTC (4 container statuses recorded)
Aug 17 04:50:29.463: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:50:29.463: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 17 04:50:29.463: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 17 04:50:29.463: INFO: 	Container thanos-querier ready: true, restart count 0
Aug 17 04:50:29.463: INFO: multus-tvx65 from openshift-multus started at 2020-08-17 02:56:38 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.463: INFO: 	Container kube-multus ready: true, restart count 0
Aug 17 04:50:29.463: INFO: tuned-nmn6j from openshift-cluster-node-tuning-operator started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.463: INFO: 	Container tuned ready: true, restart count 0
Aug 17 04:50:29.463: INFO: ibmcloud-block-storage-driver-hfn98 from kube-system started at 2020-08-17 02:56:23 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.463: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 17 04:50:29.463: INFO: calico-node-lm8qf from calico-system started at 2020-08-17 02:57:31 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.463: INFO: 	Container calico-node ready: true, restart count 0
Aug 17 04:50:29.463: INFO: cluster-samples-operator-55944b8f44-fxlq8 from openshift-cluster-samples-operator started at 2020-08-17 02:59:01 +0000 UTC (2 container statuses recorded)
Aug 17 04:50:29.464: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Aug 17 04:50:29.464: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Aug 17 04:50:29.464: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-08-17 03:05:54 +0000 UTC (7 container statuses recorded)
Aug 17 04:50:29.464: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:50:29.464: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 17 04:50:29.464: INFO: 	Container prometheus ready: true, restart count 1
Aug 17 04:50:29.464: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 17 04:50:29.464: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 17 04:50:29.464: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 17 04:50:29.464: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 17 04:50:29.464: INFO: ibm-cloud-provider-ip-169-48-239-26-5595944bf9-n8nts from ibm-system started at 2020-08-17 03:02:23 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.464: INFO: 	Container ibm-cloud-provider-ip-169-48-239-26 ready: true, restart count 0
Aug 17 04:50:29.464: INFO: console-76d8dc5894-2w62x from openshift-console started at 2020-08-17 02:59:39 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.464: INFO: 	Container console ready: true, restart count 0
Aug 17 04:50:29.464: INFO: multus-admission-controller-ncsvb from openshift-multus started at 2020-08-17 02:57:44 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.464: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 17 04:50:29.464: INFO: router-default-57d78dfb48-mzjlc from openshift-ingress started at 2020-08-17 02:59:05 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.464: INFO: 	Container router ready: true, restart count 0
Aug 17 04:50:29.464: INFO: calico-typha-579947cf56-672t9 from calico-system started at 2020-08-17 02:59:26 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.464: INFO: 	Container calico-typha ready: true, restart count 0
Aug 17 04:50:29.464: INFO: sonobuoy-systemd-logs-daemon-set-c8f140ca93fa466f-pjc77 from sonobuoy started at 2020-08-17 04:12:38 +0000 UTC (2 container statuses recorded)
Aug 17 04:50:29.464: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 04:50:29.464: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 04:50:29.464: INFO: service-serving-cert-signer-7879bf8d9f-xtqt8 from openshift-service-ca started at 2020-08-17 02:58:07 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.464: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Aug 17 04:50:29.464: INFO: configmap-cabundle-injector-8446d4b88f-9k4sq from openshift-service-ca started at 2020-08-17 02:58:07 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.464: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Aug 17 04:50:29.464: INFO: kube-state-metrics-c5f65645-855s8 from openshift-monitoring started at 2020-08-17 02:58:30 +0000 UTC (3 container statuses recorded)
Aug 17 04:50:29.464: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 17 04:50:29.464: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 17 04:50:29.464: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 17 04:50:29.464: INFO: image-registry-5669fd49dd-xphdf from openshift-image-registry started at 2020-08-17 03:00:49 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.464: INFO: 	Container registry ready: true, restart count 0
Aug 17 04:50:29.464: INFO: ibm-keepalived-watcher-ncgwx from kube-system started at 2020-08-17 02:56:14 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.464: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 17 04:50:29.464: INFO: node-ca-2lszv from openshift-image-registry started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.464: INFO: 	Container node-ca ready: true, restart count 0
Aug 17 04:50:29.464: INFO: packageserver-66768b6f89-b74q9 from openshift-operator-lifecycle-manager started at 2020-08-17 03:05:22 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.464: INFO: 	Container packageserver ready: true, restart count 0
Aug 17 04:50:29.464: INFO: 
Logging pods the kubelet thinks is on node 10.241.148.42 before test
Aug 17 04:50:29.563: INFO: console-76d8dc5894-xnhrt from openshift-console started at 2020-08-17 02:59:20 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.563: INFO: 	Container console ready: true, restart count 0
Aug 17 04:50:29.563: INFO: ibm-master-proxy-static-10.241.148.42 from kube-system started at 2020-08-17 02:56:02 +0000 UTC (2 container statuses recorded)
Aug 17 04:50:29.563: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 17 04:50:29.564: INFO: 	Container pause ready: true, restart count 0
Aug 17 04:50:29.564: INFO: multus-p85fw from openshift-multus started at 2020-08-17 02:56:38 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.564: INFO: 	Container kube-multus ready: true, restart count 0
Aug 17 04:50:29.564: INFO: calico-typha-579947cf56-86wz4 from calico-system started at 2020-08-17 02:57:30 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.564: INFO: 	Container calico-typha ready: true, restart count 0
Aug 17 04:50:29.564: INFO: apiservice-cabundle-injector-594fd4555f-j2v5w from openshift-service-ca started at 2020-08-17 02:58:07 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.564: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Aug 17 04:50:29.564: INFO: node-exporter-r6g64 from openshift-monitoring started at 2020-08-17 02:58:34 +0000 UTC (2 container statuses recorded)
Aug 17 04:50:29.564: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:50:29.564: INFO: 	Container node-exporter ready: true, restart count 0
Aug 17 04:50:29.564: INFO: router-default-57d78dfb48-8c54l from openshift-ingress started at 2020-08-17 02:59:05 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.564: INFO: 	Container router ready: true, restart count 0
Aug 17 04:50:29.564: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-08-17 03:01:11 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.564: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Aug 17 04:50:29.565: INFO: openshift-state-metrics-5849d797d8-h6gdr from openshift-monitoring started at 2020-08-17 02:58:31 +0000 UTC (3 container statuses recorded)
Aug 17 04:50:29.565: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 17 04:50:29.565: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 17 04:50:29.565: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 17 04:50:29.565: INFO: sonobuoy from sonobuoy started at 2020-08-17 04:12:31 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.565: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 17 04:50:29.565: INFO: grafana-c9c7455d7-phgf6 from openshift-monitoring started at 2020-08-17 03:04:57 +0000 UTC (2 container statuses recorded)
Aug 17 04:50:29.565: INFO: 	Container grafana ready: true, restart count 0
Aug 17 04:50:29.565: INFO: 	Container grafana-proxy ready: true, restart count 0
Aug 17 04:50:29.565: INFO: sonobuoy-e2e-job-144f6878906f41e3 from sonobuoy started at 2020-08-17 04:12:38 +0000 UTC (2 container statuses recorded)
Aug 17 04:50:29.565: INFO: 	Container e2e ready: true, restart count 0
Aug 17 04:50:29.565: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 04:50:29.565: INFO: sonobuoy-systemd-logs-daemon-set-c8f140ca93fa466f-qhfmq from sonobuoy started at 2020-08-17 04:12:38 +0000 UTC (2 container statuses recorded)
Aug 17 04:50:29.565: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 04:50:29.565: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 04:50:29.566: INFO: ibm-keepalived-watcher-xf2gw from kube-system started at 2020-08-17 02:56:05 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.566: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 17 04:50:29.566: INFO: multus-admission-controller-bskrp from openshift-multus started at 2020-08-17 02:57:45 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.566: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 17 04:50:29.566: INFO: prometheus-adapter-67dbcc5cfc-5kt76 from openshift-monitoring started at 2020-08-17 02:58:37 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.566: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 17 04:50:29.566: INFO: node-ca-j567q from openshift-image-registry started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.566: INFO: 	Container node-ca ready: true, restart count 0
Aug 17 04:50:29.566: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-08-17 03:05:18 +0000 UTC (3 container statuses recorded)
Aug 17 04:50:29.566: INFO: 	Container alertmanager ready: true, restart count 0
Aug 17 04:50:29.566: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 17 04:50:29.566: INFO: 	Container config-reloader ready: true, restart count 0
Aug 17 04:50:29.566: INFO: tigera-operator-679798d94d-qrlds from tigera-operator started at 2020-08-17 02:56:13 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.566: INFO: 	Container tigera-operator ready: true, restart count 2
Aug 17 04:50:29.566: INFO: registry-pvc-permissions-j8h5n from openshift-image-registry started at 2020-08-17 03:00:49 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.567: INFO: 	Container pvc-permissions ready: false, restart count 0
Aug 17 04:50:29.567: INFO: certified-operators-549fcd7d77-lx6ml from openshift-marketplace started at 2020-08-17 02:59:24 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.567: INFO: 	Container certified-operators ready: true, restart count 0
Aug 17 04:50:29.567: INFO: calico-node-cxl67 from calico-system started at 2020-08-17 02:57:31 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.567: INFO: 	Container calico-node ready: true, restart count 0
Aug 17 04:50:29.567: INFO: network-operator-7986644c85-9jpc4 from openshift-network-operator started at 2020-08-17 02:56:13 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.567: INFO: 	Container network-operator ready: true, restart count 0
Aug 17 04:50:29.567: INFO: tuned-b665k from openshift-cluster-node-tuning-operator started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.567: INFO: 	Container tuned ready: true, restart count 0
Aug 17 04:50:29.567: INFO: dns-default-kqx2s from openshift-dns started at 2020-08-17 02:59:01 +0000 UTC (2 container statuses recorded)
Aug 17 04:50:29.567: INFO: 	Container dns ready: true, restart count 0
Aug 17 04:50:29.567: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 17 04:50:29.567: INFO: ibmcloud-block-storage-driver-5dth8 from kube-system started at 2020-08-17 02:56:14 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.568: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 17 04:50:29.568: INFO: openshift-kube-proxy-hswpd from openshift-kube-proxy started at 2020-08-17 02:56:45 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.568: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 17 04:50:29.568: INFO: telemeter-client-76746b8bb7-hh5jv from openshift-monitoring started at 2020-08-17 03:04:49 +0000 UTC (3 container statuses recorded)
Aug 17 04:50:29.568: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:50:29.568: INFO: 	Container reload ready: true, restart count 0
Aug 17 04:50:29.568: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 17 04:50:29.568: INFO: 
Logging pods the kubelet thinks is on node 10.241.148.50 before test
Aug 17 04:50:29.702: INFO: dns-operator-6f9cf66db7-ksmws from openshift-dns-operator started at 2020-08-17 02:57:44 +0000 UTC (2 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container dns-operator ready: true, restart count 0
Aug 17 04:50:29.703: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:50:29.703: INFO: redhat-operators-85f96bd7c-69xgq from openshift-marketplace started at 2020-08-17 02:59:24 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container redhat-operators ready: true, restart count 0
Aug 17 04:50:29.703: INFO: packageserver-66768b6f89-hjpwd from openshift-operator-lifecycle-manager started at 2020-08-17 03:05:13 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container packageserver ready: true, restart count 0
Aug 17 04:50:29.703: INFO: thanos-querier-6b5fbd9754-wzzrz from openshift-monitoring started at 2020-08-17 03:05:26 +0000 UTC (4 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:50:29.703: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 17 04:50:29.703: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 17 04:50:29.703: INFO: 	Container thanos-querier ready: true, restart count 0
Aug 17 04:50:29.703: INFO: console-operator-9878d4766-7tcvg from openshift-console-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container console-operator ready: true, restart count 1
Aug 17 04:50:29.703: INFO: openshift-service-catalog-apiserver-operator-78b9dddb6f-bbw72 from openshift-service-catalog-apiserver-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container operator ready: true, restart count 1
Aug 17 04:50:29.703: INFO: ingress-operator-695bc545b9-d4trd from openshift-ingress-operator started at 2020-08-17 02:57:42 +0000 UTC (2 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container ingress-operator ready: true, restart count 0
Aug 17 04:50:29.703: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:50:29.703: INFO: downloads-678f5d6564-wxxs4 from openshift-console started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container download-server ready: true, restart count 0
Aug 17 04:50:29.703: INFO: sonobuoy-systemd-logs-daemon-set-c8f140ca93fa466f-j84lz from sonobuoy started at 2020-08-17 04:12:38 +0000 UTC (2 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 04:50:29.703: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 04:50:29.703: INFO: vpn-7b59d9f64-2rnfd from kube-system started at 2020-08-17 03:05:11 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container vpn ready: true, restart count 0
Aug 17 04:50:29.703: INFO: ibm-storage-watcher-77bf8b889-856t2 from kube-system started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 17 04:50:29.703: INFO: downloads-678f5d6564-t5r44 from openshift-console started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container download-server ready: true, restart count 0
Aug 17 04:50:29.703: INFO: node-exporter-ztsvb from openshift-monitoring started at 2020-08-17 02:58:33 +0000 UTC (2 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:50:29.703: INFO: 	Container node-exporter ready: true, restart count 0
Aug 17 04:50:29.703: INFO: calico-typha-579947cf56-87rgj from calico-system started at 2020-08-17 02:59:26 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container calico-typha ready: true, restart count 0
Aug 17 04:50:29.703: INFO: catalog-operator-85f6c659cc-6zpxp from openshift-operator-lifecycle-manager started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 17 04:50:29.703: INFO: multus-admission-controller-7xrg7 from openshift-multus started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 17 04:50:29.703: INFO: tuned-6qthk from openshift-cluster-node-tuning-operator started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container tuned ready: true, restart count 0
Aug 17 04:50:29.703: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-08-17 03:04:54 +0000 UTC (3 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container alertmanager ready: true, restart count 0
Aug 17 04:50:29.703: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 17 04:50:29.703: INFO: 	Container config-reloader ready: true, restart count 0
Aug 17 04:50:29.703: INFO: ibmcloud-block-storage-driver-6l6kb from kube-system started at 2020-08-17 02:56:30 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 17 04:50:29.703: INFO: service-ca-operator-694cfbf5d5-2dhf8 from openshift-service-ca-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container operator ready: true, restart count 0
Aug 17 04:50:29.703: INFO: olm-operator-b5f57cdbb-wcj4n from openshift-operator-lifecycle-manager started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container olm-operator ready: true, restart count 0
Aug 17 04:50:29.703: INFO: calico-kube-controllers-79d75767dd-hhcw2 from calico-system started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 17 04:50:29.703: INFO: ibmcloud-block-storage-plugin-68d5c65db9-m8qcr from kube-system started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Aug 17 04:50:29.703: INFO: ibm-cloud-provider-ip-169-48-239-26-5595944bf9-2pxq9 from ibm-system started at 2020-08-17 03:02:20 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container ibm-cloud-provider-ip-169-48-239-26 ready: true, restart count 0
Aug 17 04:50:29.703: INFO: openshift-kube-proxy-grn9g from openshift-kube-proxy started at 2020-08-17 02:56:45 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 17 04:50:29.703: INFO: calico-node-hmdtm from calico-system started at 2020-08-17 02:57:31 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container calico-node ready: true, restart count 0
Aug 17 04:50:29.703: INFO: cluster-monitoring-operator-5b5659466f-bd9vb from openshift-monitoring started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Aug 17 04:50:29.703: INFO: cluster-image-registry-operator-6cfd58b66c-xpvcp from openshift-image-registry started at 2020-08-17 02:57:42 +0000 UTC (2 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Aug 17 04:50:29.703: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Aug 17 04:50:29.703: INFO: ibm-master-proxy-static-10.241.148.50 from kube-system started at 2020-08-17 02:56:19 +0000 UTC (2 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 17 04:50:29.703: INFO: 	Container pause ready: true, restart count 0
Aug 17 04:50:29.703: INFO: ibm-keepalived-watcher-x5mxk from kube-system started at 2020-08-17 02:56:22 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 17 04:50:29.703: INFO: cluster-storage-operator-557b75f8d5-899qb from openshift-cluster-storage-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Aug 17 04:50:29.703: INFO: cluster-node-tuning-operator-b5f884945-nmndb from openshift-cluster-node-tuning-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Aug 17 04:50:29.703: INFO: dns-default-kfbzc from openshift-dns started at 2020-08-17 02:59:02 +0000 UTC (2 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container dns ready: true, restart count 0
Aug 17 04:50:29.703: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 17 04:50:29.703: INFO: node-ca-467qn from openshift-image-registry started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container node-ca ready: true, restart count 0
Aug 17 04:50:29.703: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-08-17 03:05:34 +0000 UTC (7 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 04:50:29.703: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 17 04:50:29.703: INFO: 	Container prometheus ready: true, restart count 1
Aug 17 04:50:29.703: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 17 04:50:29.703: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 17 04:50:29.703: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 17 04:50:29.703: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 17 04:50:29.703: INFO: multus-wkz57 from openshift-multus started at 2020-08-17 02:56:38 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container kube-multus ready: true, restart count 0
Aug 17 04:50:29.703: INFO: ibm-file-plugin-fdb69b446-bzpn7 from kube-system started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 17 04:50:29.703: INFO: openshift-service-catalog-controller-manager-operator-5496wjhfr from openshift-service-catalog-controller-manager-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container operator ready: true, restart count 1
Aug 17 04:50:29.703: INFO: marketplace-operator-6957767d58-6rcnt from openshift-marketplace started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 04:50:29.703: INFO: 	Container marketplace-operator ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.162bf4f5eb0a2503], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:50:30.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5962" for this suite.
Aug 17 04:50:41.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:50:43.447: INFO: namespace sched-pred-5962 deletion completed in 12.501559738s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:14.607 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:50:43.447: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 17 04:50:43.746: INFO: Waiting up to 5m0s for pod "pod-1dfba730-6e6f-4ff2-9c09-7dc01ac764b8" in namespace "emptydir-1093" to be "success or failure"
Aug 17 04:50:43.781: INFO: Pod "pod-1dfba730-6e6f-4ff2-9c09-7dc01ac764b8": Phase="Pending", Reason="", readiness=false. Elapsed: 34.87977ms
Aug 17 04:50:45.805: INFO: Pod "pod-1dfba730-6e6f-4ff2-9c09-7dc01ac764b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059045118s
Aug 17 04:50:47.924: INFO: Pod "pod-1dfba730-6e6f-4ff2-9c09-7dc01ac764b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.177405526s
Aug 17 04:50:49.983: INFO: Pod "pod-1dfba730-6e6f-4ff2-9c09-7dc01ac764b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.236485427s
STEP: Saw pod success
Aug 17 04:50:49.983: INFO: Pod "pod-1dfba730-6e6f-4ff2-9c09-7dc01ac764b8" satisfied condition "success or failure"
Aug 17 04:50:50.050: INFO: Trying to get logs from node 10.241.148.42 pod pod-1dfba730-6e6f-4ff2-9c09-7dc01ac764b8 container test-container: <nil>
STEP: delete the pod
Aug 17 04:50:50.252: INFO: Waiting for pod pod-1dfba730-6e6f-4ff2-9c09-7dc01ac764b8 to disappear
Aug 17 04:50:50.311: INFO: Pod pod-1dfba730-6e6f-4ff2-9c09-7dc01ac764b8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:50:50.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1093" for this suite.
Aug 17 04:50:58.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:51:01.167: INFO: namespace emptydir-1093 deletion completed in 10.745392925s

• [SLOW TEST:17.720 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:51:01.170: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 17 04:51:01.677: INFO: Number of nodes with available pods: 0
Aug 17 04:51:01.677: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 04:51:02.721: INFO: Number of nodes with available pods: 0
Aug 17 04:51:02.722: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 04:51:03.868: INFO: Number of nodes with available pods: 1
Aug 17 04:51:03.868: INFO: Node 10.241.148.42 is running more than one daemon pod
Aug 17 04:51:04.724: INFO: Number of nodes with available pods: 3
Aug 17 04:51:04.724: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Aug 17 04:51:04.837: INFO: Number of nodes with available pods: 2
Aug 17 04:51:04.837: INFO: Node 10.241.148.42 is running more than one daemon pod
Aug 17 04:51:05.882: INFO: Number of nodes with available pods: 2
Aug 17 04:51:05.882: INFO: Node 10.241.148.42 is running more than one daemon pod
Aug 17 04:51:06.879: INFO: Number of nodes with available pods: 2
Aug 17 04:51:06.879: INFO: Node 10.241.148.42 is running more than one daemon pod
Aug 17 04:51:07.881: INFO: Number of nodes with available pods: 2
Aug 17 04:51:07.881: INFO: Node 10.241.148.42 is running more than one daemon pod
Aug 17 04:51:08.889: INFO: Number of nodes with available pods: 2
Aug 17 04:51:08.889: INFO: Node 10.241.148.42 is running more than one daemon pod
Aug 17 04:51:09.894: INFO: Number of nodes with available pods: 2
Aug 17 04:51:09.894: INFO: Node 10.241.148.42 is running more than one daemon pod
Aug 17 04:51:10.881: INFO: Number of nodes with available pods: 2
Aug 17 04:51:10.881: INFO: Node 10.241.148.42 is running more than one daemon pod
Aug 17 04:51:11.876: INFO: Number of nodes with available pods: 2
Aug 17 04:51:11.876: INFO: Node 10.241.148.42 is running more than one daemon pod
Aug 17 04:51:12.909: INFO: Number of nodes with available pods: 2
Aug 17 04:51:12.909: INFO: Node 10.241.148.42 is running more than one daemon pod
Aug 17 04:51:13.888: INFO: Number of nodes with available pods: 3
Aug 17 04:51:13.888: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2656, will wait for the garbage collector to delete the pods
Aug 17 04:51:14.056: INFO: Deleting DaemonSet.extensions daemon-set took: 68.586241ms
Aug 17 04:51:14.156: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.396562ms
Aug 17 04:51:22.382: INFO: Number of nodes with available pods: 0
Aug 17 04:51:22.382: INFO: Number of running nodes: 0, number of available pods: 0
Aug 17 04:51:22.398: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2656/daemonsets","resourceVersion":"52294"},"items":null}

Aug 17 04:51:22.415: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2656/pods","resourceVersion":"52294"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:51:22.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2656" for this suite.
Aug 17 04:51:32.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:51:35.398: INFO: namespace daemonsets-2656 deletion completed in 12.865186334s

• [SLOW TEST:34.228 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:51:35.399: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:51:52.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8639" for this suite.
Aug 17 04:52:02.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:52:05.167: INFO: namespace resourcequota-8639 deletion completed in 12.568785178s

• [SLOW TEST:29.769 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:52:05.167: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-4166
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Aug 17 04:52:05.465: INFO: Found 0 stateful pods, waiting for 3
Aug 17 04:52:15.500: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 04:52:15.500: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 04:52:15.500: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 04:52:15.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=statefulset-4166 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 04:52:16.216: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 04:52:16.216: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 04:52:16.216: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Aug 17 04:52:26.424: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Aug 17 04:52:36.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=statefulset-4166 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:52:36.925: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 17 04:52:36.925: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 17 04:52:36.925: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 17 04:52:47.303: INFO: Waiting for StatefulSet statefulset-4166/ss2 to complete update
Aug 17 04:52:47.303: INFO: Waiting for Pod statefulset-4166/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 17 04:52:47.303: INFO: Waiting for Pod statefulset-4166/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 17 04:52:47.303: INFO: Waiting for Pod statefulset-4166/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 17 04:52:57.340: INFO: Waiting for StatefulSet statefulset-4166/ss2 to complete update
Aug 17 04:52:57.340: INFO: Waiting for Pod statefulset-4166/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 17 04:52:57.340: INFO: Waiting for Pod statefulset-4166/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 17 04:53:07.344: INFO: Waiting for StatefulSet statefulset-4166/ss2 to complete update
Aug 17 04:53:07.344: INFO: Waiting for Pod statefulset-4166/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 17 04:53:07.344: INFO: Waiting for Pod statefulset-4166/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 17 04:53:17.500: INFO: Waiting for StatefulSet statefulset-4166/ss2 to complete update
Aug 17 04:53:17.500: INFO: Waiting for Pod statefulset-4166/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 17 04:53:27.370: INFO: Waiting for StatefulSet statefulset-4166/ss2 to complete update
STEP: Rolling back to a previous revision
Aug 17 04:53:37.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=statefulset-4166 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 04:53:37.712: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 04:53:37.712: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 04:53:37.712: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 04:53:48.041: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Aug 17 04:53:48.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=statefulset-4166 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:53:49.291: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 17 04:53:49.291: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 17 04:53:49.291: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 17 04:53:49.543: INFO: Waiting for StatefulSet statefulset-4166/ss2 to complete update
Aug 17 04:53:49.543: INFO: Waiting for Pod statefulset-4166/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Aug 17 04:53:49.543: INFO: Waiting for Pod statefulset-4166/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Aug 17 04:53:49.543: INFO: Waiting for Pod statefulset-4166/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Aug 17 04:54:09.585: INFO: Waiting for StatefulSet statefulset-4166/ss2 to complete update
Aug 17 04:54:09.585: INFO: Waiting for Pod statefulset-4166/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Aug 17 04:54:19.753: INFO: Deleting all statefulset in ns statefulset-4166
Aug 17 04:54:19.827: INFO: Scaling statefulset ss2 to 0
Aug 17 04:55:00.280: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 04:55:00.297: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:55:00.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4166" for this suite.
Aug 17 04:55:10.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:55:12.982: INFO: namespace statefulset-4166 deletion completed in 12.556519452s

• [SLOW TEST:187.815 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:55:12.986: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 17 04:55:18.181: INFO: Successfully updated pod "pod-update-activedeadlineseconds-555dc280-e1d1-4ac2-b11e-82a6869fc1c5"
Aug 17 04:55:18.181: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-555dc280-e1d1-4ac2-b11e-82a6869fc1c5" in namespace "pods-3328" to be "terminated due to deadline exceeded"
Aug 17 04:55:18.260: INFO: Pod "pod-update-activedeadlineseconds-555dc280-e1d1-4ac2-b11e-82a6869fc1c5": Phase="Running", Reason="", readiness=true. Elapsed: 78.40557ms
Aug 17 04:55:20.328: INFO: Pod "pod-update-activedeadlineseconds-555dc280-e1d1-4ac2-b11e-82a6869fc1c5": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.14668229s
Aug 17 04:55:20.328: INFO: Pod "pod-update-activedeadlineseconds-555dc280-e1d1-4ac2-b11e-82a6869fc1c5" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:55:20.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3328" for this suite.
Aug 17 04:55:30.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:55:33.130: INFO: namespace pods-3328 deletion completed in 12.59662979s

• [SLOW TEST:20.144 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:55:33.130: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:55:49.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1746" for this suite.
Aug 17 04:55:58.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:56:01.395: INFO: namespace resourcequota-1746 deletion completed in 10.736022994s

• [SLOW TEST:28.265 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:56:01.397: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
Aug 17 04:56:01.612: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 17 04:57:01.882: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 04:57:01.904: INFO: Starting informer...
STEP: Starting pods...
Aug 17 04:57:02.219: INFO: Pod1 is running on 10.241.148.42. Tainting Node
Aug 17 04:57:04.564: INFO: Pod2 is running on 10.241.148.42. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Aug 17 04:57:22.369: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Aug 17 04:57:42.551: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:57:42.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-9755" for this suite.
Aug 17 04:57:52.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:57:55.266: INFO: namespace taint-multiple-pods-9755 deletion completed in 12.642250765s

• [SLOW TEST:113.869 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:57:55.266: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-e0702dea-a743-4524-9306-45e4600073f1
STEP: Creating a pod to test consume configMaps
Aug 17 04:57:55.536: INFO: Waiting up to 5m0s for pod "pod-configmaps-61219919-3518-4b45-abe9-5ff48fc202c5" in namespace "configmap-2403" to be "success or failure"
Aug 17 04:57:55.573: INFO: Pod "pod-configmaps-61219919-3518-4b45-abe9-5ff48fc202c5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.825193ms
Aug 17 04:57:57.591: INFO: Pod "pod-configmaps-61219919-3518-4b45-abe9-5ff48fc202c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.054929678s
STEP: Saw pod success
Aug 17 04:57:57.591: INFO: Pod "pod-configmaps-61219919-3518-4b45-abe9-5ff48fc202c5" satisfied condition "success or failure"
Aug 17 04:57:57.613: INFO: Trying to get logs from node 10.241.148.42 pod pod-configmaps-61219919-3518-4b45-abe9-5ff48fc202c5 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 17 04:57:57.747: INFO: Waiting for pod pod-configmaps-61219919-3518-4b45-abe9-5ff48fc202c5 to disappear
Aug 17 04:57:57.769: INFO: Pod pod-configmaps-61219919-3518-4b45-abe9-5ff48fc202c5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:57:57.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2403" for this suite.
Aug 17 04:58:05.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:58:08.296: INFO: namespace configmap-2403 deletion completed in 10.503043448s

• [SLOW TEST:13.030 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:58:08.297: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 17 04:58:08.603: INFO: Waiting up to 5m0s for pod "pod-3932117d-065a-4448-817c-6d673805d105" in namespace "emptydir-287" to be "success or failure"
Aug 17 04:58:08.625: INFO: Pod "pod-3932117d-065a-4448-817c-6d673805d105": Phase="Pending", Reason="", readiness=false. Elapsed: 21.349989ms
Aug 17 04:58:10.643: INFO: Pod "pod-3932117d-065a-4448-817c-6d673805d105": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.039683406s
STEP: Saw pod success
Aug 17 04:58:10.643: INFO: Pod "pod-3932117d-065a-4448-817c-6d673805d105" satisfied condition "success or failure"
Aug 17 04:58:10.660: INFO: Trying to get logs from node 10.241.148.42 pod pod-3932117d-065a-4448-817c-6d673805d105 container test-container: <nil>
STEP: delete the pod
Aug 17 04:58:10.752: INFO: Waiting for pod pod-3932117d-065a-4448-817c-6d673805d105 to disappear
Aug 17 04:58:10.774: INFO: Pod pod-3932117d-065a-4448-817c-6d673805d105 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:58:10.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-287" for this suite.
Aug 17 04:58:20.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:58:23.454: INFO: namespace emptydir-287 deletion completed in 12.636577603s

• [SLOW TEST:15.157 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:58:23.454: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0817 04:58:33.864845      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 17 04:58:33.865: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:58:33.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3996" for this suite.
Aug 17 04:58:43.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:58:47.438: INFO: namespace gc-3996 deletion completed in 13.536025487s

• [SLOW TEST:23.984 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:58:47.438: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:58:55.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5395" for this suite.
Aug 17 04:59:05.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:59:07.847: INFO: namespace resourcequota-5395 deletion completed in 12.723927156s

• [SLOW TEST:20.409 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:59:07.848: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Aug 17 04:59:12.211: INFO: &Pod{ObjectMeta:{send-events-a217805c-5cde-43ae-80bb-5b40730c8049  events-1816 /api/v1/namespaces/events-1816/pods/send-events-a217805c-5cde-43ae-80bb-5b40730c8049 44730142-4d10-46d8-8f74-ed1dd9607942 56211 0 2020-08-17 04:59:08 +0000 UTC <nil> <nil> map[name:foo time:73248836] map[cni.projectcalico.org/podIP:172.30.218.90/32 cni.projectcalico.org/podIPs:172.30.218.90/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.218.90"
    ],
    "dns": {}
}] openshift.io/scc:anyuid] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kj5z7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kj5z7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kj5z7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.42,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c50,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:59:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:59:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:59:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 04:59:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.42,PodIP:172.30.218.90,StartTime:2020-08-17 04:59:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-17 04:59:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:cri-o://d88d9e37f5ea1816ccfda95ade577d60c747d3e306fd54feebe5d93c7ecc8a2d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.218.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Aug 17 04:59:14.228: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Aug 17 04:59:16.269: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 04:59:16.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1816" for this suite.
Aug 17 04:59:52.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 04:59:55.260: INFO: namespace events-1816 deletion completed in 38.63045689s

• [SLOW TEST:47.412 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 04:59:55.260: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 17 04:59:56.210: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 17 04:59:58.252: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733237196, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733237196, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733237196, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733237196, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 17 05:00:01.326: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:00:11.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7985" for this suite.
Aug 17 05:00:22.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:00:24.486: INFO: namespace webhook-7985 deletion completed in 12.519438139s
STEP: Destroying namespace "webhook-7985-markers" for this suite.
Aug 17 05:00:34.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:00:36.972: INFO: namespace webhook-7985-markers deletion completed in 12.486377089s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:41.792 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:00:37.052: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Aug 17 05:00:38.053: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Aug 17 05:00:40.101: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733237238, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733237238, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733237238, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733237238, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 17 05:00:43.165: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 05:00:43.218: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:00:44.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7040" for this suite.
Aug 17 05:00:54.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:00:57.241: INFO: namespace crd-webhook-7040 deletion completed in 12.786557021s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:20.270 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:00:57.323: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Aug 17 05:01:02.158: INFO: Successfully updated pod "adopt-release-k8jg2"
STEP: Checking that the Job readopts the Pod
Aug 17 05:01:02.158: INFO: Waiting up to 15m0s for pod "adopt-release-k8jg2" in namespace "job-5334" to be "adopted"
Aug 17 05:01:02.175: INFO: Pod "adopt-release-k8jg2": Phase="Running", Reason="", readiness=true. Elapsed: 16.766367ms
Aug 17 05:01:04.197: INFO: Pod "adopt-release-k8jg2": Phase="Running", Reason="", readiness=true. Elapsed: 2.038873721s
Aug 17 05:01:04.197: INFO: Pod "adopt-release-k8jg2" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Aug 17 05:01:04.794: INFO: Successfully updated pod "adopt-release-k8jg2"
STEP: Checking that the Job releases the Pod
Aug 17 05:01:04.795: INFO: Waiting up to 15m0s for pod "adopt-release-k8jg2" in namespace "job-5334" to be "released"
Aug 17 05:01:04.811: INFO: Pod "adopt-release-k8jg2": Phase="Running", Reason="", readiness=true. Elapsed: 16.358884ms
Aug 17 05:01:06.831: INFO: Pod "adopt-release-k8jg2": Phase="Running", Reason="", readiness=true. Elapsed: 2.035780133s
Aug 17 05:01:06.831: INFO: Pod "adopt-release-k8jg2" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:01:06.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5334" for this suite.
Aug 17 05:01:56.950: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:01:59.573: INFO: namespace job-5334 deletion completed in 52.686924119s

• [SLOW TEST:62.250 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:01:59.574: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 17 05:01:59.820: INFO: Waiting up to 5m0s for pod "downwardapi-volume-56418ebf-ab94-41d3-b1ba-b2693f938d22" in namespace "downward-api-289" to be "success or failure"
Aug 17 05:01:59.837: INFO: Pod "downwardapi-volume-56418ebf-ab94-41d3-b1ba-b2693f938d22": Phase="Pending", Reason="", readiness=false. Elapsed: 16.253999ms
Aug 17 05:02:01.856: INFO: Pod "downwardapi-volume-56418ebf-ab94-41d3-b1ba-b2693f938d22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035915831s
Aug 17 05:02:03.875: INFO: Pod "downwardapi-volume-56418ebf-ab94-41d3-b1ba-b2693f938d22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054336927s
STEP: Saw pod success
Aug 17 05:02:03.875: INFO: Pod "downwardapi-volume-56418ebf-ab94-41d3-b1ba-b2693f938d22" satisfied condition "success or failure"
Aug 17 05:02:03.895: INFO: Trying to get logs from node 10.241.148.42 pod downwardapi-volume-56418ebf-ab94-41d3-b1ba-b2693f938d22 container client-container: <nil>
STEP: delete the pod
Aug 17 05:02:04.016: INFO: Waiting for pod downwardapi-volume-56418ebf-ab94-41d3-b1ba-b2693f938d22 to disappear
Aug 17 05:02:04.037: INFO: Pod downwardapi-volume-56418ebf-ab94-41d3-b1ba-b2693f938d22 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:02:04.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-289" for this suite.
Aug 17 05:02:14.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:02:17.299: INFO: namespace downward-api-289 deletion completed in 13.220842265s

• [SLOW TEST:17.725 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:02:17.299: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-d775991b-09a9-4150-a865-cd580b8d110a
STEP: Creating a pod to test consume configMaps
Aug 17 05:02:18.146: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1281146c-7d24-4613-bb53-d9d4c825bd36" in namespace "projected-8510" to be "success or failure"
Aug 17 05:02:18.252: INFO: Pod "pod-projected-configmaps-1281146c-7d24-4613-bb53-d9d4c825bd36": Phase="Pending", Reason="", readiness=false. Elapsed: 106.575576ms
Aug 17 05:02:20.314: INFO: Pod "pod-projected-configmaps-1281146c-7d24-4613-bb53-d9d4c825bd36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.167773492s
Aug 17 05:02:22.334: INFO: Pod "pod-projected-configmaps-1281146c-7d24-4613-bb53-d9d4c825bd36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.187951784s
STEP: Saw pod success
Aug 17 05:02:22.334: INFO: Pod "pod-projected-configmaps-1281146c-7d24-4613-bb53-d9d4c825bd36" satisfied condition "success or failure"
Aug 17 05:02:22.351: INFO: Trying to get logs from node 10.241.148.42 pod pod-projected-configmaps-1281146c-7d24-4613-bb53-d9d4c825bd36 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 17 05:02:22.445: INFO: Waiting for pod pod-projected-configmaps-1281146c-7d24-4613-bb53-d9d4c825bd36 to disappear
Aug 17 05:02:22.467: INFO: Pod pod-projected-configmaps-1281146c-7d24-4613-bb53-d9d4c825bd36 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:02:22.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8510" for this suite.
Aug 17 05:02:32.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:02:35.101: INFO: namespace projected-8510 deletion completed in 12.567687926s

• [SLOW TEST:17.802 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:02:35.101: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:02:35.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5157" for this suite.
Aug 17 05:03:11.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:03:14.520: INFO: namespace pods-5157 deletion completed in 39.059366839s

• [SLOW TEST:39.419 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:03:14.520: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Aug 17 05:03:14.811: INFO: Waiting up to 5m0s for pod "client-containers-b9a3a18b-4919-439f-96d6-fbce5f86c66d" in namespace "containers-7504" to be "success or failure"
Aug 17 05:03:14.829: INFO: Pod "client-containers-b9a3a18b-4919-439f-96d6-fbce5f86c66d": Phase="Pending", Reason="", readiness=false. Elapsed: 17.510395ms
Aug 17 05:03:16.945: INFO: Pod "client-containers-b9a3a18b-4919-439f-96d6-fbce5f86c66d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.134060726s
Aug 17 05:03:19.014: INFO: Pod "client-containers-b9a3a18b-4919-439f-96d6-fbce5f86c66d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.202848406s
STEP: Saw pod success
Aug 17 05:03:19.014: INFO: Pod "client-containers-b9a3a18b-4919-439f-96d6-fbce5f86c66d" satisfied condition "success or failure"
Aug 17 05:03:19.078: INFO: Trying to get logs from node 10.241.148.42 pod client-containers-b9a3a18b-4919-439f-96d6-fbce5f86c66d container test-container: <nil>
STEP: delete the pod
Aug 17 05:03:19.426: INFO: Waiting for pod client-containers-b9a3a18b-4919-439f-96d6-fbce5f86c66d to disappear
Aug 17 05:03:19.542: INFO: Pod client-containers-b9a3a18b-4919-439f-96d6-fbce5f86c66d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:03:19.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7504" for this suite.
Aug 17 05:03:30.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:03:34.324: INFO: namespace containers-7504 deletion completed in 14.151700186s

• [SLOW TEST:19.804 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:03:34.324: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-82f1a4b8-8f18-4c08-b23e-57351fb0b038
Aug 17 05:03:34.608: INFO: Pod name my-hostname-basic-82f1a4b8-8f18-4c08-b23e-57351fb0b038: Found 0 pods out of 1
Aug 17 05:03:39.639: INFO: Pod name my-hostname-basic-82f1a4b8-8f18-4c08-b23e-57351fb0b038: Found 1 pods out of 1
Aug 17 05:03:39.639: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-82f1a4b8-8f18-4c08-b23e-57351fb0b038" are running
Aug 17 05:03:39.655: INFO: Pod "my-hostname-basic-82f1a4b8-8f18-4c08-b23e-57351fb0b038-6cczx" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-17 05:03:34 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-17 05:03:36 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-17 05:03:36 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-17 05:03:34 +0000 UTC Reason: Message:}])
Aug 17 05:03:39.655: INFO: Trying to dial the pod
Aug 17 05:03:44.780: INFO: Controller my-hostname-basic-82f1a4b8-8f18-4c08-b23e-57351fb0b038: Got expected result from replica 1 [my-hostname-basic-82f1a4b8-8f18-4c08-b23e-57351fb0b038-6cczx]: "my-hostname-basic-82f1a4b8-8f18-4c08-b23e-57351fb0b038-6cczx", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:03:44.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9664" for this suite.
Aug 17 05:03:55.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:03:57.639: INFO: namespace replication-controller-9664 deletion completed in 12.730468907s

• [SLOW TEST:23.315 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:03:57.645: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 17 05:03:58.466: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 17 05:04:00.515: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733237438, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733237438, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733237438, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733237438, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 17 05:04:03.596: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 05:04:03.614: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1755-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:04:04.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3179" for this suite.
Aug 17 05:04:15.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:04:21.811: INFO: namespace webhook-3179 deletion completed in 16.775542723s
STEP: Destroying namespace "webhook-3179-markers" for this suite.
Aug 17 05:04:31.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:04:34.619: INFO: namespace webhook-3179-markers deletion completed in 12.807871422s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:37.055 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:04:34.701: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 05:04:35.027: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-1f0709e6-ed5c-4285-ba03-21c9764a069a
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-1f0709e6-ed5c-4285-ba03-21c9764a069a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:06:05.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4625" for this suite.
Aug 17 05:06:33.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:06:36.318: INFO: namespace configmap-4625 deletion completed in 30.825743953s

• [SLOW TEST:121.617 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:06:36.318: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:06:40.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5649" for this suite.
Aug 17 05:07:24.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:07:27.481: INFO: namespace kubelet-test-5649 deletion completed in 46.6274333s

• [SLOW TEST:51.163 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:07:27.482: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:07:39.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8874" for this suite.
Aug 17 05:07:47.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:07:51.549: INFO: namespace resourcequota-8874 deletion completed in 12.247135083s

• [SLOW TEST:24.067 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:07:51.549: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-cdbe24eb-67d9-46b3-8630-2f57b7cce6e9
STEP: Creating a pod to test consume configMaps
Aug 17 05:07:51.831: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9c022e1c-b643-4c45-881d-8183be66a381" in namespace "projected-3121" to be "success or failure"
Aug 17 05:07:51.848: INFO: Pod "pod-projected-configmaps-9c022e1c-b643-4c45-881d-8183be66a381": Phase="Pending", Reason="", readiness=false. Elapsed: 17.458425ms
Aug 17 05:07:53.866: INFO: Pod "pod-projected-configmaps-9c022e1c-b643-4c45-881d-8183be66a381": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035234919s
Aug 17 05:07:55.888: INFO: Pod "pod-projected-configmaps-9c022e1c-b643-4c45-881d-8183be66a381": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056708437s
STEP: Saw pod success
Aug 17 05:07:55.888: INFO: Pod "pod-projected-configmaps-9c022e1c-b643-4c45-881d-8183be66a381" satisfied condition "success or failure"
Aug 17 05:07:55.909: INFO: Trying to get logs from node 10.241.148.42 pod pod-projected-configmaps-9c022e1c-b643-4c45-881d-8183be66a381 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 17 05:07:56.003: INFO: Waiting for pod pod-projected-configmaps-9c022e1c-b643-4c45-881d-8183be66a381 to disappear
Aug 17 05:07:56.025: INFO: Pod pod-projected-configmaps-9c022e1c-b643-4c45-881d-8183be66a381 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:07:56.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3121" for this suite.
Aug 17 05:08:06.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:08:08.755: INFO: namespace projected-3121 deletion completed in 12.692101972s

• [SLOW TEST:17.206 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:08:08.755: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 05:08:09.137: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Aug 17 05:08:09.178: INFO: Number of nodes with available pods: 0
Aug 17 05:08:09.178: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Aug 17 05:08:09.301: INFO: Number of nodes with available pods: 0
Aug 17 05:08:09.301: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 05:08:10.323: INFO: Number of nodes with available pods: 0
Aug 17 05:08:10.323: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 05:08:11.333: INFO: Number of nodes with available pods: 1
Aug 17 05:08:11.333: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Aug 17 05:08:11.434: INFO: Number of nodes with available pods: 1
Aug 17 05:08:11.434: INFO: Number of running nodes: 0, number of available pods: 1
Aug 17 05:08:12.462: INFO: Number of nodes with available pods: 0
Aug 17 05:08:12.462: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Aug 17 05:08:12.507: INFO: Number of nodes with available pods: 0
Aug 17 05:08:12.507: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 05:08:13.527: INFO: Number of nodes with available pods: 0
Aug 17 05:08:13.527: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 05:08:14.526: INFO: Number of nodes with available pods: 0
Aug 17 05:08:14.526: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 05:08:15.528: INFO: Number of nodes with available pods: 0
Aug 17 05:08:15.528: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 05:08:16.575: INFO: Number of nodes with available pods: 0
Aug 17 05:08:16.575: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 05:08:17.674: INFO: Number of nodes with available pods: 0
Aug 17 05:08:17.674: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 05:08:18.561: INFO: Number of nodes with available pods: 0
Aug 17 05:08:18.561: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 05:08:19.567: INFO: Number of nodes with available pods: 0
Aug 17 05:08:19.568: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 05:08:20.671: INFO: Number of nodes with available pods: 0
Aug 17 05:08:20.671: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 05:08:21.526: INFO: Number of nodes with available pods: 0
Aug 17 05:08:21.526: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 05:08:22.535: INFO: Number of nodes with available pods: 0
Aug 17 05:08:22.535: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 05:08:23.530: INFO: Number of nodes with available pods: 0
Aug 17 05:08:23.530: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 05:08:24.528: INFO: Number of nodes with available pods: 1
Aug 17 05:08:24.528: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1072, will wait for the garbage collector to delete the pods
Aug 17 05:08:24.739: INFO: Deleting DaemonSet.extensions daemon-set took: 101.833046ms
Aug 17 05:08:25.340: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.467266ms
Aug 17 05:08:31.457: INFO: Number of nodes with available pods: 0
Aug 17 05:08:31.457: INFO: Number of running nodes: 0, number of available pods: 0
Aug 17 05:08:31.474: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1072/daemonsets","resourceVersion":"59828"},"items":null}

Aug 17 05:08:31.491: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1072/pods","resourceVersion":"59829"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:08:31.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1072" for this suite.
Aug 17 05:08:41.733: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:08:44.590: INFO: namespace daemonsets-1072 deletion completed in 12.926995318s

• [SLOW TEST:35.835 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:08:44.592: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 17 05:08:53.401: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 17 05:08:53.428: INFO: Pod pod-with-prestop-http-hook still exists
Aug 17 05:08:55.428: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 17 05:08:55.503: INFO: Pod pod-with-prestop-http-hook still exists
Aug 17 05:08:57.428: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 17 05:08:57.447: INFO: Pod pod-with-prestop-http-hook still exists
Aug 17 05:08:59.428: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 17 05:08:59.447: INFO: Pod pod-with-prestop-http-hook still exists
Aug 17 05:09:01.428: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 17 05:09:01.458: INFO: Pod pod-with-prestop-http-hook still exists
Aug 17 05:09:03.428: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 17 05:09:03.522: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:09:03.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9572" for this suite.
Aug 17 05:09:39.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:09:42.516: INFO: namespace container-lifecycle-hook-9572 deletion completed in 38.745780276s

• [SLOW TEST:57.924 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:09:42.517: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-e16c4406-77af-49c6-b8a9-7d74c9513b66
STEP: Creating a pod to test consume secrets
Aug 17 05:09:42.831: INFO: Waiting up to 5m0s for pod "pod-secrets-fe67ce03-5a2d-4bef-b8d9-5c44f847c692" in namespace "secrets-9621" to be "success or failure"
Aug 17 05:09:42.850: INFO: Pod "pod-secrets-fe67ce03-5a2d-4bef-b8d9-5c44f847c692": Phase="Pending", Reason="", readiness=false. Elapsed: 19.469265ms
Aug 17 05:09:44.876: INFO: Pod "pod-secrets-fe67ce03-5a2d-4bef-b8d9-5c44f847c692": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045045572s
Aug 17 05:09:46.965: INFO: Pod "pod-secrets-fe67ce03-5a2d-4bef-b8d9-5c44f847c692": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.1335341s
STEP: Saw pod success
Aug 17 05:09:46.965: INFO: Pod "pod-secrets-fe67ce03-5a2d-4bef-b8d9-5c44f847c692" satisfied condition "success or failure"
Aug 17 05:09:47.029: INFO: Trying to get logs from node 10.241.148.42 pod pod-secrets-fe67ce03-5a2d-4bef-b8d9-5c44f847c692 container secret-volume-test: <nil>
STEP: delete the pod
Aug 17 05:09:47.256: INFO: Waiting for pod pod-secrets-fe67ce03-5a2d-4bef-b8d9-5c44f847c692 to disappear
Aug 17 05:09:47.318: INFO: Pod pod-secrets-fe67ce03-5a2d-4bef-b8d9-5c44f847c692 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:09:47.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9621" for this suite.
Aug 17 05:09:57.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:10:00.524: INFO: namespace secrets-9621 deletion completed in 12.843540183s

• [SLOW TEST:18.007 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:10:00.524: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Aug 17 05:10:00.799: INFO: Pod name pod-release: Found 0 pods out of 1
Aug 17 05:10:05.839: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:10:05.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-778" for this suite.
Aug 17 05:10:16.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:10:20.751: INFO: namespace replication-controller-778 deletion completed in 14.689882161s

• [SLOW TEST:20.228 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:10:20.752: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 17 05:10:21.035: INFO: Waiting up to 5m0s for pod "downwardapi-volume-04378cdd-777d-4c96-959a-1e0c18e0a256" in namespace "projected-1309" to be "success or failure"
Aug 17 05:10:21.053: INFO: Pod "downwardapi-volume-04378cdd-777d-4c96-959a-1e0c18e0a256": Phase="Pending", Reason="", readiness=false. Elapsed: 17.862285ms
Aug 17 05:10:23.071: INFO: Pod "downwardapi-volume-04378cdd-777d-4c96-959a-1e0c18e0a256": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036533481s
Aug 17 05:10:25.092: INFO: Pod "downwardapi-volume-04378cdd-777d-4c96-959a-1e0c18e0a256": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057464016s
STEP: Saw pod success
Aug 17 05:10:25.092: INFO: Pod "downwardapi-volume-04378cdd-777d-4c96-959a-1e0c18e0a256" satisfied condition "success or failure"
Aug 17 05:10:25.114: INFO: Trying to get logs from node 10.241.148.42 pod downwardapi-volume-04378cdd-777d-4c96-959a-1e0c18e0a256 container client-container: <nil>
STEP: delete the pod
Aug 17 05:10:25.229: INFO: Waiting for pod downwardapi-volume-04378cdd-777d-4c96-959a-1e0c18e0a256 to disappear
Aug 17 05:10:25.254: INFO: Pod downwardapi-volume-04378cdd-777d-4c96-959a-1e0c18e0a256 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:10:25.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1309" for this suite.
Aug 17 05:10:33.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:10:36.011: INFO: namespace projected-1309 deletion completed in 10.676683876s

• [SLOW TEST:15.259 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:10:36.013: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 05:10:36.253: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 17 05:10:36.300: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 17 05:10:41.320: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 17 05:10:41.320: INFO: Creating deployment "test-rolling-update-deployment"
Aug 17 05:10:41.339: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 17 05:10:41.368: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 17 05:10:43.480: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 17 05:10:43.518: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733237841, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733237841, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733237841, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733237841, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 05:10:45.533: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Aug 17 05:10:45.576: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9942 /apis/apps/v1/namespaces/deployment-9942/deployments/test-rolling-update-deployment 70f2875e-3b36-4944-ad02-6fb4083720fb 60784 1 2020-08-17 05:10:41 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0037bc468 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-08-17 05:10:41 +0000 UTC,LastTransitionTime:2020-08-17 05:10:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2020-08-17 05:10:44 +0000 UTC,LastTransitionTime:2020-08-17 05:10:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 17 05:10:45.593: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-9942 /apis/apps/v1/namespaces/deployment-9942/replicasets/test-rolling-update-deployment-55d946486 ee45b1cf-aa00-42a1-b51a-e9221dba2132 60773 1 2020-08-17 05:10:41 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 70f2875e-3b36-4944-ad02-6fb4083720fb 0xc000772f50 0xc000772f51}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc000772fd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 17 05:10:45.593: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 17 05:10:45.594: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9942 /apis/apps/v1/namespaces/deployment-9942/replicasets/test-rolling-update-controller 10d9a4eb-fd41-4ad7-a134-c02f17209abf 60783 2 2020-08-17 05:10:36 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 70f2875e-3b36-4944-ad02-6fb4083720fb 0xc000772e77 0xc000772e78}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000772ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 17 05:10:45.613: INFO: Pod "test-rolling-update-deployment-55d946486-qcxlc" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-qcxlc test-rolling-update-deployment-55d946486- deployment-9942 /api/v1/namespaces/deployment-9942/pods/test-rolling-update-deployment-55d946486-qcxlc 45daa1fb-1e3d-4725-9506-74072814654b 60772 0 2020-08-17 05:10:41 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[cni.projectcalico.org/podIP:172.30.218.114/32 cni.projectcalico.org/podIPs:172.30.218.114/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.218.114"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 ee45b1cf-aa00-42a1-b51a-e9221dba2132 0xc000773580 0xc000773581}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-td279,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-td279,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-td279,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.42,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zvpxf,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 05:10:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 05:10:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 05:10:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 05:10:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.42,PodIP:172.30.218.114,StartTime:2020-08-17 05:10:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-17 05:10:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://2cdb64106a126b0a5e7eb68b9934418c27ffe3ba30ae9dda717341637bb28094,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.218.114,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:10:45.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9942" for this suite.
Aug 17 05:10:55.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:10:58.204: INFO: namespace deployment-9942 deletion completed in 12.512382593s

• [SLOW TEST:22.191 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:10:58.204: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:11:11.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2718" for this suite.
Aug 17 05:11:20.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:11:22.516: INFO: namespace resourcequota-2718 deletion completed in 10.497380027s

• [SLOW TEST:24.311 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:11:22.516: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-7818/configmap-test-ff34d593-3393-4920-89d0-9964a344b4a8
STEP: Creating a pod to test consume configMaps
Aug 17 05:11:22.815: INFO: Waiting up to 5m0s for pod "pod-configmaps-2d5e0cd2-11f9-4359-8eec-9a249c50cfd8" in namespace "configmap-7818" to be "success or failure"
Aug 17 05:11:22.835: INFO: Pod "pod-configmaps-2d5e0cd2-11f9-4359-8eec-9a249c50cfd8": Phase="Pending", Reason="", readiness=false. Elapsed: 19.566611ms
Aug 17 05:11:24.856: INFO: Pod "pod-configmaps-2d5e0cd2-11f9-4359-8eec-9a249c50cfd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040269794s
STEP: Saw pod success
Aug 17 05:11:24.856: INFO: Pod "pod-configmaps-2d5e0cd2-11f9-4359-8eec-9a249c50cfd8" satisfied condition "success or failure"
Aug 17 05:11:24.876: INFO: Trying to get logs from node 10.241.148.42 pod pod-configmaps-2d5e0cd2-11f9-4359-8eec-9a249c50cfd8 container env-test: <nil>
STEP: delete the pod
Aug 17 05:11:24.965: INFO: Waiting for pod pod-configmaps-2d5e0cd2-11f9-4359-8eec-9a249c50cfd8 to disappear
Aug 17 05:11:24.990: INFO: Pod pod-configmaps-2d5e0cd2-11f9-4359-8eec-9a249c50cfd8 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:11:24.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7818" for this suite.
Aug 17 05:11:33.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:11:35.537: INFO: namespace configmap-7818 deletion completed in 10.510566613s

• [SLOW TEST:13.021 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:11:35.538: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8941.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8941.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 17 05:11:40.266: INFO: DNS probes using dns-8941/dns-test-a6d822c6-e1f8-46a8-ab52-8c4b92cb02c0 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:11:40.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8941" for this suite.
Aug 17 05:11:50.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:11:52.967: INFO: namespace dns-8941 deletion completed in 12.570900663s

• [SLOW TEST:17.430 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:11:52.967: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:12:09.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-687" for this suite.
Aug 17 05:12:19.674: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:12:22.285: INFO: namespace resourcequota-687 deletion completed in 12.775941772s

• [SLOW TEST:29.318 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:12:22.285: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-dbm9
STEP: Creating a pod to test atomic-volume-subpath
Aug 17 05:12:22.661: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-dbm9" in namespace "subpath-8663" to be "success or failure"
Aug 17 05:12:22.690: INFO: Pod "pod-subpath-test-projected-dbm9": Phase="Pending", Reason="", readiness=false. Elapsed: 29.024651ms
Aug 17 05:12:24.714: INFO: Pod "pod-subpath-test-projected-dbm9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052801624s
Aug 17 05:12:26.736: INFO: Pod "pod-subpath-test-projected-dbm9": Phase="Running", Reason="", readiness=true. Elapsed: 4.075037188s
Aug 17 05:12:28.759: INFO: Pod "pod-subpath-test-projected-dbm9": Phase="Running", Reason="", readiness=true. Elapsed: 6.09832801s
Aug 17 05:12:30.777: INFO: Pod "pod-subpath-test-projected-dbm9": Phase="Running", Reason="", readiness=true. Elapsed: 8.116048102s
Aug 17 05:12:32.805: INFO: Pod "pod-subpath-test-projected-dbm9": Phase="Running", Reason="", readiness=true. Elapsed: 10.143517754s
Aug 17 05:12:34.830: INFO: Pod "pod-subpath-test-projected-dbm9": Phase="Running", Reason="", readiness=true. Elapsed: 12.168908912s
Aug 17 05:12:36.853: INFO: Pod "pod-subpath-test-projected-dbm9": Phase="Running", Reason="", readiness=true. Elapsed: 14.192261905s
Aug 17 05:12:38.874: INFO: Pod "pod-subpath-test-projected-dbm9": Phase="Running", Reason="", readiness=true. Elapsed: 16.212800311s
Aug 17 05:12:40.896: INFO: Pod "pod-subpath-test-projected-dbm9": Phase="Running", Reason="", readiness=true. Elapsed: 18.234478721s
Aug 17 05:12:42.916: INFO: Pod "pod-subpath-test-projected-dbm9": Phase="Running", Reason="", readiness=true. Elapsed: 20.254493214s
Aug 17 05:12:44.938: INFO: Pod "pod-subpath-test-projected-dbm9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.277123356s
STEP: Saw pod success
Aug 17 05:12:44.938: INFO: Pod "pod-subpath-test-projected-dbm9" satisfied condition "success or failure"
Aug 17 05:12:44.962: INFO: Trying to get logs from node 10.241.148.42 pod pod-subpath-test-projected-dbm9 container test-container-subpath-projected-dbm9: <nil>
STEP: delete the pod
Aug 17 05:12:45.068: INFO: Waiting for pod pod-subpath-test-projected-dbm9 to disappear
Aug 17 05:12:45.100: INFO: Pod pod-subpath-test-projected-dbm9 no longer exists
STEP: Deleting pod pod-subpath-test-projected-dbm9
Aug 17 05:12:45.100: INFO: Deleting pod "pod-subpath-test-projected-dbm9" in namespace "subpath-8663"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:12:45.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8663" for this suite.
Aug 17 05:12:55.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:12:57.862: INFO: namespace subpath-8663 deletion completed in 12.708877333s

• [SLOW TEST:35.577 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:12:57.863: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 05:12:58.130: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 17 05:13:03.150: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 17 05:13:03.150: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Aug 17 05:13:03.249: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7352 /apis/apps/v1/namespaces/deployment-7352/deployments/test-cleanup-deployment 8b76d708-ef9f-48d5-855c-b9743e55200a 61807 1 2020-08-17 05:13:03 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0059fcf28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Aug 17 05:13:03.271: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-7352 /apis/apps/v1/namespaces/deployment-7352/replicasets/test-cleanup-deployment-65db99849b 026c1e8b-0723-4c1b-a6b9-a6bf9f5f32e5 61809 1 2020-08-17 05:13:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 8b76d708-ef9f-48d5-855c-b9743e55200a 0xc0059fd357 0xc0059fd358}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0059fd3b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 17 05:13:03.271: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Aug 17 05:13:03.271: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-7352 /apis/apps/v1/namespaces/deployment-7352/replicasets/test-cleanup-controller c9c90c52-c9f1-4c81-aae5-ecf0df95a76f 61808 1 2020-08-17 05:12:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 8b76d708-ef9f-48d5-855c-b9743e55200a 0xc0059fd287 0xc0059fd288}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0059fd2e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 17 05:13:03.301: INFO: Pod "test-cleanup-controller-qqw8c" is available:
&Pod{ObjectMeta:{test-cleanup-controller-qqw8c test-cleanup-controller- deployment-7352 /api/v1/namespaces/deployment-7352/pods/test-cleanup-controller-qqw8c 1a39351c-be72-4d8b-b2b6-a4d04e243eef 61795 0 2020-08-17 05:12:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:172.30.218.118/32 cni.projectcalico.org/podIPs:172.30.218.118/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.218.118"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-cleanup-controller c9c90c52-c9f1-4c81-aae5-ecf0df95a76f 0xc0033d1e37 0xc0033d1e38}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v62fg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v62fg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v62fg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.42,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 05:12:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 05:13:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 05:13:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 05:12:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.42,PodIP:172.30.218.118,StartTime:2020-08-17 05:12:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-17 05:13:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://f5cb71989fceb76acf9bbc50a689a78bdd863073b0592964de7fbcd1dbe1519e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.218.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:13:03.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7352" for this suite.
Aug 17 05:13:13.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:13:16.034: INFO: namespace deployment-7352 deletion completed in 12.704274406s

• [SLOW TEST:18.171 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:13:16.034: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:13:16.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7472" for this suite.
Aug 17 05:13:25.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:13:27.553: INFO: namespace resourcequota-7472 deletion completed in 10.574289392s

• [SLOW TEST:11.519 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:13:27.553: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-lxjl
STEP: Creating a pod to test atomic-volume-subpath
Aug 17 05:13:28.029: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lxjl" in namespace "subpath-5834" to be "success or failure"
Aug 17 05:13:28.049: INFO: Pod "pod-subpath-test-configmap-lxjl": Phase="Pending", Reason="", readiness=false. Elapsed: 20.005041ms
Aug 17 05:13:30.073: INFO: Pod "pod-subpath-test-configmap-lxjl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043192876s
Aug 17 05:13:32.095: INFO: Pod "pod-subpath-test-configmap-lxjl": Phase="Running", Reason="", readiness=true. Elapsed: 4.066029716s
Aug 17 05:13:34.183: INFO: Pod "pod-subpath-test-configmap-lxjl": Phase="Running", Reason="", readiness=true. Elapsed: 6.153906165s
Aug 17 05:13:36.207: INFO: Pod "pod-subpath-test-configmap-lxjl": Phase="Running", Reason="", readiness=true. Elapsed: 8.177727934s
Aug 17 05:13:38.226: INFO: Pod "pod-subpath-test-configmap-lxjl": Phase="Running", Reason="", readiness=true. Elapsed: 10.19622619s
Aug 17 05:13:40.248: INFO: Pod "pod-subpath-test-configmap-lxjl": Phase="Running", Reason="", readiness=true. Elapsed: 12.218566364s
Aug 17 05:13:42.268: INFO: Pod "pod-subpath-test-configmap-lxjl": Phase="Running", Reason="", readiness=true. Elapsed: 14.238511568s
Aug 17 05:13:44.293: INFO: Pod "pod-subpath-test-configmap-lxjl": Phase="Running", Reason="", readiness=true. Elapsed: 16.263744867s
Aug 17 05:13:46.395: INFO: Pod "pod-subpath-test-configmap-lxjl": Phase="Running", Reason="", readiness=true. Elapsed: 18.366064697s
Aug 17 05:13:48.468: INFO: Pod "pod-subpath-test-configmap-lxjl": Phase="Running", Reason="", readiness=true. Elapsed: 20.438857288s
Aug 17 05:13:50.605: INFO: Pod "pod-subpath-test-configmap-lxjl": Phase="Running", Reason="", readiness=true. Elapsed: 22.575978543s
Aug 17 05:13:52.632: INFO: Pod "pod-subpath-test-configmap-lxjl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.602841574s
STEP: Saw pod success
Aug 17 05:13:52.632: INFO: Pod "pod-subpath-test-configmap-lxjl" satisfied condition "success or failure"
Aug 17 05:13:52.651: INFO: Trying to get logs from node 10.241.148.42 pod pod-subpath-test-configmap-lxjl container test-container-subpath-configmap-lxjl: <nil>
STEP: delete the pod
Aug 17 05:13:52.754: INFO: Waiting for pod pod-subpath-test-configmap-lxjl to disappear
Aug 17 05:13:52.771: INFO: Pod pod-subpath-test-configmap-lxjl no longer exists
STEP: Deleting pod pod-subpath-test-configmap-lxjl
Aug 17 05:13:52.771: INFO: Deleting pod "pod-subpath-test-configmap-lxjl" in namespace "subpath-5834"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:13:52.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5834" for this suite.
Aug 17 05:14:02.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:14:05.816: INFO: namespace subpath-5834 deletion completed in 12.956149523s

• [SLOW TEST:38.263 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:14:05.817: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 05:14:32.207: INFO: Container started at 2020-08-17 05:14:07 +0000 UTC, pod became ready at 2020-08-17 05:14:31 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:14:32.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5076" for this suite.
Aug 17 05:15:08.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:15:10.983: INFO: namespace container-probe-5076 deletion completed in 38.682779279s

• [SLOW TEST:65.166 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:15:10.986: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Aug 17 05:15:11.178: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:15:16.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1335" for this suite.
Aug 17 05:15:53.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:15:55.831: INFO: namespace init-container-1335 deletion completed in 38.993932413s

• [SLOW TEST:44.845 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:15:55.831: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Aug 17 05:16:00.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec pod-sharedvolume-942680e6-0454-47d2-93fd-5d6e452fba3d -c busybox-main-container --namespace=emptydir-298 -- cat /usr/share/volumeshare/shareddata.txt'
Aug 17 05:16:01.684: INFO: stderr: ""
Aug 17 05:16:01.684: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:16:01.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-298" for this suite.
Aug 17 05:16:11.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:16:14.272: INFO: namespace emptydir-298 deletion completed in 12.527017572s

• [SLOW TEST:18.442 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:16:14.274: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 05:16:14.532: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-c04e9d4a-a5b4-4b64-8a66-863ac05b5b97" in namespace "security-context-test-5695" to be "success or failure"
Aug 17 05:16:14.551: INFO: Pod "busybox-readonly-false-c04e9d4a-a5b4-4b64-8a66-863ac05b5b97": Phase="Pending", Reason="", readiness=false. Elapsed: 18.593541ms
Aug 17 05:16:16.617: INFO: Pod "busybox-readonly-false-c04e9d4a-a5b4-4b64-8a66-863ac05b5b97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.084644129s
Aug 17 05:16:18.731: INFO: Pod "busybox-readonly-false-c04e9d4a-a5b4-4b64-8a66-863ac05b5b97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.19879809s
Aug 17 05:16:18.731: INFO: Pod "busybox-readonly-false-c04e9d4a-a5b4-4b64-8a66-863ac05b5b97" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:16:18.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5695" for this suite.
Aug 17 05:16:29.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:16:32.503: INFO: namespace security-context-test-5695 deletion completed in 13.092802296s

• [SLOW TEST:18.229 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:16:32.503: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-8688
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 17 05:16:32.694: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 17 05:16:55.754: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.218.119:8080/dial?request=hostName&protocol=udp&host=172.30.218.126&port=8081&tries=1'] Namespace:pod-network-test-8688 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 17 05:16:55.754: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 05:16:56.000: INFO: Waiting for endpoints: map[]
Aug 17 05:16:56.024: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.218.119:8080/dial?request=hostName&protocol=udp&host=172.30.151.166&port=8081&tries=1'] Namespace:pod-network-test-8688 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 17 05:16:56.024: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 05:16:56.261: INFO: Waiting for endpoints: map[]
Aug 17 05:16:56.283: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.218.119:8080/dial?request=hostName&protocol=udp&host=172.30.201.226&port=8081&tries=1'] Namespace:pod-network-test-8688 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 17 05:16:56.283: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 05:16:56.547: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:16:56.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8688" for this suite.
Aug 17 05:17:06.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:17:09.066: INFO: namespace pod-network-test-8688 deletion completed in 12.473910552s

• [SLOW TEST:36.563 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:17:09.067: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 17 05:17:09.955: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 17 05:17:12.003: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238229, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238229, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238230, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238229, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 17 05:17:15.068: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:17:15.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9085" for this suite.
Aug 17 05:17:25.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:17:27.968: INFO: namespace webhook-9085 deletion completed in 12.476122734s
STEP: Destroying namespace "webhook-9085-markers" for this suite.
Aug 17 05:17:38.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:17:40.506: INFO: namespace webhook-9085-markers deletion completed in 12.537467916s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:31.538 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:17:40.606: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 05:17:41.105: INFO: Create a RollingUpdate DaemonSet
Aug 17 05:17:41.131: INFO: Check that daemon pods launch on every node of the cluster
Aug 17 05:17:41.184: INFO: Number of nodes with available pods: 0
Aug 17 05:17:41.184: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 05:17:42.244: INFO: Number of nodes with available pods: 0
Aug 17 05:17:42.244: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 05:17:43.241: INFO: Number of nodes with available pods: 1
Aug 17 05:17:43.241: INFO: Node 10.241.148.42 is running more than one daemon pod
Aug 17 05:17:44.249: INFO: Number of nodes with available pods: 3
Aug 17 05:17:44.249: INFO: Number of running nodes: 3, number of available pods: 3
Aug 17 05:17:44.249: INFO: Update the DaemonSet to trigger a rollout
Aug 17 05:17:44.378: INFO: Updating DaemonSet daemon-set
Aug 17 05:17:53.511: INFO: Roll back the DaemonSet before rollout is complete
Aug 17 05:17:53.567: INFO: Updating DaemonSet daemon-set
Aug 17 05:17:53.567: INFO: Make sure DaemonSet rollback is complete
Aug 17 05:17:53.595: INFO: Wrong image for pod: daemon-set-prtf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 17 05:17:53.595: INFO: Pod daemon-set-prtf8 is not available
Aug 17 05:17:54.651: INFO: Wrong image for pod: daemon-set-prtf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 17 05:17:54.651: INFO: Pod daemon-set-prtf8 is not available
Aug 17 05:17:55.657: INFO: Wrong image for pod: daemon-set-prtf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 17 05:17:55.657: INFO: Pod daemon-set-prtf8 is not available
Aug 17 05:17:56.653: INFO: Wrong image for pod: daemon-set-prtf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 17 05:17:56.653: INFO: Pod daemon-set-prtf8 is not available
Aug 17 05:17:57.651: INFO: Wrong image for pod: daemon-set-prtf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 17 05:17:57.651: INFO: Pod daemon-set-prtf8 is not available
Aug 17 05:17:58.651: INFO: Wrong image for pod: daemon-set-prtf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 17 05:17:58.651: INFO: Pod daemon-set-prtf8 is not available
Aug 17 05:17:59.654: INFO: Wrong image for pod: daemon-set-prtf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 17 05:17:59.654: INFO: Pod daemon-set-prtf8 is not available
Aug 17 05:18:00.650: INFO: Wrong image for pod: daemon-set-prtf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 17 05:18:00.650: INFO: Pod daemon-set-prtf8 is not available
Aug 17 05:18:01.656: INFO: Wrong image for pod: daemon-set-prtf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 17 05:18:01.656: INFO: Pod daemon-set-prtf8 is not available
Aug 17 05:18:02.658: INFO: Pod daemon-set-l6czp is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4406, will wait for the garbage collector to delete the pods
Aug 17 05:18:02.873: INFO: Deleting DaemonSet.extensions daemon-set took: 44.591174ms
Aug 17 05:18:03.573: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.300738ms
Aug 17 05:18:11.392: INFO: Number of nodes with available pods: 0
Aug 17 05:18:11.392: INFO: Number of running nodes: 0, number of available pods: 0
Aug 17 05:18:11.408: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4406/daemonsets","resourceVersion":"63993"},"items":null}

Aug 17 05:18:11.432: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4406/pods","resourceVersion":"63994"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:18:11.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4406" for this suite.
Aug 17 05:18:21.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:18:24.287: INFO: namespace daemonsets-4406 deletion completed in 12.744787847s

• [SLOW TEST:43.681 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:18:24.289: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0817 05:19:04.853929      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 17 05:19:04.854: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:19:04.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2657" for this suite.
Aug 17 05:19:17.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:19:21.425: INFO: namespace gc-2657 deletion completed in 16.540404516s

• [SLOW TEST:57.137 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:19:21.426: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-593
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 17 05:19:21.660: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 17 05:19:46.692: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.218.71:8080/dial?request=hostName&protocol=http&host=172.30.218.65&port=8080&tries=1'] Namespace:pod-network-test-593 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 17 05:19:46.692: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 05:19:46.929: INFO: Waiting for endpoints: map[]
Aug 17 05:19:47.069: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.218.71:8080/dial?request=hostName&protocol=http&host=172.30.151.171&port=8080&tries=1'] Namespace:pod-network-test-593 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 17 05:19:47.069: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 05:19:47.373: INFO: Waiting for endpoints: map[]
Aug 17 05:19:47.465: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.218.71:8080/dial?request=hostName&protocol=http&host=172.30.201.234&port=8080&tries=1'] Namespace:pod-network-test-593 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 17 05:19:47.465: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 05:19:48.105: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:19:48.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-593" for this suite.
Aug 17 05:19:58.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:20:01.174: INFO: namespace pod-network-test-593 deletion completed in 12.697200181s

• [SLOW TEST:39.749 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:20:01.175: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 05:20:01.506: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-1f39c54c-3a18-45ff-b7c2-cf41beb7126c" in namespace "security-context-test-4371" to be "success or failure"
Aug 17 05:20:01.523: INFO: Pod "alpine-nnp-false-1f39c54c-3a18-45ff-b7c2-cf41beb7126c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.650406ms
Aug 17 05:20:03.610: INFO: Pod "alpine-nnp-false-1f39c54c-3a18-45ff-b7c2-cf41beb7126c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.104030012s
Aug 17 05:20:05.628: INFO: Pod "alpine-nnp-false-1f39c54c-3a18-45ff-b7c2-cf41beb7126c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.122119317s
Aug 17 05:20:05.628: INFO: Pod "alpine-nnp-false-1f39c54c-3a18-45ff-b7c2-cf41beb7126c" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:20:05.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4371" for this suite.
Aug 17 05:20:15.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:20:20.834: INFO: namespace security-context-test-4371 deletion completed in 15.055802289s

• [SLOW TEST:19.659 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:20:20.835: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Aug 17 05:20:21.095: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Aug 17 05:20:21.656: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Aug 17 05:20:23.889: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238421, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238421, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238421, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238421, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 05:20:25.904: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238421, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238421, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238421, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238421, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 05:20:27.903: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238421, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238421, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238421, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238421, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 05:20:29.904: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238421, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238421, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238421, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238421, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 05:20:31.904: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238421, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238421, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238421, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238421, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 05:20:33.903: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238421, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238421, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238421, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238421, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 05:20:36.426: INFO: Waited 494.058226ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:20:38.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5970" for this suite.
Aug 17 05:20:48.884: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:20:51.860: INFO: namespace aggregator-5970 deletion completed in 13.158010735s

• [SLOW TEST:31.025 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:20:51.861: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-55e9bcfa-00ea-4b93-96b0-a52e80b0965a in namespace container-probe-3679
Aug 17 05:20:56.208: INFO: Started pod test-webserver-55e9bcfa-00ea-4b93-96b0-a52e80b0965a in namespace container-probe-3679
STEP: checking the pod's current state and verifying that restartCount is present
Aug 17 05:20:56.228: INFO: Initial restart count of pod test-webserver-55e9bcfa-00ea-4b93-96b0-a52e80b0965a is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:24:58.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3679" for this suite.
Aug 17 05:25:08.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:25:11.010: INFO: namespace container-probe-3679 deletion completed in 12.744248121s

• [SLOW TEST:259.149 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:25:11.011: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 17 05:25:12.307: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 17 05:25:14.370: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238712, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238712, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238712, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733238712, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 17 05:25:17.551: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:25:30.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7940" for this suite.
Aug 17 05:25:40.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:25:43.491: INFO: namespace webhook-7940 deletion completed in 12.573645428s
STEP: Destroying namespace "webhook-7940-markers" for this suite.
Aug 17 05:25:51.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:25:54.224: INFO: namespace webhook-7940-markers deletion completed in 10.732393067s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:43.288 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:25:54.298: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3234
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3234
STEP: creating replication controller externalsvc in namespace services-3234
I0817 05:25:54.700925      22 runners.go:184] Created replication controller with name: externalsvc, namespace: services-3234, replica count: 2
I0817 05:25:57.751566      22 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Aug 17 05:25:57.872: INFO: Creating new exec pod
Aug 17 05:26:01.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=services-3234 execpodnshfl -- /bin/sh -x -c nslookup nodeport-service'
Aug 17 05:26:02.590: INFO: stderr: "+ nslookup nodeport-service\n"
Aug 17 05:26:02.590: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-3234.svc.cluster.local\tcanonical name = externalsvc.services-3234.svc.cluster.local.\nName:\texternalsvc.services-3234.svc.cluster.local\nAddress: 172.21.91.173\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3234, will wait for the garbage collector to delete the pods
Aug 17 05:26:02.707: INFO: Deleting ReplicationController externalsvc took: 47.263885ms
Aug 17 05:26:03.407: INFO: Terminating ReplicationController externalsvc pods took: 700.297203ms
Aug 17 05:26:08.567: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:26:08.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3234" for this suite.
Aug 17 05:26:18.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:26:21.646: INFO: namespace services-3234 deletion completed in 12.921630281s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:27.348 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:26:21.647: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: Gathering metrics
Aug 17 05:26:22.007: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0817 05:26:22.007629      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 17 05:26:22.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5593" for this suite.
Aug 17 05:26:32.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:26:35.515: INFO: namespace gc-5593 deletion completed in 13.46900295s

• [SLOW TEST:13.869 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:26:35.516: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7749
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7749
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7749
Aug 17 05:26:35.817: INFO: Found 0 stateful pods, waiting for 1
Aug 17 05:26:45.836: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Aug 17 05:26:45.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=statefulset-7749 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 05:26:46.255: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 05:26:46.255: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 05:26:46.255: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 05:26:46.366: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 17 05:26:56.404: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 17 05:26:56.404: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 05:26:56.473: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999997109s
Aug 17 05:26:57.507: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.979891233s
Aug 17 05:26:58.531: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.945713256s
Aug 17 05:26:59.550: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.921898713s
Aug 17 05:27:00.570: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.903002486s
Aug 17 05:27:01.591: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.883182208s
Aug 17 05:27:02.609: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.862367284s
Aug 17 05:27:03.628: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.843659765s
Aug 17 05:27:04.659: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.824653681s
Aug 17 05:27:05.685: INFO: Verifying statefulset ss doesn't scale past 1 for another 793.8847ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7749
Aug 17 05:27:06.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=statefulset-7749 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 05:27:07.620: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 17 05:27:07.620: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 17 05:27:07.620: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 17 05:27:07.673: INFO: Found 1 stateful pods, waiting for 3
Aug 17 05:27:17.758: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 05:27:17.758: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 05:27:17.758: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Aug 17 05:27:17.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=statefulset-7749 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 05:27:19.011: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 05:27:19.011: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 05:27:19.011: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 05:27:19.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=statefulset-7749 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 05:27:19.424: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 05:27:19.424: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 05:27:19.424: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 05:27:19.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=statefulset-7749 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 05:27:19.907: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 05:27:19.907: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 05:27:19.907: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 05:27:19.907: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 05:27:19.961: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Aug 17 05:27:30.063: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 17 05:27:30.063: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 17 05:27:30.063: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 17 05:27:30.142: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999996992s
Aug 17 05:27:31.167: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.958973964s
Aug 17 05:27:32.191: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.933975711s
Aug 17 05:27:33.215: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.910171233s
Aug 17 05:27:34.242: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.886042068s
Aug 17 05:27:35.268: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.858349291s
Aug 17 05:27:36.291: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.83326561s
Aug 17 05:27:37.322: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.809956628s
Aug 17 05:27:38.347: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.778705699s
Aug 17 05:27:39.382: INFO: Verifying statefulset ss doesn't scale past 3 for another 754.105938ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7749
Aug 17 05:27:40.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=statefulset-7749 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 05:27:41.688: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 17 05:27:41.688: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 17 05:27:41.688: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 17 05:27:41.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=statefulset-7749 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 05:27:42.101: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 17 05:27:42.101: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 17 05:27:42.101: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 17 05:27:42.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=statefulset-7749 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 05:27:42.528: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 17 05:27:42.528: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 17 05:27:42.528: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 17 05:27:42.528: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Aug 17 05:28:12.615: INFO: Deleting all statefulset in ns statefulset-7749
Aug 17 05:28:12.633: INFO: Scaling statefulset ss to 0
Aug 17 05:28:12.683: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 05:28:12.696: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:28:12.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7749" for this suite.
Aug 17 05:28:22.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:28:25.489: INFO: namespace statefulset-7749 deletion completed in 12.595773424s

• [SLOW TEST:109.973 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:28:25.489: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 17 05:28:26.882: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 17 05:28:29.981: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:28:30.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4963" for this suite.
Aug 17 05:28:40.448: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:28:43.733: INFO: namespace webhook-4963 deletion completed in 13.371137852s
STEP: Destroying namespace "webhook-4963-markers" for this suite.
Aug 17 05:28:51.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:28:54.382: INFO: namespace webhook-4963-markers deletion completed in 10.649349863s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:28.968 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:28:54.458: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-8e1cfc5c-0f57-4de2-915d-b44be65b36b9
STEP: Creating a pod to test consume secrets
Aug 17 05:28:54.754: INFO: Waiting up to 5m0s for pod "pod-secrets-e9ce26c7-5f47-484f-9ab8-6f0d6a99d2f5" in namespace "secrets-8715" to be "success or failure"
Aug 17 05:28:54.772: INFO: Pod "pod-secrets-e9ce26c7-5f47-484f-9ab8-6f0d6a99d2f5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.144007ms
Aug 17 05:28:56.798: INFO: Pod "pod-secrets-e9ce26c7-5f47-484f-9ab8-6f0d6a99d2f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.044151244s
STEP: Saw pod success
Aug 17 05:28:56.799: INFO: Pod "pod-secrets-e9ce26c7-5f47-484f-9ab8-6f0d6a99d2f5" satisfied condition "success or failure"
Aug 17 05:28:56.817: INFO: Trying to get logs from node 10.241.148.42 pod pod-secrets-e9ce26c7-5f47-484f-9ab8-6f0d6a99d2f5 container secret-volume-test: <nil>
STEP: delete the pod
Aug 17 05:28:56.979: INFO: Waiting for pod pod-secrets-e9ce26c7-5f47-484f-9ab8-6f0d6a99d2f5 to disappear
Aug 17 05:28:57.000: INFO: Pod pod-secrets-e9ce26c7-5f47-484f-9ab8-6f0d6a99d2f5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:28:57.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8715" for this suite.
Aug 17 05:29:07.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:29:09.908: INFO: namespace secrets-8715 deletion completed in 12.830424195s

• [SLOW TEST:15.450 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:29:09.909: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 05:29:10.138: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:29:14.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6190" for this suite.
Aug 17 05:29:25.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:29:28.128: INFO: namespace custom-resource-definition-6190 deletion completed in 13.144465451s

• [SLOW TEST:18.219 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:29:28.128: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-53a57a9a-6887-420c-9add-eae216e9e283
STEP: Creating a pod to test consume secrets
Aug 17 05:29:28.555: INFO: Waiting up to 5m0s for pod "pod-secrets-504b2e60-de10-431a-8841-1382aa1b2309" in namespace "secrets-8187" to be "success or failure"
Aug 17 05:29:28.572: INFO: Pod "pod-secrets-504b2e60-de10-431a-8841-1382aa1b2309": Phase="Pending", Reason="", readiness=false. Elapsed: 16.761489ms
Aug 17 05:29:30.613: INFO: Pod "pod-secrets-504b2e60-de10-431a-8841-1382aa1b2309": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057797903s
Aug 17 05:29:32.633: INFO: Pod "pod-secrets-504b2e60-de10-431a-8841-1382aa1b2309": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.078351053s
STEP: Saw pod success
Aug 17 05:29:32.633: INFO: Pod "pod-secrets-504b2e60-de10-431a-8841-1382aa1b2309" satisfied condition "success or failure"
Aug 17 05:29:32.661: INFO: Trying to get logs from node 10.241.148.42 pod pod-secrets-504b2e60-de10-431a-8841-1382aa1b2309 container secret-volume-test: <nil>
STEP: delete the pod
Aug 17 05:29:32.754: INFO: Waiting for pod pod-secrets-504b2e60-de10-431a-8841-1382aa1b2309 to disappear
Aug 17 05:29:32.780: INFO: Pod pod-secrets-504b2e60-de10-431a-8841-1382aa1b2309 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:29:32.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8187" for this suite.
Aug 17 05:29:40.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:29:43.349: INFO: namespace secrets-8187 deletion completed in 10.504387524s

• [SLOW TEST:15.221 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:29:43.350: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7566
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-7566
Aug 17 05:29:43.739: INFO: Found 0 stateful pods, waiting for 1
Aug 17 05:29:53.760: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Aug 17 05:29:53.866: INFO: Deleting all statefulset in ns statefulset-7566
Aug 17 05:29:53.884: INFO: Scaling statefulset ss to 0
Aug 17 05:30:14.019: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 05:30:14.034: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:30:14.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7566" for this suite.
Aug 17 05:30:24.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:30:26.688: INFO: namespace statefulset-7566 deletion completed in 12.489584725s

• [SLOW TEST:43.339 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:30:26.689: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:30:31.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7542" for this suite.
Aug 17 05:30:41.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:30:44.106: INFO: namespace watch-7542 deletion completed in 12.629596412s

• [SLOW TEST:17.417 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:30:44.106: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 05:30:44.385: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-a65c4697-012a-4f76-954b-3b2cf242d936
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:30:48.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-794" for this suite.
Aug 17 05:31:05.590: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:31:08.169: INFO: namespace configmap-794 deletion completed in 18.856532022s

• [SLOW TEST:24.063 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:31:08.170: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 17 05:31:08.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-7030'
Aug 17 05:31:08.579: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 17 05:31:08.579: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Aug 17 05:31:12.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 delete deployment e2e-test-httpd-deployment --namespace=kubectl-7030'
Aug 17 05:31:12.806: INFO: stderr: ""
Aug 17 05:31:12.807: INFO: stdout: "deployment.extensions \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:31:12.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7030" for this suite.
Aug 17 05:31:28.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:31:31.400: INFO: namespace kubectl-7030 deletion completed in 18.523458349s

• [SLOW TEST:23.230 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:31:31.402: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7810
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-7810
I0817 05:31:31.875637      22 runners.go:184] Created replication controller with name: externalname-service, namespace: services-7810, replica count: 2
Aug 17 05:31:34.926: INFO: Creating new exec pod
I0817 05:31:34.926604      22 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 17 05:31:40.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=services-7810 execpodd8z9b -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Aug 17 05:31:40.619: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 17 05:31:40.619: INFO: stdout: ""
Aug 17 05:31:40.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=services-7810 execpodd8z9b -- /bin/sh -x -c nc -zv -t -w 2 172.21.181.118 80'
Aug 17 05:31:40.995: INFO: stderr: "+ nc -zv -t -w 2 172.21.181.118 80\nConnection to 172.21.181.118 80 port [tcp/http] succeeded!\n"
Aug 17 05:31:40.995: INFO: stdout: ""
Aug 17 05:31:40.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=services-7810 execpodd8z9b -- /bin/sh -x -c nc -zv -t -w 2 10.241.148.31 31157'
Aug 17 05:31:41.426: INFO: stderr: "+ nc -zv -t -w 2 10.241.148.31 31157\nConnection to 10.241.148.31 31157 port [tcp/31157] succeeded!\n"
Aug 17 05:31:41.426: INFO: stdout: ""
Aug 17 05:31:41.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=services-7810 execpodd8z9b -- /bin/sh -x -c nc -zv -t -w 2 10.241.148.42 31157'
Aug 17 05:31:42.693: INFO: stderr: "+ nc -zv -t -w 2 10.241.148.42 31157\nConnection to 10.241.148.42 31157 port [tcp/31157] succeeded!\n"
Aug 17 05:31:42.693: INFO: stdout: ""
Aug 17 05:31:42.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=services-7810 execpodd8z9b -- /bin/sh -x -c nc -zv -t -w 2 169.59.216.115 31157'
Aug 17 05:31:43.070: INFO: stderr: "+ nc -zv -t -w 2 169.59.216.115 31157\nConnection to 169.59.216.115 31157 port [tcp/31157] succeeded!\n"
Aug 17 05:31:43.070: INFO: stdout: ""
Aug 17 05:31:43.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=services-7810 execpodd8z9b -- /bin/sh -x -c nc -zv -t -w 2 169.59.216.123 31157'
Aug 17 05:31:43.451: INFO: stderr: "+ nc -zv -t -w 2 169.59.216.123 31157\nConnection to 169.59.216.123 31157 port [tcp/31157] succeeded!\n"
Aug 17 05:31:43.451: INFO: stdout: ""
Aug 17 05:31:43.451: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:31:43.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7810" for this suite.
Aug 17 05:31:53.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:31:56.177: INFO: namespace services-7810 deletion completed in 12.551322798s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:24.776 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:31:56.179: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Aug 17 05:31:56.448: INFO: Waiting up to 5m0s for pod "pod-bf473247-39e5-4834-b0b3-d87bb2cc50fb" in namespace "emptydir-2026" to be "success or failure"
Aug 17 05:31:56.464: INFO: Pod "pod-bf473247-39e5-4834-b0b3-d87bb2cc50fb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.64562ms
Aug 17 05:31:58.502: INFO: Pod "pod-bf473247-39e5-4834-b0b3-d87bb2cc50fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.053861556s
STEP: Saw pod success
Aug 17 05:31:58.502: INFO: Pod "pod-bf473247-39e5-4834-b0b3-d87bb2cc50fb" satisfied condition "success or failure"
Aug 17 05:31:58.519: INFO: Trying to get logs from node 10.241.148.42 pod pod-bf473247-39e5-4834-b0b3-d87bb2cc50fb container test-container: <nil>
STEP: delete the pod
Aug 17 05:31:58.643: INFO: Waiting for pod pod-bf473247-39e5-4834-b0b3-d87bb2cc50fb to disappear
Aug 17 05:31:58.663: INFO: Pod pod-bf473247-39e5-4834-b0b3-d87bb2cc50fb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:31:58.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2026" for this suite.
Aug 17 05:32:08.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:32:11.536: INFO: namespace emptydir-2026 deletion completed in 12.833159648s

• [SLOW TEST:15.357 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:32:11.539: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Aug 17 05:32:15.931: INFO: Pod pod-hostip-9636fe4e-a3e5-47d9-b88d-a0cbe4679b85 has hostIP: 10.241.148.42
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:32:15.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8828" for this suite.
Aug 17 05:32:32.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:32:35.111: INFO: namespace pods-8828 deletion completed in 19.042838579s

• [SLOW TEST:23.572 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:32:35.111: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-2148
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-2148
STEP: Deleting pre-stop pod
Aug 17 05:32:46.725: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:32:46.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-2148" for this suite.
Aug 17 05:33:23.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:33:26.079: INFO: namespace prestop-2148 deletion completed in 38.527700228s

• [SLOW TEST:50.968 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:33:26.079: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 17 05:33:26.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-4591'
Aug 17 05:33:26.632: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 17 05:33:26.632: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Aug 17 05:33:26.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 delete jobs e2e-test-httpd-job --namespace=kubectl-4591'
Aug 17 05:33:26.835: INFO: stderr: ""
Aug 17 05:33:26.835: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:33:26.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4591" for this suite.
Aug 17 05:33:34.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:33:37.476: INFO: namespace kubectl-4591 deletion completed in 10.601473436s

• [SLOW TEST:11.397 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:33:37.479: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:33:41.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4916" for this suite.
Aug 17 05:33:52.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:33:54.510: INFO: namespace kubelet-test-4916 deletion completed in 12.556079074s

• [SLOW TEST:17.031 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:33:54.510: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:33:59.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-308" for this suite.
Aug 17 05:34:09.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:34:11.876: INFO: namespace emptydir-wrapper-308 deletion completed in 12.656593019s

• [SLOW TEST:17.366 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:34:11.876: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-67ca33a1-9bd5-4d58-90e8-971abc82aafb
STEP: Creating a pod to test consume secrets
Aug 17 05:34:12.218: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9179cac8-cf24-4a30-8227-1e0661624bb9" in namespace "projected-889" to be "success or failure"
Aug 17 05:34:12.240: INFO: Pod "pod-projected-secrets-9179cac8-cf24-4a30-8227-1e0661624bb9": Phase="Pending", Reason="", readiness=false. Elapsed: 21.470636ms
Aug 17 05:34:14.265: INFO: Pod "pod-projected-secrets-9179cac8-cf24-4a30-8227-1e0661624bb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046733084s
Aug 17 05:34:16.348: INFO: Pod "pod-projected-secrets-9179cac8-cf24-4a30-8227-1e0661624bb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.128955449s
STEP: Saw pod success
Aug 17 05:34:16.348: INFO: Pod "pod-projected-secrets-9179cac8-cf24-4a30-8227-1e0661624bb9" satisfied condition "success or failure"
Aug 17 05:34:16.420: INFO: Trying to get logs from node 10.241.148.42 pod pod-projected-secrets-9179cac8-cf24-4a30-8227-1e0661624bb9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 17 05:34:16.767: INFO: Waiting for pod pod-projected-secrets-9179cac8-cf24-4a30-8227-1e0661624bb9 to disappear
Aug 17 05:34:16.839: INFO: Pod pod-projected-secrets-9179cac8-cf24-4a30-8227-1e0661624bb9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:34:16.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-889" for this suite.
Aug 17 05:34:25.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:34:27.979: INFO: namespace projected-889 deletion completed in 10.733999891s

• [SLOW TEST:16.103 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:34:27.984: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 05:34:28.187: INFO: Creating deployment "test-recreate-deployment"
Aug 17 05:34:28.212: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 17 05:34:28.265: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Aug 17 05:34:30.337: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 17 05:34:30.374: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733239268, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733239268, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733239268, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733239268, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 05:34:32.400: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 17 05:34:32.442: INFO: Updating deployment test-recreate-deployment
Aug 17 05:34:32.442: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Aug 17 05:34:32.679: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-3550 /apis/apps/v1/namespaces/deployment-3550/deployments/test-recreate-deployment c3ce7b83-6e59-499c-9965-0391932ef462 70984 2 2020-08-17 05:34:28 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002d53878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-08-17 05:34:32 +0000 UTC,LastTransitionTime:2020-08-17 05:34:32 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-08-17 05:34:32 +0000 UTC,LastTransitionTime:2020-08-17 05:34:28 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Aug 17 05:34:32.696: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-3550 /apis/apps/v1/namespaces/deployment-3550/replicasets/test-recreate-deployment-5f94c574ff c3d8f0a5-a3cc-4c8a-8e2d-d6987bbc07c9 70982 1 2020-08-17 05:34:32 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment c3ce7b83-6e59-499c-9965-0391932ef462 0xc003d901c7 0xc003d901c8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003d90228 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 17 05:34:32.696: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 17 05:34:32.697: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-3550 /apis/apps/v1/namespaces/deployment-3550/replicasets/test-recreate-deployment-68fc85c7bb 8b80a9fe-2a47-4afb-87db-4d0ccef50842 70973 2 2020-08-17 05:34:28 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment c3ce7b83-6e59-499c-9965-0391932ef462 0xc003d90297 0xc003d90298}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003d902f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 17 05:34:32.715: INFO: Pod "test-recreate-deployment-5f94c574ff-mtzxd" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-mtzxd test-recreate-deployment-5f94c574ff- deployment-3550 /api/v1/namespaces/deployment-3550/pods/test-recreate-deployment-5f94c574ff-mtzxd a0fdcbed-86f2-4b79-aef2-2fbcd0efb07d 70985 0 2020-08-17 05:34:32 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff c3d8f0a5-a3cc-4c8a-8e2d-d6987bbc07c9 0xc002d53c77 0xc002d53c78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp5qr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp5qr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp5qr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.42,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-p7l9h,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 05:34:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 05:34:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 05:34:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 05:34:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.42,PodIP:,StartTime:2020-08-17 05:34:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:34:32.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3550" for this suite.
Aug 17 05:34:42.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:34:45.393: INFO: namespace deployment-3550 deletion completed in 12.617743025s

• [SLOW TEST:17.410 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:34:45.394: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 17 05:34:45.750: INFO: Waiting up to 5m0s for pod "downwardapi-volume-80cbf4f4-4450-42bc-a1fe-7260258fbc97" in namespace "projected-3243" to be "success or failure"
Aug 17 05:34:45.819: INFO: Pod "downwardapi-volume-80cbf4f4-4450-42bc-a1fe-7260258fbc97": Phase="Pending", Reason="", readiness=false. Elapsed: 68.222794ms
Aug 17 05:34:47.900: INFO: Pod "downwardapi-volume-80cbf4f4-4450-42bc-a1fe-7260258fbc97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.149704653s
Aug 17 05:34:49.973: INFO: Pod "downwardapi-volume-80cbf4f4-4450-42bc-a1fe-7260258fbc97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.222150412s
STEP: Saw pod success
Aug 17 05:34:49.973: INFO: Pod "downwardapi-volume-80cbf4f4-4450-42bc-a1fe-7260258fbc97" satisfied condition "success or failure"
Aug 17 05:34:50.062: INFO: Trying to get logs from node 10.241.148.42 pod downwardapi-volume-80cbf4f4-4450-42bc-a1fe-7260258fbc97 container client-container: <nil>
STEP: delete the pod
Aug 17 05:34:50.334: INFO: Waiting for pod downwardapi-volume-80cbf4f4-4450-42bc-a1fe-7260258fbc97 to disappear
Aug 17 05:34:50.416: INFO: Pod downwardapi-volume-80cbf4f4-4450-42bc-a1fe-7260258fbc97 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:34:50.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3243" for this suite.
Aug 17 05:35:00.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:35:03.217: INFO: namespace projected-3243 deletion completed in 12.472982641s

• [SLOW TEST:17.823 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:35:03.217: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Aug 17 05:35:03.701: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4203 /api/v1/namespaces/watch-4203/configmaps/e2e-watch-test-resource-version 987e7550-5a88-457c-89da-2e82457cb09a 71245 0 2020-08-17 05:35:03 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 17 05:35:03.701: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4203 /api/v1/namespaces/watch-4203/configmaps/e2e-watch-test-resource-version 987e7550-5a88-457c-89da-2e82457cb09a 71246 0 2020-08-17 05:35:03 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:35:03.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4203" for this suite.
Aug 17 05:35:11.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:35:14.324: INFO: namespace watch-4203 deletion completed in 10.555946828s

• [SLOW TEST:11.107 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:35:14.324: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 17 05:35:23.008: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 17 05:35:23.051: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 17 05:35:25.051: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 17 05:35:25.076: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 17 05:35:27.056: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 17 05:35:27.074: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:35:27.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7416" for this suite.
Aug 17 05:36:01.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:36:04.091: INFO: namespace container-lifecycle-hook-7416 deletion completed in 36.974108543s

• [SLOW TEST:49.767 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:36:04.092: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 05:36:04.456: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-b7973dac-8f20-4747-9dec-978e2b5116ae" in namespace "security-context-test-6792" to be "success or failure"
Aug 17 05:36:04.473: INFO: Pod "busybox-privileged-false-b7973dac-8f20-4747-9dec-978e2b5116ae": Phase="Pending", Reason="", readiness=false. Elapsed: 17.118108ms
Aug 17 05:36:06.503: INFO: Pod "busybox-privileged-false-b7973dac-8f20-4747-9dec-978e2b5116ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046932559s
Aug 17 05:36:06.503: INFO: Pod "busybox-privileged-false-b7973dac-8f20-4747-9dec-978e2b5116ae" satisfied condition "success or failure"
Aug 17 05:36:06.540: INFO: Got logs for pod "busybox-privileged-false-b7973dac-8f20-4747-9dec-978e2b5116ae": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:36:06.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6792" for this suite.
Aug 17 05:36:16.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:36:21.004: INFO: namespace security-context-test-6792 deletion completed in 14.373741206s

• [SLOW TEST:16.913 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:36:21.006: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 17 05:36:21.379: INFO: Waiting up to 5m0s for pod "pod-dea767f4-3749-47ee-b412-a6e859a9da18" in namespace "emptydir-991" to be "success or failure"
Aug 17 05:36:21.407: INFO: Pod "pod-dea767f4-3749-47ee-b412-a6e859a9da18": Phase="Pending", Reason="", readiness=false. Elapsed: 27.481204ms
Aug 17 05:36:23.426: INFO: Pod "pod-dea767f4-3749-47ee-b412-a6e859a9da18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.047329354s
STEP: Saw pod success
Aug 17 05:36:23.426: INFO: Pod "pod-dea767f4-3749-47ee-b412-a6e859a9da18" satisfied condition "success or failure"
Aug 17 05:36:23.447: INFO: Trying to get logs from node 10.241.148.42 pod pod-dea767f4-3749-47ee-b412-a6e859a9da18 container test-container: <nil>
STEP: delete the pod
Aug 17 05:36:23.550: INFO: Waiting for pod pod-dea767f4-3749-47ee-b412-a6e859a9da18 to disappear
Aug 17 05:36:23.571: INFO: Pod pod-dea767f4-3749-47ee-b412-a6e859a9da18 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:36:23.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-991" for this suite.
Aug 17 05:36:33.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:36:36.134: INFO: namespace emptydir-991 deletion completed in 12.5077014s

• [SLOW TEST:15.128 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:36:36.134: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-e43d6f95-43b4-413c-ac87-f2c12cb8ee55
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:36:36.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2951" for this suite.
Aug 17 05:36:44.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:36:49.139: INFO: namespace secrets-2951 deletion completed in 12.751899297s

• [SLOW TEST:13.005 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:36:49.139: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 05:36:50.220: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-d1b86118-d4bd-48bb-a09f-9b99e55854a7
STEP: Creating configMap with name cm-test-opt-upd-e9c7e759-d588-4d1c-ad50-639100dc0201
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-d1b86118-d4bd-48bb-a09f-9b99e55854a7
STEP: Updating configmap cm-test-opt-upd-e9c7e759-d588-4d1c-ad50-639100dc0201
STEP: Creating configMap with name cm-test-opt-create-9c70ac36-1a8b-49da-b6cd-5202f1a86166
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:36:57.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8798" for this suite.
Aug 17 05:37:33.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:37:36.289: INFO: namespace projected-8798 deletion completed in 39.027885827s

• [SLOW TEST:47.150 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:37:36.290: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 17 05:37:37.212: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 17 05:37:39.258: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733239457, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733239457, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733239457, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733239457, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 17 05:37:42.332: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:37:42.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6255" for this suite.
Aug 17 05:37:52.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:37:55.081: INFO: namespace webhook-6255 deletion completed in 12.648857778s
STEP: Destroying namespace "webhook-6255-markers" for this suite.
Aug 17 05:38:03.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:38:05.808: INFO: namespace webhook-6255-markers deletion completed in 10.726343655s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:29.599 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:38:05.889: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Aug 17 05:38:06.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 create -f - --namespace=kubectl-6978'
Aug 17 05:38:07.002: INFO: stderr: ""
Aug 17 05:38:07.002: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 17 05:38:08.020: INFO: Selector matched 1 pods for map[app:redis]
Aug 17 05:38:08.020: INFO: Found 0 / 1
Aug 17 05:38:09.021: INFO: Selector matched 1 pods for map[app:redis]
Aug 17 05:38:09.021: INFO: Found 0 / 1
Aug 17 05:38:10.021: INFO: Selector matched 1 pods for map[app:redis]
Aug 17 05:38:10.021: INFO: Found 1 / 1
Aug 17 05:38:10.021: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Aug 17 05:38:10.039: INFO: Selector matched 1 pods for map[app:redis]
Aug 17 05:38:10.039: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 17 05:38:10.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 patch pod redis-master-2x6xr --namespace=kubectl-6978 -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 17 05:38:10.224: INFO: stderr: ""
Aug 17 05:38:10.224: INFO: stdout: "pod/redis-master-2x6xr patched\n"
STEP: checking annotations
Aug 17 05:38:10.246: INFO: Selector matched 1 pods for map[app:redis]
Aug 17 05:38:10.246: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:38:10.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6978" for this suite.
Aug 17 05:38:26.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:38:28.911: INFO: namespace kubectl-6978 deletion completed in 18.615359434s

• [SLOW TEST:23.022 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:38:28.911: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Aug 17 05:38:29.182: INFO: Waiting up to 5m0s for pod "downward-api-f8a78454-bf7e-433f-b3da-6fb0ca60f52d" in namespace "downward-api-6496" to be "success or failure"
Aug 17 05:38:29.208: INFO: Pod "downward-api-f8a78454-bf7e-433f-b3da-6fb0ca60f52d": Phase="Pending", Reason="", readiness=false. Elapsed: 26.124009ms
Aug 17 05:38:31.230: INFO: Pod "downward-api-f8a78454-bf7e-433f-b3da-6fb0ca60f52d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048424327s
Aug 17 05:38:33.248: INFO: Pod "downward-api-f8a78454-bf7e-433f-b3da-6fb0ca60f52d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066460625s
STEP: Saw pod success
Aug 17 05:38:33.248: INFO: Pod "downward-api-f8a78454-bf7e-433f-b3da-6fb0ca60f52d" satisfied condition "success or failure"
Aug 17 05:38:33.279: INFO: Trying to get logs from node 10.241.148.42 pod downward-api-f8a78454-bf7e-433f-b3da-6fb0ca60f52d container dapi-container: <nil>
STEP: delete the pod
Aug 17 05:38:33.588: INFO: Waiting for pod downward-api-f8a78454-bf7e-433f-b3da-6fb0ca60f52d to disappear
Aug 17 05:38:33.609: INFO: Pod downward-api-f8a78454-bf7e-433f-b3da-6fb0ca60f52d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:38:33.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6496" for this suite.
Aug 17 05:38:43.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:38:46.440: INFO: namespace downward-api-6496 deletion completed in 12.762465647s

• [SLOW TEST:17.529 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:38:46.441: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 17 05:38:50.674: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:38:51.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8905" for this suite.
Aug 17 05:39:02.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:39:05.007: INFO: namespace container-runtime-8905 deletion completed in 13.134846004s

• [SLOW TEST:18.565 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:39:05.007: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-91d5c3e3-a922-43e1-a747-ee4b64c3bd9a
STEP: Creating a pod to test consume secrets
Aug 17 05:39:05.355: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b4aa3738-ecc5-45d3-a80c-ae0642734427" in namespace "projected-8325" to be "success or failure"
Aug 17 05:39:05.377: INFO: Pod "pod-projected-secrets-b4aa3738-ecc5-45d3-a80c-ae0642734427": Phase="Pending", Reason="", readiness=false. Elapsed: 22.121461ms
Aug 17 05:39:07.406: INFO: Pod "pod-projected-secrets-b4aa3738-ecc5-45d3-a80c-ae0642734427": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051276878s
Aug 17 05:39:09.423: INFO: Pod "pod-projected-secrets-b4aa3738-ecc5-45d3-a80c-ae0642734427": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068063688s
STEP: Saw pod success
Aug 17 05:39:09.423: INFO: Pod "pod-projected-secrets-b4aa3738-ecc5-45d3-a80c-ae0642734427" satisfied condition "success or failure"
Aug 17 05:39:09.440: INFO: Trying to get logs from node 10.241.148.42 pod pod-projected-secrets-b4aa3738-ecc5-45d3-a80c-ae0642734427 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 17 05:39:09.535: INFO: Waiting for pod pod-projected-secrets-b4aa3738-ecc5-45d3-a80c-ae0642734427 to disappear
Aug 17 05:39:09.551: INFO: Pod pod-projected-secrets-b4aa3738-ecc5-45d3-a80c-ae0642734427 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:39:09.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8325" for this suite.
Aug 17 05:39:17.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:39:22.074: INFO: namespace projected-8325 deletion completed in 12.465383496s

• [SLOW TEST:17.067 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:39:22.074: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Aug 17 05:39:22.375: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-9775" to be "success or failure"
Aug 17 05:39:22.399: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 23.745614ms
Aug 17 05:39:24.418: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043237291s
Aug 17 05:39:26.444: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069305611s
STEP: Saw pod success
Aug 17 05:39:26.444: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Aug 17 05:39:26.466: INFO: Trying to get logs from node 10.241.148.42 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Aug 17 05:39:26.599: INFO: Waiting for pod pod-host-path-test to disappear
Aug 17 05:39:26.618: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:39:26.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-9775" for this suite.
Aug 17 05:39:36.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:39:39.204: INFO: namespace hostpath-9775 deletion completed in 12.533710571s

• [SLOW TEST:17.130 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:39:39.204: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Aug 17 05:39:39.424: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 05:39:49.126: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:40:23.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3761" for this suite.
Aug 17 05:40:33.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:40:35.709: INFO: namespace crd-publish-openapi-3761 deletion completed in 12.596033334s

• [SLOW TEST:56.504 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:40:35.709: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 17 05:40:42.177: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 17 05:40:42.211: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 17 05:40:44.212: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 17 05:40:44.231: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 17 05:40:46.212: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 17 05:40:46.234: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 17 05:40:48.212: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 17 05:40:48.281: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 17 05:40:50.212: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 17 05:40:50.270: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 17 05:40:52.212: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 17 05:40:52.241: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 17 05:40:54.212: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 17 05:40:54.233: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:40:54.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1291" for this suite.
Aug 17 05:41:10.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:41:12.823: INFO: namespace container-lifecycle-hook-1291 deletion completed in 18.477273595s

• [SLOW TEST:37.114 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:41:12.824: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-7f57cb94-b09e-4aea-9d19-6f18a540a4bf
STEP: Creating a pod to test consume configMaps
Aug 17 05:41:13.107: INFO: Waiting up to 5m0s for pod "pod-configmaps-9eb77918-cd59-478a-b434-e3cdaed7fe04" in namespace "configmap-695" to be "success or failure"
Aug 17 05:41:13.130: INFO: Pod "pod-configmaps-9eb77918-cd59-478a-b434-e3cdaed7fe04": Phase="Pending", Reason="", readiness=false. Elapsed: 23.206155ms
Aug 17 05:41:15.153: INFO: Pod "pod-configmaps-9eb77918-cd59-478a-b434-e3cdaed7fe04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046569118s
STEP: Saw pod success
Aug 17 05:41:15.153: INFO: Pod "pod-configmaps-9eb77918-cd59-478a-b434-e3cdaed7fe04" satisfied condition "success or failure"
Aug 17 05:41:15.178: INFO: Trying to get logs from node 10.241.148.42 pod pod-configmaps-9eb77918-cd59-478a-b434-e3cdaed7fe04 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 17 05:41:15.278: INFO: Waiting for pod pod-configmaps-9eb77918-cd59-478a-b434-e3cdaed7fe04 to disappear
Aug 17 05:41:15.297: INFO: Pod pod-configmaps-9eb77918-cd59-478a-b434-e3cdaed7fe04 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:41:15.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-695" for this suite.
Aug 17 05:41:23.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:41:25.848: INFO: namespace configmap-695 deletion completed in 10.516851439s

• [SLOW TEST:13.024 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:41:25.849: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 17 05:41:26.118: INFO: Waiting up to 5m0s for pod "pod-f7fe836f-6643-4feb-82a2-79c15ba5bd0e" in namespace "emptydir-7608" to be "success or failure"
Aug 17 05:41:26.140: INFO: Pod "pod-f7fe836f-6643-4feb-82a2-79c15ba5bd0e": Phase="Pending", Reason="", readiness=false. Elapsed: 22.304055ms
Aug 17 05:41:28.157: INFO: Pod "pod-f7fe836f-6643-4feb-82a2-79c15ba5bd0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.039235123s
STEP: Saw pod success
Aug 17 05:41:28.157: INFO: Pod "pod-f7fe836f-6643-4feb-82a2-79c15ba5bd0e" satisfied condition "success or failure"
Aug 17 05:41:28.174: INFO: Trying to get logs from node 10.241.148.42 pod pod-f7fe836f-6643-4feb-82a2-79c15ba5bd0e container test-container: <nil>
STEP: delete the pod
Aug 17 05:41:28.298: INFO: Waiting for pod pod-f7fe836f-6643-4feb-82a2-79c15ba5bd0e to disappear
Aug 17 05:41:28.326: INFO: Pod pod-f7fe836f-6643-4feb-82a2-79c15ba5bd0e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:41:28.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7608" for this suite.
Aug 17 05:41:38.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:41:40.936: INFO: namespace emptydir-7608 deletion completed in 12.553118878s

• [SLOW TEST:15.087 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:41:40.936: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 17 05:41:41.225: INFO: Waiting up to 5m0s for pod "pod-896cc94e-f947-4d46-a5b9-3b84d5ccb79a" in namespace "emptydir-9789" to be "success or failure"
Aug 17 05:41:41.249: INFO: Pod "pod-896cc94e-f947-4d46-a5b9-3b84d5ccb79a": Phase="Pending", Reason="", readiness=false. Elapsed: 24.018335ms
Aug 17 05:41:43.268: INFO: Pod "pod-896cc94e-f947-4d46-a5b9-3b84d5ccb79a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.042174451s
STEP: Saw pod success
Aug 17 05:41:43.268: INFO: Pod "pod-896cc94e-f947-4d46-a5b9-3b84d5ccb79a" satisfied condition "success or failure"
Aug 17 05:41:43.289: INFO: Trying to get logs from node 10.241.148.42 pod pod-896cc94e-f947-4d46-a5b9-3b84d5ccb79a container test-container: <nil>
STEP: delete the pod
Aug 17 05:41:43.382: INFO: Waiting for pod pod-896cc94e-f947-4d46-a5b9-3b84d5ccb79a to disappear
Aug 17 05:41:43.404: INFO: Pod pod-896cc94e-f947-4d46-a5b9-3b84d5ccb79a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:41:43.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9789" for this suite.
Aug 17 05:41:53.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:41:56.672: INFO: namespace emptydir-9789 deletion completed in 13.196782435s

• [SLOW TEST:15.736 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:41:56.674: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Aug 17 05:41:56.972: INFO: namespace kubectl-1198
Aug 17 05:41:56.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 create -f - --namespace=kubectl-1198'
Aug 17 05:41:58.018: INFO: stderr: ""
Aug 17 05:41:58.018: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 17 05:41:59.106: INFO: Selector matched 1 pods for map[app:redis]
Aug 17 05:41:59.106: INFO: Found 0 / 1
Aug 17 05:42:00.126: INFO: Selector matched 1 pods for map[app:redis]
Aug 17 05:42:00.126: INFO: Found 0 / 1
Aug 17 05:42:01.136: INFO: Selector matched 1 pods for map[app:redis]
Aug 17 05:42:01.136: INFO: Found 0 / 1
Aug 17 05:42:02.100: INFO: Selector matched 1 pods for map[app:redis]
Aug 17 05:42:02.100: INFO: Found 1 / 1
Aug 17 05:42:02.101: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 17 05:42:02.179: INFO: Selector matched 1 pods for map[app:redis]
Aug 17 05:42:02.179: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 17 05:42:02.179: INFO: wait on redis-master startup in kubectl-1198 
Aug 17 05:42:02.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 logs redis-master-nhnt4 redis-master --namespace=kubectl-1198'
Aug 17 05:42:02.423: INFO: stderr: ""
Aug 17 05:42:02.423: INFO: stdout: "1:C 17 Aug 2020 05:42:01.263 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 17 Aug 2020 05:42:01.263 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 17 Aug 2020 05:42:01.263 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 17 Aug 2020 05:42:01.265 * Running mode=standalone, port=6379.\n1:M 17 Aug 2020 05:42:01.266 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 17 Aug 2020 05:42:01.266 # Server initialized\n1:M 17 Aug 2020 05:42:01.266 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 17 Aug 2020 05:42:01.266 * Ready to accept connections\n"
STEP: exposing RC
Aug 17 05:42:02.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-1198'
Aug 17 05:42:02.626: INFO: stderr: ""
Aug 17 05:42:02.626: INFO: stdout: "service/rm2 exposed\n"
Aug 17 05:42:02.723: INFO: Service rm2 in namespace kubectl-1198 found.
STEP: exposing service
Aug 17 05:42:04.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-1198'
Aug 17 05:42:05.519: INFO: stderr: ""
Aug 17 05:42:05.519: INFO: stdout: "service/rm3 exposed\n"
Aug 17 05:42:05.554: INFO: Service rm3 in namespace kubectl-1198 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:42:07.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1198" for this suite.
Aug 17 05:42:23.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:42:26.412: INFO: namespace kubectl-1198 deletion completed in 18.700603409s

• [SLOW TEST:29.739 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:42:26.413: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-33531cd0-145f-4f8b-b45f-24d63b99496b
STEP: Creating a pod to test consume configMaps
Aug 17 05:42:26.761: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7b1eff64-a2f9-44ff-9c10-334a3cf4d260" in namespace "projected-654" to be "success or failure"
Aug 17 05:42:26.785: INFO: Pod "pod-projected-configmaps-7b1eff64-a2f9-44ff-9c10-334a3cf4d260": Phase="Pending", Reason="", readiness=false. Elapsed: 24.399225ms
Aug 17 05:42:28.802: INFO: Pod "pod-projected-configmaps-7b1eff64-a2f9-44ff-9c10-334a3cf4d260": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041744045s
Aug 17 05:42:30.820: INFO: Pod "pod-projected-configmaps-7b1eff64-a2f9-44ff-9c10-334a3cf4d260": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058973964s
STEP: Saw pod success
Aug 17 05:42:30.820: INFO: Pod "pod-projected-configmaps-7b1eff64-a2f9-44ff-9c10-334a3cf4d260" satisfied condition "success or failure"
Aug 17 05:42:30.844: INFO: Trying to get logs from node 10.241.148.42 pod pod-projected-configmaps-7b1eff64-a2f9-44ff-9c10-334a3cf4d260 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 17 05:42:30.944: INFO: Waiting for pod pod-projected-configmaps-7b1eff64-a2f9-44ff-9c10-334a3cf4d260 to disappear
Aug 17 05:42:30.963: INFO: Pod pod-projected-configmaps-7b1eff64-a2f9-44ff-9c10-334a3cf4d260 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:42:30.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-654" for this suite.
Aug 17 05:42:39.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:42:41.816: INFO: namespace projected-654 deletion completed in 10.725056456s

• [SLOW TEST:15.403 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:42:41.816: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-2077
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Aug 17 05:42:42.135: INFO: Found 0 stateful pods, waiting for 3
Aug 17 05:42:52.159: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 05:42:52.159: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 05:42:52.159: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Aug 17 05:42:52.266: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Aug 17 05:43:02.370: INFO: Updating stateful set ss2
Aug 17 05:43:02.411: INFO: Waiting for Pod statefulset-2077/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Aug 17 05:43:12.610: INFO: Found 2 stateful pods, waiting for 3
Aug 17 05:43:22.633: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 05:43:22.633: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 05:43:22.633: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Aug 17 05:43:22.710: INFO: Updating stateful set ss2
Aug 17 05:43:22.745: INFO: Waiting for Pod statefulset-2077/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 17 05:43:32.833: INFO: Updating stateful set ss2
Aug 17 05:43:32.870: INFO: Waiting for StatefulSet statefulset-2077/ss2 to complete update
Aug 17 05:43:32.870: INFO: Waiting for Pod statefulset-2077/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 17 05:43:42.910: INFO: Waiting for StatefulSet statefulset-2077/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Aug 17 05:43:52.911: INFO: Deleting all statefulset in ns statefulset-2077
Aug 17 05:43:52.932: INFO: Scaling statefulset ss2 to 0
Aug 17 05:44:13.079: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 05:44:13.093: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:44:13.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2077" for this suite.
Aug 17 05:44:23.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:44:25.851: INFO: namespace statefulset-2077 deletion completed in 12.649953533s

• [SLOW TEST:104.035 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:44:25.852: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-abe1844f-136a-4e4a-8b9e-04541477f58b
STEP: Creating a pod to test consume configMaps
Aug 17 05:44:26.142: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-eff96bd5-6fad-4c07-b647-b9e7b0b66289" in namespace "projected-6647" to be "success or failure"
Aug 17 05:44:26.160: INFO: Pod "pod-projected-configmaps-eff96bd5-6fad-4c07-b647-b9e7b0b66289": Phase="Pending", Reason="", readiness=false. Elapsed: 17.90099ms
Aug 17 05:44:28.180: INFO: Pod "pod-projected-configmaps-eff96bd5-6fad-4c07-b647-b9e7b0b66289": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037291844s
Aug 17 05:44:30.218: INFO: Pod "pod-projected-configmaps-eff96bd5-6fad-4c07-b647-b9e7b0b66289": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076245163s
STEP: Saw pod success
Aug 17 05:44:30.219: INFO: Pod "pod-projected-configmaps-eff96bd5-6fad-4c07-b647-b9e7b0b66289" satisfied condition "success or failure"
Aug 17 05:44:30.251: INFO: Trying to get logs from node 10.241.148.42 pod pod-projected-configmaps-eff96bd5-6fad-4c07-b647-b9e7b0b66289 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 17 05:44:30.499: INFO: Waiting for pod pod-projected-configmaps-eff96bd5-6fad-4c07-b647-b9e7b0b66289 to disappear
Aug 17 05:44:30.521: INFO: Pod pod-projected-configmaps-eff96bd5-6fad-4c07-b647-b9e7b0b66289 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:44:30.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6647" for this suite.
Aug 17 05:44:40.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:44:43.303: INFO: namespace projected-6647 deletion completed in 12.68895972s

• [SLOW TEST:17.451 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:44:43.304: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Aug 17 05:44:43.530: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 17 05:44:43.677: INFO: Waiting for terminating namespaces to be deleted...
Aug 17 05:44:43.711: INFO: 
Logging pods the kubelet thinks is on node 10.241.148.31 before test
Aug 17 05:44:43.884: INFO: router-default-57d78dfb48-mzjlc from openshift-ingress started at 2020-08-17 02:59:05 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.884: INFO: 	Container router ready: true, restart count 0
Aug 17 05:44:43.885: INFO: calico-typha-579947cf56-672t9 from calico-system started at 2020-08-17 02:59:26 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.885: INFO: 	Container calico-typha ready: true, restart count 0
Aug 17 05:44:43.885: INFO: sonobuoy-systemd-logs-daemon-set-c8f140ca93fa466f-pjc77 from sonobuoy started at 2020-08-17 04:12:38 +0000 UTC (2 container statuses recorded)
Aug 17 05:44:43.885: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 17 05:44:43.885: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 05:44:43.885: INFO: telemeter-client-76746b8bb7-dnjsr from openshift-monitoring started at 2020-08-17 04:57:05 +0000 UTC (3 container statuses recorded)
Aug 17 05:44:43.885: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 05:44:43.885: INFO: 	Container reload ready: true, restart count 0
Aug 17 05:44:43.885: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 17 05:44:43.885: INFO: multus-admission-controller-ncsvb from openshift-multus started at 2020-08-17 02:57:44 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.885: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 17 05:44:43.885: INFO: configmap-cabundle-injector-8446d4b88f-9k4sq from openshift-service-ca started at 2020-08-17 02:58:07 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.885: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Aug 17 05:44:43.885: INFO: kube-state-metrics-c5f65645-855s8 from openshift-monitoring started at 2020-08-17 02:58:30 +0000 UTC (3 container statuses recorded)
Aug 17 05:44:43.885: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 17 05:44:43.885: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 17 05:44:43.886: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 17 05:44:43.886: INFO: image-registry-5669fd49dd-xphdf from openshift-image-registry started at 2020-08-17 03:00:49 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.886: INFO: 	Container registry ready: true, restart count 0
Aug 17 05:44:43.886: INFO: service-serving-cert-signer-7879bf8d9f-xtqt8 from openshift-service-ca started at 2020-08-17 02:58:07 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.886: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Aug 17 05:44:43.886: INFO: node-ca-2lszv from openshift-image-registry started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.886: INFO: 	Container node-ca ready: true, restart count 0
Aug 17 05:44:43.886: INFO: packageserver-66768b6f89-b74q9 from openshift-operator-lifecycle-manager started at 2020-08-17 03:05:22 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.886: INFO: 	Container packageserver ready: true, restart count 0
Aug 17 05:44:43.886: INFO: ibm-keepalived-watcher-ncgwx from kube-system started at 2020-08-17 02:56:14 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.886: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 17 05:44:43.886: INFO: node-exporter-kkwj6 from openshift-monitoring started at 2020-08-17 02:58:34 +0000 UTC (2 container statuses recorded)
Aug 17 05:44:43.886: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 05:44:43.886: INFO: 	Container node-exporter ready: true, restart count 0
Aug 17 05:44:43.886: INFO: dns-default-sxjmd from openshift-dns started at 2020-08-17 02:59:01 +0000 UTC (2 container statuses recorded)
Aug 17 05:44:43.887: INFO: 	Container dns ready: true, restart count 0
Aug 17 05:44:43.887: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 17 05:44:43.887: INFO: prometheus-operator-56d9d699cb-m5pmc from openshift-monitoring started at 2020-08-17 03:04:39 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.887: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 17 05:44:43.887: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-08-17 03:05:11 +0000 UTC (3 container statuses recorded)
Aug 17 05:44:43.887: INFO: 	Container alertmanager ready: true, restart count 0
Aug 17 05:44:43.887: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 17 05:44:43.887: INFO: 	Container config-reloader ready: true, restart count 0
Aug 17 05:44:43.887: INFO: openshift-state-metrics-5849d797d8-2f9sd from openshift-monitoring started at 2020-08-17 04:57:05 +0000 UTC (3 container statuses recorded)
Aug 17 05:44:43.887: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 17 05:44:43.887: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 17 05:44:43.887: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 17 05:44:43.887: INFO: ibm-master-proxy-static-10.241.148.31 from kube-system started at 2020-08-17 02:56:11 +0000 UTC (2 container statuses recorded)
Aug 17 05:44:43.887: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 17 05:44:43.887: INFO: 	Container pause ready: true, restart count 0
Aug 17 05:44:43.887: INFO: prometheus-adapter-67dbcc5cfc-xk5l6 from openshift-monitoring started at 2020-08-17 02:58:37 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.887: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 17 05:44:43.888: INFO: network-operator-7986644c85-79z2f from openshift-network-operator started at 2020-08-17 04:57:04 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.888: INFO: 	Container network-operator ready: true, restart count 0
Aug 17 05:44:43.888: INFO: community-operators-65d9d5c9d6-zh8cl from openshift-marketplace started at 2020-08-17 02:59:24 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.888: INFO: 	Container community-operators ready: true, restart count 0
Aug 17 05:44:43.888: INFO: apiservice-cabundle-injector-594fd4555f-znbpx from openshift-service-ca started at 2020-08-17 04:57:04 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.888: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Aug 17 05:44:43.888: INFO: openshift-kube-proxy-5nh74 from openshift-kube-proxy started at 2020-08-17 02:56:45 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.888: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 17 05:44:43.888: INFO: grafana-c9c7455d7-5lnv7 from openshift-monitoring started at 2020-08-17 04:57:05 +0000 UTC (2 container statuses recorded)
Aug 17 05:44:43.888: INFO: 	Container grafana ready: true, restart count 0
Aug 17 05:44:43.888: INFO: 	Container grafana-proxy ready: true, restart count 0
Aug 17 05:44:43.888: INFO: thanos-querier-6b5fbd9754-ctb66 from openshift-monitoring started at 2020-08-17 03:05:37 +0000 UTC (4 container statuses recorded)
Aug 17 05:44:43.888: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 05:44:43.888: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 17 05:44:43.888: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 17 05:44:43.888: INFO: 	Container thanos-querier ready: true, restart count 0
Aug 17 05:44:43.889: INFO: tuned-nmn6j from openshift-cluster-node-tuning-operator started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.889: INFO: 	Container tuned ready: true, restart count 0
Aug 17 05:44:43.889: INFO: certified-operators-549fcd7d77-n48c7 from openshift-marketplace started at 2020-08-17 04:57:05 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.889: INFO: 	Container certified-operators ready: true, restart count 0
Aug 17 05:44:43.889: INFO: multus-tvx65 from openshift-multus started at 2020-08-17 02:56:38 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.889: INFO: 	Container kube-multus ready: true, restart count 0
Aug 17 05:44:43.889: INFO: calico-node-lm8qf from calico-system started at 2020-08-17 02:57:31 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.889: INFO: 	Container calico-node ready: true, restart count 0
Aug 17 05:44:43.889: INFO: cluster-samples-operator-55944b8f44-fxlq8 from openshift-cluster-samples-operator started at 2020-08-17 02:59:01 +0000 UTC (2 container statuses recorded)
Aug 17 05:44:43.889: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Aug 17 05:44:43.889: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Aug 17 05:44:43.889: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-08-17 03:05:54 +0000 UTC (7 container statuses recorded)
Aug 17 05:44:43.889: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 05:44:43.889: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 17 05:44:43.889: INFO: 	Container prometheus ready: true, restart count 1
Aug 17 05:44:43.889: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 17 05:44:43.890: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 17 05:44:43.890: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 17 05:44:43.890: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 17 05:44:43.890: INFO: ibm-cloud-provider-ip-169-48-239-26-5595944bf9-n8nts from ibm-system started at 2020-08-17 03:02:23 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.890: INFO: 	Container ibm-cloud-provider-ip-169-48-239-26 ready: true, restart count 0
Aug 17 05:44:43.890: INFO: console-76d8dc5894-2w62x from openshift-console started at 2020-08-17 02:59:39 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.890: INFO: 	Container console ready: true, restart count 0
Aug 17 05:44:43.890: INFO: ibmcloud-block-storage-driver-hfn98 from kube-system started at 2020-08-17 02:56:23 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.890: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 17 05:44:43.890: INFO: 
Logging pods the kubelet thinks is on node 10.241.148.42 before test
Aug 17 05:44:43.968: INFO: ibmcloud-block-storage-driver-5dth8 from kube-system started at 2020-08-17 02:56:14 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.968: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 17 05:44:43.968: INFO: openshift-kube-proxy-hswpd from openshift-kube-proxy started at 2020-08-17 02:56:45 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.968: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 17 05:44:43.968: INFO: multus-admission-controller-sr4ms from openshift-multus started at 2020-08-17 04:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.968: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 17 05:44:43.968: INFO: ibm-master-proxy-static-10.241.148.42 from kube-system started at 2020-08-17 02:56:02 +0000 UTC (2 container statuses recorded)
Aug 17 05:44:43.969: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 17 05:44:43.969: INFO: 	Container pause ready: true, restart count 0
Aug 17 05:44:43.969: INFO: multus-p85fw from openshift-multus started at 2020-08-17 02:56:38 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.969: INFO: 	Container kube-multus ready: true, restart count 0
Aug 17 05:44:43.969: INFO: calico-typha-579947cf56-86wz4 from calico-system started at 2020-08-17 02:57:30 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.969: INFO: 	Container calico-typha ready: true, restart count 0
Aug 17 05:44:43.969: INFO: node-exporter-r6g64 from openshift-monitoring started at 2020-08-17 02:58:34 +0000 UTC (2 container statuses recorded)
Aug 17 05:44:43.969: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 05:44:43.969: INFO: 	Container node-exporter ready: true, restart count 0
Aug 17 05:44:43.969: INFO: sonobuoy from sonobuoy started at 2020-08-17 04:12:31 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.969: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 17 05:44:43.969: INFO: sonobuoy-e2e-job-144f6878906f41e3 from sonobuoy started at 2020-08-17 04:12:38 +0000 UTC (2 container statuses recorded)
Aug 17 05:44:43.969: INFO: 	Container e2e ready: true, restart count 0
Aug 17 05:44:43.969: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 05:44:43.969: INFO: sonobuoy-systemd-logs-daemon-set-c8f140ca93fa466f-qhfmq from sonobuoy started at 2020-08-17 04:12:38 +0000 UTC (2 container statuses recorded)
Aug 17 05:44:43.969: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 17 05:44:43.970: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 05:44:43.970: INFO: ibm-keepalived-watcher-xf2gw from kube-system started at 2020-08-17 02:56:05 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.970: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 17 05:44:43.970: INFO: dns-default-phmhp from openshift-dns started at 2020-08-17 04:57:42 +0000 UTC (2 container statuses recorded)
Aug 17 05:44:43.970: INFO: 	Container dns ready: true, restart count 0
Aug 17 05:44:43.970: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 17 05:44:43.970: INFO: node-ca-j567q from openshift-image-registry started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.970: INFO: 	Container node-ca ready: true, restart count 0
Aug 17 05:44:43.970: INFO: registry-pvc-permissions-j8h5n from openshift-image-registry started at 2020-08-17 03:00:49 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.970: INFO: 	Container pvc-permissions ready: false, restart count 0
Aug 17 05:44:43.970: INFO: calico-node-cxl67 from calico-system started at 2020-08-17 02:57:31 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.970: INFO: 	Container calico-node ready: true, restart count 0
Aug 17 05:44:43.970: INFO: tuned-b665k from openshift-cluster-node-tuning-operator started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:43.970: INFO: 	Container tuned ready: true, restart count 0
Aug 17 05:44:43.970: INFO: 
Logging pods the kubelet thinks is on node 10.241.148.50 before test
Aug 17 05:44:44.135: INFO: olm-operator-b5f57cdbb-wcj4n from openshift-operator-lifecycle-manager started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container olm-operator ready: true, restart count 0
Aug 17 05:44:44.135: INFO: console-76d8dc5894-x8wzx from openshift-console started at 2020-08-17 04:57:05 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container console ready: true, restart count 0
Aug 17 05:44:44.135: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-08-17 03:04:54 +0000 UTC (3 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container alertmanager ready: true, restart count 0
Aug 17 05:44:44.135: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 17 05:44:44.135: INFO: 	Container config-reloader ready: true, restart count 0
Aug 17 05:44:44.135: INFO: ibmcloud-block-storage-driver-6l6kb from kube-system started at 2020-08-17 02:56:30 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 17 05:44:44.135: INFO: service-ca-operator-694cfbf5d5-2dhf8 from openshift-service-ca-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container operator ready: true, restart count 0
Aug 17 05:44:44.135: INFO: cluster-monitoring-operator-5b5659466f-bd9vb from openshift-monitoring started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Aug 17 05:44:44.135: INFO: cluster-image-registry-operator-6cfd58b66c-xpvcp from openshift-image-registry started at 2020-08-17 02:57:42 +0000 UTC (2 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Aug 17 05:44:44.135: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Aug 17 05:44:44.135: INFO: calico-kube-controllers-79d75767dd-hhcw2 from calico-system started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 17 05:44:44.135: INFO: ibmcloud-block-storage-plugin-68d5c65db9-m8qcr from kube-system started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Aug 17 05:44:44.135: INFO: ibm-cloud-provider-ip-169-48-239-26-5595944bf9-2pxq9 from ibm-system started at 2020-08-17 03:02:20 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container ibm-cloud-provider-ip-169-48-239-26 ready: true, restart count 0
Aug 17 05:44:44.135: INFO: openshift-kube-proxy-grn9g from openshift-kube-proxy started at 2020-08-17 02:56:45 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 17 05:44:44.135: INFO: calico-node-hmdtm from calico-system started at 2020-08-17 02:57:31 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container calico-node ready: true, restart count 0
Aug 17 05:44:44.135: INFO: cluster-storage-operator-557b75f8d5-899qb from openshift-cluster-storage-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Aug 17 05:44:44.135: INFO: prometheus-adapter-67dbcc5cfc-pz99g from openshift-monitoring started at 2020-08-17 04:57:05 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 17 05:44:44.135: INFO: ibm-master-proxy-static-10.241.148.50 from kube-system started at 2020-08-17 02:56:19 +0000 UTC (2 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 17 05:44:44.135: INFO: 	Container pause ready: true, restart count 0
Aug 17 05:44:44.135: INFO: ibm-keepalived-watcher-x5mxk from kube-system started at 2020-08-17 02:56:22 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 17 05:44:44.135: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-08-17 04:57:12 +0000 UTC (3 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container alertmanager ready: true, restart count 0
Aug 17 05:44:44.135: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 17 05:44:44.135: INFO: 	Container config-reloader ready: true, restart count 0
Aug 17 05:44:44.135: INFO: cluster-node-tuning-operator-b5f884945-nmndb from openshift-cluster-node-tuning-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Aug 17 05:44:44.135: INFO: dns-default-kfbzc from openshift-dns started at 2020-08-17 02:59:02 +0000 UTC (2 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container dns ready: true, restart count 0
Aug 17 05:44:44.135: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 17 05:44:44.135: INFO: openshift-service-catalog-controller-manager-operator-5496wjhfr from openshift-service-catalog-controller-manager-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container operator ready: true, restart count 1
Aug 17 05:44:44.135: INFO: marketplace-operator-6957767d58-6rcnt from openshift-marketplace started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container marketplace-operator ready: true, restart count 0
Aug 17 05:44:44.135: INFO: node-ca-467qn from openshift-image-registry started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container node-ca ready: true, restart count 0
Aug 17 05:44:44.135: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-08-17 03:05:34 +0000 UTC (7 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 05:44:44.135: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 17 05:44:44.135: INFO: 	Container prometheus ready: true, restart count 1
Aug 17 05:44:44.135: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 17 05:44:44.135: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 17 05:44:44.135: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 17 05:44:44.135: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 17 05:44:44.135: INFO: multus-wkz57 from openshift-multus started at 2020-08-17 02:56:38 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container kube-multus ready: true, restart count 0
Aug 17 05:44:44.135: INFO: ibm-file-plugin-fdb69b446-bzpn7 from kube-system started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 17 05:44:44.135: INFO: ingress-operator-695bc545b9-d4trd from openshift-ingress-operator started at 2020-08-17 02:57:42 +0000 UTC (2 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container ingress-operator ready: true, restart count 0
Aug 17 05:44:44.135: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 05:44:44.135: INFO: downloads-678f5d6564-wxxs4 from openshift-console started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container download-server ready: true, restart count 0
Aug 17 05:44:44.135: INFO: dns-operator-6f9cf66db7-ksmws from openshift-dns-operator started at 2020-08-17 02:57:44 +0000 UTC (2 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container dns-operator ready: true, restart count 0
Aug 17 05:44:44.135: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 05:44:44.135: INFO: redhat-operators-85f96bd7c-69xgq from openshift-marketplace started at 2020-08-17 02:59:24 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container redhat-operators ready: true, restart count 0
Aug 17 05:44:44.135: INFO: packageserver-66768b6f89-hjpwd from openshift-operator-lifecycle-manager started at 2020-08-17 03:05:13 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container packageserver ready: true, restart count 0
Aug 17 05:44:44.135: INFO: thanos-querier-6b5fbd9754-wzzrz from openshift-monitoring started at 2020-08-17 03:05:26 +0000 UTC (4 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 05:44:44.135: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 17 05:44:44.135: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 17 05:44:44.135: INFO: 	Container thanos-querier ready: true, restart count 0
Aug 17 05:44:44.135: INFO: console-operator-9878d4766-7tcvg from openshift-console-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container console-operator ready: true, restart count 1
Aug 17 05:44:44.135: INFO: openshift-service-catalog-apiserver-operator-78b9dddb6f-bbw72 from openshift-service-catalog-apiserver-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container operator ready: true, restart count 1
Aug 17 05:44:44.135: INFO: tigera-operator-679798d94d-rj7fl from tigera-operator started at 2020-08-17 04:57:05 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container tigera-operator ready: true, restart count 0
Aug 17 05:44:44.135: INFO: node-exporter-ztsvb from openshift-monitoring started at 2020-08-17 02:58:33 +0000 UTC (2 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 05:44:44.135: INFO: 	Container node-exporter ready: true, restart count 0
Aug 17 05:44:44.135: INFO: calico-typha-579947cf56-87rgj from calico-system started at 2020-08-17 02:59:26 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container calico-typha ready: true, restart count 0
Aug 17 05:44:44.135: INFO: sonobuoy-systemd-logs-daemon-set-c8f140ca93fa466f-j84lz from sonobuoy started at 2020-08-17 04:12:38 +0000 UTC (2 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 17 05:44:44.135: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 05:44:44.135: INFO: vpn-7b59d9f64-2rnfd from kube-system started at 2020-08-17 03:05:11 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container vpn ready: true, restart count 0
Aug 17 05:44:44.135: INFO: router-default-57d78dfb48-x8wzz from openshift-ingress started at 2020-08-17 04:57:04 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container router ready: true, restart count 0
Aug 17 05:44:44.135: INFO: ibm-storage-watcher-77bf8b889-856t2 from kube-system started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 17 05:44:44.135: INFO: downloads-678f5d6564-t5r44 from openshift-console started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container download-server ready: true, restart count 0
Aug 17 05:44:44.135: INFO: tuned-6qthk from openshift-cluster-node-tuning-operator started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container tuned ready: true, restart count 0
Aug 17 05:44:44.135: INFO: catalog-operator-85f6c659cc-6zpxp from openshift-operator-lifecycle-manager started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 17 05:44:44.135: INFO: multus-admission-controller-7xrg7 from openshift-multus started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 05:44:44.135: INFO: 	Container multus-admission-controller ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node 10.241.148.31
STEP: verifying the node has the label node 10.241.148.42
STEP: verifying the node has the label node 10.241.148.50
Aug 17 05:44:44.424: INFO: Pod calico-kube-controllers-79d75767dd-hhcw2 requesting resource cpu=0m on Node 10.241.148.50
Aug 17 05:44:44.424: INFO: Pod calico-node-cxl67 requesting resource cpu=0m on Node 10.241.148.42
Aug 17 05:44:44.424: INFO: Pod calico-node-hmdtm requesting resource cpu=0m on Node 10.241.148.50
Aug 17 05:44:44.424: INFO: Pod calico-node-lm8qf requesting resource cpu=0m on Node 10.241.148.31
Aug 17 05:44:44.424: INFO: Pod calico-typha-579947cf56-672t9 requesting resource cpu=0m on Node 10.241.148.31
Aug 17 05:44:44.424: INFO: Pod calico-typha-579947cf56-86wz4 requesting resource cpu=0m on Node 10.241.148.42
Aug 17 05:44:44.424: INFO: Pod calico-typha-579947cf56-87rgj requesting resource cpu=0m on Node 10.241.148.50
Aug 17 05:44:44.424: INFO: Pod ibm-cloud-provider-ip-169-48-239-26-5595944bf9-2pxq9 requesting resource cpu=5m on Node 10.241.148.50
Aug 17 05:44:44.424: INFO: Pod ibm-cloud-provider-ip-169-48-239-26-5595944bf9-n8nts requesting resource cpu=5m on Node 10.241.148.31
Aug 17 05:44:44.424: INFO: Pod ibm-file-plugin-fdb69b446-bzpn7 requesting resource cpu=50m on Node 10.241.148.50
Aug 17 05:44:44.424: INFO: Pod ibm-keepalived-watcher-ncgwx requesting resource cpu=5m on Node 10.241.148.31
Aug 17 05:44:44.424: INFO: Pod ibm-keepalived-watcher-x5mxk requesting resource cpu=5m on Node 10.241.148.50
Aug 17 05:44:44.424: INFO: Pod ibm-keepalived-watcher-xf2gw requesting resource cpu=5m on Node 10.241.148.42
Aug 17 05:44:44.424: INFO: Pod ibm-master-proxy-static-10.241.148.31 requesting resource cpu=25m on Node 10.241.148.31
Aug 17 05:44:44.424: INFO: Pod ibm-master-proxy-static-10.241.148.42 requesting resource cpu=25m on Node 10.241.148.42
Aug 17 05:44:44.424: INFO: Pod ibm-master-proxy-static-10.241.148.50 requesting resource cpu=25m on Node 10.241.148.50
Aug 17 05:44:44.424: INFO: Pod ibm-storage-watcher-77bf8b889-856t2 requesting resource cpu=50m on Node 10.241.148.50
Aug 17 05:44:44.424: INFO: Pod ibmcloud-block-storage-driver-5dth8 requesting resource cpu=50m on Node 10.241.148.42
Aug 17 05:44:44.424: INFO: Pod ibmcloud-block-storage-driver-6l6kb requesting resource cpu=50m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod ibmcloud-block-storage-driver-hfn98 requesting resource cpu=50m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod ibmcloud-block-storage-plugin-68d5c65db9-m8qcr requesting resource cpu=50m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod vpn-7b59d9f64-2rnfd requesting resource cpu=5m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod cluster-node-tuning-operator-b5f884945-nmndb requesting resource cpu=10m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod tuned-6qthk requesting resource cpu=10m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod tuned-b665k requesting resource cpu=10m on Node 10.241.148.42
Aug 17 05:44:44.425: INFO: Pod tuned-nmn6j requesting resource cpu=10m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod cluster-samples-operator-55944b8f44-fxlq8 requesting resource cpu=20m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod cluster-storage-operator-557b75f8d5-899qb requesting resource cpu=10m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod console-operator-9878d4766-7tcvg requesting resource cpu=10m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod console-76d8dc5894-2w62x requesting resource cpu=10m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod console-76d8dc5894-x8wzx requesting resource cpu=10m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod downloads-678f5d6564-t5r44 requesting resource cpu=10m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod downloads-678f5d6564-wxxs4 requesting resource cpu=10m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod dns-operator-6f9cf66db7-ksmws requesting resource cpu=20m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod dns-default-kfbzc requesting resource cpu=110m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod dns-default-phmhp requesting resource cpu=110m on Node 10.241.148.42
Aug 17 05:44:44.425: INFO: Pod dns-default-sxjmd requesting resource cpu=110m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod cluster-image-registry-operator-6cfd58b66c-xpvcp requesting resource cpu=20m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod image-registry-5669fd49dd-xphdf requesting resource cpu=100m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod node-ca-2lszv requesting resource cpu=10m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod node-ca-467qn requesting resource cpu=10m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod node-ca-j567q requesting resource cpu=10m on Node 10.241.148.42
Aug 17 05:44:44.425: INFO: Pod ingress-operator-695bc545b9-d4trd requesting resource cpu=20m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod router-default-57d78dfb48-mzjlc requesting resource cpu=100m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod router-default-57d78dfb48-x8wzz requesting resource cpu=100m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod openshift-kube-proxy-5nh74 requesting resource cpu=100m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod openshift-kube-proxy-grn9g requesting resource cpu=100m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod openshift-kube-proxy-hswpd requesting resource cpu=100m on Node 10.241.148.42
Aug 17 05:44:44.425: INFO: Pod certified-operators-549fcd7d77-n48c7 requesting resource cpu=10m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod community-operators-65d9d5c9d6-zh8cl requesting resource cpu=10m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod marketplace-operator-6957767d58-6rcnt requesting resource cpu=10m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod redhat-operators-85f96bd7c-69xgq requesting resource cpu=10m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod alertmanager-main-0 requesting resource cpu=6m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod alertmanager-main-1 requesting resource cpu=6m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod alertmanager-main-2 requesting resource cpu=6m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod cluster-monitoring-operator-5b5659466f-bd9vb requesting resource cpu=10m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod grafana-c9c7455d7-5lnv7 requesting resource cpu=5m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod kube-state-metrics-c5f65645-855s8 requesting resource cpu=4m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod node-exporter-kkwj6 requesting resource cpu=9m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod node-exporter-r6g64 requesting resource cpu=9m on Node 10.241.148.42
Aug 17 05:44:44.425: INFO: Pod node-exporter-ztsvb requesting resource cpu=9m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod openshift-state-metrics-5849d797d8-2f9sd requesting resource cpu=3m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod prometheus-adapter-67dbcc5cfc-pz99g requesting resource cpu=1m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod prometheus-adapter-67dbcc5cfc-xk5l6 requesting resource cpu=1m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod prometheus-k8s-0 requesting resource cpu=76m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod prometheus-k8s-1 requesting resource cpu=76m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod prometheus-operator-56d9d699cb-m5pmc requesting resource cpu=5m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod telemeter-client-76746b8bb7-dnjsr requesting resource cpu=3m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod thanos-querier-6b5fbd9754-ctb66 requesting resource cpu=8m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod thanos-querier-6b5fbd9754-wzzrz requesting resource cpu=8m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod multus-admission-controller-7xrg7 requesting resource cpu=10m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod multus-admission-controller-ncsvb requesting resource cpu=10m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod multus-admission-controller-sr4ms requesting resource cpu=10m on Node 10.241.148.42
Aug 17 05:44:44.425: INFO: Pod multus-p85fw requesting resource cpu=10m on Node 10.241.148.42
Aug 17 05:44:44.425: INFO: Pod multus-tvx65 requesting resource cpu=10m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod multus-wkz57 requesting resource cpu=10m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod network-operator-7986644c85-79z2f requesting resource cpu=10m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod catalog-operator-85f6c659cc-6zpxp requesting resource cpu=10m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod olm-operator-b5f57cdbb-wcj4n requesting resource cpu=10m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod packageserver-66768b6f89-b74q9 requesting resource cpu=10m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod packageserver-66768b6f89-hjpwd requesting resource cpu=10m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod service-ca-operator-694cfbf5d5-2dhf8 requesting resource cpu=10m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod apiservice-cabundle-injector-594fd4555f-znbpx requesting resource cpu=10m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod configmap-cabundle-injector-8446d4b88f-9k4sq requesting resource cpu=10m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod service-serving-cert-signer-7879bf8d9f-xtqt8 requesting resource cpu=10m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod openshift-service-catalog-apiserver-operator-78b9dddb6f-bbw72 requesting resource cpu=0m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod openshift-service-catalog-controller-manager-operator-5496wjhfr requesting resource cpu=10m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.241.148.42
Aug 17 05:44:44.425: INFO: Pod sonobuoy-e2e-job-144f6878906f41e3 requesting resource cpu=0m on Node 10.241.148.42
Aug 17 05:44:44.425: INFO: Pod sonobuoy-systemd-logs-daemon-set-c8f140ca93fa466f-j84lz requesting resource cpu=0m on Node 10.241.148.50
Aug 17 05:44:44.425: INFO: Pod sonobuoy-systemd-logs-daemon-set-c8f140ca93fa466f-pjc77 requesting resource cpu=0m on Node 10.241.148.31
Aug 17 05:44:44.425: INFO: Pod sonobuoy-systemd-logs-daemon-set-c8f140ca93fa466f-qhfmq requesting resource cpu=0m on Node 10.241.148.42
Aug 17 05:44:44.425: INFO: Pod tigera-operator-679798d94d-rj7fl requesting resource cpu=100m on Node 10.241.148.50
STEP: Starting Pods to consume most of the cluster CPU.
Aug 17 05:44:44.425: INFO: Creating a pod which consumes cpu=2208m on Node 10.241.148.31
Aug 17 05:44:44.481: INFO: Creating a pod which consumes cpu=2499m on Node 10.241.148.42
Aug 17 05:44:44.528: INFO: Creating a pod which consumes cpu=2039m on Node 10.241.148.50
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-05546e94-e761-4b00-9ea6-c3416f73dfed.162bf7ebb76298c0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5666/filler-pod-05546e94-e761-4b00-9ea6-c3416f73dfed to 10.241.148.50]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-05546e94-e761-4b00-9ea6-c3416f73dfed.162bf7ebfd902ee9], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-05546e94-e761-4b00-9ea6-c3416f73dfed.162bf7ec1d369afd], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-05546e94-e761-4b00-9ea6-c3416f73dfed.162bf7ec2c71cb9e], Reason = [Created], Message = [Created container filler-pod-05546e94-e761-4b00-9ea6-c3416f73dfed]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-05546e94-e761-4b00-9ea6-c3416f73dfed.162bf7ec2f8df759], Reason = [Started], Message = [Started container filler-pod-05546e94-e761-4b00-9ea6-c3416f73dfed]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1b79c509-31f5-4987-867f-6107fee638b5.162bf7ebb1eca01e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5666/filler-pod-1b79c509-31f5-4987-867f-6107fee638b5 to 10.241.148.31]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1b79c509-31f5-4987-867f-6107fee638b5.162bf7ebf5cfe7e3], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1b79c509-31f5-4987-867f-6107fee638b5.162bf7ec03014440], Reason = [Created], Message = [Created container filler-pod-1b79c509-31f5-4987-867f-6107fee638b5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1b79c509-31f5-4987-867f-6107fee638b5.162bf7ec05d1208c], Reason = [Started], Message = [Started container filler-pod-1b79c509-31f5-4987-867f-6107fee638b5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9ed13cd3-03ac-469d-a64e-35da9cd114c6.162bf7ebb491473b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5666/filler-pod-9ed13cd3-03ac-469d-a64e-35da9cd114c6 to 10.241.148.42]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9ed13cd3-03ac-469d-a64e-35da9cd114c6.162bf7ebf8580ddb], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9ed13cd3-03ac-469d-a64e-35da9cd114c6.162bf7ec05513029], Reason = [Created], Message = [Created container filler-pod-9ed13cd3-03ac-469d-a64e-35da9cd114c6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9ed13cd3-03ac-469d-a64e-35da9cd114c6.162bf7ec08853362], Reason = [Started], Message = [Started container filler-pod-9ed13cd3-03ac-469d-a64e-35da9cd114c6]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.162bf7ecc3154c75], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.241.148.31
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.241.148.42
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.241.148.50
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:44:51.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5666" for this suite.
Aug 17 05:45:01.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:45:03.953: INFO: namespace sched-pred-5666 deletion completed in 12.660849676s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:20.650 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:45:03.954: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8355
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8355
STEP: creating replication controller externalsvc in namespace services-8355
I0817 05:45:04.320486      22 runners.go:184] Created replication controller with name: externalsvc, namespace: services-8355, replica count: 2
I0817 05:45:07.371083      22 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Aug 17 05:45:07.471: INFO: Creating new exec pod
Aug 17 05:45:09.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=services-8355 execpod896hq -- /bin/sh -x -c nslookup clusterip-service'
Aug 17 05:45:10.015: INFO: stderr: "+ nslookup clusterip-service\n"
Aug 17 05:45:10.015: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-8355.svc.cluster.local\tcanonical name = externalsvc.services-8355.svc.cluster.local.\nName:\texternalsvc.services-8355.svc.cluster.local\nAddress: 172.21.186.100\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8355, will wait for the garbage collector to delete the pods
Aug 17 05:45:10.133: INFO: Deleting ReplicationController externalsvc took: 48.882582ms
Aug 17 05:45:10.733: INFO: Terminating ReplicationController externalsvc pods took: 600.672651ms
Aug 17 05:45:22.543: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:45:22.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8355" for this suite.
Aug 17 05:45:32.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:45:35.183: INFO: namespace services-8355 deletion completed in 12.520843476s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:31.229 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:45:35.186: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 17 05:45:37.726: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:45:37.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-214" for this suite.
Aug 17 05:45:48.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:45:51.555: INFO: namespace container-runtime-214 deletion completed in 13.686917312s

• [SLOW TEST:16.369 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:45:51.555: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-3475ae91-8a82-4426-906b-7e7ce4acb3c5
STEP: Creating a pod to test consume secrets
Aug 17 05:45:52.058: INFO: Waiting up to 5m0s for pod "pod-secrets-9651bf8a-107b-4853-a421-e5b8d4b19212" in namespace "secrets-5422" to be "success or failure"
Aug 17 05:45:52.089: INFO: Pod "pod-secrets-9651bf8a-107b-4853-a421-e5b8d4b19212": Phase="Pending", Reason="", readiness=false. Elapsed: 31.040577ms
Aug 17 05:45:54.110: INFO: Pod "pod-secrets-9651bf8a-107b-4853-a421-e5b8d4b19212": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.052876753s
STEP: Saw pod success
Aug 17 05:45:54.111: INFO: Pod "pod-secrets-9651bf8a-107b-4853-a421-e5b8d4b19212" satisfied condition "success or failure"
Aug 17 05:45:54.128: INFO: Trying to get logs from node 10.241.148.42 pod pod-secrets-9651bf8a-107b-4853-a421-e5b8d4b19212 container secret-volume-test: <nil>
STEP: delete the pod
Aug 17 05:45:54.244: INFO: Waiting for pod pod-secrets-9651bf8a-107b-4853-a421-e5b8d4b19212 to disappear
Aug 17 05:45:54.274: INFO: Pod pod-secrets-9651bf8a-107b-4853-a421-e5b8d4b19212 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:45:54.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5422" for this suite.
Aug 17 05:46:04.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:46:06.907: INFO: namespace secrets-5422 deletion completed in 12.506557622s
STEP: Destroying namespace "secret-namespace-6420" for this suite.
Aug 17 05:46:14.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:46:21.860: INFO: namespace secret-namespace-6420 deletion completed in 14.952346187s

• [SLOW TEST:30.305 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:46:21.860: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Aug 17 05:46:22.182: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:47:11.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4934" for this suite.
Aug 17 05:47:21.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:47:23.919: INFO: namespace crd-publish-openapi-4934 deletion completed in 12.765395395s

• [SLOW TEST:62.060 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:47:23.920: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 05:47:26.418: INFO: Waiting up to 5m0s for pod "client-envvars-ea71df14-9ac6-4dc8-b4f7-aff93830cdfc" in namespace "pods-3757" to be "success or failure"
Aug 17 05:47:26.443: INFO: Pod "client-envvars-ea71df14-9ac6-4dc8-b4f7-aff93830cdfc": Phase="Pending", Reason="", readiness=false. Elapsed: 25.294837ms
Aug 17 05:47:28.465: INFO: Pod "client-envvars-ea71df14-9ac6-4dc8-b4f7-aff93830cdfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046737167s
Aug 17 05:47:30.491: INFO: Pod "client-envvars-ea71df14-9ac6-4dc8-b4f7-aff93830cdfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072949299s
STEP: Saw pod success
Aug 17 05:47:30.491: INFO: Pod "client-envvars-ea71df14-9ac6-4dc8-b4f7-aff93830cdfc" satisfied condition "success or failure"
Aug 17 05:47:30.511: INFO: Trying to get logs from node 10.241.148.42 pod client-envvars-ea71df14-9ac6-4dc8-b4f7-aff93830cdfc container env3cont: <nil>
STEP: delete the pod
Aug 17 05:47:30.646: INFO: Waiting for pod client-envvars-ea71df14-9ac6-4dc8-b4f7-aff93830cdfc to disappear
Aug 17 05:47:30.664: INFO: Pod client-envvars-ea71df14-9ac6-4dc8-b4f7-aff93830cdfc no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:47:30.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3757" for this suite.
Aug 17 05:47:46.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:47:51.971: INFO: namespace pods-3757 deletion completed in 21.277959357s

• [SLOW TEST:28.051 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:47:51.972: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 05:47:52.166: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Creating first CR 
Aug 17 05:47:52.440: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-17T05:47:52Z generation:1 name:name1 resourceVersion:76880 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:df8a42aa-7569-444a-845c-9ca1c6a9a687] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Aug 17 05:48:02.465: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-17T05:48:02Z generation:1 name:name2 resourceVersion:76925 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:7317419e-aedb-4dd1-895a-657b57bf2155] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Aug 17 05:48:12.491: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-17T05:47:52Z generation:2 name:name1 resourceVersion:76974 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:df8a42aa-7569-444a-845c-9ca1c6a9a687] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Aug 17 05:48:22.535: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-17T05:48:02Z generation:2 name:name2 resourceVersion:77020 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:7317419e-aedb-4dd1-895a-657b57bf2155] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Aug 17 05:48:32.574: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-17T05:47:52Z generation:2 name:name1 resourceVersion:77061 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:df8a42aa-7569-444a-845c-9ca1c6a9a687] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Aug 17 05:48:42.628: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-17T05:48:02Z generation:2 name:name2 resourceVersion:77107 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:7317419e-aedb-4dd1-895a-657b57bf2155] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:48:53.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-4377" for this suite.
Aug 17 05:49:03.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:49:06.310: INFO: namespace crd-watch-4377 deletion completed in 12.601911181s

• [SLOW TEST:74.339 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:49:06.311: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0817 05:49:12.752626      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 17 05:49:12.752: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:49:12.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4519" for this suite.
Aug 17 05:49:22.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:49:25.335: INFO: namespace gc-4519 deletion completed in 12.550078442s

• [SLOW TEST:19.024 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:49:25.335: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Aug 17 05:49:25.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 --namespace=kubectl-929 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Aug 17 05:49:29.981: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Aug 17 05:49:29.981: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:49:32.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-929" for this suite.
Aug 17 05:49:40.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:49:42.700: INFO: namespace kubectl-929 deletion completed in 10.603027713s

• [SLOW TEST:17.365 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:49:42.701: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 17 05:49:42.997: INFO: Waiting up to 5m0s for pod "pod-56888f07-4a1d-4b6a-b8ad-5a807651ff56" in namespace "emptydir-9211" to be "success or failure"
Aug 17 05:49:43.014: INFO: Pod "pod-56888f07-4a1d-4b6a-b8ad-5a807651ff56": Phase="Pending", Reason="", readiness=false. Elapsed: 16.649137ms
Aug 17 05:49:45.032: INFO: Pod "pod-56888f07-4a1d-4b6a-b8ad-5a807651ff56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034939844s
Aug 17 05:49:47.172: INFO: Pod "pod-56888f07-4a1d-4b6a-b8ad-5a807651ff56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.175044367s
STEP: Saw pod success
Aug 17 05:49:47.172: INFO: Pod "pod-56888f07-4a1d-4b6a-b8ad-5a807651ff56" satisfied condition "success or failure"
Aug 17 05:49:47.302: INFO: Trying to get logs from node 10.241.148.42 pod pod-56888f07-4a1d-4b6a-b8ad-5a807651ff56 container test-container: <nil>
STEP: delete the pod
Aug 17 05:49:47.730: INFO: Waiting for pod pod-56888f07-4a1d-4b6a-b8ad-5a807651ff56 to disappear
Aug 17 05:49:47.791: INFO: Pod pod-56888f07-4a1d-4b6a-b8ad-5a807651ff56 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:49:47.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9211" for this suite.
Aug 17 05:49:56.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:49:58.788: INFO: namespace emptydir-9211 deletion completed in 10.671418121s

• [SLOW TEST:16.088 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:49:58.790: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Aug 17 05:49:59.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 create -f - --namespace=kubectl-8668'
Aug 17 05:49:59.681: INFO: stderr: ""
Aug 17 05:49:59.681: INFO: stdout: "pod/pause created\n"
Aug 17 05:49:59.681: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 17 05:49:59.681: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8668" to be "running and ready"
Aug 17 05:49:59.705: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 23.458607ms
Aug 17 05:50:01.723: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.041302751s
Aug 17 05:50:01.723: INFO: Pod "pause" satisfied condition "running and ready"
Aug 17 05:50:01.723: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Aug 17 05:50:01.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 label pods pause testing-label=testing-label-value --namespace=kubectl-8668'
Aug 17 05:50:01.933: INFO: stderr: ""
Aug 17 05:50:01.933: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Aug 17 05:50:01.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pod pause -L testing-label --namespace=kubectl-8668'
Aug 17 05:50:02.074: INFO: stderr: ""
Aug 17 05:50:02.074: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Aug 17 05:50:02.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 label pods pause testing-label- --namespace=kubectl-8668'
Aug 17 05:50:02.269: INFO: stderr: ""
Aug 17 05:50:02.269: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Aug 17 05:50:02.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pod pause -L testing-label --namespace=kubectl-8668'
Aug 17 05:50:02.783: INFO: stderr: ""
Aug 17 05:50:02.783: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Aug 17 05:50:02.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 delete --grace-period=0 --force -f - --namespace=kubectl-8668'
Aug 17 05:50:02.976: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 05:50:02.976: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 17 05:50:02.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get rc,svc -l name=pause --no-headers --namespace=kubectl-8668'
Aug 17 05:50:03.144: INFO: stderr: "No resources found in kubectl-8668 namespace.\n"
Aug 17 05:50:03.144: INFO: stdout: ""
Aug 17 05:50:03.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods -l name=pause --namespace=kubectl-8668 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 17 05:50:03.280: INFO: stderr: ""
Aug 17 05:50:03.280: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:50:03.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8668" for this suite.
Aug 17 05:50:11.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:50:13.987: INFO: namespace kubectl-8668 deletion completed in 10.671843009s

• [SLOW TEST:15.198 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:50:13.989: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-8df6bea7-0811-4ed7-9555-22f24adfa352
STEP: Creating a pod to test consume secrets
Aug 17 05:50:14.280: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7d40bdc2-39b3-42a5-8efa-24dbbf70ba46" in namespace "projected-6630" to be "success or failure"
Aug 17 05:50:14.300: INFO: Pod "pod-projected-secrets-7d40bdc2-39b3-42a5-8efa-24dbbf70ba46": Phase="Pending", Reason="", readiness=false. Elapsed: 19.646788ms
Aug 17 05:50:16.360: INFO: Pod "pod-projected-secrets-7d40bdc2-39b3-42a5-8efa-24dbbf70ba46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.079684256s
Aug 17 05:50:18.435: INFO: Pod "pod-projected-secrets-7d40bdc2-39b3-42a5-8efa-24dbbf70ba46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.154720162s
STEP: Saw pod success
Aug 17 05:50:18.435: INFO: Pod "pod-projected-secrets-7d40bdc2-39b3-42a5-8efa-24dbbf70ba46" satisfied condition "success or failure"
Aug 17 05:50:18.497: INFO: Trying to get logs from node 10.241.148.42 pod pod-projected-secrets-7d40bdc2-39b3-42a5-8efa-24dbbf70ba46 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 17 05:50:18.713: INFO: Waiting for pod pod-projected-secrets-7d40bdc2-39b3-42a5-8efa-24dbbf70ba46 to disappear
Aug 17 05:50:18.774: INFO: Pod pod-projected-secrets-7d40bdc2-39b3-42a5-8efa-24dbbf70ba46 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:50:18.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6630" for this suite.
Aug 17 05:50:29.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:50:31.738: INFO: namespace projected-6630 deletion completed in 12.620464945s

• [SLOW TEST:17.749 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:50:31.739: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:50:41.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7849" for this suite.
Aug 17 05:50:52.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:50:54.625: INFO: namespace job-7849 deletion completed in 12.620253545s

• [SLOW TEST:22.886 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:50:54.626: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:50:54.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-282" for this suite.
Aug 17 05:51:02.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:51:05.521: INFO: namespace tables-282 deletion completed in 10.608969743s

• [SLOW TEST:10.895 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:51:05.521: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:52:05.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4863" for this suite.
Aug 17 05:52:39.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:52:42.578: INFO: namespace container-probe-4863 deletion completed in 36.668661337s

• [SLOW TEST:97.057 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:52:42.579: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 05:52:42.830: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-b1c54619-b315-493c-bfcd-b909da48fe05
STEP: Creating secret with name s-test-opt-upd-5e3f29dd-ab6c-4a79-906b-d58fd83639ee
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-b1c54619-b315-493c-bfcd-b909da48fe05
STEP: Updating secret s-test-opt-upd-5e3f29dd-ab6c-4a79-906b-d58fd83639ee
STEP: Creating secret with name s-test-opt-create-f32dc9fe-2686-429f-bc32-7d81c5c670b3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:54:09.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8265" for this suite.
Aug 17 05:54:46.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:54:51.545: INFO: namespace projected-8265 deletion completed in 41.571486245s

• [SLOW TEST:128.966 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:54:51.545: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
Aug 17 05:54:51.758: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 17 05:55:52.141: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 05:55:52.164: INFO: Starting informer...
STEP: Starting pod...
Aug 17 05:55:52.458: INFO: Pod is running on 10.241.148.42. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Aug 17 05:55:52.533: INFO: Pod wasn't evicted. Proceeding
Aug 17 05:55:52.533: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Aug 17 05:57:07.598: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:57:07.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-6850" for this suite.
Aug 17 05:57:23.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:57:26.507: INFO: namespace taint-single-pod-6850 deletion completed in 18.878787184s

• [SLOW TEST:154.962 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:57:26.507: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Aug 17 05:57:29.949: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:57:30.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8121" for this suite.
Aug 17 05:57:46.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:57:51.659: INFO: namespace replicaset-8121 deletion completed in 21.473977578s

• [SLOW TEST:25.152 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:57:51.659: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Aug 17 05:57:54.683: INFO: Successfully updated pod "labelsupdate2140cd77-8aad-468f-8c11-4f1bee7fb8a1"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:57:56.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-876" for this suite.
Aug 17 05:58:32.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:58:35.491: INFO: namespace downward-api-876 deletion completed in 38.65889089s

• [SLOW TEST:43.832 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:58:35.492: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-54981bbe-0dfb-4d8c-a2b2-93b6d570a307
STEP: Creating a pod to test consume configMaps
Aug 17 05:58:35.822: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-883b017a-dbb4-45ed-aeea-341c209e3064" in namespace "projected-9199" to be "success or failure"
Aug 17 05:58:35.844: INFO: Pod "pod-projected-configmaps-883b017a-dbb4-45ed-aeea-341c209e3064": Phase="Pending", Reason="", readiness=false. Elapsed: 22.225478ms
Aug 17 05:58:37.864: INFO: Pod "pod-projected-configmaps-883b017a-dbb4-45ed-aeea-341c209e3064": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041395488s
Aug 17 05:58:39.886: INFO: Pod "pod-projected-configmaps-883b017a-dbb4-45ed-aeea-341c209e3064": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06334543s
STEP: Saw pod success
Aug 17 05:58:39.886: INFO: Pod "pod-projected-configmaps-883b017a-dbb4-45ed-aeea-341c209e3064" satisfied condition "success or failure"
Aug 17 05:58:39.910: INFO: Trying to get logs from node 10.241.148.42 pod pod-projected-configmaps-883b017a-dbb4-45ed-aeea-341c209e3064 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 17 05:58:40.002: INFO: Waiting for pod pod-projected-configmaps-883b017a-dbb4-45ed-aeea-341c209e3064 to disappear
Aug 17 05:58:40.023: INFO: Pod pod-projected-configmaps-883b017a-dbb4-45ed-aeea-341c209e3064 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:58:40.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9199" for this suite.
Aug 17 05:58:50.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:58:53.159: INFO: namespace projected-9199 deletion completed in 13.051802145s

• [SLOW TEST:17.667 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:58:53.159: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Aug 17 05:58:53.349: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 17 05:58:53.486: INFO: Waiting for terminating namespaces to be deleted...
Aug 17 05:58:53.516: INFO: 
Logging pods the kubelet thinks is on node 10.241.148.31 before test
Aug 17 05:58:53.694: INFO: community-operators-65d9d5c9d6-zh8cl from openshift-marketplace started at 2020-08-17 02:59:24 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.694: INFO: 	Container community-operators ready: true, restart count 0
Aug 17 05:58:53.694: INFO: apiservice-cabundle-injector-594fd4555f-znbpx from openshift-service-ca started at 2020-08-17 04:57:04 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.694: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Aug 17 05:58:53.694: INFO: openshift-kube-proxy-5nh74 from openshift-kube-proxy started at 2020-08-17 02:56:45 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.694: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 17 05:58:53.694: INFO: prometheus-adapter-67dbcc5cfc-xk5l6 from openshift-monitoring started at 2020-08-17 02:58:37 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.694: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 17 05:58:53.694: INFO: network-operator-7986644c85-79z2f from openshift-network-operator started at 2020-08-17 04:57:04 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.694: INFO: 	Container network-operator ready: true, restart count 0
Aug 17 05:58:53.694: INFO: thanos-querier-6b5fbd9754-ctb66 from openshift-monitoring started at 2020-08-17 03:05:37 +0000 UTC (4 container statuses recorded)
Aug 17 05:58:53.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 05:58:53.694: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 17 05:58:53.694: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 17 05:58:53.694: INFO: 	Container thanos-querier ready: true, restart count 0
Aug 17 05:58:53.694: INFO: grafana-c9c7455d7-5lnv7 from openshift-monitoring started at 2020-08-17 04:57:05 +0000 UTC (2 container statuses recorded)
Aug 17 05:58:53.694: INFO: 	Container grafana ready: true, restart count 0
Aug 17 05:58:53.694: INFO: 	Container grafana-proxy ready: true, restart count 0
Aug 17 05:58:53.694: INFO: multus-tvx65 from openshift-multus started at 2020-08-17 02:56:38 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.694: INFO: 	Container kube-multus ready: true, restart count 0
Aug 17 05:58:53.695: INFO: tuned-nmn6j from openshift-cluster-node-tuning-operator started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.695: INFO: 	Container tuned ready: true, restart count 0
Aug 17 05:58:53.695: INFO: certified-operators-549fcd7d77-n48c7 from openshift-marketplace started at 2020-08-17 04:57:05 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.695: INFO: 	Container certified-operators ready: true, restart count 0
Aug 17 05:58:53.695: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-08-17 03:05:54 +0000 UTC (7 container statuses recorded)
Aug 17 05:58:53.695: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 05:58:53.695: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 17 05:58:53.695: INFO: 	Container prometheus ready: true, restart count 1
Aug 17 05:58:53.695: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 17 05:58:53.695: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 17 05:58:53.695: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 17 05:58:53.695: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 17 05:58:53.695: INFO: ibm-cloud-provider-ip-169-48-239-26-5595944bf9-n8nts from ibm-system started at 2020-08-17 03:02:23 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.695: INFO: 	Container ibm-cloud-provider-ip-169-48-239-26 ready: true, restart count 0
Aug 17 05:58:53.695: INFO: console-76d8dc5894-2w62x from openshift-console started at 2020-08-17 02:59:39 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.695: INFO: 	Container console ready: true, restart count 0
Aug 17 05:58:53.695: INFO: ibmcloud-block-storage-driver-hfn98 from kube-system started at 2020-08-17 02:56:23 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.695: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 17 05:58:53.695: INFO: calico-node-lm8qf from calico-system started at 2020-08-17 02:57:31 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.695: INFO: 	Container calico-node ready: true, restart count 0
Aug 17 05:58:53.695: INFO: cluster-samples-operator-55944b8f44-fxlq8 from openshift-cluster-samples-operator started at 2020-08-17 02:59:01 +0000 UTC (2 container statuses recorded)
Aug 17 05:58:53.695: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Aug 17 05:58:53.695: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Aug 17 05:58:53.695: INFO: sonobuoy-systemd-logs-daemon-set-c8f140ca93fa466f-pjc77 from sonobuoy started at 2020-08-17 04:12:38 +0000 UTC (2 container statuses recorded)
Aug 17 05:58:53.695: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 17 05:58:53.695: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 05:58:53.695: INFO: telemeter-client-76746b8bb7-dnjsr from openshift-monitoring started at 2020-08-17 04:57:05 +0000 UTC (3 container statuses recorded)
Aug 17 05:58:53.695: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 05:58:53.695: INFO: 	Container reload ready: true, restart count 0
Aug 17 05:58:53.695: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 17 05:58:53.695: INFO: multus-admission-controller-ncsvb from openshift-multus started at 2020-08-17 02:57:44 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.695: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 17 05:58:53.695: INFO: router-default-57d78dfb48-mzjlc from openshift-ingress started at 2020-08-17 02:59:05 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.695: INFO: 	Container router ready: true, restart count 0
Aug 17 05:58:53.695: INFO: calico-typha-579947cf56-672t9 from calico-system started at 2020-08-17 02:59:26 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.695: INFO: 	Container calico-typha ready: true, restart count 0
Aug 17 05:58:53.695: INFO: image-registry-5669fd49dd-xphdf from openshift-image-registry started at 2020-08-17 03:00:49 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.695: INFO: 	Container registry ready: true, restart count 0
Aug 17 05:58:53.695: INFO: service-serving-cert-signer-7879bf8d9f-xtqt8 from openshift-service-ca started at 2020-08-17 02:58:07 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.695: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Aug 17 05:58:53.695: INFO: configmap-cabundle-injector-8446d4b88f-9k4sq from openshift-service-ca started at 2020-08-17 02:58:07 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.695: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Aug 17 05:58:53.695: INFO: kube-state-metrics-c5f65645-855s8 from openshift-monitoring started at 2020-08-17 02:58:30 +0000 UTC (3 container statuses recorded)
Aug 17 05:58:53.695: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 17 05:58:53.695: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 17 05:58:53.695: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 17 05:58:53.695: INFO: ibm-keepalived-watcher-ncgwx from kube-system started at 2020-08-17 02:56:14 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.695: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 17 05:58:53.695: INFO: node-ca-2lszv from openshift-image-registry started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.695: INFO: 	Container node-ca ready: true, restart count 0
Aug 17 05:58:53.695: INFO: packageserver-66768b6f89-b74q9 from openshift-operator-lifecycle-manager started at 2020-08-17 03:05:22 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.695: INFO: 	Container packageserver ready: true, restart count 0
Aug 17 05:58:53.695: INFO: prometheus-operator-56d9d699cb-m5pmc from openshift-monitoring started at 2020-08-17 03:04:39 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.695: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 17 05:58:53.695: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-08-17 03:05:11 +0000 UTC (3 container statuses recorded)
Aug 17 05:58:53.695: INFO: 	Container alertmanager ready: true, restart count 0
Aug 17 05:58:53.695: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 17 05:58:53.695: INFO: 	Container config-reloader ready: true, restart count 0
Aug 17 05:58:53.695: INFO: openshift-state-metrics-5849d797d8-2f9sd from openshift-monitoring started at 2020-08-17 04:57:05 +0000 UTC (3 container statuses recorded)
Aug 17 05:58:53.695: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 17 05:58:53.695: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 17 05:58:53.695: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 17 05:58:53.695: INFO: ibm-master-proxy-static-10.241.148.31 from kube-system started at 2020-08-17 02:56:11 +0000 UTC (2 container statuses recorded)
Aug 17 05:58:53.695: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 17 05:58:53.695: INFO: 	Container pause ready: true, restart count 0
Aug 17 05:58:53.695: INFO: node-exporter-kkwj6 from openshift-monitoring started at 2020-08-17 02:58:34 +0000 UTC (2 container statuses recorded)
Aug 17 05:58:53.695: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 05:58:53.695: INFO: 	Container node-exporter ready: true, restart count 0
Aug 17 05:58:53.695: INFO: dns-default-sxjmd from openshift-dns started at 2020-08-17 02:59:01 +0000 UTC (2 container statuses recorded)
Aug 17 05:58:53.695: INFO: 	Container dns ready: true, restart count 0
Aug 17 05:58:53.695: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 17 05:58:53.695: INFO: 
Logging pods the kubelet thinks is on node 10.241.148.42 before test
Aug 17 05:58:53.762: INFO: tuned-b665k from openshift-cluster-node-tuning-operator started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.762: INFO: 	Container tuned ready: true, restart count 0
Aug 17 05:58:53.762: INFO: openshift-kube-proxy-hswpd from openshift-kube-proxy started at 2020-08-17 02:56:45 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.762: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 17 05:58:53.762: INFO: ibmcloud-block-storage-driver-5dth8 from kube-system started at 2020-08-17 02:56:14 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.762: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 17 05:58:53.762: INFO: multus-p85fw from openshift-multus started at 2020-08-17 02:56:38 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.762: INFO: 	Container kube-multus ready: true, restart count 0
Aug 17 05:58:53.762: INFO: calico-typha-579947cf56-86wz4 from calico-system started at 2020-08-17 02:57:30 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.762: INFO: 	Container calico-typha ready: true, restart count 0
Aug 17 05:58:53.762: INFO: node-exporter-r6g64 from openshift-monitoring started at 2020-08-17 02:58:34 +0000 UTC (2 container statuses recorded)
Aug 17 05:58:53.762: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 05:58:53.762: INFO: 	Container node-exporter ready: true, restart count 0
Aug 17 05:58:53.762: INFO: ibm-master-proxy-static-10.241.148.42 from kube-system started at 2020-08-17 02:56:02 +0000 UTC (2 container statuses recorded)
Aug 17 05:58:53.762: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 17 05:58:53.762: INFO: 	Container pause ready: true, restart count 0
Aug 17 05:58:53.762: INFO: sonobuoy from sonobuoy started at 2020-08-17 04:12:31 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.762: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 17 05:58:53.762: INFO: sonobuoy-e2e-job-144f6878906f41e3 from sonobuoy started at 2020-08-17 04:12:38 +0000 UTC (2 container statuses recorded)
Aug 17 05:58:53.762: INFO: 	Container e2e ready: true, restart count 0
Aug 17 05:58:53.762: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 05:58:53.762: INFO: sonobuoy-systemd-logs-daemon-set-c8f140ca93fa466f-qhfmq from sonobuoy started at 2020-08-17 04:12:38 +0000 UTC (2 container statuses recorded)
Aug 17 05:58:53.762: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 17 05:58:53.762: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 05:58:53.762: INFO: node-ca-j567q from openshift-image-registry started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.762: INFO: 	Container node-ca ready: true, restart count 0
Aug 17 05:58:53.762: INFO: ibm-keepalived-watcher-xf2gw from kube-system started at 2020-08-17 02:56:05 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.762: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 17 05:58:53.762: INFO: registry-pvc-permissions-j8h5n from openshift-image-registry started at 2020-08-17 03:00:49 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.762: INFO: 	Container pvc-permissions ready: false, restart count 0
Aug 17 05:58:53.762: INFO: multus-admission-controller-wrxb5 from openshift-multus started at 2020-08-17 05:56:32 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.762: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 17 05:58:53.762: INFO: dns-default-kb7xp from openshift-dns started at 2020-08-17 05:56:02 +0000 UTC (2 container statuses recorded)
Aug 17 05:58:53.762: INFO: 	Container dns ready: true, restart count 0
Aug 17 05:58:53.762: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 17 05:58:53.762: INFO: calico-node-cxl67 from calico-system started at 2020-08-17 02:57:31 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.762: INFO: 	Container calico-node ready: true, restart count 0
Aug 17 05:58:53.762: INFO: 
Logging pods the kubelet thinks is on node 10.241.148.50 before test
Aug 17 05:58:53.926: INFO: console-operator-9878d4766-7tcvg from openshift-console-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container console-operator ready: true, restart count 1
Aug 17 05:58:53.926: INFO: openshift-service-catalog-apiserver-operator-78b9dddb6f-bbw72 from openshift-service-catalog-apiserver-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container operator ready: true, restart count 1
Aug 17 05:58:53.926: INFO: ingress-operator-695bc545b9-d4trd from openshift-ingress-operator started at 2020-08-17 02:57:42 +0000 UTC (2 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container ingress-operator ready: true, restart count 0
Aug 17 05:58:53.926: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 05:58:53.926: INFO: downloads-678f5d6564-wxxs4 from openshift-console started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container download-server ready: true, restart count 0
Aug 17 05:58:53.926: INFO: dns-operator-6f9cf66db7-ksmws from openshift-dns-operator started at 2020-08-17 02:57:44 +0000 UTC (2 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container dns-operator ready: true, restart count 0
Aug 17 05:58:53.926: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 05:58:53.926: INFO: redhat-operators-85f96bd7c-69xgq from openshift-marketplace started at 2020-08-17 02:59:24 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container redhat-operators ready: true, restart count 0
Aug 17 05:58:53.926: INFO: packageserver-66768b6f89-hjpwd from openshift-operator-lifecycle-manager started at 2020-08-17 03:05:13 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container packageserver ready: true, restart count 0
Aug 17 05:58:53.926: INFO: thanos-querier-6b5fbd9754-wzzrz from openshift-monitoring started at 2020-08-17 03:05:26 +0000 UTC (4 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 05:58:53.926: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 17 05:58:53.926: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 17 05:58:53.926: INFO: 	Container thanos-querier ready: true, restart count 0
Aug 17 05:58:53.926: INFO: tigera-operator-679798d94d-rj7fl from tigera-operator started at 2020-08-17 04:57:05 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container tigera-operator ready: true, restart count 0
Aug 17 05:58:53.926: INFO: ibm-storage-watcher-77bf8b889-856t2 from kube-system started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 17 05:58:53.926: INFO: downloads-678f5d6564-t5r44 from openshift-console started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container download-server ready: true, restart count 0
Aug 17 05:58:53.926: INFO: node-exporter-ztsvb from openshift-monitoring started at 2020-08-17 02:58:33 +0000 UTC (2 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 05:58:53.926: INFO: 	Container node-exporter ready: true, restart count 0
Aug 17 05:58:53.926: INFO: calico-typha-579947cf56-87rgj from calico-system started at 2020-08-17 02:59:26 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container calico-typha ready: true, restart count 0
Aug 17 05:58:53.926: INFO: sonobuoy-systemd-logs-daemon-set-c8f140ca93fa466f-j84lz from sonobuoy started at 2020-08-17 04:12:38 +0000 UTC (2 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 17 05:58:53.926: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 05:58:53.926: INFO: vpn-7b59d9f64-2rnfd from kube-system started at 2020-08-17 03:05:11 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container vpn ready: true, restart count 0
Aug 17 05:58:53.926: INFO: router-default-57d78dfb48-x8wzz from openshift-ingress started at 2020-08-17 04:57:04 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container router ready: true, restart count 0
Aug 17 05:58:53.926: INFO: catalog-operator-85f6c659cc-6zpxp from openshift-operator-lifecycle-manager started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 17 05:58:53.926: INFO: multus-admission-controller-7xrg7 from openshift-multus started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 17 05:58:53.926: INFO: tuned-6qthk from openshift-cluster-node-tuning-operator started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container tuned ready: true, restart count 0
Aug 17 05:58:53.926: INFO: ibmcloud-block-storage-driver-6l6kb from kube-system started at 2020-08-17 02:56:30 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 17 05:58:53.926: INFO: service-ca-operator-694cfbf5d5-2dhf8 from openshift-service-ca-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container operator ready: true, restart count 0
Aug 17 05:58:53.926: INFO: olm-operator-b5f57cdbb-wcj4n from openshift-operator-lifecycle-manager started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container olm-operator ready: true, restart count 0
Aug 17 05:58:53.926: INFO: console-76d8dc5894-x8wzx from openshift-console started at 2020-08-17 04:57:05 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container console ready: true, restart count 0
Aug 17 05:58:53.926: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-08-17 03:04:54 +0000 UTC (3 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container alertmanager ready: true, restart count 0
Aug 17 05:58:53.926: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 17 05:58:53.926: INFO: 	Container config-reloader ready: true, restart count 0
Aug 17 05:58:53.926: INFO: openshift-kube-proxy-grn9g from openshift-kube-proxy started at 2020-08-17 02:56:45 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 17 05:58:53.926: INFO: calico-node-hmdtm from calico-system started at 2020-08-17 02:57:31 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container calico-node ready: true, restart count 0
Aug 17 05:58:53.926: INFO: cluster-monitoring-operator-5b5659466f-bd9vb from openshift-monitoring started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Aug 17 05:58:53.926: INFO: cluster-image-registry-operator-6cfd58b66c-xpvcp from openshift-image-registry started at 2020-08-17 02:57:42 +0000 UTC (2 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Aug 17 05:58:53.926: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Aug 17 05:58:53.926: INFO: calico-kube-controllers-79d75767dd-hhcw2 from calico-system started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 17 05:58:53.926: INFO: ibmcloud-block-storage-plugin-68d5c65db9-m8qcr from kube-system started at 2020-08-17 02:57:43 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Aug 17 05:58:53.926: INFO: ibm-cloud-provider-ip-169-48-239-26-5595944bf9-2pxq9 from ibm-system started at 2020-08-17 03:02:20 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container ibm-cloud-provider-ip-169-48-239-26 ready: true, restart count 0
Aug 17 05:58:53.926: INFO: ibm-master-proxy-static-10.241.148.50 from kube-system started at 2020-08-17 02:56:19 +0000 UTC (2 container statuses recorded)
Aug 17 05:58:53.926: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 17 05:58:53.927: INFO: 	Container pause ready: true, restart count 0
Aug 17 05:58:53.927: INFO: ibm-keepalived-watcher-x5mxk from kube-system started at 2020-08-17 02:56:22 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.927: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 17 05:58:53.927: INFO: cluster-storage-operator-557b75f8d5-899qb from openshift-cluster-storage-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.927: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Aug 17 05:58:53.927: INFO: prometheus-adapter-67dbcc5cfc-pz99g from openshift-monitoring started at 2020-08-17 04:57:05 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.927: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 17 05:58:53.927: INFO: cluster-node-tuning-operator-b5f884945-nmndb from openshift-cluster-node-tuning-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.927: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Aug 17 05:58:53.927: INFO: dns-default-kfbzc from openshift-dns started at 2020-08-17 02:59:02 +0000 UTC (2 container statuses recorded)
Aug 17 05:58:53.927: INFO: 	Container dns ready: true, restart count 0
Aug 17 05:58:53.927: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 17 05:58:53.927: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-08-17 04:57:12 +0000 UTC (3 container statuses recorded)
Aug 17 05:58:53.927: INFO: 	Container alertmanager ready: true, restart count 0
Aug 17 05:58:53.927: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 17 05:58:53.927: INFO: 	Container config-reloader ready: true, restart count 0
Aug 17 05:58:53.927: INFO: multus-wkz57 from openshift-multus started at 2020-08-17 02:56:38 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.927: INFO: 	Container kube-multus ready: true, restart count 0
Aug 17 05:58:53.927: INFO: ibm-file-plugin-fdb69b446-bzpn7 from kube-system started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.927: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 17 05:58:53.927: INFO: openshift-service-catalog-controller-manager-operator-5496wjhfr from openshift-service-catalog-controller-manager-operator started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.927: INFO: 	Container operator ready: true, restart count 1
Aug 17 05:58:53.927: INFO: marketplace-operator-6957767d58-6rcnt from openshift-marketplace started at 2020-08-17 02:57:42 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.927: INFO: 	Container marketplace-operator ready: true, restart count 0
Aug 17 05:58:53.927: INFO: node-ca-467qn from openshift-image-registry started at 2020-08-17 02:59:01 +0000 UTC (1 container statuses recorded)
Aug 17 05:58:53.927: INFO: 	Container node-ca ready: true, restart count 0
Aug 17 05:58:53.927: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-08-17 03:05:34 +0000 UTC (7 container statuses recorded)
Aug 17 05:58:53.927: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 17 05:58:53.927: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 17 05:58:53.927: INFO: 	Container prometheus ready: true, restart count 1
Aug 17 05:58:53.927: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 17 05:58:53.927: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 17 05:58:53.927: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 17 05:58:53.927: INFO: 	Container thanos-sidecar ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-9dd9ff15-7524-4a70-a8b7-5d5cfc572c8d 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-9dd9ff15-7524-4a70-a8b7-5d5cfc572c8d off the node 10.241.148.42
STEP: verifying the node doesn't have the label kubernetes.io/e2e-9dd9ff15-7524-4a70-a8b7-5d5cfc572c8d
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:59:00.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2021" for this suite.
Aug 17 05:59:14.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:59:19.021: INFO: namespace sched-pred-2021 deletion completed in 18.634074668s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:25.862 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:59:19.022: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 17 05:59:19.455: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3593c387-516c-485a-9d9f-f15bffb59d79" in namespace "downward-api-9932" to be "success or failure"
Aug 17 05:59:19.512: INFO: Pod "downwardapi-volume-3593c387-516c-485a-9d9f-f15bffb59d79": Phase="Pending", Reason="", readiness=false. Elapsed: 57.338345ms
Aug 17 05:59:21.531: INFO: Pod "downwardapi-volume-3593c387-516c-485a-9d9f-f15bffb59d79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076088538s
Aug 17 05:59:23.550: INFO: Pod "downwardapi-volume-3593c387-516c-485a-9d9f-f15bffb59d79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.095326583s
STEP: Saw pod success
Aug 17 05:59:23.550: INFO: Pod "downwardapi-volume-3593c387-516c-485a-9d9f-f15bffb59d79" satisfied condition "success or failure"
Aug 17 05:59:23.574: INFO: Trying to get logs from node 10.241.148.42 pod downwardapi-volume-3593c387-516c-485a-9d9f-f15bffb59d79 container client-container: <nil>
STEP: delete the pod
Aug 17 05:59:23.663: INFO: Waiting for pod downwardapi-volume-3593c387-516c-485a-9d9f-f15bffb59d79 to disappear
Aug 17 05:59:23.690: INFO: Pod downwardapi-volume-3593c387-516c-485a-9d9f-f15bffb59d79 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 05:59:23.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9932" for this suite.
Aug 17 05:59:33.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 05:59:36.373: INFO: namespace downward-api-9932 deletion completed in 12.641319889s

• [SLOW TEST:17.351 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 05:59:36.374: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Aug 17 05:59:36.597: INFO: PodSpec: initContainers in spec.initContainers
Aug 17 06:00:23.344: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-4d328cfd-3365-49b7-872e-ed32dba29f0c", GenerateName:"", Namespace:"init-container-1402", SelfLink:"/api/v1/namespaces/init-container-1402/pods/pod-init-4d328cfd-3365-49b7-872e-ed32dba29f0c", UID:"34f4eae1-cf02-46fb-961c-8cf044223169", ResourceVersion:"81557", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63733240776, loc:(*time.Location)(0x84c02a0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"597597804"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.30.218.106/32", "cni.projectcalico.org/podIPs":"172.30.218.106/32", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.218.106\"\n    ],\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-nhch4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002494040), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-nhch4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc005036140), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-nhch4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc005036280), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-nhch4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc005036050), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001a600a8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.241.148.42", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00a76a000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001a60160)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001a60180)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001a6019c), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001a601a0), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733240776, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733240776, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733240776, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733240776, loc:(*time.Location)(0x84c02a0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.241.148.42", PodIP:"172.30.218.106", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.218.106"}}, StartTime:(*v1.Time)(0xc003182060), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0008180e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000818150)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"cri-o://df350e1802d4bd92895dae1daa644a335afc225bc00453a80f9fadb0e6bfb0ee", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0031820a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003182080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc001a60214)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:00:23.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1402" for this suite.
Aug 17 06:00:39.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:00:42.057: INFO: namespace init-container-1402 deletion completed in 18.650100171s

• [SLOW TEST:65.683 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:00:42.059: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Aug 17 06:00:42.363: INFO: Waiting up to 5m0s for pod "var-expansion-83101730-f618-4c23-bf63-436f55af922d" in namespace "var-expansion-2623" to be "success or failure"
Aug 17 06:00:42.382: INFO: Pod "var-expansion-83101730-f618-4c23-bf63-436f55af922d": Phase="Pending", Reason="", readiness=false. Elapsed: 18.442638ms
Aug 17 06:00:44.403: INFO: Pod "var-expansion-83101730-f618-4c23-bf63-436f55af922d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039172273s
Aug 17 06:00:46.459: INFO: Pod "var-expansion-83101730-f618-4c23-bf63-436f55af922d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.095152896s
STEP: Saw pod success
Aug 17 06:00:46.459: INFO: Pod "var-expansion-83101730-f618-4c23-bf63-436f55af922d" satisfied condition "success or failure"
Aug 17 06:00:46.520: INFO: Trying to get logs from node 10.241.148.42 pod var-expansion-83101730-f618-4c23-bf63-436f55af922d container dapi-container: <nil>
STEP: delete the pod
Aug 17 06:00:46.775: INFO: Waiting for pod var-expansion-83101730-f618-4c23-bf63-436f55af922d to disappear
Aug 17 06:00:46.854: INFO: Pod var-expansion-83101730-f618-4c23-bf63-436f55af922d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:00:46.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2623" for this suite.
Aug 17 06:00:57.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:01:00.315: INFO: namespace var-expansion-2623 deletion completed in 12.754303295s

• [SLOW TEST:18.257 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:01:00.316: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 17 06:01:00.644: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f895c469-9a2f-45c6-97b2-2b17cad175d9" in namespace "projected-7369" to be "success or failure"
Aug 17 06:01:00.669: INFO: Pod "downwardapi-volume-f895c469-9a2f-45c6-97b2-2b17cad175d9": Phase="Pending", Reason="", readiness=false. Elapsed: 25.067179ms
Aug 17 06:01:02.689: INFO: Pod "downwardapi-volume-f895c469-9a2f-45c6-97b2-2b17cad175d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.044747006s
STEP: Saw pod success
Aug 17 06:01:02.689: INFO: Pod "downwardapi-volume-f895c469-9a2f-45c6-97b2-2b17cad175d9" satisfied condition "success or failure"
Aug 17 06:01:02.709: INFO: Trying to get logs from node 10.241.148.42 pod downwardapi-volume-f895c469-9a2f-45c6-97b2-2b17cad175d9 container client-container: <nil>
STEP: delete the pod
Aug 17 06:01:02.822: INFO: Waiting for pod downwardapi-volume-f895c469-9a2f-45c6-97b2-2b17cad175d9 to disappear
Aug 17 06:01:02.840: INFO: Pod downwardapi-volume-f895c469-9a2f-45c6-97b2-2b17cad175d9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:01:02.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7369" for this suite.
Aug 17 06:01:12.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:01:15.425: INFO: namespace projected-7369 deletion completed in 12.514126471s

• [SLOW TEST:15.109 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:01:15.426: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 17 06:01:15.709: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cfea0006-a7fc-4f0b-9b54-5169835c4dc7" in namespace "projected-155" to be "success or failure"
Aug 17 06:01:15.738: INFO: Pod "downwardapi-volume-cfea0006-a7fc-4f0b-9b54-5169835c4dc7": Phase="Pending", Reason="", readiness=false. Elapsed: 27.902921ms
Aug 17 06:01:17.797: INFO: Pod "downwardapi-volume-cfea0006-a7fc-4f0b-9b54-5169835c4dc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.087324764s
STEP: Saw pod success
Aug 17 06:01:17.797: INFO: Pod "downwardapi-volume-cfea0006-a7fc-4f0b-9b54-5169835c4dc7" satisfied condition "success or failure"
Aug 17 06:01:17.848: INFO: Trying to get logs from node 10.241.148.42 pod downwardapi-volume-cfea0006-a7fc-4f0b-9b54-5169835c4dc7 container client-container: <nil>
STEP: delete the pod
Aug 17 06:01:17.996: INFO: Waiting for pod downwardapi-volume-cfea0006-a7fc-4f0b-9b54-5169835c4dc7 to disappear
Aug 17 06:01:18.058: INFO: Pod downwardapi-volume-cfea0006-a7fc-4f0b-9b54-5169835c4dc7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:01:18.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-155" for this suite.
Aug 17 06:01:28.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:01:31.210: INFO: namespace projected-155 deletion completed in 12.730237628s

• [SLOW TEST:15.784 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:01:31.210: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-7579/configmap-test-423fbc17-6403-4444-b3e1-1d4aa682081b
STEP: Creating a pod to test consume configMaps
Aug 17 06:01:31.504: INFO: Waiting up to 5m0s for pod "pod-configmaps-2507db48-8696-475c-9463-1d7516cec74e" in namespace "configmap-7579" to be "success or failure"
Aug 17 06:01:31.520: INFO: Pod "pod-configmaps-2507db48-8696-475c-9463-1d7516cec74e": Phase="Pending", Reason="", readiness=false. Elapsed: 15.708095ms
Aug 17 06:01:33.546: INFO: Pod "pod-configmaps-2507db48-8696-475c-9463-1d7516cec74e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041477623s
Aug 17 06:01:35.565: INFO: Pod "pod-configmaps-2507db48-8696-475c-9463-1d7516cec74e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061029498s
STEP: Saw pod success
Aug 17 06:01:35.565: INFO: Pod "pod-configmaps-2507db48-8696-475c-9463-1d7516cec74e" satisfied condition "success or failure"
Aug 17 06:01:35.586: INFO: Trying to get logs from node 10.241.148.42 pod pod-configmaps-2507db48-8696-475c-9463-1d7516cec74e container env-test: <nil>
STEP: delete the pod
Aug 17 06:01:35.675: INFO: Waiting for pod pod-configmaps-2507db48-8696-475c-9463-1d7516cec74e to disappear
Aug 17 06:01:35.696: INFO: Pod pod-configmaps-2507db48-8696-475c-9463-1d7516cec74e no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:01:35.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7579" for this suite.
Aug 17 06:01:45.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:01:50.949: INFO: namespace configmap-7579 deletion completed in 15.178603792s

• [SLOW TEST:19.739 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:01:50.949: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 17 06:01:51.276: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dae8f5ee-e538-498b-949b-63536a26ecad" in namespace "projected-1267" to be "success or failure"
Aug 17 06:01:51.299: INFO: Pod "downwardapi-volume-dae8f5ee-e538-498b-949b-63536a26ecad": Phase="Pending", Reason="", readiness=false. Elapsed: 22.902217ms
Aug 17 06:01:53.321: INFO: Pod "downwardapi-volume-dae8f5ee-e538-498b-949b-63536a26ecad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044469447s
Aug 17 06:01:55.339: INFO: Pod "downwardapi-volume-dae8f5ee-e538-498b-949b-63536a26ecad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062356889s
STEP: Saw pod success
Aug 17 06:01:55.339: INFO: Pod "downwardapi-volume-dae8f5ee-e538-498b-949b-63536a26ecad" satisfied condition "success or failure"
Aug 17 06:01:55.358: INFO: Trying to get logs from node 10.241.148.42 pod downwardapi-volume-dae8f5ee-e538-498b-949b-63536a26ecad container client-container: <nil>
STEP: delete the pod
Aug 17 06:01:55.459: INFO: Waiting for pod downwardapi-volume-dae8f5ee-e538-498b-949b-63536a26ecad to disappear
Aug 17 06:01:55.475: INFO: Pod downwardapi-volume-dae8f5ee-e538-498b-949b-63536a26ecad no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:01:55.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1267" for this suite.
Aug 17 06:02:03.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:02:06.015: INFO: namespace projected-1267 deletion completed in 10.497148107s

• [SLOW TEST:15.066 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:02:06.015: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:02:06.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4877" for this suite.
Aug 17 06:02:14.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:02:18.152: INFO: namespace services-4877 deletion completed in 11.823877304s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:12.137 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:02:18.153: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:02:20.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-125" for this suite.
Aug 17 06:03:17.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:03:21.124: INFO: namespace kubelet-test-125 deletion completed in 1m0.184779706s

• [SLOW TEST:62.971 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:03:21.124: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Aug 17 06:03:21.327: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:03:25.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6229" for this suite.
Aug 17 06:03:35.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:03:38.437: INFO: namespace init-container-6229 deletion completed in 12.52548715s

• [SLOW TEST:17.313 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:03:38.442: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-5ce20a7f-2425-4421-ad0a-972f454eb791
STEP: Creating a pod to test consume configMaps
Aug 17 06:03:38.767: INFO: Waiting up to 5m0s for pod "pod-configmaps-ea1e4419-761e-4000-a37d-db2a8766ca41" in namespace "configmap-7357" to be "success or failure"
Aug 17 06:03:38.783: INFO: Pod "pod-configmaps-ea1e4419-761e-4000-a37d-db2a8766ca41": Phase="Pending", Reason="", readiness=false. Elapsed: 16.745072ms
Aug 17 06:03:40.806: INFO: Pod "pod-configmaps-ea1e4419-761e-4000-a37d-db2a8766ca41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038990422s
Aug 17 06:03:42.828: INFO: Pod "pod-configmaps-ea1e4419-761e-4000-a37d-db2a8766ca41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061777866s
STEP: Saw pod success
Aug 17 06:03:42.828: INFO: Pod "pod-configmaps-ea1e4419-761e-4000-a37d-db2a8766ca41" satisfied condition "success or failure"
Aug 17 06:03:42.848: INFO: Trying to get logs from node 10.241.148.42 pod pod-configmaps-ea1e4419-761e-4000-a37d-db2a8766ca41 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 17 06:03:42.968: INFO: Waiting for pod pod-configmaps-ea1e4419-761e-4000-a37d-db2a8766ca41 to disappear
Aug 17 06:03:42.992: INFO: Pod pod-configmaps-ea1e4419-761e-4000-a37d-db2a8766ca41 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:03:42.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7357" for this suite.
Aug 17 06:03:53.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:03:55.743: INFO: namespace configmap-7357 deletion completed in 12.606581084s

• [SLOW TEST:17.301 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:03:55.743: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 17 06:03:56.015: INFO: Waiting up to 5m0s for pod "pod-484a86a4-8dc8-4a6b-9479-83c278342ad4" in namespace "emptydir-3525" to be "success or failure"
Aug 17 06:03:56.041: INFO: Pod "pod-484a86a4-8dc8-4a6b-9479-83c278342ad4": Phase="Pending", Reason="", readiness=false. Elapsed: 25.793019ms
Aug 17 06:03:58.063: INFO: Pod "pod-484a86a4-8dc8-4a6b-9479-83c278342ad4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.047535253s
STEP: Saw pod success
Aug 17 06:03:58.063: INFO: Pod "pod-484a86a4-8dc8-4a6b-9479-83c278342ad4" satisfied condition "success or failure"
Aug 17 06:03:58.083: INFO: Trying to get logs from node 10.241.148.42 pod pod-484a86a4-8dc8-4a6b-9479-83c278342ad4 container test-container: <nil>
STEP: delete the pod
Aug 17 06:03:58.186: INFO: Waiting for pod pod-484a86a4-8dc8-4a6b-9479-83c278342ad4 to disappear
Aug 17 06:03:58.213: INFO: Pod pod-484a86a4-8dc8-4a6b-9479-83c278342ad4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:03:58.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3525" for this suite.
Aug 17 06:04:08.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:04:10.944: INFO: namespace emptydir-3525 deletion completed in 12.63763723s

• [SLOW TEST:15.201 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:04:10.945: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 06:04:11.254: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-37467ce3-4531-4585-af5b-549fa158258f
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-37467ce3-4531-4585-af5b-549fa158258f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:04:15.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5810" for this suite.
Aug 17 06:04:31.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:04:34.523: INFO: namespace projected-5810 deletion completed in 18.854966948s

• [SLOW TEST:23.578 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:04:34.523: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-c541fbca-29b6-4d50-b46b-0a5d00841689
STEP: Creating a pod to test consume configMaps
Aug 17 06:04:34.841: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f3c0117d-3459-421f-9f2a-81437973e543" in namespace "projected-8003" to be "success or failure"
Aug 17 06:04:34.862: INFO: Pod "pod-projected-configmaps-f3c0117d-3459-421f-9f2a-81437973e543": Phase="Pending", Reason="", readiness=false. Elapsed: 20.983859ms
Aug 17 06:04:36.882: INFO: Pod "pod-projected-configmaps-f3c0117d-3459-421f-9f2a-81437973e543": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041111729s
Aug 17 06:04:38.905: INFO: Pod "pod-projected-configmaps-f3c0117d-3459-421f-9f2a-81437973e543": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06383763s
STEP: Saw pod success
Aug 17 06:04:38.905: INFO: Pod "pod-projected-configmaps-f3c0117d-3459-421f-9f2a-81437973e543" satisfied condition "success or failure"
Aug 17 06:04:38.923: INFO: Trying to get logs from node 10.241.148.42 pod pod-projected-configmaps-f3c0117d-3459-421f-9f2a-81437973e543 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 17 06:04:39.026: INFO: Waiting for pod pod-projected-configmaps-f3c0117d-3459-421f-9f2a-81437973e543 to disappear
Aug 17 06:04:39.044: INFO: Pod pod-projected-configmaps-f3c0117d-3459-421f-9f2a-81437973e543 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:04:39.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8003" for this suite.
Aug 17 06:04:49.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:04:52.027: INFO: namespace projected-8003 deletion completed in 12.926645062s

• [SLOW TEST:17.504 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:04:52.027: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Aug 17 06:05:22.440: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0817 06:05:22.440366      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:05:22.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8586" for this suite.
Aug 17 06:05:32.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:05:35.125: INFO: namespace gc-8586 deletion completed in 12.648928835s

• [SLOW TEST:43.098 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:05:35.130: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-d3b164e7-7161-4015-bacb-b71dddffee69 in namespace container-probe-4525
Aug 17 06:05:37.544: INFO: Started pod liveness-d3b164e7-7161-4015-bacb-b71dddffee69 in namespace container-probe-4525
STEP: checking the pod's current state and verifying that restartCount is present
Aug 17 06:05:37.565: INFO: Initial restart count of pod liveness-d3b164e7-7161-4015-bacb-b71dddffee69 is 0
Aug 17 06:05:49.788: INFO: Restart count of pod container-probe-4525/liveness-d3b164e7-7161-4015-bacb-b71dddffee69 is now 1 (12.223289373s elapsed)
Aug 17 06:06:10.015: INFO: Restart count of pod container-probe-4525/liveness-d3b164e7-7161-4015-bacb-b71dddffee69 is now 2 (32.450069852s elapsed)
Aug 17 06:06:30.305: INFO: Restart count of pod container-probe-4525/liveness-d3b164e7-7161-4015-bacb-b71dddffee69 is now 3 (52.740536444s elapsed)
Aug 17 06:06:48.676: INFO: Restart count of pod container-probe-4525/liveness-d3b164e7-7161-4015-bacb-b71dddffee69 is now 4 (1m11.111231167s elapsed)
Aug 17 06:07:49.779: INFO: Restart count of pod container-probe-4525/liveness-d3b164e7-7161-4015-bacb-b71dddffee69 is now 5 (2m12.214289138s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:07:49.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4525" for this suite.
Aug 17 06:08:00.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:08:03.087: INFO: namespace container-probe-4525 deletion completed in 12.825044419s

• [SLOW TEST:147.958 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:08:03.088: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-4470
STEP: creating replication controller nodeport-test in namespace services-4470
I0817 06:08:03.512369      22 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-4470, replica count: 2
I0817 06:08:06.563356      22 runners.go:184] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0817 06:08:09.564043      22 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 17 06:08:09.564: INFO: Creating new exec pod
Aug 17 06:08:14.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=services-4470 execpodhkpjj -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Aug 17 06:08:15.394: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Aug 17 06:08:15.394: INFO: stdout: ""
Aug 17 06:08:15.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=services-4470 execpodhkpjj -- /bin/sh -x -c nc -zv -t -w 2 172.21.38.154 80'
Aug 17 06:08:15.834: INFO: stderr: "+ nc -zv -t -w 2 172.21.38.154 80\nConnection to 172.21.38.154 80 port [tcp/http] succeeded!\n"
Aug 17 06:08:15.834: INFO: stdout: ""
Aug 17 06:08:15.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=services-4470 execpodhkpjj -- /bin/sh -x -c nc -zv -t -w 2 10.241.148.31 30868'
Aug 17 06:08:16.264: INFO: stderr: "+ nc -zv -t -w 2 10.241.148.31 30868\nConnection to 10.241.148.31 30868 port [tcp/30868] succeeded!\n"
Aug 17 06:08:16.264: INFO: stdout: ""
Aug 17 06:08:16.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=services-4470 execpodhkpjj -- /bin/sh -x -c nc -zv -t -w 2 10.241.148.42 30868'
Aug 17 06:08:16.981: INFO: stderr: "+ nc -zv -t -w 2 10.241.148.42 30868\nConnection to 10.241.148.42 30868 port [tcp/30868] succeeded!\n"
Aug 17 06:08:16.981: INFO: stdout: ""
Aug 17 06:08:16.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=services-4470 execpodhkpjj -- /bin/sh -x -c nc -zv -t -w 2 169.59.216.115 30868'
Aug 17 06:08:17.380: INFO: stderr: "+ nc -zv -t -w 2 169.59.216.115 30868\nConnection to 169.59.216.115 30868 port [tcp/30868] succeeded!\n"
Aug 17 06:08:17.380: INFO: stdout: ""
Aug 17 06:08:17.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=services-4470 execpodhkpjj -- /bin/sh -x -c nc -zv -t -w 2 169.59.216.123 30868'
Aug 17 06:08:18.093: INFO: stderr: "+ nc -zv -t -w 2 169.59.216.123 30868\nConnection to 169.59.216.123 30868 port [tcp/30868] succeeded!\n"
Aug 17 06:08:18.093: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:08:18.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4470" for this suite.
Aug 17 06:08:28.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:08:30.994: INFO: namespace services-4470 deletion completed in 12.655758525s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:27.906 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:08:30.994: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-470f5cf7-644b-4f84-8975-59e7c0005f6c
STEP: Creating a pod to test consume secrets
Aug 17 06:08:31.321: INFO: Waiting up to 5m0s for pod "pod-secrets-a19cb2a0-565b-48a4-b1f5-83e46131d9bc" in namespace "secrets-8437" to be "success or failure"
Aug 17 06:08:31.349: INFO: Pod "pod-secrets-a19cb2a0-565b-48a4-b1f5-83e46131d9bc": Phase="Pending", Reason="", readiness=false. Elapsed: 27.989398ms
Aug 17 06:08:33.413: INFO: Pod "pod-secrets-a19cb2a0-565b-48a4-b1f5-83e46131d9bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.092452941s
Aug 17 06:08:35.455: INFO: Pod "pod-secrets-a19cb2a0-565b-48a4-b1f5-83e46131d9bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.133746684s
STEP: Saw pod success
Aug 17 06:08:35.455: INFO: Pod "pod-secrets-a19cb2a0-565b-48a4-b1f5-83e46131d9bc" satisfied condition "success or failure"
Aug 17 06:08:35.510: INFO: Trying to get logs from node 10.241.148.42 pod pod-secrets-a19cb2a0-565b-48a4-b1f5-83e46131d9bc container secret-volume-test: <nil>
STEP: delete the pod
Aug 17 06:08:35.737: INFO: Waiting for pod pod-secrets-a19cb2a0-565b-48a4-b1f5-83e46131d9bc to disappear
Aug 17 06:08:35.760: INFO: Pod pod-secrets-a19cb2a0-565b-48a4-b1f5-83e46131d9bc no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:08:35.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8437" for this suite.
Aug 17 06:08:45.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:08:51.298: INFO: namespace secrets-8437 deletion completed in 15.412743813s

• [SLOW TEST:20.305 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:08:51.301: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-4699/secret-test-128a76e0-064c-43f1-8111-877e94a22411
STEP: Creating a pod to test consume secrets
Aug 17 06:08:51.670: INFO: Waiting up to 5m0s for pod "pod-configmaps-e6153047-d78b-4888-8dae-f7234fc5a570" in namespace "secrets-4699" to be "success or failure"
Aug 17 06:08:51.729: INFO: Pod "pod-configmaps-e6153047-d78b-4888-8dae-f7234fc5a570": Phase="Pending", Reason="", readiness=false. Elapsed: 59.444138ms
Aug 17 06:08:53.754: INFO: Pod "pod-configmaps-e6153047-d78b-4888-8dae-f7234fc5a570": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.084142777s
STEP: Saw pod success
Aug 17 06:08:53.754: INFO: Pod "pod-configmaps-e6153047-d78b-4888-8dae-f7234fc5a570" satisfied condition "success or failure"
Aug 17 06:08:53.783: INFO: Trying to get logs from node 10.241.148.42 pod pod-configmaps-e6153047-d78b-4888-8dae-f7234fc5a570 container env-test: <nil>
STEP: delete the pod
Aug 17 06:08:54.033: INFO: Waiting for pod pod-configmaps-e6153047-d78b-4888-8dae-f7234fc5a570 to disappear
Aug 17 06:08:54.071: INFO: Pod pod-configmaps-e6153047-d78b-4888-8dae-f7234fc5a570 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:08:54.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4699" for this suite.
Aug 17 06:09:04.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:09:06.838: INFO: namespace secrets-4699 deletion completed in 12.65849713s

• [SLOW TEST:15.537 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:09:06.838: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2141.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2141.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2141.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2141.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2141.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2141.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2141.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2141.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2141.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2141.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2141.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2141.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2141.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 99.197.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.197.99_udp@PTR;check="$$(dig +tcp +noall +answer +search 99.197.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.197.99_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2141.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2141.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2141.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2141.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2141.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2141.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2141.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2141.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2141.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2141.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2141.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2141.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2141.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 99.197.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.197.99_udp@PTR;check="$$(dig +tcp +noall +answer +search 99.197.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.197.99_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 17 06:09:11.660: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2141.svc.cluster.local from pod dns-2141/dns-test-71413199-b52f-4d2d-9a21-25675e4da1c0: the server could not find the requested resource (get pods dns-test-71413199-b52f-4d2d-9a21-25675e4da1c0)
Aug 17 06:09:11.713: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2141.svc.cluster.local from pod dns-2141/dns-test-71413199-b52f-4d2d-9a21-25675e4da1c0: the server could not find the requested resource (get pods dns-test-71413199-b52f-4d2d-9a21-25675e4da1c0)
Aug 17 06:09:11.966: INFO: Unable to read jessie_udp@dns-test-service.dns-2141.svc.cluster.local from pod dns-2141/dns-test-71413199-b52f-4d2d-9a21-25675e4da1c0: the server could not find the requested resource (get pods dns-test-71413199-b52f-4d2d-9a21-25675e4da1c0)
Aug 17 06:09:12.001: INFO: Unable to read jessie_tcp@dns-test-service.dns-2141.svc.cluster.local from pod dns-2141/dns-test-71413199-b52f-4d2d-9a21-25675e4da1c0: the server could not find the requested resource (get pods dns-test-71413199-b52f-4d2d-9a21-25675e4da1c0)
Aug 17 06:09:12.038: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2141.svc.cluster.local from pod dns-2141/dns-test-71413199-b52f-4d2d-9a21-25675e4da1c0: the server could not find the requested resource (get pods dns-test-71413199-b52f-4d2d-9a21-25675e4da1c0)
Aug 17 06:09:12.111: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2141.svc.cluster.local from pod dns-2141/dns-test-71413199-b52f-4d2d-9a21-25675e4da1c0: the server could not find the requested resource (get pods dns-test-71413199-b52f-4d2d-9a21-25675e4da1c0)
Aug 17 06:09:12.431: INFO: Lookups using dns-2141/dns-test-71413199-b52f-4d2d-9a21-25675e4da1c0 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-2141.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2141.svc.cluster.local jessie_udp@dns-test-service.dns-2141.svc.cluster.local jessie_tcp@dns-test-service.dns-2141.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2141.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2141.svc.cluster.local]

Aug 17 06:09:19.469: INFO: DNS probes using dns-2141/dns-test-71413199-b52f-4d2d-9a21-25675e4da1c0 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:09:20.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2141" for this suite.
Aug 17 06:09:30.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:09:33.106: INFO: namespace dns-2141 deletion completed in 12.589432311s

• [SLOW TEST:26.268 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:09:33.106: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Aug 17 06:09:33.525: INFO: Waiting up to 5m0s for pod "downward-api-9f181015-83d1-4488-8228-8167e0fe0d3c" in namespace "downward-api-1778" to be "success or failure"
Aug 17 06:09:33.621: INFO: Pod "downward-api-9f181015-83d1-4488-8228-8167e0fe0d3c": Phase="Pending", Reason="", readiness=false. Elapsed: 95.65935ms
Aug 17 06:09:35.640: INFO: Pod "downward-api-9f181015-83d1-4488-8228-8167e0fe0d3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.114411464s
Aug 17 06:09:37.662: INFO: Pod "downward-api-9f181015-83d1-4488-8228-8167e0fe0d3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.137162595s
STEP: Saw pod success
Aug 17 06:09:37.662: INFO: Pod "downward-api-9f181015-83d1-4488-8228-8167e0fe0d3c" satisfied condition "success or failure"
Aug 17 06:09:37.683: INFO: Trying to get logs from node 10.241.148.42 pod downward-api-9f181015-83d1-4488-8228-8167e0fe0d3c container dapi-container: <nil>
STEP: delete the pod
Aug 17 06:09:37.779: INFO: Waiting for pod downward-api-9f181015-83d1-4488-8228-8167e0fe0d3c to disappear
Aug 17 06:09:37.796: INFO: Pod downward-api-9f181015-83d1-4488-8228-8167e0fe0d3c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:09:37.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1778" for this suite.
Aug 17 06:09:47.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:09:51.466: INFO: namespace downward-api-1778 deletion completed in 13.631974787s

• [SLOW TEST:18.359 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:09:51.467: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-eabc7637-39a2-4f37-bd30-d97c4a9d3bee in namespace container-probe-4193
Aug 17 06:09:55.813: INFO: Started pod liveness-eabc7637-39a2-4f37-bd30-d97c4a9d3bee in namespace container-probe-4193
STEP: checking the pod's current state and verifying that restartCount is present
Aug 17 06:09:55.832: INFO: Initial restart count of pod liveness-eabc7637-39a2-4f37-bd30-d97c4a9d3bee is 0
Aug 17 06:10:10.031: INFO: Restart count of pod container-probe-4193/liveness-eabc7637-39a2-4f37-bd30-d97c4a9d3bee is now 1 (14.198424582s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:10:10.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4193" for this suite.
Aug 17 06:10:20.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:10:22.707: INFO: namespace container-probe-4193 deletion completed in 12.571295265s

• [SLOW TEST:31.240 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:10:22.707: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:10:23.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6545" for this suite.
Aug 17 06:10:33.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:10:35.774: INFO: namespace kubelet-test-6545 deletion completed in 12.604890386s

• [SLOW TEST:13.067 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:10:35.776: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 17 06:10:36.491: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 17 06:10:38.540: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241436, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241436, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241436, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241436, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 17 06:10:41.619: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 06:10:41.640: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9557-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:10:43.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3360" for this suite.
Aug 17 06:10:53.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:10:55.987: INFO: namespace webhook-3360 deletion completed in 12.673667351s
STEP: Destroying namespace "webhook-3360-markers" for this suite.
Aug 17 06:11:04.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:11:06.463: INFO: namespace webhook-3360-markers deletion completed in 10.476348649s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:30.763 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:11:06.541: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Aug 17 06:11:06.854: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3243 /api/v1/namespaces/watch-3243/configmaps/e2e-watch-test-configmap-a 08a91fcc-0e94-44ce-9e27-abaa4b7cc938 86038 0 2020-08-17 06:11:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 17 06:11:06.854: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3243 /api/v1/namespaces/watch-3243/configmaps/e2e-watch-test-configmap-a 08a91fcc-0e94-44ce-9e27-abaa4b7cc938 86038 0 2020-08-17 06:11:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Aug 17 06:11:17.116: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3243 /api/v1/namespaces/watch-3243/configmaps/e2e-watch-test-configmap-a 08a91fcc-0e94-44ce-9e27-abaa4b7cc938 86079 0 2020-08-17 06:11:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug 17 06:11:17.116: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3243 /api/v1/namespaces/watch-3243/configmaps/e2e-watch-test-configmap-a 08a91fcc-0e94-44ce-9e27-abaa4b7cc938 86079 0 2020-08-17 06:11:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Aug 17 06:11:27.157: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3243 /api/v1/namespaces/watch-3243/configmaps/e2e-watch-test-configmap-a 08a91fcc-0e94-44ce-9e27-abaa4b7cc938 86116 0 2020-08-17 06:11:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 17 06:11:27.158: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3243 /api/v1/namespaces/watch-3243/configmaps/e2e-watch-test-configmap-a 08a91fcc-0e94-44ce-9e27-abaa4b7cc938 86116 0 2020-08-17 06:11:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Aug 17 06:11:37.200: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3243 /api/v1/namespaces/watch-3243/configmaps/e2e-watch-test-configmap-a 08a91fcc-0e94-44ce-9e27-abaa4b7cc938 86153 0 2020-08-17 06:11:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 17 06:11:37.200: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3243 /api/v1/namespaces/watch-3243/configmaps/e2e-watch-test-configmap-a 08a91fcc-0e94-44ce-9e27-abaa4b7cc938 86153 0 2020-08-17 06:11:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Aug 17 06:11:47.350: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3243 /api/v1/namespaces/watch-3243/configmaps/e2e-watch-test-configmap-b cea7a5de-e2b0-4694-b1a5-f93e0e671584 86191 0 2020-08-17 06:11:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 17 06:11:47.350: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3243 /api/v1/namespaces/watch-3243/configmaps/e2e-watch-test-configmap-b cea7a5de-e2b0-4694-b1a5-f93e0e671584 86191 0 2020-08-17 06:11:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Aug 17 06:11:57.416: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3243 /api/v1/namespaces/watch-3243/configmaps/e2e-watch-test-configmap-b cea7a5de-e2b0-4694-b1a5-f93e0e671584 86231 0 2020-08-17 06:11:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 17 06:11:57.422: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3243 /api/v1/namespaces/watch-3243/configmaps/e2e-watch-test-configmap-b cea7a5de-e2b0-4694-b1a5-f93e0e671584 86231 0 2020-08-17 06:11:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:12:07.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3243" for this suite.
Aug 17 06:12:17.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:12:21.162: INFO: namespace watch-3243 deletion completed in 13.674119288s

• [SLOW TEST:74.621 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:12:21.163: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Aug 17 06:12:21.489: INFO: Waiting up to 5m0s for pod "var-expansion-59e86ca2-44a4-458d-87f0-9b71d49eabf7" in namespace "var-expansion-9826" to be "success or failure"
Aug 17 06:12:21.513: INFO: Pod "var-expansion-59e86ca2-44a4-458d-87f0-9b71d49eabf7": Phase="Pending", Reason="", readiness=false. Elapsed: 23.483184ms
Aug 17 06:12:23.540: INFO: Pod "var-expansion-59e86ca2-44a4-458d-87f0-9b71d49eabf7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.050237746s
STEP: Saw pod success
Aug 17 06:12:23.540: INFO: Pod "var-expansion-59e86ca2-44a4-458d-87f0-9b71d49eabf7" satisfied condition "success or failure"
Aug 17 06:12:23.557: INFO: Trying to get logs from node 10.241.148.42 pod var-expansion-59e86ca2-44a4-458d-87f0-9b71d49eabf7 container dapi-container: <nil>
STEP: delete the pod
Aug 17 06:12:23.712: INFO: Waiting for pod var-expansion-59e86ca2-44a4-458d-87f0-9b71d49eabf7 to disappear
Aug 17 06:12:23.729: INFO: Pod var-expansion-59e86ca2-44a4-458d-87f0-9b71d49eabf7 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:12:23.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9826" for this suite.
Aug 17 06:12:31.838: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:12:35.051: INFO: namespace var-expansion-9826 deletion completed in 11.275650669s

• [SLOW TEST:13.888 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:12:35.052: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 17 06:12:35.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-9376'
Aug 17 06:12:35.529: INFO: stderr: ""
Aug 17 06:12:35.529: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Aug 17 06:12:35.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 delete pods e2e-test-httpd-pod --namespace=kubectl-9376'
Aug 17 06:12:42.394: INFO: stderr: ""
Aug 17 06:12:42.394: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:12:42.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9376" for this suite.
Aug 17 06:12:52.531: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:12:55.007: INFO: namespace kubectl-9376 deletion completed in 12.53858568s

• [SLOW TEST:19.955 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:12:55.007: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 17 06:12:56.089: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 17 06:12:58.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241576, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241576, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241576, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241576, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 17 06:13:01.203: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Aug 17 06:13:05.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 attach --namespace=webhook-9301 to-be-attached-pod -i -c=container1'
Aug 17 06:13:06.053: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:13:06.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9301" for this suite.
Aug 17 06:13:42.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:13:44.651: INFO: namespace webhook-9301 deletion completed in 38.491676102s
STEP: Destroying namespace "webhook-9301-markers" for this suite.
Aug 17 06:13:54.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:13:57.175: INFO: namespace webhook-9301-markers deletion completed in 12.523839981s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:62.242 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:13:57.250: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 17 06:13:57.536: INFO: Waiting up to 5m0s for pod "pod-8a54188c-56e6-474a-bdbc-717aafe9d44d" in namespace "emptydir-2426" to be "success or failure"
Aug 17 06:13:57.560: INFO: Pod "pod-8a54188c-56e6-474a-bdbc-717aafe9d44d": Phase="Pending", Reason="", readiness=false. Elapsed: 24.025914ms
Aug 17 06:13:59.579: INFO: Pod "pod-8a54188c-56e6-474a-bdbc-717aafe9d44d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.042828511s
STEP: Saw pod success
Aug 17 06:13:59.579: INFO: Pod "pod-8a54188c-56e6-474a-bdbc-717aafe9d44d" satisfied condition "success or failure"
Aug 17 06:13:59.599: INFO: Trying to get logs from node 10.241.148.42 pod pod-8a54188c-56e6-474a-bdbc-717aafe9d44d container test-container: <nil>
STEP: delete the pod
Aug 17 06:13:59.805: INFO: Waiting for pod pod-8a54188c-56e6-474a-bdbc-717aafe9d44d to disappear
Aug 17 06:13:59.822: INFO: Pod pod-8a54188c-56e6-474a-bdbc-717aafe9d44d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:13:59.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2426" for this suite.
Aug 17 06:14:09.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:14:12.385: INFO: namespace emptydir-2426 deletion completed in 12.494627372s

• [SLOW TEST:15.136 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:14:12.388: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:14:14.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9249" for this suite.
Aug 17 06:14:26.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:14:29.354: INFO: namespace containers-9249 deletion completed in 14.502637868s

• [SLOW TEST:16.966 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:14:29.357: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 06:14:29.638: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Aug 17 06:14:38.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 --namespace=crd-publish-openapi-3973 create -f -'
Aug 17 06:14:39.253: INFO: stderr: ""
Aug 17 06:14:39.253: INFO: stdout: "e2e-test-crd-publish-openapi-6849-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 17 06:14:39.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 --namespace=crd-publish-openapi-3973 delete e2e-test-crd-publish-openapi-6849-crds test-foo'
Aug 17 06:14:39.482: INFO: stderr: ""
Aug 17 06:14:39.482: INFO: stdout: "e2e-test-crd-publish-openapi-6849-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Aug 17 06:14:39.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 --namespace=crd-publish-openapi-3973 apply -f -'
Aug 17 06:14:40.095: INFO: stderr: ""
Aug 17 06:14:40.095: INFO: stdout: "e2e-test-crd-publish-openapi-6849-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 17 06:14:40.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 --namespace=crd-publish-openapi-3973 delete e2e-test-crd-publish-openapi-6849-crds test-foo'
Aug 17 06:14:40.288: INFO: stderr: ""
Aug 17 06:14:40.288: INFO: stdout: "e2e-test-crd-publish-openapi-6849-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Aug 17 06:14:40.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 --namespace=crd-publish-openapi-3973 create -f -'
Aug 17 06:14:40.670: INFO: rc: 1
Aug 17 06:14:40.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 --namespace=crd-publish-openapi-3973 apply -f -'
Aug 17 06:14:41.238: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Aug 17 06:14:41.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 --namespace=crd-publish-openapi-3973 create -f -'
Aug 17 06:14:41.847: INFO: rc: 1
Aug 17 06:14:41.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 --namespace=crd-publish-openapi-3973 apply -f -'
Aug 17 06:14:42.370: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Aug 17 06:14:42.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 explain e2e-test-crd-publish-openapi-6849-crds'
Aug 17 06:14:42.708: INFO: stderr: ""
Aug 17 06:14:42.708: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6849-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Aug 17 06:14:42.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 explain e2e-test-crd-publish-openapi-6849-crds.metadata'
Aug 17 06:14:43.323: INFO: stderr: ""
Aug 17 06:14:43.323: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6849-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Aug 17 06:14:43.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 explain e2e-test-crd-publish-openapi-6849-crds.spec'
Aug 17 06:14:43.909: INFO: stderr: ""
Aug 17 06:14:43.909: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6849-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Aug 17 06:14:43.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 explain e2e-test-crd-publish-openapi-6849-crds.spec.bars'
Aug 17 06:14:44.260: INFO: stderr: ""
Aug 17 06:14:44.260: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6849-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Aug 17 06:14:44.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 explain e2e-test-crd-publish-openapi-6849-crds.spec.bars2'
Aug 17 06:14:44.860: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:14:53.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3973" for this suite.
Aug 17 06:15:01.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:15:04.479: INFO: namespace crd-publish-openapi-3973 deletion completed in 10.711265763s

• [SLOW TEST:35.123 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:15:04.483: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 17 06:15:04.808: INFO: Waiting up to 5m0s for pod "downwardapi-volume-300b4c52-6e2f-4197-b335-1884137a3869" in namespace "downward-api-7141" to be "success or failure"
Aug 17 06:15:04.842: INFO: Pod "downwardapi-volume-300b4c52-6e2f-4197-b335-1884137a3869": Phase="Pending", Reason="", readiness=false. Elapsed: 33.741296ms
Aug 17 06:15:06.860: INFO: Pod "downwardapi-volume-300b4c52-6e2f-4197-b335-1884137a3869": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052334982s
Aug 17 06:15:08.882: INFO: Pod "downwardapi-volume-300b4c52-6e2f-4197-b335-1884137a3869": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.073535003s
STEP: Saw pod success
Aug 17 06:15:08.882: INFO: Pod "downwardapi-volume-300b4c52-6e2f-4197-b335-1884137a3869" satisfied condition "success or failure"
Aug 17 06:15:08.902: INFO: Trying to get logs from node 10.241.148.42 pod downwardapi-volume-300b4c52-6e2f-4197-b335-1884137a3869 container client-container: <nil>
STEP: delete the pod
Aug 17 06:15:09.004: INFO: Waiting for pod downwardapi-volume-300b4c52-6e2f-4197-b335-1884137a3869 to disappear
Aug 17 06:15:09.025: INFO: Pod downwardapi-volume-300b4c52-6e2f-4197-b335-1884137a3869 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:15:09.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7141" for this suite.
Aug 17 06:15:19.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:15:22.058: INFO: namespace downward-api-7141 deletion completed in 12.990728423s

• [SLOW TEST:17.576 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:15:22.059: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-c01d339f-e878-4090-bf54-56ab887236f8
STEP: Creating a pod to test consume secrets
Aug 17 06:15:22.349: INFO: Waiting up to 5m0s for pod "pod-secrets-4b461755-c409-4881-8cb7-dc658c699a4f" in namespace "secrets-8713" to be "success or failure"
Aug 17 06:15:22.373: INFO: Pod "pod-secrets-4b461755-c409-4881-8cb7-dc658c699a4f": Phase="Pending", Reason="", readiness=false. Elapsed: 24.082401ms
Aug 17 06:15:24.391: INFO: Pod "pod-secrets-4b461755-c409-4881-8cb7-dc658c699a4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041894587s
Aug 17 06:15:26.426: INFO: Pod "pod-secrets-4b461755-c409-4881-8cb7-dc658c699a4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076419353s
STEP: Saw pod success
Aug 17 06:15:26.426: INFO: Pod "pod-secrets-4b461755-c409-4881-8cb7-dc658c699a4f" satisfied condition "success or failure"
Aug 17 06:15:26.447: INFO: Trying to get logs from node 10.241.148.42 pod pod-secrets-4b461755-c409-4881-8cb7-dc658c699a4f container secret-volume-test: <nil>
STEP: delete the pod
Aug 17 06:15:26.541: INFO: Waiting for pod pod-secrets-4b461755-c409-4881-8cb7-dc658c699a4f to disappear
Aug 17 06:15:26.559: INFO: Pod pod-secrets-4b461755-c409-4881-8cb7-dc658c699a4f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:15:26.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8713" for this suite.
Aug 17 06:15:36.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:15:39.235: INFO: namespace secrets-8713 deletion completed in 12.605039996s

• [SLOW TEST:17.177 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:15:39.236: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Aug 17 06:15:40.468: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Aug 17 06:15:42.519: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241740, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241740, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241740, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241740, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 17 06:15:45.582: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 06:15:45.603: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:15:46.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9354" for this suite.
Aug 17 06:15:57.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:15:59.726: INFO: namespace crd-webhook-9354 deletion completed in 12.764051467s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:20.560 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:15:59.797: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 06:16:00.119: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 17 06:16:05.153: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 17 06:16:05.153: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 17 06:16:07.171: INFO: Creating deployment "test-rollover-deployment"
Aug 17 06:16:07.209: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 17 06:16:09.236: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 17 06:16:09.265: INFO: Ensure that both replica sets have 1 created replica
Aug 17 06:16:09.298: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 17 06:16:09.329: INFO: Updating deployment test-rollover-deployment
Aug 17 06:16:09.329: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 17 06:16:11.355: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 17 06:16:11.392: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 17 06:16:11.426: INFO: all replica sets need to contain the pod-template-hash label
Aug 17 06:16:11.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241767, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241767, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241769, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241767, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 06:16:13.462: INFO: all replica sets need to contain the pod-template-hash label
Aug 17 06:16:13.462: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241767, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241767, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241771, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241767, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 06:16:15.475: INFO: all replica sets need to contain the pod-template-hash label
Aug 17 06:16:15.476: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241767, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241767, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241771, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241767, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 06:16:17.517: INFO: all replica sets need to contain the pod-template-hash label
Aug 17 06:16:17.518: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241767, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241767, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241771, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241767, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 06:16:19.517: INFO: all replica sets need to contain the pod-template-hash label
Aug 17 06:16:19.517: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241767, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241767, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241771, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241767, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 06:16:21.457: INFO: all replica sets need to contain the pod-template-hash label
Aug 17 06:16:21.458: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241767, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241767, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241771, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733241767, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 06:16:23.463: INFO: 
Aug 17 06:16:23.463: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Aug 17 06:16:23.518: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9066 /apis/apps/v1/namespaces/deployment-9066/deployments/test-rollover-deployment 37646e4e-6e73-4c97-a933-0b8cf34afb9c 88227 2 2020-08-17 06:16:07 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002ff14f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-08-17 06:16:07 +0000 UTC,LastTransitionTime:2020-08-17 06:16:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2020-08-17 06:16:22 +0000 UTC,LastTransitionTime:2020-08-17 06:16:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 17 06:16:23.564: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-9066 /apis/apps/v1/namespaces/deployment-9066/replicasets/test-rollover-deployment-7d7dc6548c c655cf43-1f72-416c-8d8c-ad3219d85549 88216 2 2020-08-17 06:16:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 37646e4e-6e73-4c97-a933-0b8cf34afb9c 0xc00816fd27 0xc00816fd28}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00816fd98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 17 06:16:23.564: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 17 06:16:23.564: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9066 /apis/apps/v1/namespaces/deployment-9066/replicasets/test-rollover-controller 472c1fac-bc02-4981-a132-b462ca724051 88225 2 2020-08-17 06:16:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 37646e4e-6e73-4c97-a933-0b8cf34afb9c 0xc00816fb07 0xc00816fb08}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00816fc38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 17 06:16:23.564: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-9066 /apis/apps/v1/namespaces/deployment-9066/replicasets/test-rollover-deployment-f6c94f66c da11ac9d-85aa-43a0-a929-a1ef832268af 88150 2 2020-08-17 06:16:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 37646e4e-6e73-4c97-a933-0b8cf34afb9c 0xc00816fef0 0xc00816fef1}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00816ffc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 17 06:16:23.592: INFO: Pod "test-rollover-deployment-7d7dc6548c-xsn8d" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-xsn8d test-rollover-deployment-7d7dc6548c- deployment-9066 /api/v1/namespaces/deployment-9066/pods/test-rollover-deployment-7d7dc6548c-xsn8d 120b80d8-412f-4ea4-ba5d-dc311a66949e 88178 0 2020-08-17 06:16:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[cni.projectcalico.org/podIP:172.30.218.83/32 cni.projectcalico.org/podIPs:172.30.218.83/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.218.83"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c c655cf43-1f72-416c-8d8c-ad3219d85549 0xc00aaaa9d7 0xc00aaaa9d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ffg9w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ffg9w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ffg9w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.148.42,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lwthg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 06:16:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 06:16:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 06:16:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-17 06:16:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.148.42,PodIP:172.30.218.83,StartTime:2020-08-17 06:16:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-17 06:16:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://4e581acfdac3e5b84838a647be2f1f9c7983ae899c87f70483a8e826e46c4830,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.218.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:16:23.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9066" for this suite.
Aug 17 06:16:33.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:16:36.173: INFO: namespace deployment-9066 deletion completed in 12.516712481s

• [SLOW TEST:36.376 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:16:36.173: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 06:16:36.369: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Aug 17 06:16:37.725: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:16:37.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7247" for this suite.
Aug 17 06:16:47.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:16:51.921: INFO: namespace replication-controller-7247 deletion completed in 14.06171238s

• [SLOW TEST:15.748 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:16:51.921: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-9393
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9393 to expose endpoints map[]
Aug 17 06:16:52.223: INFO: successfully validated that service multi-endpoint-test in namespace services-9393 exposes endpoints map[] (20.597243ms elapsed)
STEP: Creating pod pod1 in namespace services-9393
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9393 to expose endpoints map[pod1:[100]]
Aug 17 06:16:54.403: INFO: successfully validated that service multi-endpoint-test in namespace services-9393 exposes endpoints map[pod1:[100]] (2.115649402s elapsed)
STEP: Creating pod pod2 in namespace services-9393
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9393 to expose endpoints map[pod1:[100] pod2:[101]]
Aug 17 06:16:58.807: INFO: successfully validated that service multi-endpoint-test in namespace services-9393 exposes endpoints map[pod1:[100] pod2:[101]] (4.335865636s elapsed)
STEP: Deleting pod pod1 in namespace services-9393
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9393 to expose endpoints map[pod2:[101]]
Aug 17 06:16:58.896: INFO: successfully validated that service multi-endpoint-test in namespace services-9393 exposes endpoints map[pod2:[101]] (49.994888ms elapsed)
STEP: Deleting pod pod2 in namespace services-9393
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9393 to expose endpoints map[]
Aug 17 06:16:59.998: INFO: successfully validated that service multi-endpoint-test in namespace services-9393 exposes endpoints map[] (1.065774532s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:17:00.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9393" for this suite.
Aug 17 06:17:18.224: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:17:21.650: INFO: namespace services-9393 deletion completed in 21.521621016s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:29.729 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:17:21.652: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-8c201109-cadf-493f-b0fc-2ba730c5134f
STEP: Creating a pod to test consume configMaps
Aug 17 06:17:21.972: INFO: Waiting up to 5m0s for pod "pod-configmaps-ee88d1c0-4797-4df1-9bb7-fefc1d17c995" in namespace "configmap-2310" to be "success or failure"
Aug 17 06:17:21.990: INFO: Pod "pod-configmaps-ee88d1c0-4797-4df1-9bb7-fefc1d17c995": Phase="Pending", Reason="", readiness=false. Elapsed: 18.108789ms
Aug 17 06:17:24.010: INFO: Pod "pod-configmaps-ee88d1c0-4797-4df1-9bb7-fefc1d17c995": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038129828s
Aug 17 06:17:26.029: INFO: Pod "pod-configmaps-ee88d1c0-4797-4df1-9bb7-fefc1d17c995": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056719457s
STEP: Saw pod success
Aug 17 06:17:26.029: INFO: Pod "pod-configmaps-ee88d1c0-4797-4df1-9bb7-fefc1d17c995" satisfied condition "success or failure"
Aug 17 06:17:26.048: INFO: Trying to get logs from node 10.241.148.42 pod pod-configmaps-ee88d1c0-4797-4df1-9bb7-fefc1d17c995 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 17 06:17:26.197: INFO: Waiting for pod pod-configmaps-ee88d1c0-4797-4df1-9bb7-fefc1d17c995 to disappear
Aug 17 06:17:26.230: INFO: Pod pod-configmaps-ee88d1c0-4797-4df1-9bb7-fefc1d17c995 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:17:26.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2310" for this suite.
Aug 17 06:17:34.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:17:36.851: INFO: namespace configmap-2310 deletion completed in 10.580385591s

• [SLOW TEST:15.199 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:17:36.851: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Aug 17 06:17:37.026: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:18:24.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2911" for this suite.
Aug 17 06:18:34.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:18:37.130: INFO: namespace crd-publish-openapi-2911 deletion completed in 13.026140574s

• [SLOW TEST:60.279 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:18:37.131: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Aug 17 06:18:40.103: INFO: Successfully updated pod "labelsupdatef235e83e-4c8e-49cb-9a97-6cd65badf3a5"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:18:42.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7150" for this suite.
Aug 17 06:19:18.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:19:25.365: INFO: namespace projected-7150 deletion completed in 43.119235157s

• [SLOW TEST:48.234 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:19:25.365: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Aug 17 06:19:25.937: INFO: Waiting up to 5m0s for pod "downward-api-1ab88228-b9e9-48f9-b459-cc3113e3ffa3" in namespace "downward-api-6186" to be "success or failure"
Aug 17 06:19:26.018: INFO: Pod "downward-api-1ab88228-b9e9-48f9-b459-cc3113e3ffa3": Phase="Pending", Reason="", readiness=false. Elapsed: 80.615137ms
Aug 17 06:19:28.037: INFO: Pod "downward-api-1ab88228-b9e9-48f9-b459-cc3113e3ffa3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.099485848s
Aug 17 06:19:30.055: INFO: Pod "downward-api-1ab88228-b9e9-48f9-b459-cc3113e3ffa3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.117822833s
STEP: Saw pod success
Aug 17 06:19:30.055: INFO: Pod "downward-api-1ab88228-b9e9-48f9-b459-cc3113e3ffa3" satisfied condition "success or failure"
Aug 17 06:19:30.074: INFO: Trying to get logs from node 10.241.148.42 pod downward-api-1ab88228-b9e9-48f9-b459-cc3113e3ffa3 container dapi-container: <nil>
STEP: delete the pod
Aug 17 06:19:30.220: INFO: Waiting for pod downward-api-1ab88228-b9e9-48f9-b459-cc3113e3ffa3 to disappear
Aug 17 06:19:30.244: INFO: Pod downward-api-1ab88228-b9e9-48f9-b459-cc3113e3ffa3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:19:30.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6186" for this suite.
Aug 17 06:19:40.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:19:42.745: INFO: namespace downward-api-6186 deletion completed in 12.459362205s

• [SLOW TEST:17.379 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:19:42.745: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 06:19:42.957: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:19:47.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5993" for this suite.
Aug 17 06:20:36.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:20:38.797: INFO: namespace pods-5993 deletion completed in 50.65117105s

• [SLOW TEST:56.052 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:20:38.798: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Aug 17 06:20:39.176: INFO: Waiting up to 5m0s for pod "client-containers-417d0e8f-5256-4388-9ec1-b5e97c5adf8a" in namespace "containers-3949" to be "success or failure"
Aug 17 06:20:39.219: INFO: Pod "client-containers-417d0e8f-5256-4388-9ec1-b5e97c5adf8a": Phase="Pending", Reason="", readiness=false. Elapsed: 42.981797ms
Aug 17 06:20:41.238: INFO: Pod "client-containers-417d0e8f-5256-4388-9ec1-b5e97c5adf8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.062535882s
STEP: Saw pod success
Aug 17 06:20:41.238: INFO: Pod "client-containers-417d0e8f-5256-4388-9ec1-b5e97c5adf8a" satisfied condition "success or failure"
Aug 17 06:20:41.263: INFO: Trying to get logs from node 10.241.148.42 pod client-containers-417d0e8f-5256-4388-9ec1-b5e97c5adf8a container test-container: <nil>
STEP: delete the pod
Aug 17 06:20:41.360: INFO: Waiting for pod client-containers-417d0e8f-5256-4388-9ec1-b5e97c5adf8a to disappear
Aug 17 06:20:41.381: INFO: Pod client-containers-417d0e8f-5256-4388-9ec1-b5e97c5adf8a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:20:41.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3949" for this suite.
Aug 17 06:20:51.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:20:55.893: INFO: namespace containers-3949 deletion completed in 14.450394507s

• [SLOW TEST:17.094 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:20:55.893: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Aug 17 06:20:56.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 create -f - --namespace=kubectl-1485'
Aug 17 06:20:56.724: INFO: stderr: ""
Aug 17 06:20:56.724: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 17 06:20:56.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1485'
Aug 17 06:20:56.882: INFO: stderr: ""
Aug 17 06:20:56.883: INFO: stdout: "update-demo-nautilus-9hzqw update-demo-nautilus-vcdhc "
Aug 17 06:20:56.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods update-demo-nautilus-9hzqw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1485'
Aug 17 06:20:57.022: INFO: stderr: ""
Aug 17 06:20:57.022: INFO: stdout: ""
Aug 17 06:20:57.022: INFO: update-demo-nautilus-9hzqw is created but not running
Aug 17 06:21:02.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1485'
Aug 17 06:21:02.380: INFO: stderr: ""
Aug 17 06:21:02.380: INFO: stdout: "update-demo-nautilus-9hzqw update-demo-nautilus-vcdhc "
Aug 17 06:21:02.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods update-demo-nautilus-9hzqw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1485'
Aug 17 06:21:02.530: INFO: stderr: ""
Aug 17 06:21:02.530: INFO: stdout: "true"
Aug 17 06:21:02.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods update-demo-nautilus-9hzqw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1485'
Aug 17 06:21:02.675: INFO: stderr: ""
Aug 17 06:21:02.675: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 17 06:21:02.675: INFO: validating pod update-demo-nautilus-9hzqw
Aug 17 06:21:02.722: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 17 06:21:02.722: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 17 06:21:02.722: INFO: update-demo-nautilus-9hzqw is verified up and running
Aug 17 06:21:02.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods update-demo-nautilus-vcdhc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1485'
Aug 17 06:21:03.116: INFO: stderr: ""
Aug 17 06:21:03.116: INFO: stdout: "true"
Aug 17 06:21:03.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods update-demo-nautilus-vcdhc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1485'
Aug 17 06:21:03.253: INFO: stderr: ""
Aug 17 06:21:03.253: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 17 06:21:03.253: INFO: validating pod update-demo-nautilus-vcdhc
Aug 17 06:21:03.299: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 17 06:21:03.299: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 17 06:21:03.299: INFO: update-demo-nautilus-vcdhc is verified up and running
STEP: rolling-update to new replication controller
Aug 17 06:21:03.305: INFO: scanned /root for discovery docs: <nil>
Aug 17 06:21:03.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-1485'
Aug 17 06:21:26.767: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug 17 06:21:26.767: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 17 06:21:26.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1485'
Aug 17 06:21:27.372: INFO: stderr: ""
Aug 17 06:21:27.372: INFO: stdout: "update-demo-kitten-76dc2 update-demo-kitten-g4b5h "
Aug 17 06:21:27.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods update-demo-kitten-76dc2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1485'
Aug 17 06:21:27.500: INFO: stderr: ""
Aug 17 06:21:27.500: INFO: stdout: "true"
Aug 17 06:21:27.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods update-demo-kitten-76dc2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1485'
Aug 17 06:21:27.658: INFO: stderr: ""
Aug 17 06:21:27.658: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug 17 06:21:27.658: INFO: validating pod update-demo-kitten-76dc2
Aug 17 06:21:27.692: INFO: got data: {
  "image": "kitten.jpg"
}

Aug 17 06:21:27.692: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug 17 06:21:27.692: INFO: update-demo-kitten-76dc2 is verified up and running
Aug 17 06:21:27.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods update-demo-kitten-g4b5h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1485'
Aug 17 06:21:27.851: INFO: stderr: ""
Aug 17 06:21:27.851: INFO: stdout: "true"
Aug 17 06:21:27.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods update-demo-kitten-g4b5h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1485'
Aug 17 06:21:28.013: INFO: stderr: ""
Aug 17 06:21:28.013: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug 17 06:21:28.013: INFO: validating pod update-demo-kitten-g4b5h
Aug 17 06:21:28.048: INFO: got data: {
  "image": "kitten.jpg"
}

Aug 17 06:21:28.048: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug 17 06:21:28.048: INFO: update-demo-kitten-g4b5h is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:21:28.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1485" for this suite.
Aug 17 06:21:46.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:21:54.526: INFO: namespace kubectl-1485 deletion completed in 26.361472264s

• [SLOW TEST:58.632 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:21:54.527: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Aug 17 06:21:59.816: INFO: Successfully updated pod "annotationupdate8af8bbc4-8106-4c0a-a9d3-edf449a81795"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:22:01.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9546" for this suite.
Aug 17 06:22:26.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:22:28.651: INFO: namespace downward-api-9546 deletion completed in 26.656985914s

• [SLOW TEST:34.125 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:22:28.654: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 17 06:22:37.168: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 17 06:22:37.220: INFO: Pod pod-with-poststart-http-hook still exists
Aug 17 06:22:39.220: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 17 06:22:39.243: INFO: Pod pod-with-poststart-http-hook still exists
Aug 17 06:22:41.220: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 17 06:22:41.238: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:22:41.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8515" for this suite.
Aug 17 06:22:57.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:22:59.778: INFO: namespace container-lifecycle-hook-8515 deletion completed in 18.485590522s

• [SLOW TEST:31.124 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:22:59.779: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-d85e361a-059f-4918-b66c-3d37ccb33e7f
STEP: Creating a pod to test consume secrets
Aug 17 06:23:00.047: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0b23aa35-0705-4639-82d9-1a00a32a233d" in namespace "projected-6224" to be "success or failure"
Aug 17 06:23:00.069: INFO: Pod "pod-projected-secrets-0b23aa35-0705-4639-82d9-1a00a32a233d": Phase="Pending", Reason="", readiness=false. Elapsed: 22.158644ms
Aug 17 06:23:02.091: INFO: Pod "pod-projected-secrets-0b23aa35-0705-4639-82d9-1a00a32a233d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043974566s
Aug 17 06:23:04.175: INFO: Pod "pod-projected-secrets-0b23aa35-0705-4639-82d9-1a00a32a233d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.127853788s
STEP: Saw pod success
Aug 17 06:23:04.175: INFO: Pod "pod-projected-secrets-0b23aa35-0705-4639-82d9-1a00a32a233d" satisfied condition "success or failure"
Aug 17 06:23:04.240: INFO: Trying to get logs from node 10.241.148.42 pod pod-projected-secrets-0b23aa35-0705-4639-82d9-1a00a32a233d container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 17 06:23:04.367: INFO: Waiting for pod pod-projected-secrets-0b23aa35-0705-4639-82d9-1a00a32a233d to disappear
Aug 17 06:23:04.427: INFO: Pod pod-projected-secrets-0b23aa35-0705-4639-82d9-1a00a32a233d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:23:04.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6224" for this suite.
Aug 17 06:23:14.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:23:19.601: INFO: namespace projected-6224 deletion completed in 14.851112968s

• [SLOW TEST:19.822 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:23:19.601: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Aug 17 06:23:20.138: INFO: Waiting up to 5m0s for pod "pod-feb5bdb3-64b6-4513-8ddf-daacd97ea220" in namespace "emptydir-6155" to be "success or failure"
Aug 17 06:23:20.227: INFO: Pod "pod-feb5bdb3-64b6-4513-8ddf-daacd97ea220": Phase="Pending", Reason="", readiness=false. Elapsed: 89.387352ms
Aug 17 06:23:22.247: INFO: Pod "pod-feb5bdb3-64b6-4513-8ddf-daacd97ea220": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.109054443s
STEP: Saw pod success
Aug 17 06:23:22.247: INFO: Pod "pod-feb5bdb3-64b6-4513-8ddf-daacd97ea220" satisfied condition "success or failure"
Aug 17 06:23:22.266: INFO: Trying to get logs from node 10.241.148.42 pod pod-feb5bdb3-64b6-4513-8ddf-daacd97ea220 container test-container: <nil>
STEP: delete the pod
Aug 17 06:23:22.373: INFO: Waiting for pod pod-feb5bdb3-64b6-4513-8ddf-daacd97ea220 to disappear
Aug 17 06:23:22.402: INFO: Pod pod-feb5bdb3-64b6-4513-8ddf-daacd97ea220 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:23:22.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6155" for this suite.
Aug 17 06:23:30.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:23:32.972: INFO: namespace emptydir-6155 deletion completed in 10.503409273s

• [SLOW TEST:13.371 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:23:32.974: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 17 06:23:33.258: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dd82b1a4-82f7-4581-aea2-8310319ea87e" in namespace "downward-api-5596" to be "success or failure"
Aug 17 06:23:33.288: INFO: Pod "downwardapi-volume-dd82b1a4-82f7-4581-aea2-8310319ea87e": Phase="Pending", Reason="", readiness=false. Elapsed: 30.582294ms
Aug 17 06:23:35.317: INFO: Pod "downwardapi-volume-dd82b1a4-82f7-4581-aea2-8310319ea87e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.059042464s
STEP: Saw pod success
Aug 17 06:23:35.317: INFO: Pod "downwardapi-volume-dd82b1a4-82f7-4581-aea2-8310319ea87e" satisfied condition "success or failure"
Aug 17 06:23:35.337: INFO: Trying to get logs from node 10.241.148.42 pod downwardapi-volume-dd82b1a4-82f7-4581-aea2-8310319ea87e container client-container: <nil>
STEP: delete the pod
Aug 17 06:23:35.457: INFO: Waiting for pod downwardapi-volume-dd82b1a4-82f7-4581-aea2-8310319ea87e to disappear
Aug 17 06:23:35.483: INFO: Pod downwardapi-volume-dd82b1a4-82f7-4581-aea2-8310319ea87e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:23:35.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5596" for this suite.
Aug 17 06:23:45.652: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:23:51.225: INFO: namespace downward-api-5596 deletion completed in 15.666501842s

• [SLOW TEST:18.251 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:23:51.226: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-3686
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3686 to expose endpoints map[]
Aug 17 06:23:52.108: INFO: successfully validated that service endpoint-test2 in namespace services-3686 exposes endpoints map[] (126.939327ms elapsed)
STEP: Creating pod pod1 in namespace services-3686
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3686 to expose endpoints map[pod1:[80]]
Aug 17 06:23:56.114: INFO: successfully validated that service endpoint-test2 in namespace services-3686 exposes endpoints map[pod1:[80]] (3.840317539s elapsed)
STEP: Creating pod pod2 in namespace services-3686
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3686 to expose endpoints map[pod1:[80] pod2:[80]]
Aug 17 06:24:00.740: INFO: successfully validated that service endpoint-test2 in namespace services-3686 exposes endpoints map[pod1:[80] pod2:[80]] (4.545845322s elapsed)
STEP: Deleting pod pod1 in namespace services-3686
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3686 to expose endpoints map[pod2:[80]]
Aug 17 06:24:01.869: INFO: successfully validated that service endpoint-test2 in namespace services-3686 exposes endpoints map[pod2:[80]] (1.098055147s elapsed)
STEP: Deleting pod pod2 in namespace services-3686
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3686 to expose endpoints map[]
Aug 17 06:24:02.976: INFO: successfully validated that service endpoint-test2 in namespace services-3686 exposes endpoints map[] (1.065848464s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:24:03.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3686" for this suite.
Aug 17 06:24:19.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:24:25.514: INFO: namespace services-3686 deletion completed in 22.293339783s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:34.289 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:24:25.515: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 17 06:24:26.572: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Aug 17 06:24:28.619: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733242266, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733242266, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733242266, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733242266, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 17 06:24:31.700: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 06:24:31.723: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:24:33.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2833" for this suite.
Aug 17 06:24:43.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:24:46.255: INFO: namespace webhook-2833 deletion completed in 12.611537471s
STEP: Destroying namespace "webhook-2833-markers" for this suite.
Aug 17 06:24:56.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:24:59.004: INFO: namespace webhook-2833-markers deletion completed in 12.748241601s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:33.561 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:24:59.077: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Aug 17 06:24:59.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 api-versions'
Aug 17 06:24:59.474: INFO: stderr: ""
Aug 17 06:24:59.474: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1beta1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.openshift.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:24:59.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9711" for this suite.
Aug 17 06:25:09.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:25:12.000: INFO: namespace kubectl-9711 deletion completed in 12.490170999s

• [SLOW TEST:12.923 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:25:12.000: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 17 06:25:12.306: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3bb50d51-608d-4954-895a-6fd3fc2d9346" in namespace "projected-9597" to be "success or failure"
Aug 17 06:25:12.329: INFO: Pod "downwardapi-volume-3bb50d51-608d-4954-895a-6fd3fc2d9346": Phase="Pending", Reason="", readiness=false. Elapsed: 22.838722ms
Aug 17 06:25:14.350: INFO: Pod "downwardapi-volume-3bb50d51-608d-4954-895a-6fd3fc2d9346": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043876348s
Aug 17 06:25:16.391: INFO: Pod "downwardapi-volume-3bb50d51-608d-4954-895a-6fd3fc2d9346": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.085029359s
STEP: Saw pod success
Aug 17 06:25:16.391: INFO: Pod "downwardapi-volume-3bb50d51-608d-4954-895a-6fd3fc2d9346" satisfied condition "success or failure"
Aug 17 06:25:16.445: INFO: Trying to get logs from node 10.241.148.42 pod downwardapi-volume-3bb50d51-608d-4954-895a-6fd3fc2d9346 container client-container: <nil>
STEP: delete the pod
Aug 17 06:25:16.838: INFO: Waiting for pod downwardapi-volume-3bb50d51-608d-4954-895a-6fd3fc2d9346 to disappear
Aug 17 06:25:16.915: INFO: Pod downwardapi-volume-3bb50d51-608d-4954-895a-6fd3fc2d9346 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:25:16.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9597" for this suite.
Aug 17 06:25:27.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:25:30.115: INFO: namespace projected-9597 deletion completed in 12.629727864s

• [SLOW TEST:18.115 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:25:30.115: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 06:25:30.398: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-7aa19c87-3ea0-4224-bba1-badd2c48bd1b
STEP: Creating configMap with name cm-test-opt-upd-20428e21-7dff-4399-b2bd-e886a7dd125b
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-7aa19c87-3ea0-4224-bba1-badd2c48bd1b
STEP: Updating configmap cm-test-opt-upd-20428e21-7dff-4399-b2bd-e886a7dd125b
STEP: Creating configMap with name cm-test-opt-create-eb06dbac-01f3-415d-9cff-e61098a4c743
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:26:44.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5367" for this suite.
Aug 17 06:27:00.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:27:03.192: INFO: namespace configmap-5367 deletion completed in 18.46878336s

• [SLOW TEST:93.077 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:27:03.194: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 17 06:27:06.850: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:27:06.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5957" for this suite.
Aug 17 06:27:17.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:27:21.573: INFO: namespace container-runtime-5957 deletion completed in 14.568563277s

• [SLOW TEST:18.378 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:27:21.574: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 17 06:27:21.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-3384'
Aug 17 06:27:22.370: INFO: stderr: ""
Aug 17 06:27:22.370: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Aug 17 06:27:27.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pod e2e-test-httpd-pod --namespace=kubectl-3384 -o json'
Aug 17 06:27:27.564: INFO: stderr: ""
Aug 17 06:27:27.564: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.30.218.92/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.218.92/32\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.218.92\\\"\\n    ],\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2020-08-17T06:27:22Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3384\",\n        \"resourceVersion\": \"92728\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-3384/pods/e2e-test-httpd-pod\",\n        \"uid\": \"1b8edb7f-2ef3-4af7-a870-bc52ccc01063\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-t8hhh\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-qqpfh\"\n            }\n        ],\n        \"nodeName\": \"10.241.148.42\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c65,c30\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-t8hhh\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-t8hhh\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-17T06:27:22Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-17T06:27:23Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-17T06:27:23Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-17T06:27:22Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://4c340ec3cd33b0605c6735914564bd05e08e91afcb9dfd185a6c0679125d17f8\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-08-17T06:27:23Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.241.148.42\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.218.92\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.218.92\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-08-17T06:27:22Z\"\n    }\n}\n"
STEP: replace the image in the pod
Aug 17 06:27:27.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 replace -f - --namespace=kubectl-3384'
Aug 17 06:27:28.198: INFO: stderr: ""
Aug 17 06:27:28.198: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Aug 17 06:27:28.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 delete pods e2e-test-httpd-pod --namespace=kubectl-3384'
Aug 17 06:27:32.424: INFO: stderr: ""
Aug 17 06:27:32.424: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:27:32.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3384" for this suite.
Aug 17 06:27:40.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:27:43.822: INFO: namespace kubectl-3384 deletion completed in 11.350293718s

• [SLOW TEST:22.249 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:27:43.823: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Aug 17 06:27:44.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 create -f - --namespace=kubectl-8129'
Aug 17 06:27:44.641: INFO: stderr: ""
Aug 17 06:27:44.641: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 17 06:27:44.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8129'
Aug 17 06:27:44.785: INFO: stderr: ""
Aug 17 06:27:44.785: INFO: stdout: ""
STEP: Replicas for name=update-demo: expected=2 actual=0
Aug 17 06:27:49.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8129'
Aug 17 06:27:50.076: INFO: stderr: ""
Aug 17 06:27:50.076: INFO: stdout: "update-demo-nautilus-dg8zz update-demo-nautilus-nh859 "
Aug 17 06:27:50.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods update-demo-nautilus-dg8zz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8129'
Aug 17 06:27:50.242: INFO: stderr: ""
Aug 17 06:27:50.242: INFO: stdout: "true"
Aug 17 06:27:50.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods update-demo-nautilus-dg8zz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8129'
Aug 17 06:27:50.379: INFO: stderr: ""
Aug 17 06:27:50.379: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 17 06:27:50.379: INFO: validating pod update-demo-nautilus-dg8zz
Aug 17 06:27:50.454: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 17 06:27:50.454: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 17 06:27:50.454: INFO: update-demo-nautilus-dg8zz is verified up and running
Aug 17 06:27:50.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods update-demo-nautilus-nh859 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8129'
Aug 17 06:27:50.737: INFO: stderr: ""
Aug 17 06:27:50.737: INFO: stdout: "true"
Aug 17 06:27:50.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods update-demo-nautilus-nh859 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8129'
Aug 17 06:27:50.880: INFO: stderr: ""
Aug 17 06:27:50.880: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 17 06:27:50.880: INFO: validating pod update-demo-nautilus-nh859
Aug 17 06:27:50.945: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 17 06:27:50.945: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 17 06:27:50.945: INFO: update-demo-nautilus-nh859 is verified up and running
STEP: using delete to clean up resources
Aug 17 06:27:50.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 delete --grace-period=0 --force -f - --namespace=kubectl-8129'
Aug 17 06:27:51.116: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 06:27:51.116: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 17 06:27:51.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8129'
Aug 17 06:27:51.364: INFO: stderr: "No resources found in kubectl-8129 namespace.\n"
Aug 17 06:27:51.364: INFO: stdout: ""
Aug 17 06:27:51.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods -l name=update-demo --namespace=kubectl-8129 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 17 06:27:51.519: INFO: stderr: ""
Aug 17 06:27:51.519: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:27:51.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8129" for this suite.
Aug 17 06:28:09.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:28:12.051: INFO: namespace kubectl-8129 deletion completed in 20.471539579s

• [SLOW TEST:28.229 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:28:12.052: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-1760
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 17 06:28:12.235: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 17 06:28:36.872: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.218.105:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1760 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 17 06:28:36.872: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 06:28:37.172: INFO: Found all expected endpoints: [netserver-0]
Aug 17 06:28:37.191: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.201.245:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1760 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 17 06:28:37.191: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 06:28:37.486: INFO: Found all expected endpoints: [netserver-1]
Aug 17 06:28:37.509: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.151.173:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1760 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 17 06:28:37.509: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 06:28:37.807: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:28:37.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1760" for this suite.
Aug 17 06:28:48.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:28:52.420: INFO: namespace pod-network-test-1760 deletion completed in 14.581119715s

• [SLOW TEST:40.368 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:28:52.421: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-0c565e57-f7fe-446c-845d-99bdf0b8ce6f in namespace container-probe-7401
Aug 17 06:28:54.786: INFO: Started pod busybox-0c565e57-f7fe-446c-845d-99bdf0b8ce6f in namespace container-probe-7401
STEP: checking the pod's current state and verifying that restartCount is present
Aug 17 06:28:54.804: INFO: Initial restart count of pod busybox-0c565e57-f7fe-446c-845d-99bdf0b8ce6f is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:32:55.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7401" for this suite.
Aug 17 06:33:03.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:33:05.898: INFO: namespace container-probe-7401 deletion completed in 10.598981714s

• [SLOW TEST:253.477 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:33:05.898: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-9fae20fb-99af-47dc-baba-5f1600e256fe
STEP: Creating a pod to test consume secrets
Aug 17 06:33:06.217: INFO: Waiting up to 5m0s for pod "pod-secrets-137dd406-11c0-446f-8e2d-899417dfd565" in namespace "secrets-8150" to be "success or failure"
Aug 17 06:33:06.240: INFO: Pod "pod-secrets-137dd406-11c0-446f-8e2d-899417dfd565": Phase="Pending", Reason="", readiness=false. Elapsed: 22.649382ms
Aug 17 06:33:08.272: INFO: Pod "pod-secrets-137dd406-11c0-446f-8e2d-899417dfd565": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.054662152s
STEP: Saw pod success
Aug 17 06:33:08.272: INFO: Pod "pod-secrets-137dd406-11c0-446f-8e2d-899417dfd565" satisfied condition "success or failure"
Aug 17 06:33:08.294: INFO: Trying to get logs from node 10.241.148.42 pod pod-secrets-137dd406-11c0-446f-8e2d-899417dfd565 container secret-env-test: <nil>
STEP: delete the pod
Aug 17 06:33:08.502: INFO: Waiting for pod pod-secrets-137dd406-11c0-446f-8e2d-899417dfd565 to disappear
Aug 17 06:33:08.520: INFO: Pod pod-secrets-137dd406-11c0-446f-8e2d-899417dfd565 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:33:08.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8150" for this suite.
Aug 17 06:33:16.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:33:22.240: INFO: namespace secrets-8150 deletion completed in 13.677125407s

• [SLOW TEST:16.342 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:33:22.241: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Aug 17 06:33:22.515: INFO: Waiting up to 5m0s for pod "var-expansion-35aed941-680c-4f4e-9b68-a2673308e938" in namespace "var-expansion-6853" to be "success or failure"
Aug 17 06:33:22.540: INFO: Pod "var-expansion-35aed941-680c-4f4e-9b68-a2673308e938": Phase="Pending", Reason="", readiness=false. Elapsed: 24.844311ms
Aug 17 06:33:24.566: INFO: Pod "var-expansion-35aed941-680c-4f4e-9b68-a2673308e938": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050589029s
Aug 17 06:33:26.597: INFO: Pod "var-expansion-35aed941-680c-4f4e-9b68-a2673308e938": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.081606049s
STEP: Saw pod success
Aug 17 06:33:26.597: INFO: Pod "var-expansion-35aed941-680c-4f4e-9b68-a2673308e938" satisfied condition "success or failure"
Aug 17 06:33:26.618: INFO: Trying to get logs from node 10.241.148.42 pod var-expansion-35aed941-680c-4f4e-9b68-a2673308e938 container dapi-container: <nil>
STEP: delete the pod
Aug 17 06:33:26.720: INFO: Waiting for pod var-expansion-35aed941-680c-4f4e-9b68-a2673308e938 to disappear
Aug 17 06:33:26.738: INFO: Pod var-expansion-35aed941-680c-4f4e-9b68-a2673308e938 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:33:26.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6853" for this suite.
Aug 17 06:33:36.884: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:33:39.400: INFO: namespace var-expansion-6853 deletion completed in 12.632829633s

• [SLOW TEST:17.159 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:33:39.402: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 17 06:33:40.700: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9e3b52f4-869b-4053-b048-d78470cba40c" in namespace "downward-api-6337" to be "success or failure"
Aug 17 06:33:40.717: INFO: Pod "downwardapi-volume-9e3b52f4-869b-4053-b048-d78470cba40c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.181511ms
Aug 17 06:33:42.738: INFO: Pod "downwardapi-volume-9e3b52f4-869b-4053-b048-d78470cba40c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037244021s
Aug 17 06:33:44.756: INFO: Pod "downwardapi-volume-9e3b52f4-869b-4053-b048-d78470cba40c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055131598s
STEP: Saw pod success
Aug 17 06:33:44.756: INFO: Pod "downwardapi-volume-9e3b52f4-869b-4053-b048-d78470cba40c" satisfied condition "success or failure"
Aug 17 06:33:44.778: INFO: Trying to get logs from node 10.241.148.42 pod downwardapi-volume-9e3b52f4-869b-4053-b048-d78470cba40c container client-container: <nil>
STEP: delete the pod
Aug 17 06:33:44.886: INFO: Waiting for pod downwardapi-volume-9e3b52f4-869b-4053-b048-d78470cba40c to disappear
Aug 17 06:33:44.913: INFO: Pod downwardapi-volume-9e3b52f4-869b-4053-b048-d78470cba40c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:33:44.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6337" for this suite.
Aug 17 06:33:55.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:33:57.957: INFO: namespace downward-api-6337 deletion completed in 12.904436728s

• [SLOW TEST:18.555 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:33:57.957: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Aug 17 06:33:58.218: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:34:12.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7476" for this suite.
Aug 17 06:34:22.467: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:34:24.866: INFO: namespace pods-7476 deletion completed in 12.456018837s

• [SLOW TEST:26.910 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:34:24.869: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:34:36.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1472" for this suite.
Aug 17 06:34:46.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:34:51.688: INFO: namespace resourcequota-1472 deletion completed in 15.197562804s

• [SLOW TEST:26.819 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:34:51.690: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 06:34:52.021: INFO: (0) /api/v1/nodes/10.241.148.31:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 74.767142ms)
Aug 17 06:34:52.043: INFO: (1) /api/v1/nodes/10.241.148.31:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 22.412272ms)
Aug 17 06:34:52.066: INFO: (2) /api/v1/nodes/10.241.148.31:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 21.930014ms)
Aug 17 06:34:52.091: INFO: (3) /api/v1/nodes/10.241.148.31:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 25.116067ms)
Aug 17 06:34:52.116: INFO: (4) /api/v1/nodes/10.241.148.31:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 24.410692ms)
Aug 17 06:34:52.140: INFO: (5) /api/v1/nodes/10.241.148.31:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 23.835185ms)
Aug 17 06:34:52.166: INFO: (6) /api/v1/nodes/10.241.148.31:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 25.861335ms)
Aug 17 06:34:52.190: INFO: (7) /api/v1/nodes/10.241.148.31:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 23.924821ms)
Aug 17 06:34:52.220: INFO: (8) /api/v1/nodes/10.241.148.31:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 29.45948ms)
Aug 17 06:34:52.251: INFO: (9) /api/v1/nodes/10.241.148.31:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 31.575524ms)
Aug 17 06:34:52.278: INFO: (10) /api/v1/nodes/10.241.148.31:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 26.029685ms)
Aug 17 06:34:52.309: INFO: (11) /api/v1/nodes/10.241.148.31:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 31.292869ms)
Aug 17 06:34:52.334: INFO: (12) /api/v1/nodes/10.241.148.31:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 25.329057ms)
Aug 17 06:34:52.362: INFO: (13) /api/v1/nodes/10.241.148.31:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 27.70469ms)
Aug 17 06:34:52.389: INFO: (14) /api/v1/nodes/10.241.148.31:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 26.22989ms)
Aug 17 06:34:52.423: INFO: (15) /api/v1/nodes/10.241.148.31:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 34.00176ms)
Aug 17 06:34:52.450: INFO: (16) /api/v1/nodes/10.241.148.31:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 27.711747ms)
Aug 17 06:34:52.475: INFO: (17) /api/v1/nodes/10.241.148.31:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 24.440292ms)
Aug 17 06:34:52.523: INFO: (18) /api/v1/nodes/10.241.148.31:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 47.956389ms)
Aug 17 06:34:52.557: INFO: (19) /api/v1/nodes/10.241.148.31:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 34.565958ms)
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:34:52.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4402" for this suite.
Aug 17 06:35:00.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:35:02.200: INFO: namespace proxy-4402 deletion completed in 9.602757284s

• [SLOW TEST:10.511 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:35:02.204: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-1500
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-1500
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1500
Aug 17 06:35:02.495: INFO: Found 0 stateful pods, waiting for 1
Aug 17 06:35:12.515: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Aug 17 06:35:12.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=statefulset-1500 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 06:35:13.727: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 06:35:13.728: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 06:35:13.728: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 06:35:13.747: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 17 06:35:23.767: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 17 06:35:23.767: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 06:35:23.827: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Aug 17 06:35:23.827: INFO: ss-0  10.241.148.42  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:02 +0000 UTC  }]
Aug 17 06:35:23.827: INFO: 
Aug 17 06:35:23.827: INFO: StatefulSet ss has not reached scale 3, at 1
Aug 17 06:35:24.851: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.982384286s
Aug 17 06:35:25.878: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.958790378s
Aug 17 06:35:26.915: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.931469023s
Aug 17 06:35:27.945: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.895093429s
Aug 17 06:35:28.974: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.864665821s
Aug 17 06:35:30.002: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.83534583s
Aug 17 06:35:31.026: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.808265739s
Aug 17 06:35:32.059: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.783998145s
Aug 17 06:35:33.088: INFO: Verifying statefulset ss doesn't scale past 3 for another 751.107568ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1500
Aug 17 06:35:34.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=statefulset-1500 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 06:35:34.551: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 17 06:35:34.551: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 17 06:35:34.551: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 17 06:35:34.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=statefulset-1500 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 06:35:34.977: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 17 06:35:34.977: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 17 06:35:34.977: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 17 06:35:34.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=statefulset-1500 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 06:35:35.426: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 17 06:35:35.426: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 17 06:35:35.426: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 17 06:35:35.449: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 06:35:35.449: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 06:35:35.449: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Aug 17 06:35:35.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=statefulset-1500 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 06:35:35.947: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 06:35:35.947: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 06:35:35.947: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 06:35:35.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=statefulset-1500 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 06:35:36.393: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 06:35:36.393: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 06:35:36.393: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 06:35:36.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 exec --namespace=statefulset-1500 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 06:35:37.211: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 06:35:37.211: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 06:35:37.211: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 06:35:37.211: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 06:35:37.224: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Aug 17 06:35:47.366: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 17 06:35:47.366: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 17 06:35:47.366: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 17 06:35:47.583: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Aug 17 06:35:47.583: INFO: ss-0  10.241.148.42  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:02 +0000 UTC  }]
Aug 17 06:35:47.583: INFO: ss-1  10.241.148.31  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:23 +0000 UTC  }]
Aug 17 06:35:47.583: INFO: ss-2  10.241.148.50  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:23 +0000 UTC  }]
Aug 17 06:35:47.583: INFO: 
Aug 17 06:35:47.583: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 17 06:35:48.882: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Aug 17 06:35:48.882: INFO: ss-0  10.241.148.42  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:02 +0000 UTC  }]
Aug 17 06:35:48.882: INFO: ss-1  10.241.148.31  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:23 +0000 UTC  }]
Aug 17 06:35:48.882: INFO: ss-2  10.241.148.50  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:23 +0000 UTC  }]
Aug 17 06:35:48.882: INFO: 
Aug 17 06:35:48.882: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 17 06:35:49.993: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Aug 17 06:35:49.993: INFO: ss-0  10.241.148.42  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:02 +0000 UTC  }]
Aug 17 06:35:49.993: INFO: ss-1  10.241.148.31  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:23 +0000 UTC  }]
Aug 17 06:35:49.993: INFO: ss-2  10.241.148.50  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:23 +0000 UTC  }]
Aug 17 06:35:49.993: INFO: 
Aug 17 06:35:49.993: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 17 06:35:51.012: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Aug 17 06:35:51.012: INFO: ss-1  10.241.148.31  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-17 06:35:23 +0000 UTC  }]
Aug 17 06:35:51.012: INFO: 
Aug 17 06:35:51.012: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 17 06:35:52.033: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.462206563s
Aug 17 06:35:53.057: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.440989661s
Aug 17 06:35:54.074: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.41796643s
Aug 17 06:35:55.095: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.400435499s
Aug 17 06:35:56.115: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.379712438s
Aug 17 06:35:57.135: INFO: Verifying statefulset ss doesn't scale past 0 for another 359.487491ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1500
Aug 17 06:35:58.153: INFO: Scaling statefulset ss to 0
Aug 17 06:35:58.200: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Aug 17 06:35:58.214: INFO: Deleting all statefulset in ns statefulset-1500
Aug 17 06:35:58.225: INFO: Scaling statefulset ss to 0
Aug 17 06:35:58.272: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 06:35:58.285: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:35:58.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1500" for this suite.
Aug 17 06:36:08.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:36:11.122: INFO: namespace statefulset-1500 deletion completed in 12.731179208s

• [SLOW TEST:68.919 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:36:11.122: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 17 06:36:11.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-7687'
Aug 17 06:36:11.525: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 17 06:36:11.525: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Aug 17 06:36:13.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 delete deployment e2e-test-httpd-deployment --namespace=kubectl-7687'
Aug 17 06:36:13.743: INFO: stderr: ""
Aug 17 06:36:13.743: INFO: stdout: "deployment.extensions \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:36:13.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7687" for this suite.
Aug 17 06:36:29.877: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:36:32.550: INFO: namespace kubectl-7687 deletion completed in 18.738937833s

• [SLOW TEST:21.427 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:36:32.550: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Aug 17 06:36:32.831: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-667650646 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:36:32.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3511" for this suite.
Aug 17 06:36:41.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:36:43.543: INFO: namespace kubectl-3511 deletion completed in 10.525974143s

• [SLOW TEST:10.993 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:36:43.543: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 06:36:43.777: INFO: Creating ReplicaSet my-hostname-basic-1d93bdcd-0f8b-4358-9c94-189953c0b1d8
Aug 17 06:36:43.830: INFO: Pod name my-hostname-basic-1d93bdcd-0f8b-4358-9c94-189953c0b1d8: Found 0 pods out of 1
Aug 17 06:36:48.900: INFO: Pod name my-hostname-basic-1d93bdcd-0f8b-4358-9c94-189953c0b1d8: Found 1 pods out of 1
Aug 17 06:36:48.900: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-1d93bdcd-0f8b-4358-9c94-189953c0b1d8" is running
Aug 17 06:36:48.989: INFO: Pod "my-hostname-basic-1d93bdcd-0f8b-4358-9c94-189953c0b1d8-kpx5t" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-17 06:36:43 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-17 06:36:45 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-17 06:36:45 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-17 06:36:43 +0000 UTC Reason: Message:}])
Aug 17 06:36:48.989: INFO: Trying to dial the pod
Aug 17 06:36:54.063: INFO: Controller my-hostname-basic-1d93bdcd-0f8b-4358-9c94-189953c0b1d8: Got expected result from replica 1 [my-hostname-basic-1d93bdcd-0f8b-4358-9c94-189953c0b1d8-kpx5t]: "my-hostname-basic-1d93bdcd-0f8b-4358-9c94-189953c0b1d8-kpx5t", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:36:54.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3811" for this suite.
Aug 17 06:37:04.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:37:06.783: INFO: namespace replicaset-3811 deletion completed in 12.682951235s

• [SLOW TEST:23.240 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:37:06.784: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Aug 17 06:37:07.275: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3463 /api/v1/namespaces/watch-3463/configmaps/e2e-watch-test-label-changed 3f5f56bd-a7c2-4c10-b24f-49d21e480060 96285 0 2020-08-17 06:37:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 17 06:37:07.275: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3463 /api/v1/namespaces/watch-3463/configmaps/e2e-watch-test-label-changed 3f5f56bd-a7c2-4c10-b24f-49d21e480060 96287 0 2020-08-17 06:37:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug 17 06:37:07.275: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3463 /api/v1/namespaces/watch-3463/configmaps/e2e-watch-test-label-changed 3f5f56bd-a7c2-4c10-b24f-49d21e480060 96288 0 2020-08-17 06:37:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Aug 17 06:37:17.753: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3463 /api/v1/namespaces/watch-3463/configmaps/e2e-watch-test-label-changed 3f5f56bd-a7c2-4c10-b24f-49d21e480060 96330 0 2020-08-17 06:37:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 17 06:37:17.754: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3463 /api/v1/namespaces/watch-3463/configmaps/e2e-watch-test-label-changed 3f5f56bd-a7c2-4c10-b24f-49d21e480060 96331 0 2020-08-17 06:37:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Aug 17 06:37:17.798: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3463 /api/v1/namespaces/watch-3463/configmaps/e2e-watch-test-label-changed 3f5f56bd-a7c2-4c10-b24f-49d21e480060 96332 0 2020-08-17 06:37:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:37:17.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3463" for this suite.
Aug 17 06:37:26.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:37:29.164: INFO: namespace watch-3463 deletion completed in 10.719955319s

• [SLOW TEST:22.380 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:37:29.164: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 06:37:29.496: INFO: (0) /api/v1/nodes/10.241.148.31/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 63.25391ms)
Aug 17 06:37:29.522: INFO: (1) /api/v1/nodes/10.241.148.31/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 26.128502ms)
Aug 17 06:37:29.546: INFO: (2) /api/v1/nodes/10.241.148.31/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 23.516732ms)
Aug 17 06:37:29.573: INFO: (3) /api/v1/nodes/10.241.148.31/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 26.831562ms)
Aug 17 06:37:29.597: INFO: (4) /api/v1/nodes/10.241.148.31/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 24.43483ms)
Aug 17 06:37:29.627: INFO: (5) /api/v1/nodes/10.241.148.31/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 29.643014ms)
Aug 17 06:37:29.663: INFO: (6) /api/v1/nodes/10.241.148.31/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 35.66589ms)
Aug 17 06:37:29.691: INFO: (7) /api/v1/nodes/10.241.148.31/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 27.604797ms)
Aug 17 06:37:29.722: INFO: (8) /api/v1/nodes/10.241.148.31/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 31.123877ms)
Aug 17 06:37:29.748: INFO: (9) /api/v1/nodes/10.241.148.31/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 25.696375ms)
Aug 17 06:37:29.771: INFO: (10) /api/v1/nodes/10.241.148.31/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 23.576568ms)
Aug 17 06:37:29.795: INFO: (11) /api/v1/nodes/10.241.148.31/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 24.086533ms)
Aug 17 06:37:29.821: INFO: (12) /api/v1/nodes/10.241.148.31/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 25.300811ms)
Aug 17 06:37:29.846: INFO: (13) /api/v1/nodes/10.241.148.31/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 24.920592ms)
Aug 17 06:37:29.872: INFO: (14) /api/v1/nodes/10.241.148.31/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 26.113654ms)
Aug 17 06:37:29.897: INFO: (15) /api/v1/nodes/10.241.148.31/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 25.117352ms)
Aug 17 06:37:29.919: INFO: (16) /api/v1/nodes/10.241.148.31/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 21.9175ms)
Aug 17 06:37:29.939: INFO: (17) /api/v1/nodes/10.241.148.31/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.204299ms)
Aug 17 06:37:29.966: INFO: (18) /api/v1/nodes/10.241.148.31/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 26.724227ms)
Aug 17 06:37:30.007: INFO: (19) /api/v1/nodes/10.241.148.31/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 40.854285ms)
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:37:30.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5048" for this suite.
Aug 17 06:37:38.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:37:39.656: INFO: namespace proxy-5048 deletion completed in 9.607659537s

• [SLOW TEST:10.491 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:37:39.656: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 17 06:37:39.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-2876'
Aug 17 06:37:41.013: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 17 06:37:41.013: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Aug 17 06:37:41.044: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Aug 17 06:37:41.093: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Aug 17 06:37:41.155: INFO: scanned /root for discovery docs: <nil>
Aug 17 06:37:41.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-2876'
Aug 17 06:37:56.621: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug 17 06:37:56.621: INFO: stdout: "Created e2e-test-httpd-rc-3e0d5fe5429d5da6c5699bfa18a562c5\nScaling up e2e-test-httpd-rc-3e0d5fe5429d5da6c5699bfa18a562c5 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-3e0d5fe5429d5da6c5699bfa18a562c5 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-3e0d5fe5429d5da6c5699bfa18a562c5 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Aug 17 06:37:56.621: INFO: stdout: "Created e2e-test-httpd-rc-3e0d5fe5429d5da6c5699bfa18a562c5\nScaling up e2e-test-httpd-rc-3e0d5fe5429d5da6c5699bfa18a562c5 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-3e0d5fe5429d5da6c5699bfa18a562c5 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-3e0d5fe5429d5da6c5699bfa18a562c5 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Aug 17 06:37:56.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-2876'
Aug 17 06:37:56.768: INFO: stderr: ""
Aug 17 06:37:56.768: INFO: stdout: "e2e-test-httpd-rc-3e0d5fe5429d5da6c5699bfa18a562c5-ncvtk "
Aug 17 06:37:56.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods e2e-test-httpd-rc-3e0d5fe5429d5da6c5699bfa18a562c5-ncvtk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2876'
Aug 17 06:37:56.920: INFO: stderr: ""
Aug 17 06:37:56.920: INFO: stdout: "true"
Aug 17 06:37:56.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 get pods e2e-test-httpd-rc-3e0d5fe5429d5da6c5699bfa18a562c5-ncvtk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2876'
Aug 17 06:37:57.077: INFO: stderr: ""
Aug 17 06:37:57.077: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Aug 17 06:37:57.077: INFO: e2e-test-httpd-rc-3e0d5fe5429d5da6c5699bfa18a562c5-ncvtk is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Aug 17 06:37:57.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 delete rc e2e-test-httpd-rc --namespace=kubectl-2876'
Aug 17 06:37:57.263: INFO: stderr: ""
Aug 17 06:37:57.263: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:37:57.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2876" for this suite.
Aug 17 06:38:07.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:38:09.873: INFO: namespace kubectl-2876 deletion completed in 12.545233611s

• [SLOW TEST:30.218 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:38:09.874: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 17 06:38:10.678: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 17 06:38:12.720: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733243090, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733243090, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733243090, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733243090, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 17 06:38:15.787: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:38:17.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7251" for this suite.
Aug 17 06:38:27.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:38:30.047: INFO: namespace webhook-7251 deletion completed in 12.61813826s
STEP: Destroying namespace "webhook-7251-markers" for this suite.
Aug 17 06:38:40.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:38:42.476: INFO: namespace webhook-7251-markers deletion completed in 12.429033265s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:32.673 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:38:42.547: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Aug 17 06:38:44.016: INFO: Pod name wrapped-volume-race-e39ff1c0-f14a-45b0-98f3-38888f348a0b: Found 0 pods out of 5
Aug 17 06:38:49.325: INFO: Pod name wrapped-volume-race-e39ff1c0-f14a-45b0-98f3-38888f348a0b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e39ff1c0-f14a-45b0-98f3-38888f348a0b in namespace emptydir-wrapper-7562, will wait for the garbage collector to delete the pods
Aug 17 06:38:51.961: INFO: Deleting ReplicationController wrapped-volume-race-e39ff1c0-f14a-45b0-98f3-38888f348a0b took: 76.856658ms
Aug 17 06:38:52.161: INFO: Terminating ReplicationController wrapped-volume-race-e39ff1c0-f14a-45b0-98f3-38888f348a0b pods took: 200.307425ms
STEP: Creating RC which spawns configmap-volume pods
Aug 17 06:39:31.864: INFO: Pod name wrapped-volume-race-2cb050cb-56b3-4503-9278-3fa9cfe7731c: Found 0 pods out of 5
Aug 17 06:39:36.923: INFO: Pod name wrapped-volume-race-2cb050cb-56b3-4503-9278-3fa9cfe7731c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2cb050cb-56b3-4503-9278-3fa9cfe7731c in namespace emptydir-wrapper-7562, will wait for the garbage collector to delete the pods
Aug 17 06:39:37.151: INFO: Deleting ReplicationController wrapped-volume-race-2cb050cb-56b3-4503-9278-3fa9cfe7731c took: 46.500694ms
Aug 17 06:39:37.951: INFO: Terminating ReplicationController wrapped-volume-race-2cb050cb-56b3-4503-9278-3fa9cfe7731c pods took: 800.461715ms
STEP: Creating RC which spawns configmap-volume pods
Aug 17 06:40:21.941: INFO: Pod name wrapped-volume-race-b281d3e8-fa80-4e65-b8a5-c58f78bf4b79: Found 0 pods out of 5
Aug 17 06:40:26.989: INFO: Pod name wrapped-volume-race-b281d3e8-fa80-4e65-b8a5-c58f78bf4b79: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b281d3e8-fa80-4e65-b8a5-c58f78bf4b79 in namespace emptydir-wrapper-7562, will wait for the garbage collector to delete the pods
Aug 17 06:40:27.208: INFO: Deleting ReplicationController wrapped-volume-race-b281d3e8-fa80-4e65-b8a5-c58f78bf4b79 took: 42.714733ms
Aug 17 06:40:27.908: INFO: Terminating ReplicationController wrapped-volume-race-b281d3e8-fa80-4e65-b8a5-c58f78bf4b79 pods took: 700.320735ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:41:13.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7562" for this suite.
Aug 17 06:41:26.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:41:28.554: INFO: namespace emptydir-wrapper-7562 deletion completed in 14.588509923s

• [SLOW TEST:166.007 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:41:28.561: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Aug 17 06:41:28.792: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:41:33.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5155" for this suite.
Aug 17 06:41:43.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:41:45.655: INFO: namespace init-container-5155 deletion completed in 12.466785631s

• [SLOW TEST:17.094 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:41:45.658: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 17 06:41:47.047: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733243306, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733243306, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733243306, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733243306, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 06:41:49.116: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733243306, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733243306, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733243306, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733243306, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 17 06:41:52.228: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:41:52.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7272" for this suite.
Aug 17 06:42:03.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:42:05.607: INFO: namespace webhook-7272 deletion completed in 12.565097374s
STEP: Destroying namespace "webhook-7272-markers" for this suite.
Aug 17 06:42:15.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:42:21.060: INFO: namespace webhook-7272-markers deletion completed in 15.452910614s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:35.477 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:42:21.135: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Aug 17 06:42:21.499: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c5f3e63b-d24f-4c0e-ac07-2a28d943ebb1" in namespace "downward-api-3064" to be "success or failure"
Aug 17 06:42:21.529: INFO: Pod "downwardapi-volume-c5f3e63b-d24f-4c0e-ac07-2a28d943ebb1": Phase="Pending", Reason="", readiness=false. Elapsed: 30.182504ms
Aug 17 06:42:23.555: INFO: Pod "downwardapi-volume-c5f3e63b-d24f-4c0e-ac07-2a28d943ebb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.05595329s
STEP: Saw pod success
Aug 17 06:42:23.555: INFO: Pod "downwardapi-volume-c5f3e63b-d24f-4c0e-ac07-2a28d943ebb1" satisfied condition "success or failure"
Aug 17 06:42:23.577: INFO: Trying to get logs from node 10.241.148.42 pod downwardapi-volume-c5f3e63b-d24f-4c0e-ac07-2a28d943ebb1 container client-container: <nil>
STEP: delete the pod
Aug 17 06:42:23.736: INFO: Waiting for pod downwardapi-volume-c5f3e63b-d24f-4c0e-ac07-2a28d943ebb1 to disappear
Aug 17 06:42:23.756: INFO: Pod downwardapi-volume-c5f3e63b-d24f-4c0e-ac07-2a28d943ebb1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:42:23.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3064" for this suite.
Aug 17 06:42:33.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:42:36.393: INFO: namespace downward-api-3064 deletion completed in 12.533510805s

• [SLOW TEST:15.258 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:42:36.394: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:42:45.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2644" for this suite.
Aug 17 06:42:55.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:42:57.931: INFO: namespace namespaces-2644 deletion completed in 12.557664157s
STEP: Destroying namespace "nsdeletetest-5151" for this suite.
Aug 17 06:42:57.954: INFO: Namespace nsdeletetest-5151 was already deleted
STEP: Destroying namespace "nsdeletetest-3483" for this suite.
Aug 17 06:43:06.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:43:08.465: INFO: namespace nsdeletetest-3483 deletion completed in 10.510506969s

• [SLOW TEST:32.071 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:43:08.465: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Aug 17 06:43:08.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-5325 -- logs-generator --log-lines-total 100 --run-duration 20s'
Aug 17 06:43:08.863: INFO: stderr: ""
Aug 17 06:43:08.864: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Aug 17 06:43:08.864: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Aug 17 06:43:08.864: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5325" to be "running and ready, or succeeded"
Aug 17 06:43:08.884: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 19.995541ms
Aug 17 06:43:10.907: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.043428527s
Aug 17 06:43:10.907: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Aug 17 06:43:10.907: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Aug 17 06:43:10.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 logs logs-generator logs-generator --namespace=kubectl-5325'
Aug 17 06:43:11.125: INFO: stderr: ""
Aug 17 06:43:11.125: INFO: stdout: "I0817 06:43:10.189814       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/22np 290\nI0817 06:43:10.390037       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/n22x 566\nI0817 06:43:10.590047       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/qns 508\nI0817 06:43:10.790041       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/km8 399\nI0817 06:43:10.990112       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/fjg6 542\n"
STEP: limiting log lines
Aug 17 06:43:11.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 logs logs-generator logs-generator --namespace=kubectl-5325 --tail=1'
Aug 17 06:43:11.848: INFO: stderr: ""
Aug 17 06:43:11.848: INFO: stdout: "I0817 06:43:11.590080       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/6hjw 398\n"
STEP: limiting log bytes
Aug 17 06:43:11.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 logs logs-generator logs-generator --namespace=kubectl-5325 --limit-bytes=1'
Aug 17 06:43:12.031: INFO: stderr: ""
Aug 17 06:43:12.031: INFO: stdout: "I"
STEP: exposing timestamps
Aug 17 06:43:12.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 logs logs-generator logs-generator --namespace=kubectl-5325 --tail=1 --timestamps'
Aug 17 06:43:12.216: INFO: stderr: ""
Aug 17 06:43:12.216: INFO: stdout: "2020-08-17T01:43:12.19012301-05:00 I0817 06:43:12.190027       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/sdtn 309\n"
STEP: restricting to a time range
Aug 17 06:43:14.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 logs logs-generator logs-generator --namespace=kubectl-5325 --since=1s'
Aug 17 06:43:14.912: INFO: stderr: ""
Aug 17 06:43:14.912: INFO: stdout: "I0817 06:43:13.990126       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/sqpt 418\nI0817 06:43:14.190057       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/kwr7 317\nI0817 06:43:14.390069       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/kk9 458\nI0817 06:43:14.590038       1 logs_generator.go:76] 22 GET /api/v1/namespaces/ns/pods/s4n 563\nI0817 06:43:14.790030       1 logs_generator.go:76] 23 POST /api/v1/namespaces/default/pods/fl2 391\n"
Aug 17 06:43:14.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 logs logs-generator logs-generator --namespace=kubectl-5325 --since=24h'
Aug 17 06:43:15.138: INFO: stderr: ""
Aug 17 06:43:15.138: INFO: stdout: "I0817 06:43:10.189814       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/22np 290\nI0817 06:43:10.390037       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/n22x 566\nI0817 06:43:10.590047       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/qns 508\nI0817 06:43:10.790041       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/km8 399\nI0817 06:43:10.990112       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/fjg6 542\nI0817 06:43:11.190775       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/pv4 307\nI0817 06:43:11.390323       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/qm4 306\nI0817 06:43:11.590080       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/6hjw 398\nI0817 06:43:11.790047       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/79r5 225\nI0817 06:43:11.994033       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/fjw 442\nI0817 06:43:12.190027       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/sdtn 309\nI0817 06:43:12.390035       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/qsmc 204\nI0817 06:43:12.590046       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/wzp 417\nI0817 06:43:12.790049       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/zzxh 232\nI0817 06:43:12.989975       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/qmc 282\nI0817 06:43:13.190123       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/ql8 235\nI0817 06:43:13.390121       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/65p6 347\nI0817 06:43:13.589994       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/gptn 580\nI0817 06:43:13.790119       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/6vkx 267\nI0817 06:43:13.990126       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/sqpt 418\nI0817 06:43:14.190057       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/kwr7 317\nI0817 06:43:14.390069       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/kk9 458\nI0817 06:43:14.590038       1 logs_generator.go:76] 22 GET /api/v1/namespaces/ns/pods/s4n 563\nI0817 06:43:14.790030       1 logs_generator.go:76] 23 POST /api/v1/namespaces/default/pods/fl2 391\nI0817 06:43:14.990241       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/default/pods/gbct 472\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Aug 17 06:43:15.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 delete pod logs-generator --namespace=kubectl-5325'
Aug 17 06:43:22.391: INFO: stderr: ""
Aug 17 06:43:22.391: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:43:22.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5325" for this suite.
Aug 17 06:43:32.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:43:35.505: INFO: namespace kubectl-5325 deletion completed in 13.057716529s

• [SLOW TEST:27.040 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:43:35.507: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Aug 17 06:43:40.462: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3829 pod-service-account-9961854a-cf89-4445-8345-68b1d2ef8437 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Aug 17 06:43:40.885: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3829 pod-service-account-9961854a-cf89-4445-8345-68b1d2ef8437 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Aug 17 06:43:41.313: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3829 pod-service-account-9961854a-cf89-4445-8345-68b1d2ef8437 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:43:41.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3829" for this suite.
Aug 17 06:43:51.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:43:54.624: INFO: namespace svcaccounts-3829 deletion completed in 12.831058333s

• [SLOW TEST:19.117 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:43:54.627: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-1db80c79-0b39-402b-86f7-f6fc87f8269e
STEP: Creating a pod to test consume configMaps
Aug 17 06:43:54.990: INFO: Waiting up to 5m0s for pod "pod-configmaps-b24c0780-7d77-4f5c-8d7e-6985472870f1" in namespace "configmap-7614" to be "success or failure"
Aug 17 06:43:55.011: INFO: Pod "pod-configmaps-b24c0780-7d77-4f5c-8d7e-6985472870f1": Phase="Pending", Reason="", readiness=false. Elapsed: 21.063807ms
Aug 17 06:43:57.033: INFO: Pod "pod-configmaps-b24c0780-7d77-4f5c-8d7e-6985472870f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043510194s
Aug 17 06:43:59.073: INFO: Pod "pod-configmaps-b24c0780-7d77-4f5c-8d7e-6985472870f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.082733701s
STEP: Saw pod success
Aug 17 06:43:59.073: INFO: Pod "pod-configmaps-b24c0780-7d77-4f5c-8d7e-6985472870f1" satisfied condition "success or failure"
Aug 17 06:43:59.109: INFO: Trying to get logs from node 10.241.148.42 pod pod-configmaps-b24c0780-7d77-4f5c-8d7e-6985472870f1 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 17 06:43:59.209: INFO: Waiting for pod pod-configmaps-b24c0780-7d77-4f5c-8d7e-6985472870f1 to disappear
Aug 17 06:43:59.244: INFO: Pod pod-configmaps-b24c0780-7d77-4f5c-8d7e-6985472870f1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:43:59.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7614" for this suite.
Aug 17 06:44:09.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:44:11.936: INFO: namespace configmap-7614 deletion completed in 12.608885782s

• [SLOW TEST:17.309 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:44:11.936: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-3114, will wait for the garbage collector to delete the pods
Aug 17 06:44:16.616: INFO: Deleting Job.batch foo took: 152.511304ms
Aug 17 06:44:17.216: INFO: Terminating Job.batch foo pods took: 600.218638ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:45:02.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3114" for this suite.
Aug 17 06:45:12.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:45:15.088: INFO: namespace job-3114 deletion completed in 12.57831296s

• [SLOW TEST:63.152 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:45:15.090: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Aug 17 06:45:16.041: INFO: created pod pod-service-account-defaultsa
Aug 17 06:45:16.041: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 17 06:45:16.096: INFO: created pod pod-service-account-mountsa
Aug 17 06:45:16.096: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 17 06:45:16.163: INFO: created pod pod-service-account-nomountsa
Aug 17 06:45:16.163: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 17 06:45:16.222: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 17 06:45:16.222: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 17 06:45:16.309: INFO: created pod pod-service-account-mountsa-mountspec
Aug 17 06:45:16.309: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 17 06:45:16.402: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 17 06:45:16.402: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 17 06:45:16.479: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 17 06:45:16.479: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 17 06:45:16.572: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 17 06:45:16.572: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 17 06:45:16.679: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 17 06:45:16.679: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:45:16.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3797" for this suite.
Aug 17 06:45:27.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:45:29.454: INFO: namespace svcaccounts-3797 deletion completed in 12.576493412s

• [SLOW TEST:14.364 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:45:29.454: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-b6b0f356-924c-4200-9174-402cebf2c65f
STEP: Creating a pod to test consume configMaps
Aug 17 06:45:29.757: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-660b5e4c-f6fe-4cd0-bac7-2da574d63d10" in namespace "projected-2632" to be "success or failure"
Aug 17 06:45:29.779: INFO: Pod "pod-projected-configmaps-660b5e4c-f6fe-4cd0-bac7-2da574d63d10": Phase="Pending", Reason="", readiness=false. Elapsed: 22.555972ms
Aug 17 06:45:31.805: INFO: Pod "pod-projected-configmaps-660b5e4c-f6fe-4cd0-bac7-2da574d63d10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047962745s
Aug 17 06:45:33.823: INFO: Pod "pod-projected-configmaps-660b5e4c-f6fe-4cd0-bac7-2da574d63d10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066353813s
STEP: Saw pod success
Aug 17 06:45:33.823: INFO: Pod "pod-projected-configmaps-660b5e4c-f6fe-4cd0-bac7-2da574d63d10" satisfied condition "success or failure"
Aug 17 06:45:33.844: INFO: Trying to get logs from node 10.241.148.42 pod pod-projected-configmaps-660b5e4c-f6fe-4cd0-bac7-2da574d63d10 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 17 06:45:33.958: INFO: Waiting for pod pod-projected-configmaps-660b5e4c-f6fe-4cd0-bac7-2da574d63d10 to disappear
Aug 17 06:45:33.973: INFO: Pod pod-projected-configmaps-660b5e4c-f6fe-4cd0-bac7-2da574d63d10 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:45:33.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2632" for this suite.
Aug 17 06:45:44.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:45:47.201: INFO: namespace projected-2632 deletion completed in 13.194805229s

• [SLOW TEST:17.748 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:45:47.202: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 17 06:45:49.217: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733243548, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733243548, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733243548, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733243548, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 06:45:51.243: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733243548, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733243548, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733243548, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733243548, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 17 06:45:54.302: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:45:54.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5205" for this suite.
Aug 17 06:46:04.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:46:07.460: INFO: namespace webhook-5205 deletion completed in 12.703706902s
STEP: Destroying namespace "webhook-5205-markers" for this suite.
Aug 17 06:46:15.556: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:46:20.849: INFO: namespace webhook-5205-markers deletion completed in 13.389277438s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:33.801 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:46:21.003: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-1711
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1711
STEP: Creating statefulset with conflicting port in namespace statefulset-1711
STEP: Waiting until pod test-pod will start running in namespace statefulset-1711
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1711
Aug 17 06:46:25.582: INFO: Observed stateful pod in namespace: statefulset-1711, name: ss-0, uid: f7885ca5-bd7a-4f6a-aa3a-87b0c0dac642, status phase: Pending. Waiting for statefulset controller to delete.
Aug 17 06:46:25.620: INFO: Observed stateful pod in namespace: statefulset-1711, name: ss-0, uid: f7885ca5-bd7a-4f6a-aa3a-87b0c0dac642, status phase: Failed. Waiting for statefulset controller to delete.
Aug 17 06:46:25.646: INFO: Observed stateful pod in namespace: statefulset-1711, name: ss-0, uid: f7885ca5-bd7a-4f6a-aa3a-87b0c0dac642, status phase: Failed. Waiting for statefulset controller to delete.
Aug 17 06:46:25.679: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1711
STEP: Removing pod with conflicting port in namespace statefulset-1711
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1711 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Aug 17 06:46:29.812: INFO: Deleting all statefulset in ns statefulset-1711
Aug 17 06:46:29.825: INFO: Scaling statefulset ss to 0
Aug 17 06:46:39.905: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 06:46:39.918: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:46:39.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1711" for this suite.
Aug 17 06:46:50.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:46:52.856: INFO: namespace statefulset-1711 deletion completed in 12.796542302s

• [SLOW TEST:31.853 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:46:52.859: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 17 06:46:53.696: INFO: Number of nodes with available pods: 0
Aug 17 06:46:53.696: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 06:46:54.763: INFO: Number of nodes with available pods: 0
Aug 17 06:46:54.763: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 06:46:55.743: INFO: Number of nodes with available pods: 1
Aug 17 06:46:55.743: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 06:46:56.771: INFO: Number of nodes with available pods: 3
Aug 17 06:46:56.771: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Aug 17 06:46:56.908: INFO: Number of nodes with available pods: 2
Aug 17 06:46:56.908: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 06:46:57.960: INFO: Number of nodes with available pods: 2
Aug 17 06:46:57.960: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 06:46:58.958: INFO: Number of nodes with available pods: 2
Aug 17 06:46:58.958: INFO: Node 10.241.148.31 is running more than one daemon pod
Aug 17 06:47:00.029: INFO: Number of nodes with available pods: 3
Aug 17 06:47:00.030: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6490, will wait for the garbage collector to delete the pods
Aug 17 06:47:00.249: INFO: Deleting DaemonSet.extensions daemon-set took: 57.779004ms
Aug 17 06:47:00.949: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.278789ms
Aug 17 06:47:12.478: INFO: Number of nodes with available pods: 0
Aug 17 06:47:12.478: INFO: Number of running nodes: 0, number of available pods: 0
Aug 17 06:47:12.510: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6490/daemonsets","resourceVersion":"101412"},"items":null}

Aug 17 06:47:12.532: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6490/pods","resourceVersion":"101412"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:47:12.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6490" for this suite.
Aug 17 06:47:22.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:47:25.271: INFO: namespace daemonsets-6490 deletion completed in 12.606059332s

• [SLOW TEST:32.412 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:47:25.271: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Aug 17 06:47:25.471: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Aug 17 06:47:58.286: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
Aug 17 06:48:07.538: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:48:45.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2258" for this suite.
Aug 17 06:48:56.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:48:58.432: INFO: namespace crd-publish-openapi-2258 deletion completed in 12.455763018s

• [SLOW TEST:93.161 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:48:58.433: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1008
I0817 06:48:58.658578      22 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1008, replica count: 1
I0817 06:48:59.709522      22 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0817 06:49:00.709815      22 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0817 06:49:01.710118      22 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 17 06:49:01.865: INFO: Created: latency-svc-4bhf5
Aug 17 06:49:01.885: INFO: Got endpoints: latency-svc-4bhf5 [74.92327ms]
Aug 17 06:49:01.935: INFO: Created: latency-svc-n9mnr
Aug 17 06:49:01.962: INFO: Got endpoints: latency-svc-n9mnr [76.539189ms]
Aug 17 06:49:01.973: INFO: Created: latency-svc-49qhj
Aug 17 06:49:01.994: INFO: Got endpoints: latency-svc-49qhj [108.391791ms]
Aug 17 06:49:02.011: INFO: Created: latency-svc-lwlnf
Aug 17 06:49:02.031: INFO: Got endpoints: latency-svc-lwlnf [144.77889ms]
Aug 17 06:49:02.040: INFO: Created: latency-svc-86c8d
Aug 17 06:49:02.082: INFO: Got endpoints: latency-svc-86c8d [196.59654ms]
Aug 17 06:49:02.087: INFO: Created: latency-svc-gqmkt
Aug 17 06:49:02.106: INFO: Got endpoints: latency-svc-gqmkt [219.081222ms]
Aug 17 06:49:02.109: INFO: Created: latency-svc-h6nlx
Aug 17 06:49:02.135: INFO: Got endpoints: latency-svc-h6nlx [249.226249ms]
Aug 17 06:49:02.139: INFO: Created: latency-svc-jbvjr
Aug 17 06:49:02.172: INFO: Got endpoints: latency-svc-jbvjr [285.600209ms]
Aug 17 06:49:02.173: INFO: Created: latency-svc-pnd48
Aug 17 06:49:02.194: INFO: Got endpoints: latency-svc-pnd48 [308.063045ms]
Aug 17 06:49:02.198: INFO: Created: latency-svc-tvvnr
Aug 17 06:49:02.257: INFO: Created: latency-svc-2b9cx
Aug 17 06:49:02.258: INFO: Got endpoints: latency-svc-2b9cx [369.664784ms]
Aug 17 06:49:02.259: INFO: Got endpoints: latency-svc-tvvnr [371.730978ms]
Aug 17 06:49:02.267: INFO: Created: latency-svc-cg5bq
Aug 17 06:49:02.294: INFO: Got endpoints: latency-svc-cg5bq [406.930909ms]
Aug 17 06:49:02.296: INFO: Created: latency-svc-4c9rv
Aug 17 06:49:02.328: INFO: Got endpoints: latency-svc-4c9rv [439.665743ms]
Aug 17 06:49:02.339: INFO: Created: latency-svc-9sxwj
Aug 17 06:49:02.367: INFO: Got endpoints: latency-svc-9sxwj [479.306848ms]
Aug 17 06:49:02.371: INFO: Created: latency-svc-dssf4
Aug 17 06:49:02.394: INFO: Got endpoints: latency-svc-dssf4 [505.60314ms]
Aug 17 06:49:02.404: INFO: Created: latency-svc-r56rb
Aug 17 06:49:02.431: INFO: Got endpoints: latency-svc-r56rb [543.517021ms]
Aug 17 06:49:02.435: INFO: Created: latency-svc-fbhvr
Aug 17 06:49:02.454: INFO: Got endpoints: latency-svc-fbhvr [492.32264ms]
Aug 17 06:49:02.465: INFO: Created: latency-svc-tbqzm
Aug 17 06:49:02.493: INFO: Got endpoints: latency-svc-tbqzm [498.678345ms]
Aug 17 06:49:02.499: INFO: Created: latency-svc-mptsc
Aug 17 06:49:02.521: INFO: Got endpoints: latency-svc-mptsc [490.625613ms]
Aug 17 06:49:02.529: INFO: Created: latency-svc-wfd5n
Aug 17 06:49:02.550: INFO: Got endpoints: latency-svc-wfd5n [468.037107ms]
Aug 17 06:49:02.553: INFO: Created: latency-svc-qtx94
Aug 17 06:49:02.577: INFO: Got endpoints: latency-svc-qtx94 [471.02634ms]
Aug 17 06:49:02.579: INFO: Created: latency-svc-vnrmp
Aug 17 06:49:02.603: INFO: Got endpoints: latency-svc-vnrmp [467.341717ms]
Aug 17 06:49:02.606: INFO: Created: latency-svc-gjjpl
Aug 17 06:49:02.628: INFO: Got endpoints: latency-svc-gjjpl [455.453507ms]
Aug 17 06:49:02.640: INFO: Created: latency-svc-cdpc7
Aug 17 06:49:02.660: INFO: Got endpoints: latency-svc-cdpc7 [466.251144ms]
Aug 17 06:49:02.666: INFO: Created: latency-svc-5rzdf
Aug 17 06:49:02.690: INFO: Created: latency-svc-5t25s
Aug 17 06:49:02.700: INFO: Got endpoints: latency-svc-5rzdf [441.001008ms]
Aug 17 06:49:02.716: INFO: Got endpoints: latency-svc-5t25s [457.911015ms]
Aug 17 06:49:02.723: INFO: Created: latency-svc-nzxnc
Aug 17 06:49:02.748: INFO: Got endpoints: latency-svc-nzxnc [453.751768ms]
Aug 17 06:49:02.748: INFO: Created: latency-svc-w85vf
Aug 17 06:49:02.769: INFO: Got endpoints: latency-svc-w85vf [440.82236ms]
Aug 17 06:49:02.772: INFO: Created: latency-svc-ncsq4
Aug 17 06:49:02.803: INFO: Got endpoints: latency-svc-ncsq4 [435.802605ms]
Aug 17 06:49:02.809: INFO: Created: latency-svc-622gg
Aug 17 06:49:02.829: INFO: Got endpoints: latency-svc-622gg [80.869426ms]
Aug 17 06:49:02.832: INFO: Created: latency-svc-pbggl
Aug 17 06:49:02.854: INFO: Got endpoints: latency-svc-pbggl [460.009824ms]
Aug 17 06:49:02.855: INFO: Created: latency-svc-8wt59
Aug 17 06:49:02.880: INFO: Got endpoints: latency-svc-8wt59 [449.229235ms]
Aug 17 06:49:02.883: INFO: Created: latency-svc-d92xv
Aug 17 06:49:02.903: INFO: Got endpoints: latency-svc-d92xv [449.223508ms]
Aug 17 06:49:02.913: INFO: Created: latency-svc-6tj8t
Aug 17 06:49:02.932: INFO: Got endpoints: latency-svc-6tj8t [438.583836ms]
Aug 17 06:49:02.943: INFO: Created: latency-svc-4qv84
Aug 17 06:49:02.968: INFO: Got endpoints: latency-svc-4qv84 [446.967169ms]
Aug 17 06:49:02.975: INFO: Created: latency-svc-xswhj
Aug 17 06:49:02.992: INFO: Got endpoints: latency-svc-xswhj [441.852016ms]
Aug 17 06:49:03.000: INFO: Created: latency-svc-hkbj2
Aug 17 06:49:03.023: INFO: Created: latency-svc-vdnbx
Aug 17 06:49:03.036: INFO: Got endpoints: latency-svc-hkbj2 [458.917409ms]
Aug 17 06:49:03.054: INFO: Got endpoints: latency-svc-vdnbx [451.223189ms]
Aug 17 06:49:03.073: INFO: Created: latency-svc-8726h
Aug 17 06:49:03.089: INFO: Got endpoints: latency-svc-8726h [461.094049ms]
Aug 17 06:49:03.092: INFO: Created: latency-svc-t9847
Aug 17 06:49:03.126: INFO: Got endpoints: latency-svc-t9847 [465.775461ms]
Aug 17 06:49:03.132: INFO: Created: latency-svc-5ctdm
Aug 17 06:49:03.161: INFO: Created: latency-svc-dq7fz
Aug 17 06:49:03.161: INFO: Got endpoints: latency-svc-5ctdm [460.59481ms]
Aug 17 06:49:03.202: INFO: Got endpoints: latency-svc-dq7fz [485.387913ms]
Aug 17 06:49:03.216: INFO: Created: latency-svc-t7glf
Aug 17 06:49:03.238: INFO: Got endpoints: latency-svc-t7glf [469.413281ms]
Aug 17 06:49:03.242: INFO: Created: latency-svc-c9v5k
Aug 17 06:49:03.262: INFO: Got endpoints: latency-svc-c9v5k [459.220605ms]
Aug 17 06:49:03.267: INFO: Created: latency-svc-6drvb
Aug 17 06:49:03.291: INFO: Got endpoints: latency-svc-6drvb [462.040933ms]
Aug 17 06:49:03.297: INFO: Created: latency-svc-wxrls
Aug 17 06:49:03.320: INFO: Got endpoints: latency-svc-wxrls [465.778184ms]
Aug 17 06:49:03.323: INFO: Created: latency-svc-ks4h6
Aug 17 06:49:03.353: INFO: Got endpoints: latency-svc-ks4h6 [472.558585ms]
Aug 17 06:49:03.354: INFO: Created: latency-svc-k9vpj
Aug 17 06:49:03.411: INFO: Got endpoints: latency-svc-k9vpj [507.637429ms]
Aug 17 06:49:03.421: INFO: Created: latency-svc-qv88h
Aug 17 06:49:03.454: INFO: Got endpoints: latency-svc-qv88h [522.118965ms]
Aug 17 06:49:03.462: INFO: Created: latency-svc-n9rs4
Aug 17 06:49:03.491: INFO: Got endpoints: latency-svc-n9rs4 [522.432397ms]
Aug 17 06:49:03.500: INFO: Created: latency-svc-krct6
Aug 17 06:49:03.516: INFO: Got endpoints: latency-svc-krct6 [523.578261ms]
Aug 17 06:49:03.517: INFO: Created: latency-svc-lnzwz
Aug 17 06:49:03.524: INFO: Got endpoints: latency-svc-lnzwz [488.047995ms]
Aug 17 06:49:03.530: INFO: Created: latency-svc-xtdtm
Aug 17 06:49:03.539: INFO: Got endpoints: latency-svc-xtdtm [485.210492ms]
Aug 17 06:49:03.540: INFO: Created: latency-svc-946rj
Aug 17 06:49:03.557: INFO: Got endpoints: latency-svc-946rj [468.522507ms]
Aug 17 06:49:03.562: INFO: Created: latency-svc-t96wt
Aug 17 06:49:03.590: INFO: Got endpoints: latency-svc-t96wt [463.967839ms]
Aug 17 06:49:03.602: INFO: Created: latency-svc-6r49l
Aug 17 06:49:03.625: INFO: Got endpoints: latency-svc-6r49l [464.16925ms]
Aug 17 06:49:03.629: INFO: Created: latency-svc-lggtr
Aug 17 06:49:03.648: INFO: Got endpoints: latency-svc-lggtr [445.855148ms]
Aug 17 06:49:03.649: INFO: Created: latency-svc-hb8mr
Aug 17 06:49:03.681: INFO: Got endpoints: latency-svc-hb8mr [442.619563ms]
Aug 17 06:49:03.690: INFO: Created: latency-svc-cgtmg
Aug 17 06:49:03.713: INFO: Got endpoints: latency-svc-cgtmg [450.954713ms]
Aug 17 06:49:03.715: INFO: Created: latency-svc-4xkg9
Aug 17 06:49:03.745: INFO: Created: latency-svc-4kpdl
Aug 17 06:49:03.746: INFO: Got endpoints: latency-svc-4xkg9 [454.546215ms]
Aug 17 06:49:03.763: INFO: Got endpoints: latency-svc-4kpdl [442.666508ms]
Aug 17 06:49:03.769: INFO: Created: latency-svc-dsbpx
Aug 17 06:49:03.792: INFO: Got endpoints: latency-svc-dsbpx [439.705708ms]
Aug 17 06:49:03.803: INFO: Created: latency-svc-gmpds
Aug 17 06:49:03.825: INFO: Got endpoints: latency-svc-gmpds [414.247875ms]
Aug 17 06:49:03.832: INFO: Created: latency-svc-jmr29
Aug 17 06:49:03.857: INFO: Got endpoints: latency-svc-jmr29 [402.715963ms]
Aug 17 06:49:03.861: INFO: Created: latency-svc-c9cmm
Aug 17 06:49:03.883: INFO: Got endpoints: latency-svc-c9cmm [392.255179ms]
Aug 17 06:49:03.889: INFO: Created: latency-svc-ddbx6
Aug 17 06:49:03.914: INFO: Got endpoints: latency-svc-ddbx6 [398.232644ms]
Aug 17 06:49:03.916: INFO: Created: latency-svc-94w5l
Aug 17 06:49:03.939: INFO: Got endpoints: latency-svc-94w5l [415.022215ms]
Aug 17 06:49:03.940: INFO: Created: latency-svc-d45tr
Aug 17 06:49:03.972: INFO: Got endpoints: latency-svc-d45tr [432.466086ms]
Aug 17 06:49:03.979: INFO: Created: latency-svc-65c9s
Aug 17 06:49:03.997: INFO: Got endpoints: latency-svc-65c9s [439.602776ms]
Aug 17 06:49:04.003: INFO: Created: latency-svc-qtgsp
Aug 17 06:49:04.035: INFO: Created: latency-svc-6bmpv
Aug 17 06:49:04.044: INFO: Got endpoints: latency-svc-qtgsp [452.997966ms]
Aug 17 06:49:04.058: INFO: Created: latency-svc-xg525
Aug 17 06:49:04.059: INFO: Got endpoints: latency-svc-6bmpv [432.939404ms]
Aug 17 06:49:04.112: INFO: Got endpoints: latency-svc-xg525 [464.421729ms]
Aug 17 06:49:04.115: INFO: Created: latency-svc-rjqgw
Aug 17 06:49:04.156: INFO: Created: latency-svc-xp5b6
Aug 17 06:49:04.157: INFO: Got endpoints: latency-svc-rjqgw [476.171116ms]
Aug 17 06:49:04.164: INFO: Created: latency-svc-8rx9d
Aug 17 06:49:04.164: INFO: Got endpoints: latency-svc-xp5b6 [450.208811ms]
Aug 17 06:49:04.170: INFO: Created: latency-svc-d7wxk
Aug 17 06:49:04.171: INFO: Got endpoints: latency-svc-8rx9d [424.913022ms]
Aug 17 06:49:04.183: INFO: Got endpoints: latency-svc-d7wxk [419.994283ms]
Aug 17 06:49:04.186: INFO: Created: latency-svc-hjr4k
Aug 17 06:49:04.215: INFO: Got endpoints: latency-svc-hjr4k [422.649223ms]
Aug 17 06:49:04.220: INFO: Created: latency-svc-ftscj
Aug 17 06:49:04.241: INFO: Got endpoints: latency-svc-ftscj [415.491925ms]
Aug 17 06:49:04.242: INFO: Created: latency-svc-g55n6
Aug 17 06:49:04.267: INFO: Got endpoints: latency-svc-g55n6 [410.149864ms]
Aug 17 06:49:04.275: INFO: Created: latency-svc-wdz9d
Aug 17 06:49:04.300: INFO: Got endpoints: latency-svc-wdz9d [416.211847ms]
Aug 17 06:49:04.301: INFO: Created: latency-svc-sdbml
Aug 17 06:49:04.333: INFO: Created: latency-svc-whfvw
Aug 17 06:49:04.333: INFO: Got endpoints: latency-svc-sdbml [418.902476ms]
Aug 17 06:49:04.351: INFO: Got endpoints: latency-svc-whfvw [411.976922ms]
Aug 17 06:49:04.360: INFO: Created: latency-svc-2ct54
Aug 17 06:49:04.380: INFO: Got endpoints: latency-svc-2ct54 [408.043171ms]
Aug 17 06:49:04.385: INFO: Created: latency-svc-27p8t
Aug 17 06:49:04.406: INFO: Got endpoints: latency-svc-27p8t [408.273884ms]
Aug 17 06:49:04.416: INFO: Created: latency-svc-2trsn
Aug 17 06:49:04.428: INFO: Got endpoints: latency-svc-2trsn [384.256161ms]
Aug 17 06:49:04.438: INFO: Created: latency-svc-rmnjn
Aug 17 06:49:04.459: INFO: Created: latency-svc-w4w4s
Aug 17 06:49:04.469: INFO: Got endpoints: latency-svc-rmnjn [410.759548ms]
Aug 17 06:49:04.483: INFO: Created: latency-svc-qbp2z
Aug 17 06:49:04.484: INFO: Got endpoints: latency-svc-w4w4s [371.293738ms]
Aug 17 06:49:04.506: INFO: Created: latency-svc-jnx4n
Aug 17 06:49:04.507: INFO: Got endpoints: latency-svc-qbp2z [349.554804ms]
Aug 17 06:49:04.528: INFO: Got endpoints: latency-svc-jnx4n [363.927399ms]
Aug 17 06:49:04.534: INFO: Created: latency-svc-t2hc8
Aug 17 06:49:04.587: INFO: Got endpoints: latency-svc-t2hc8 [416.123409ms]
Aug 17 06:49:04.591: INFO: Created: latency-svc-kmtt5
Aug 17 06:49:04.615: INFO: Got endpoints: latency-svc-kmtt5 [431.390721ms]
Aug 17 06:49:04.624: INFO: Created: latency-svc-dw5vh
Aug 17 06:49:04.641: INFO: Got endpoints: latency-svc-dw5vh [425.432452ms]
Aug 17 06:49:04.642: INFO: Created: latency-svc-g5jf8
Aug 17 06:49:04.660: INFO: Got endpoints: latency-svc-g5jf8 [418.827532ms]
Aug 17 06:49:04.664: INFO: Created: latency-svc-kmn6d
Aug 17 06:49:04.683: INFO: Got endpoints: latency-svc-kmn6d [415.568448ms]
Aug 17 06:49:04.696: INFO: Created: latency-svc-hcq5b
Aug 17 06:49:04.713: INFO: Got endpoints: latency-svc-hcq5b [412.970443ms]
Aug 17 06:49:04.717: INFO: Created: latency-svc-kvsl7
Aug 17 06:49:04.738: INFO: Got endpoints: latency-svc-kvsl7 [404.342608ms]
Aug 17 06:49:04.748: INFO: Created: latency-svc-ncww9
Aug 17 06:49:04.764: INFO: Created: latency-svc-7m4gw
Aug 17 06:49:04.764: INFO: Got endpoints: latency-svc-ncww9 [412.896067ms]
Aug 17 06:49:04.793: INFO: Got endpoints: latency-svc-7m4gw [412.358859ms]
Aug 17 06:49:04.793: INFO: Created: latency-svc-bgmcx
Aug 17 06:49:04.816: INFO: Got endpoints: latency-svc-bgmcx [410.203256ms]
Aug 17 06:49:04.826: INFO: Created: latency-svc-cgrvj
Aug 17 06:49:04.864: INFO: Got endpoints: latency-svc-cgrvj [435.308704ms]
Aug 17 06:49:04.864: INFO: Created: latency-svc-bfwhv
Aug 17 06:49:04.894: INFO: Created: latency-svc-dpnp6
Aug 17 06:49:04.895: INFO: Got endpoints: latency-svc-bfwhv [425.082181ms]
Aug 17 06:49:04.916: INFO: Created: latency-svc-kh7m9
Aug 17 06:49:04.917: INFO: Got endpoints: latency-svc-dpnp6 [433.15486ms]
Aug 17 06:49:04.941: INFO: Got endpoints: latency-svc-kh7m9 [434.032651ms]
Aug 17 06:49:04.947: INFO: Created: latency-svc-m22mm
Aug 17 06:49:04.979: INFO: Got endpoints: latency-svc-m22mm [450.776412ms]
Aug 17 06:49:04.983: INFO: Created: latency-svc-fv9nv
Aug 17 06:49:04.997: INFO: Created: latency-svc-9fxj8
Aug 17 06:49:05.003: INFO: Got endpoints: latency-svc-fv9nv [416.541689ms]
Aug 17 06:49:05.035: INFO: Created: latency-svc-r4jbr
Aug 17 06:49:05.035: INFO: Got endpoints: latency-svc-9fxj8 [420.515741ms]
Aug 17 06:49:05.039: INFO: Got endpoints: latency-svc-r4jbr [397.837609ms]
Aug 17 06:49:05.042: INFO: Created: latency-svc-cflmp
Aug 17 06:49:05.060: INFO: Got endpoints: latency-svc-cflmp [399.485965ms]
Aug 17 06:49:05.064: INFO: Created: latency-svc-n7857
Aug 17 06:49:05.087: INFO: Got endpoints: latency-svc-n7857 [403.817649ms]
Aug 17 06:49:05.097: INFO: Created: latency-svc-whtrx
Aug 17 06:49:05.118: INFO: Got endpoints: latency-svc-whtrx [405.123691ms]
Aug 17 06:49:05.119: INFO: Created: latency-svc-qsb55
Aug 17 06:49:05.136: INFO: Got endpoints: latency-svc-qsb55 [397.632542ms]
Aug 17 06:49:05.144: INFO: Created: latency-svc-ng5sj
Aug 17 06:49:05.171: INFO: Got endpoints: latency-svc-ng5sj [406.731352ms]
Aug 17 06:49:05.172: INFO: Created: latency-svc-8gcd4
Aug 17 06:49:05.198: INFO: Got endpoints: latency-svc-8gcd4 [404.520275ms]
Aug 17 06:49:05.205: INFO: Created: latency-svc-5gr7b
Aug 17 06:49:05.237: INFO: Got endpoints: latency-svc-5gr7b [420.916938ms]
Aug 17 06:49:05.238: INFO: Created: latency-svc-nccvz
Aug 17 06:49:05.260: INFO: Got endpoints: latency-svc-nccvz [396.786478ms]
Aug 17 06:49:05.264: INFO: Created: latency-svc-97jkm
Aug 17 06:49:05.286: INFO: Got endpoints: latency-svc-97jkm [391.51965ms]
Aug 17 06:49:05.307: INFO: Created: latency-svc-qkmd8
Aug 17 06:49:05.325: INFO: Created: latency-svc-2bdjq
Aug 17 06:49:05.328: INFO: Got endpoints: latency-svc-qkmd8 [411.529682ms]
Aug 17 06:49:05.345: INFO: Got endpoints: latency-svc-2bdjq [403.665884ms]
Aug 17 06:49:05.349: INFO: Created: latency-svc-hhmcx
Aug 17 06:49:05.370: INFO: Got endpoints: latency-svc-hhmcx [391.036642ms]
Aug 17 06:49:05.377: INFO: Created: latency-svc-qcwp7
Aug 17 06:49:05.392: INFO: Got endpoints: latency-svc-qcwp7 [388.094195ms]
Aug 17 06:49:05.392: INFO: Created: latency-svc-6jhvg
Aug 17 06:49:05.415: INFO: Got endpoints: latency-svc-6jhvg [379.791921ms]
Aug 17 06:49:05.428: INFO: Created: latency-svc-4mn5b
Aug 17 06:49:05.442: INFO: Got endpoints: latency-svc-4mn5b [402.712834ms]
Aug 17 06:49:05.447: INFO: Created: latency-svc-md4m7
Aug 17 06:49:05.469: INFO: Got endpoints: latency-svc-md4m7 [409.232518ms]
Aug 17 06:49:05.473: INFO: Created: latency-svc-cn97m
Aug 17 06:49:05.496: INFO: Got endpoints: latency-svc-cn97m [409.457449ms]
Aug 17 06:49:05.497: INFO: Created: latency-svc-9fxw6
Aug 17 06:49:05.547: INFO: Got endpoints: latency-svc-9fxw6 [429.082596ms]
Aug 17 06:49:05.554: INFO: Created: latency-svc-hdcpr
Aug 17 06:49:05.574: INFO: Got endpoints: latency-svc-hdcpr [438.287343ms]
Aug 17 06:49:05.589: INFO: Created: latency-svc-v2ttz
Aug 17 06:49:05.611: INFO: Got endpoints: latency-svc-v2ttz [439.736546ms]
Aug 17 06:49:05.621: INFO: Created: latency-svc-z6gvw
Aug 17 06:49:05.651: INFO: Got endpoints: latency-svc-z6gvw [452.629477ms]
Aug 17 06:49:05.666: INFO: Created: latency-svc-chj4w
Aug 17 06:49:05.691: INFO: Created: latency-svc-pnm6q
Aug 17 06:49:05.692: INFO: Got endpoints: latency-svc-chj4w [454.375752ms]
Aug 17 06:49:05.715: INFO: Got endpoints: latency-svc-pnm6q [454.138899ms]
Aug 17 06:49:05.719: INFO: Created: latency-svc-wswrm
Aug 17 06:49:05.741: INFO: Got endpoints: latency-svc-wswrm [454.932335ms]
Aug 17 06:49:05.744: INFO: Created: latency-svc-jsvcw
Aug 17 06:49:05.764: INFO: Got endpoints: latency-svc-jsvcw [435.408586ms]
Aug 17 06:49:05.787: INFO: Created: latency-svc-ppzrp
Aug 17 06:49:05.803: INFO: Got endpoints: latency-svc-ppzrp [458.585792ms]
Aug 17 06:49:05.819: INFO: Created: latency-svc-52dnx
Aug 17 06:49:05.849: INFO: Created: latency-svc-dm7kl
Aug 17 06:49:05.860: INFO: Got endpoints: latency-svc-52dnx [489.711682ms]
Aug 17 06:49:05.874: INFO: Got endpoints: latency-svc-dm7kl [481.847374ms]
Aug 17 06:49:05.883: INFO: Created: latency-svc-gp5kq
Aug 17 06:49:05.901: INFO: Created: latency-svc-dwflt
Aug 17 06:49:05.908: INFO: Got endpoints: latency-svc-gp5kq [493.217459ms]
Aug 17 06:49:05.924: INFO: Got endpoints: latency-svc-dwflt [481.757066ms]
Aug 17 06:49:05.932: INFO: Created: latency-svc-jrmhj
Aug 17 06:49:05.959: INFO: Got endpoints: latency-svc-jrmhj [489.481879ms]
Aug 17 06:49:05.970: INFO: Created: latency-svc-mktqp
Aug 17 06:49:05.989: INFO: Got endpoints: latency-svc-mktqp [492.000759ms]
Aug 17 06:49:05.996: INFO: Created: latency-svc-nrlpc
Aug 17 06:49:06.021: INFO: Got endpoints: latency-svc-nrlpc [473.753265ms]
Aug 17 06:49:06.029: INFO: Created: latency-svc-5c2k4
Aug 17 06:49:06.051: INFO: Got endpoints: latency-svc-5c2k4 [477.030462ms]
Aug 17 06:49:06.055: INFO: Created: latency-svc-m4qlp
Aug 17 06:49:06.079: INFO: Got endpoints: latency-svc-m4qlp [468.09767ms]
Aug 17 06:49:06.086: INFO: Created: latency-svc-z5d5j
Aug 17 06:49:06.104: INFO: Got endpoints: latency-svc-z5d5j [453.037425ms]
Aug 17 06:49:06.120: INFO: Created: latency-svc-jzs6d
Aug 17 06:49:06.146: INFO: Got endpoints: latency-svc-jzs6d [453.954344ms]
Aug 17 06:49:06.156: INFO: Created: latency-svc-2qw87
Aug 17 06:49:06.177: INFO: Got endpoints: latency-svc-2qw87 [461.598811ms]
Aug 17 06:49:06.189: INFO: Created: latency-svc-d4th7
Aug 17 06:49:06.211: INFO: Created: latency-svc-pphsf
Aug 17 06:49:06.211: INFO: Got endpoints: latency-svc-d4th7 [469.607659ms]
Aug 17 06:49:06.241: INFO: Got endpoints: latency-svc-pphsf [476.803672ms]
Aug 17 06:49:06.246: INFO: Created: latency-svc-mfp9t
Aug 17 06:49:06.272: INFO: Got endpoints: latency-svc-mfp9t [468.49792ms]
Aug 17 06:49:06.289: INFO: Created: latency-svc-qm278
Aug 17 06:49:06.319: INFO: Got endpoints: latency-svc-qm278 [458.99065ms]
Aug 17 06:49:06.333: INFO: Created: latency-svc-lb86r
Aug 17 06:49:06.353: INFO: Got endpoints: latency-svc-lb86r [479.121684ms]
Aug 17 06:49:06.361: INFO: Created: latency-svc-tnfmp
Aug 17 06:49:06.389: INFO: Created: latency-svc-49nrn
Aug 17 06:49:06.389: INFO: Got endpoints: latency-svc-tnfmp [480.560154ms]
Aug 17 06:49:06.415: INFO: Got endpoints: latency-svc-49nrn [491.112431ms]
Aug 17 06:49:06.421: INFO: Created: latency-svc-v7qxx
Aug 17 06:49:06.446: INFO: Created: latency-svc-xp69k
Aug 17 06:49:06.446: INFO: Got endpoints: latency-svc-v7qxx [486.691665ms]
Aug 17 06:49:06.473: INFO: Got endpoints: latency-svc-xp69k [484.061372ms]
Aug 17 06:49:06.475: INFO: Created: latency-svc-f6v9r
Aug 17 06:49:06.508: INFO: Created: latency-svc-pwjsf
Aug 17 06:49:06.508: INFO: Got endpoints: latency-svc-f6v9r [487.043142ms]
Aug 17 06:49:06.534: INFO: Got endpoints: latency-svc-pwjsf [482.870782ms]
Aug 17 06:49:06.546: INFO: Created: latency-svc-86nww
Aug 17 06:49:06.571: INFO: Got endpoints: latency-svc-86nww [491.773113ms]
Aug 17 06:49:06.581: INFO: Created: latency-svc-fbv8n
Aug 17 06:49:06.611: INFO: Got endpoints: latency-svc-fbv8n [506.899358ms]
Aug 17 06:49:06.618: INFO: Created: latency-svc-vdfqn
Aug 17 06:49:06.642: INFO: Got endpoints: latency-svc-vdfqn [496.510136ms]
Aug 17 06:49:06.646: INFO: Created: latency-svc-xd9dm
Aug 17 06:49:06.670: INFO: Created: latency-svc-h9fgp
Aug 17 06:49:06.687: INFO: Got endpoints: latency-svc-xd9dm [510.19465ms]
Aug 17 06:49:06.713: INFO: Got endpoints: latency-svc-h9fgp [501.815256ms]
Aug 17 06:49:06.735: INFO: Created: latency-svc-9phwv
Aug 17 06:49:06.767: INFO: Got endpoints: latency-svc-9phwv [525.983364ms]
Aug 17 06:49:06.782: INFO: Created: latency-svc-97dhh
Aug 17 06:49:06.811: INFO: Got endpoints: latency-svc-97dhh [538.661883ms]
Aug 17 06:49:06.813: INFO: Created: latency-svc-sdjb4
Aug 17 06:49:06.857: INFO: Got endpoints: latency-svc-sdjb4 [538.244844ms]
Aug 17 06:49:06.871: INFO: Created: latency-svc-nhpdw
Aug 17 06:49:06.908: INFO: Got endpoints: latency-svc-nhpdw [554.796805ms]
Aug 17 06:49:06.927: INFO: Created: latency-svc-b6v6b
Aug 17 06:49:06.962: INFO: Got endpoints: latency-svc-b6v6b [572.962307ms]
Aug 17 06:49:06.980: INFO: Created: latency-svc-pvhjt
Aug 17 06:49:07.008: INFO: Created: latency-svc-r952h
Aug 17 06:49:07.008: INFO: Got endpoints: latency-svc-pvhjt [593.511137ms]
Aug 17 06:49:07.032: INFO: Got endpoints: latency-svc-r952h [586.191733ms]
Aug 17 06:49:07.046: INFO: Created: latency-svc-4ft92
Aug 17 06:49:07.087: INFO: Got endpoints: latency-svc-4ft92 [614.034535ms]
Aug 17 06:49:07.088: INFO: Created: latency-svc-29qd8
Aug 17 06:49:07.105: INFO: Got endpoints: latency-svc-29qd8 [596.191032ms]
Aug 17 06:49:07.114: INFO: Created: latency-svc-v66hp
Aug 17 06:49:07.134: INFO: Got endpoints: latency-svc-v66hp [599.587061ms]
Aug 17 06:49:07.143: INFO: Created: latency-svc-7kqk9
Aug 17 06:49:07.170: INFO: Got endpoints: latency-svc-7kqk9 [599.50993ms]
Aug 17 06:49:07.171: INFO: Created: latency-svc-482cs
Aug 17 06:49:07.189: INFO: Got endpoints: latency-svc-482cs [578.370631ms]
Aug 17 06:49:07.202: INFO: Created: latency-svc-np5wz
Aug 17 06:49:07.213: INFO: Got endpoints: latency-svc-np5wz [570.375367ms]
Aug 17 06:49:07.223: INFO: Created: latency-svc-5tzx7
Aug 17 06:49:07.247: INFO: Created: latency-svc-zhbnz
Aug 17 06:49:07.248: INFO: Got endpoints: latency-svc-5tzx7 [561.232355ms]
Aug 17 06:49:07.265: INFO: Got endpoints: latency-svc-zhbnz [551.28444ms]
Aug 17 06:49:07.267: INFO: Created: latency-svc-wczzj
Aug 17 06:49:07.294: INFO: Got endpoints: latency-svc-wczzj [526.975134ms]
Aug 17 06:49:07.300: INFO: Created: latency-svc-bh4q2
Aug 17 06:49:07.328: INFO: Got endpoints: latency-svc-bh4q2 [517.016449ms]
Aug 17 06:49:07.333: INFO: Created: latency-svc-2qmfh
Aug 17 06:49:07.350: INFO: Got endpoints: latency-svc-2qmfh [492.222217ms]
Aug 17 06:49:07.363: INFO: Created: latency-svc-hhgdw
Aug 17 06:49:07.379: INFO: Got endpoints: latency-svc-hhgdw [471.316438ms]
Aug 17 06:49:07.390: INFO: Created: latency-svc-rgd4l
Aug 17 06:49:07.416: INFO: Got endpoints: latency-svc-rgd4l [453.531516ms]
Aug 17 06:49:07.422: INFO: Created: latency-svc-gzw44
Aug 17 06:49:07.458: INFO: Created: latency-svc-ssrzh
Aug 17 06:49:07.475: INFO: Got endpoints: latency-svc-gzw44 [466.481946ms]
Aug 17 06:49:07.483: INFO: Got endpoints: latency-svc-ssrzh [451.034087ms]
Aug 17 06:49:07.493: INFO: Created: latency-svc-b64f9
Aug 17 06:49:07.508: INFO: Got endpoints: latency-svc-b64f9 [420.124164ms]
Aug 17 06:49:07.523: INFO: Created: latency-svc-6dkv5
Aug 17 06:49:07.556: INFO: Got endpoints: latency-svc-6dkv5 [450.857107ms]
Aug 17 06:49:07.620: INFO: Created: latency-svc-t59xz
Aug 17 06:49:07.620: INFO: Created: latency-svc-4tjns
Aug 17 06:49:07.639: INFO: Created: latency-svc-q8w8s
Aug 17 06:49:07.659: INFO: Got endpoints: latency-svc-4tjns [525.327976ms]
Aug 17 06:49:07.660: INFO: Got endpoints: latency-svc-t59xz [489.47649ms]
Aug 17 06:49:07.670: INFO: Created: latency-svc-qlrcl
Aug 17 06:49:07.670: INFO: Got endpoints: latency-svc-q8w8s [480.44455ms]
Aug 17 06:49:07.688: INFO: Got endpoints: latency-svc-qlrcl [475.171858ms]
Aug 17 06:49:07.702: INFO: Created: latency-svc-jd9gx
Aug 17 06:49:07.722: INFO: Got endpoints: latency-svc-jd9gx [473.061802ms]
Aug 17 06:49:07.725: INFO: Created: latency-svc-r8v8n
Aug 17 06:49:07.750: INFO: Got endpoints: latency-svc-r8v8n [485.33408ms]
Aug 17 06:49:07.752: INFO: Created: latency-svc-xz9nk
Aug 17 06:49:07.786: INFO: Created: latency-svc-zmsq9
Aug 17 06:49:07.795: INFO: Got endpoints: latency-svc-xz9nk [500.70095ms]
Aug 17 06:49:07.811: INFO: Got endpoints: latency-svc-zmsq9 [482.538294ms]
Aug 17 06:49:07.819: INFO: Created: latency-svc-28dbj
Aug 17 06:49:07.837: INFO: Got endpoints: latency-svc-28dbj [486.996754ms]
Aug 17 06:49:07.843: INFO: Created: latency-svc-npfjl
Aug 17 06:49:07.868: INFO: Got endpoints: latency-svc-npfjl [488.164319ms]
Aug 17 06:49:07.868: INFO: Created: latency-svc-b4kmg
Aug 17 06:49:07.892: INFO: Created: latency-svc-wjtgg
Aug 17 06:49:07.893: INFO: Got endpoints: latency-svc-b4kmg [476.625226ms]
Aug 17 06:49:07.913: INFO: Got endpoints: latency-svc-wjtgg [437.85542ms]
Aug 17 06:49:07.920: INFO: Created: latency-svc-8ktgx
Aug 17 06:49:07.936: INFO: Got endpoints: latency-svc-8ktgx [452.517173ms]
Aug 17 06:49:07.940: INFO: Created: latency-svc-57t8t
Aug 17 06:49:07.970: INFO: Got endpoints: latency-svc-57t8t [461.814828ms]
Aug 17 06:49:07.979: INFO: Created: latency-svc-6s2zf
Aug 17 06:49:07.997: INFO: Got endpoints: latency-svc-6s2zf [441.029436ms]
Aug 17 06:49:08.007: INFO: Created: latency-svc-6j8qc
Aug 17 06:49:08.033: INFO: Got endpoints: latency-svc-6j8qc [373.263834ms]
Aug 17 06:49:08.033: INFO: Latencies: [76.539189ms 80.869426ms 108.391791ms 144.77889ms 196.59654ms 219.081222ms 249.226249ms 285.600209ms 308.063045ms 349.554804ms 363.927399ms 369.664784ms 371.293738ms 371.730978ms 373.263834ms 379.791921ms 384.256161ms 388.094195ms 391.036642ms 391.51965ms 392.255179ms 396.786478ms 397.632542ms 397.837609ms 398.232644ms 399.485965ms 402.712834ms 402.715963ms 403.665884ms 403.817649ms 404.342608ms 404.520275ms 405.123691ms 406.731352ms 406.930909ms 408.043171ms 408.273884ms 409.232518ms 409.457449ms 410.149864ms 410.203256ms 410.759548ms 411.529682ms 411.976922ms 412.358859ms 412.896067ms 412.970443ms 414.247875ms 415.022215ms 415.491925ms 415.568448ms 416.123409ms 416.211847ms 416.541689ms 418.827532ms 418.902476ms 419.994283ms 420.124164ms 420.515741ms 420.916938ms 422.649223ms 424.913022ms 425.082181ms 425.432452ms 429.082596ms 431.390721ms 432.466086ms 432.939404ms 433.15486ms 434.032651ms 435.308704ms 435.408586ms 435.802605ms 437.85542ms 438.287343ms 438.583836ms 439.602776ms 439.665743ms 439.705708ms 439.736546ms 440.82236ms 441.001008ms 441.029436ms 441.852016ms 442.619563ms 442.666508ms 445.855148ms 446.967169ms 449.223508ms 449.229235ms 450.208811ms 450.776412ms 450.857107ms 450.954713ms 451.034087ms 451.223189ms 452.517173ms 452.629477ms 452.997966ms 453.037425ms 453.531516ms 453.751768ms 453.954344ms 454.138899ms 454.375752ms 454.546215ms 454.932335ms 455.453507ms 457.911015ms 458.585792ms 458.917409ms 458.99065ms 459.220605ms 460.009824ms 460.59481ms 461.094049ms 461.598811ms 461.814828ms 462.040933ms 463.967839ms 464.16925ms 464.421729ms 465.775461ms 465.778184ms 466.251144ms 466.481946ms 467.341717ms 468.037107ms 468.09767ms 468.49792ms 468.522507ms 469.413281ms 469.607659ms 471.02634ms 471.316438ms 472.558585ms 473.061802ms 473.753265ms 475.171858ms 476.171116ms 476.625226ms 476.803672ms 477.030462ms 479.121684ms 479.306848ms 480.44455ms 480.560154ms 481.757066ms 481.847374ms 482.538294ms 482.870782ms 484.061372ms 485.210492ms 485.33408ms 485.387913ms 486.691665ms 486.996754ms 487.043142ms 488.047995ms 488.164319ms 489.47649ms 489.481879ms 489.711682ms 490.625613ms 491.112431ms 491.773113ms 492.000759ms 492.222217ms 492.32264ms 493.217459ms 496.510136ms 498.678345ms 500.70095ms 501.815256ms 505.60314ms 506.899358ms 507.637429ms 510.19465ms 517.016449ms 522.118965ms 522.432397ms 523.578261ms 525.327976ms 525.983364ms 526.975134ms 538.244844ms 538.661883ms 543.517021ms 551.28444ms 554.796805ms 561.232355ms 570.375367ms 572.962307ms 578.370631ms 586.191733ms 593.511137ms 596.191032ms 599.50993ms 599.587061ms 614.034535ms]
Aug 17 06:49:08.033: INFO: 50 %ile: 453.531516ms
Aug 17 06:49:08.033: INFO: 90 %ile: 522.432397ms
Aug 17 06:49:08.033: INFO: 99 %ile: 599.587061ms
Aug 17 06:49:08.033: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:49:08.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1008" for this suite.
Aug 17 06:49:38.171: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:49:40.865: INFO: namespace svc-latency-1008 deletion completed in 32.766174733s

• [SLOW TEST:42.433 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:49:40.866: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Aug 17 06:49:52.611: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0817 06:49:52.611343      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:49:52.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3902" for this suite.
Aug 17 06:50:03.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:50:05.726: INFO: namespace gc-3902 deletion completed in 12.870728388s

• [SLOW TEST:24.860 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:50:05.729: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-3f640b6e-e14c-4b7b-aff2-98ae84843e17
STEP: Creating a pod to test consume secrets
Aug 17 06:50:06.126: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7b9e60b7-8944-470d-bd43-4b960645096e" in namespace "projected-7761" to be "success or failure"
Aug 17 06:50:06.143: INFO: Pod "pod-projected-secrets-7b9e60b7-8944-470d-bd43-4b960645096e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.757428ms
Aug 17 06:50:08.162: INFO: Pod "pod-projected-secrets-7b9e60b7-8944-470d-bd43-4b960645096e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035944637s
STEP: Saw pod success
Aug 17 06:50:08.162: INFO: Pod "pod-projected-secrets-7b9e60b7-8944-470d-bd43-4b960645096e" satisfied condition "success or failure"
Aug 17 06:50:08.179: INFO: Trying to get logs from node 10.241.148.42 pod pod-projected-secrets-7b9e60b7-8944-470d-bd43-4b960645096e container secret-volume-test: <nil>
STEP: delete the pod
Aug 17 06:50:08.318: INFO: Waiting for pod pod-projected-secrets-7b9e60b7-8944-470d-bd43-4b960645096e to disappear
Aug 17 06:50:08.337: INFO: Pod pod-projected-secrets-7b9e60b7-8944-470d-bd43-4b960645096e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:50:08.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7761" for this suite.
Aug 17 06:50:18.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:50:22.128: INFO: namespace projected-7761 deletion completed in 13.758677012s

• [SLOW TEST:16.399 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:50:22.129: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 17 06:50:22.876: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 17 06:50:24.923: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733243822, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733243822, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733243822, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733243822, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 17 06:50:27.998: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:50:28.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4126" for this suite.
Aug 17 06:50:38.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:50:40.719: INFO: namespace webhook-4126 deletion completed in 12.4684642s
STEP: Destroying namespace "webhook-4126-markers" for this suite.
Aug 17 06:50:48.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:50:51.692: INFO: namespace webhook-4126-markers deletion completed in 10.9724674s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:29.662 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:50:51.791: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-1109dbc7-59f8-43f2-89a1-efd7c6e7ea3e
STEP: Creating a pod to test consume secrets
Aug 17 06:50:52.095: INFO: Waiting up to 5m0s for pod "pod-secrets-2d6aa41d-b501-4307-8bca-81fd1fd0bc18" in namespace "secrets-5465" to be "success or failure"
Aug 17 06:50:52.112: INFO: Pod "pod-secrets-2d6aa41d-b501-4307-8bca-81fd1fd0bc18": Phase="Pending", Reason="", readiness=false. Elapsed: 17.050498ms
Aug 17 06:50:54.130: INFO: Pod "pod-secrets-2d6aa41d-b501-4307-8bca-81fd1fd0bc18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0353815s
Aug 17 06:50:56.150: INFO: Pod "pod-secrets-2d6aa41d-b501-4307-8bca-81fd1fd0bc18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054921257s
STEP: Saw pod success
Aug 17 06:50:56.150: INFO: Pod "pod-secrets-2d6aa41d-b501-4307-8bca-81fd1fd0bc18" satisfied condition "success or failure"
Aug 17 06:50:56.169: INFO: Trying to get logs from node 10.241.148.42 pod pod-secrets-2d6aa41d-b501-4307-8bca-81fd1fd0bc18 container secret-volume-test: <nil>
STEP: delete the pod
Aug 17 06:50:56.264: INFO: Waiting for pod pod-secrets-2d6aa41d-b501-4307-8bca-81fd1fd0bc18 to disappear
Aug 17 06:50:56.280: INFO: Pod pod-secrets-2d6aa41d-b501-4307-8bca-81fd1fd0bc18 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:50:56.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5465" for this suite.
Aug 17 06:51:06.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:51:09.053: INFO: namespace secrets-5465 deletion completed in 12.716324491s

• [SLOW TEST:17.263 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:51:09.054: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 06:51:09.390: INFO: Waiting up to 5m0s for pod "busybox-user-65534-d797e00c-c862-4941-afb3-716b1ff92601" in namespace "security-context-test-7621" to be "success or failure"
Aug 17 06:51:09.407: INFO: Pod "busybox-user-65534-d797e00c-c862-4941-afb3-716b1ff92601": Phase="Pending", Reason="", readiness=false. Elapsed: 17.589834ms
Aug 17 06:51:11.429: INFO: Pod "busybox-user-65534-d797e00c-c862-4941-afb3-716b1ff92601": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0393086s
Aug 17 06:51:13.451: INFO: Pod "busybox-user-65534-d797e00c-c862-4941-afb3-716b1ff92601": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061542946s
Aug 17 06:51:13.451: INFO: Pod "busybox-user-65534-d797e00c-c862-4941-afb3-716b1ff92601" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:51:13.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7621" for this suite.
Aug 17 06:51:23.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:51:26.167: INFO: namespace security-context-test-7621 deletion completed in 12.648290777s

• [SLOW TEST:17.113 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:51:26.167: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Aug 17 06:51:26.357: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Aug 17 06:51:26.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 create -f - --namespace=kubectl-8830'
Aug 17 06:51:27.275: INFO: stderr: ""
Aug 17 06:51:27.275: INFO: stdout: "service/redis-slave created\n"
Aug 17 06:51:27.275: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Aug 17 06:51:27.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 create -f - --namespace=kubectl-8830'
Aug 17 06:51:27.674: INFO: stderr: ""
Aug 17 06:51:27.674: INFO: stdout: "service/redis-master created\n"
Aug 17 06:51:27.674: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 17 06:51:27.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 create -f - --namespace=kubectl-8830'
Aug 17 06:51:28.287: INFO: stderr: ""
Aug 17 06:51:28.287: INFO: stdout: "service/frontend created\n"
Aug 17 06:51:28.287: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Aug 17 06:51:28.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 create -f - --namespace=kubectl-8830'
Aug 17 06:51:28.972: INFO: stderr: ""
Aug 17 06:51:28.972: INFO: stdout: "deployment.apps/frontend created\n"
Aug 17 06:51:28.972: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 17 06:51:28.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 create -f - --namespace=kubectl-8830'
Aug 17 06:51:29.355: INFO: stderr: ""
Aug 17 06:51:29.355: INFO: stdout: "deployment.apps/redis-master created\n"
Aug 17 06:51:29.355: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Aug 17 06:51:29.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 create -f - --namespace=kubectl-8830'
Aug 17 06:51:29.746: INFO: stderr: ""
Aug 17 06:51:29.746: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Aug 17 06:51:29.746: INFO: Waiting for all frontend pods to be Running.
Aug 17 06:51:49.797: INFO: Waiting for frontend to serve content.
Aug 17 06:51:49.884: INFO: Trying to add a new entry to the guestbook.
Aug 17 06:51:49.979: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Aug 17 06:51:50.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 delete --grace-period=0 --force -f - --namespace=kubectl-8830'
Aug 17 06:51:50.850: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 06:51:50.850: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Aug 17 06:51:50.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 delete --grace-period=0 --force -f - --namespace=kubectl-8830'
Aug 17 06:51:51.113: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 06:51:51.113: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 17 06:51:51.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 delete --grace-period=0 --force -f - --namespace=kubectl-8830'
Aug 17 06:51:51.357: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 06:51:51.357: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 17 06:51:51.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 delete --grace-period=0 --force -f - --namespace=kubectl-8830'
Aug 17 06:51:51.565: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 06:51:51.565: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 17 06:51:51.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 delete --grace-period=0 --force -f - --namespace=kubectl-8830'
Aug 17 06:51:51.743: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 06:51:51.743: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 17 06:51:51.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-667650646 delete --grace-period=0 --force -f - --namespace=kubectl-8830'
Aug 17 06:51:51.922: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 06:51:51.922: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:51:51.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8830" for this suite.
Aug 17 06:52:10.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:52:12.649: INFO: namespace kubectl-8830 deletion completed in 20.662398075s

• [SLOW TEST:46.482 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:52:12.650: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-fd423d13-1c28-483d-9d8b-fc684c502393
STEP: Creating a pod to test consume configMaps
Aug 17 06:52:12.984: INFO: Waiting up to 5m0s for pod "pod-configmaps-270ed276-767c-45fd-8384-42b0f8a1f420" in namespace "configmap-3613" to be "success or failure"
Aug 17 06:52:13.002: INFO: Pod "pod-configmaps-270ed276-767c-45fd-8384-42b0f8a1f420": Phase="Pending", Reason="", readiness=false. Elapsed: 18.24185ms
Aug 17 06:52:15.028: INFO: Pod "pod-configmaps-270ed276-767c-45fd-8384-42b0f8a1f420": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043883336s
Aug 17 06:52:17.080: INFO: Pod "pod-configmaps-270ed276-767c-45fd-8384-42b0f8a1f420": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.096511793s
STEP: Saw pod success
Aug 17 06:52:17.080: INFO: Pod "pod-configmaps-270ed276-767c-45fd-8384-42b0f8a1f420" satisfied condition "success or failure"
Aug 17 06:52:17.128: INFO: Trying to get logs from node 10.241.148.42 pod pod-configmaps-270ed276-767c-45fd-8384-42b0f8a1f420 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 17 06:52:17.311: INFO: Waiting for pod pod-configmaps-270ed276-767c-45fd-8384-42b0f8a1f420 to disappear
Aug 17 06:52:17.366: INFO: Pod pod-configmaps-270ed276-767c-45fd-8384-42b0f8a1f420 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:52:17.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3613" for this suite.
Aug 17 06:52:25.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:52:28.477: INFO: namespace configmap-3613 deletion completed in 10.789649836s

• [SLOW TEST:15.827 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 17 06:52:28.477: INFO: >>> kubeConfig: /tmp/kubeconfig-667650646
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Aug 17 06:52:28.934: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"134756e7-8de0-46d4-adec-ac9eeef77d4b", Controller:(*bool)(0xc0030c7252), BlockOwnerDeletion:(*bool)(0xc0030c7253)}}
Aug 17 06:52:28.960: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"c9bdae45-b21c-4490-86c1-b644e8d89fc7", Controller:(*bool)(0xc00298eb12), BlockOwnerDeletion:(*bool)(0xc00298eb13)}}
Aug 17 06:52:29.004: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"02c5dce1-1e52-4d0b-84a2-949031e48480", Controller:(*bool)(0xc00298ef62), BlockOwnerDeletion:(*bool)(0xc00298ef63)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 17 06:52:34.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5782" for this suite.
Aug 17 06:52:44.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 17 06:52:48.513: INFO: namespace gc-5782 deletion completed in 14.210238476s

• [SLOW TEST:20.036 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSAug 17 06:52:48.513: INFO: Running AfterSuite actions on all nodes
Aug 17 06:52:48.513: INFO: Running AfterSuite actions on node 1
Aug 17 06:52:48.513: INFO: Skipping dumping logs from cluster

Ran 276 of 4897 Specs in 9577.712 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Pending | 4621 Skipped
PASS

Ginkgo ran 1 suite in 2h39m39.64522793s
Test Suite Passed
